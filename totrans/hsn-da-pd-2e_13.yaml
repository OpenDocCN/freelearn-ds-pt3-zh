- en: '*Chapter 9*: Getting Started with Machine Learning in Python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will expose us to the vernacular of machine learning and the common
    tasks that machine learning can be used to solve. Afterward, we will learn how
    we can prepare our data for use in machine learning models. We have discussed
    data cleaning already, but only for human consumption—machine learning models
    require different `scikit-learn` to build preprocessing pipelines that streamline
    this procedure, since our models will only be as good as the data they are trained
    on.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will walk through how we can use `scikit-learn` to build a model and
    evaluate its performance. Scikit-learn has a very user-friendly API, so once we
    know how to build one model, we can build any number of them. We won't be going
    into any of the mathematics behind the models; there are entire books on this,
    and the goal of this chapter is to serve as an introduction to the topic. By the
    end of this chapter, we will be able to identify what type of problem we are looking
    to solve and some algorithms that can help us, as well as how to implement them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the machine learning landscape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing exploratory data analysis using skills learned in previous chapters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing data for use in a machine learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering to help understand unlabeled data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning when regression is appropriate and how to implement it with scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding classification tasks and learning how to use logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter materials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be working with three datasets. The first two come
    from data on wine quality that was donated to the UCI Machine Learning Data Repository
    ([http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php))
    by P. Cortez, A. Cerdeira, F. Almeida, T. Matos, and J. Reis, which contains information
    on the chemical properties of various wine samples, along with a rating of the
    quality from a blind tasting by a panel of wine experts. These files can be found
    in the `data/` folder inside this chapter's folder in the GitHub repository ([https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_09](https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_09))
    as `winequality-red.csv` and `winequality-white.csv` for red and white wine, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Our third dataset was collected using the Open Exoplanet Catalogue database,
    which can be found at [https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/](https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/).
    This database provides data in `planet_data_collection.ipynb` notebook on GitHub
    contains the code that was used to parse this information into the CSV files we
    will use in this chapter; while we won't be going over this explicitly, I encourage
    you to take a look at it. The data files can be found in the `data/` folder, as
    well. We will use `planets.csv` for this chapter; however, the parsed data for
    the other hierarchies is provided for exercises and further exploration. These
    are `binaries.csv`, `stars.csv`, and `systems.csv`, which contain data on binaries
    (stars or binaries forming a group of two), data on a single star, and data on
    planetary systems, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the `red_wine.ipynb` notebook to predict red wine quality,
    the `wine.ipynb` notebook to classify wines as red or white based on their chemical
    properties, and the `planets_ml.ipynb` notebook to build a regression model to
    predict the year length of planets and perform clustering to find similar planet
    groups. We will use the `preprocessing.ipynb` notebook for the section on preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Back in [*Chapter 1*](B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015), *Introduction
    to Data Analysis*, when we set up our environment, we installed a package from
    GitHub called `ml_utils`. This package contains utility functions and classes
    that we will use for our three chapters on machine learning. Unlike the last two
    chapters, we won't be discussing how to make this package; however, those interested
    can look through the code at https://github.com/stefmolin/ml-utils/tree/2nd_edition
    and follow the instructions from [*Chapter 7*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146),
    *Financial Analysis – Bitcoin and the Stock Market*, to install it in editable
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the reference links for the data sources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Open Exoplanet Catalogue database*, available at [https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/#data-structure](https://github.com/OpenExoplanetCatalogue/open_exoplanet_catalogue/#data-structure).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences
    by data mining from physicochemical properties. In Decision Support Systems, Elsevier,
    47(4):547-553, 2009.* Available online at [http://archive.ics.uci.edu/ml/datasets/Wine+Quality](http://archive.ics.uci.edu/ml/datasets/Wine+Quality).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [*[http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php)*].
    Irvine, CA: University of California, School of Information and Computer Science.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of the machine learning landscape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine learning** is a subset of **artificial intelligence** (**AI**) whereby
    an algorithm can learn to predict values from input data without explicitly being
    taught rules. These algorithms rely on statistics to make inferences as they learn;
    they then use what they learn to make predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Applying for a loan, using a search engine, sending a robot vacuum to clean
    a specific room with a voice command—machine learning can be found everywhere
    we look. This is because it can be used for many purposes, for example, voice
    recognition by AI assistants such as Alexa, Siri, or Google Assistant, mapping
    floor plans by exploring surroundings, determining who will default on a loan,
    figuring out which search results are relevant, and even painting ([https://www.boredpanda.com/computer-deep-learning-algorithm-painting-masters/](https://www.boredpanda.com/computer-deep-learning-algorithm-painting-masters/)).
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning models can be made to adapt to changes in the input over time
    and are a huge help in making decisions without the need for a human each time.
    Think about applying for a loan or a credit line increase on a credit card; the
    bank or credit card company will rely on a machine learning algorithm to look
    up things from the applicant's credit score and history with them to determine
    whether the applicant should be approved. Most likely, they will only approve
    the applicant at that moment if the model predicts a strong chance he or she can
    be trusted with the loan or new credit limit. In the case where the model can't
    be so sure, they can send it over to a human to make the final decision. This
    reduces the amount of applications employees have to sift through to just the
    borderline cases, while also providing faster answers for those non-borderline
    cases (the process can be nearly instantaneous).
  prefs: []
  type: TYPE_NORMAL
- en: One important thing to call out here is that models that are used for tasks
    such as loan approvals, by law, have to be interpretable. There needs to be a
    way to explain to the applicant why they were rejected—sometimes, reasons beyond
    technology can influence and limit what approaches or data we use.
  prefs: []
  type: TYPE_NORMAL
- en: Types of machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine learning is typically divided into three categories: unsupervised learning,
    supervised learning, and reinforcement learning. We use **unsupervised learning**
    when we don''t have labeled data telling us what our model should say for each
    data point. In many cases, gathering labeled data is costly or just not feasible,
    so unsupervised learning will be used. Note that it is more difficult to optimize
    the performance of these models because we don''t know how well they are performing.
    If we do have access to the labels, we can use **supervised learning**; this makes
    it much easier for us to evaluate our models and look to improve them since we
    can calculate metrics on their performance compared to the true labels.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Since unsupervised learning looks to find meaning in the data without a correct
    answer, it can be used to learn more about the data as a part of the analysis
    or before moving on to supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reinforcement learning** is concerned with reacting to feedback from the
    environment; this is used for things such as robots and AI in games. It is well
    beyond the scope of this book, but there are resources in the *Further reading*
    section for more information.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that not all machine learning approaches fit neatly into the aforementioned
    categories. One example is **deep learning**, which aims to learn data representations
    using methods such as **neural networks**. Deep learning methods are often seen
    as black boxes, which has prevented their use in certain domains where interpretable
    models are required; however, they are used for tasks such as speech recognition
    and image classification. Deep learning is also beyond the scope of this book,
    but it is good to be aware that it is also machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Interpretable machine learning is an active area of research. Check out the
    resources in the *Further reading* section for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Common tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most common machine learning tasks are clustering, classification, and regression.
    In **clustering**, we look to assign data into groups, with the goal being that
    the groups are well-defined, meaning that members of the group are close together
    and groups are separated from other groups. Clustering can be used in an unsupervised
    manner in an attempt to gain a better understanding of the data, or in a supervised
    manner to try to predict which cluster data belongs to (essentially classification).
    Note that clustering can be used for prediction in an unsupervised manner; however,
    we will need to decipher what each cluster means. Labels that are obtained from
    clustering can even be used as the input for a supervised learner to model how
    observations are mapped to each group; this is called **semi-supervised learning**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification**, as we discussed in the previous chapter, looks to assign
    a class label to the data, such as *benign* or *malicious*. This may sound like
    assigning it to a cluster, however, we aren''t worried about how similar the values
    that are assigned to *benign* are, just marking them as *benign*. Since we are
    assigning to a class or category, this class of models is used to predict a discrete
    label. **Regression**, on the other hand, is for predicting numeric values, such
    as housing prices or book sales; it models the strength and magnitude of the relationships
    between variables. Both can be performed as unsupervised or supervised learning;
    however, supervised models are more likely to perform better.'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we know what machine learning is, we need to know how we can build
    our own models. Python offers many packages for building machine learning models;
    some libraries we should be aware of include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scikit-learn`: Easy to use (and learn), it features a consistent API for machine
    learning in Python ([https://scikit-learn.org/stable/index.html](https://scikit-learn.org/stable/index.html))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`statsmodels`: A statistical modeling library that also provides statistical
    tests ([https://www.statsmodels.org/stable/index.html](https://www.statsmodels.org/stable/index.html))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tensorflow`: A machine learning library developed by Google that features
    faster calculations ([https://www.tensorflow.org/](https://www.tensorflow.org/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keras`: A high-level API for running deep learning from libraries such as
    TensorFlow ([https://keras.io/](https://keras.io/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytorch`: A deep learning library developed by Facebook ([https://pytorch.org](https://pytorch.org))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Most of these libraries use NumPy and SciPy, a library built on top of NumPy
    for statistics, mathematics, and engineering purposes. SciPy can be used to handle
    linear algebra, interpolation, integration, and clustering algorithms, among other
    things. More information on SciPy can be found at [https://docs.scipy.org/doc/scipy/reference/tutorial/general.html](https://docs.scipy.org/doc/scipy/reference/tutorial/general.html).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this book, we will be using `scikit-learn` for its user-friendly API. In
    `scikit-learn`, our base class is an `fit()` method. We use `transform()` method—transforming
    the data into something `predict()` method. The `score()` method. Knowing just
    these four methods, we can easily build any machine learning model offered by
    `scikit-learn`. More information on this design pattern can be found at [https://scikit-learn.org/stable/developers/develop.html](https://scikit-learn.org/stable/developers/develop.html).
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have learned throughout this book, our first step should be to engage
    in some **exploratory data analysis** (**EDA**) to get familiar with our data.
    In the interest of brevity, this section will include a subset of the EDA that's
    available in each of the notebooks—be sure to check out the respective notebooks
    for the full version.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: While we will use `pandas` code to perform our EDA, be sure to check out the
    `pandas-profiling` package ([https://github.com/pandas-profiling/pandas-profiling](https://github.com/pandas-profiling/pandas-profiling)),
    which can be used to quickly perform some initial EDA on the data via an interactive
    HTML report.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with our imports, which will be the same across the notebooks
    we will use in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will start our EDA with the wine quality data before moving on to the planets.
  prefs: []
  type: TYPE_NORMAL
- en: Red wine quality data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s read in our red wine data and do some EDA using techniques we have learned
    throughout this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We have data on 11 different chemical properties of red wine, along with a
    column indicating the quality score from the wine experts that participated in
    the blind taste testing. We can try to predict the quality score by looking at
    the chemical properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Red wine dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.1_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – Red wine dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what the distribution of the `quality` column looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The information on the dataset says that `quality` varies from 0 (terrible)
    to 10 (excellent); however, we only have values in the middle of that range. An
    interesting task for this dataset could be to see if we can predict high-quality
    red wines (a quality score of 7 or higher):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Distribution of red wine quality scores'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.2_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2 – Distribution of red wine quality scores
  prefs: []
  type: TYPE_NORMAL
- en: 'All of our data is numeric, so we don''t have to worry about handling text
    values; we also don''t have any missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use `describe()` to get an idea of what scale each of the columns is
    on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The result indicates that we will definitely have to do some scaling if our
    model uses distance metrics for anything because our columns aren''t all on the
    same range:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Summary statistics for the red wine dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.3_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.3 – Summary statistics for the red wine dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, let''s use `pd.cut()` to bin our high-quality red wines (roughly 14%
    of the data) for later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'We are stopping our EDA here for brevity; however, we should make sure to fully
    explore our data and consult domain experts before attempting any modeling. One
    thing to pay particular attention to is correlations between variables and what
    we are trying to predict (high-quality red wine, in this case). Variables with
    strong correlations may be good features to include in a model. However, note
    that correlation does not imply causation. We already learned a few ways to use
    visualizations to look for correlations: the scatter matrix we discussed in [*Chapter
    5*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106), *Visualizing Data with Pandas
    and Matplotlib*, and the heatmap and pair plot from [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*. A pair plot is included
    in the `red_wine.ipynb` notebook.'
  prefs: []
  type: TYPE_NORMAL
- en: White and red wine chemical properties data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s look at the red and white wine data together. Since the data comes
    in separate files, we need to read in both and concatenate them into a single
    dataframe. The white wine file is actually semi-colon (`;`) separated, so we must
    provide the `sep` argument to `pd.read_csv()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also look at the quality scores of the white wines, just as we did with
    the red ones, and we will find that the white wines tend to be rated higher overall.
    This might bring us to question whether the judges preferred white wine over red
    wine, thus creating a bias in their ratings. As it is, the rating system that
    was used seems to be pretty subjective:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Distribution of white wine quality scores'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.4_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.4 – Distribution of white wine quality scores
  prefs: []
  type: TYPE_NORMAL
- en: 'Both of these dataframes have the same columns, so we can combine them without
    further work. Here, we use `pd.concat()` to stack the white wine data on top of
    the red wine data after adding a column to identify which wine type each observation
    belongs to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As we did with the red wine dataset, we can run `info()` to check whether we
    need to perform type conversion or whether we are missing any data; thankfully,
    we have no need here either. Our combined wine dataset looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Combined wine dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.5_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.5 – Combined wine dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `value_counts()`, we can see that we have many more white wines than
    red wines in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, let''s examine box plots for each chemical property broken out by wine
    type using `seaborn`. This can help us identify **features** (model inputs) that
    will be helpful when building our model to distinguish between red and white wine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Given the following result, we might look to use fixed acidity, volatile acidity,
    total sulfur dioxide, and sulphates when building a model since they seem to be
    distributed differently for red and white wines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Comparing red and white wine on a chemical level'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.6_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.6 – Comparing red and white wine on a chemical level
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the distributions of variables across classes can help inform feature
    selection for our model. If we see that the distribution for a variable is very
    different between classes, that variable may be very useful to include in our
    model. It is essential that we perform an in-depth exploration of our data before
    moving on to modeling. Be sure to use the visualizations we covered in [*Chapter
    5*](B16834_05_Final_SK_ePub.xhtml#_idTextAnchor106), *Visualizing Data with Pandas
    and Matplotlib*, and [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125),
    *Plotting with Seaborn and Customization Techniques*, as they will prove invaluable
    for this process.
  prefs: []
  type: TYPE_NORMAL
- en: We will come back to this visualization in [*Chapter 10*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217),
    *Making Better Predictions – Optimizing Models*, when we examine incorrect predictions
    made by our model. Now, let's take a look at the other dataset we will be working
    with.
  prefs: []
  type: TYPE_NORMAL
- en: Planets and exoplanets data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An **exoplanet** is simply a planet that orbits a star outside of our solar
    system, so from here on out we will refer to both collectively as **planets**.
    Let''s read in our planets data now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Some interesting tasks we can do with this data would be to find clusters of
    similar planets based on their orbits and try to predict the orbit period (how
    long a year is on a planet), in Earth days:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Planets dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.7_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.7 – Planets dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'We can build a correlation matrix heatmap to help find the best features to
    use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The heatmap shows us that the semi-major axis of a planet''s orbit is highly
    positively correlated to the length of its period, which makes sense since the
    semi-major axis (along with eccentricity) helps define the path that a planet
    travels around its star:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Correlations between features in the planets dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.8_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.8 – Correlations between features in the planets dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'To predict `period`, we probably want to look at `semimajoraxis`, `mass`, and
    `eccentricity`. The orbit eccentricity quantifies how much the orbit differs from
    a perfect circle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Understanding eccentricity'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.9_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.9 – Understanding eccentricity
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what shapes the orbits we have are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'It looks like nearly everything is an ellipse, which we would expect since
    these are planets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Distribution of orbit eccentricities'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.10_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.10 – Distribution of orbit eccentricities
  prefs: []
  type: TYPE_NORMAL
- en: 'An ellipse, being an elongated circle, has two axes: *major* and *minor* for
    the longest and shortest ones, respectively. The semi-major axis is half the major
    axis. When compared to a circle, the axes are analogous to the diameter, crossing
    the entire shape, and the semi-axes are akin to the radius, being half the diameter.
    The following is how this would look in the case where the planet orbited a star
    that was exactly in the center of its elliptical orbit (due to gravity from other
    objects, in reality, the star can be anywhere inside the orbit path):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 – Understanding the semi-major axis'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.11_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.11 – Understanding the semi-major axis
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we understand what these columns mean, let''s do some more EDA. This
    data isn''t as clean as our wine data was—it''s certainly much easier to measure
    everything when we can reach out and touch it. We only have `eccentricity`, `semimajoraxis`,
    or `mass` data for a fraction of the planets, despite knowing most of the `period`
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If we were to drop data where any of these columns was null, we would be left
    with about 30% of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If we are simply looking for a way to predict the length of the year (when
    we have these values available) to learn more about their relationship, we wouldn''t
    necessarily worry about throwing out the missing data. Imputing it here could
    be far worse for our model. At least everything is properly encoded as a decimal
    (`float64`); however, let''s check whether we need to do some scaling (beneficial
    if our model is sensitive to differences in magnitude):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows us that, depending on our model, we will definitely have to do some
    scaling because the values in the `period` column are much larger than the others:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Summary statistics for the planets dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.12_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.12 – Summary statistics for the planets dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'We could also look at some scatter plots. Note that there is a `list` column
    for the group the planet belongs to, such as `Solar System` or `Controversial`.
    We might want to see if the period (and distance from the star) influences this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The controversial planets appear to be spread throughout and have larger semi-major
    axes and periods. Perhaps they are controversial because they are very far from
    their star:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.13 – Planet period versus semi-major axis'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.13_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.13 – Planet period versus semi-major axis
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, we can see that the scale of `period` is making this pretty
    difficult to read, so we could try a log transformation on the *y*-axis to get
    more separation in the denser section on the lower-left. Let''s just point out
    the planets in our solar system this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'There were certainly a lot of planets hiding in that lower-left corner of the
    plot. We can see many planets with years shorter than Mercury''s 88 Earth-day
    year now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.14 – Our solar system compared to exoplanets'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.14_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.14 – Our solar system compared to exoplanets
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a feel for the data we will be working with, let's learn how
    to prepare it for use in a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be working in the `preprocessing.ipynb` notebook before
    we return to the notebooks we used for EDA. We will begin with our imports and
    read in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Machine learning models follow the garbage in, garbage out principle. We have
    to make sure that we **train** our models (have them learn) on the best possible
    version of the data. What this means will depend on the model we choose. For instance,
    models that use a distance metric to calculate how similar observations are will
    easily be confused if our features are on wildly different scales. Unless we are
    working with a **natural language processing** (**NLP**) problem to try and understand
    the meaning of words, our model will have no use for—or worse, be unable to interpret—textual
    values. Missing or invalid data will also cause problems; we will have to decide
    whether to drop them or impute them. All of the adjustments we make to our data
    before giving it to our model to learn from are collectively called **preprocessing**.
  prefs: []
  type: TYPE_NORMAL
- en: Training and testing sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, machine learning sounds pretty great, though—we can build a model that
    will learn how to perform a task for us. Therefore, we should give it all the
    data we have so that it learns well, right? Unfortunately, it's not that simple.
    If we give the model all of our data, we risk **overfitting** it, meaning that
    it won't be able to generalize well to new data points because it was fit to the
    sample rather than the population. On the other hand, if we don't give it enough
    data, it will **underfit** and be unable to capture the underlying information
    in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: When a model fits the randomness in the data, it is said to fit the **noise**
    in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another thing to consider is that if we use all of our data to train the model,
    how can we evaluate its performance? If we test it on the data we used for training,
    we will be overestimating how good it is because our model will always perform
    better on the training data. For these reasons, it''s important to split our data
    into a **training set** and **testing set**. To do so, we could shuffle our dataframe
    and select the top *x*% of the rows for training and leave the rest for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This would work, but it''s a lot to write every time. Thankfully, `scikit-learn`
    provides us with the `train_test_split()` function in the `model_selection` module,
    which is a more robust, easier-to-use solution. It requires us to separate our
    input data (`X`) from our output data (`y`) beforehand. Here, we will pick 75%
    of the data to be used for the training set (`X_train`, `y_train`) and 25% for
    the testing set (`X_test`, `y_test`). We will set a seed (`random_state=0`) so
    that the split is reproducible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'While there are no specific criteria for what constitutes a good size for the
    test set, a rule of thumb is usually between 10% and 30% of the data. However,
    if we don''t have much data, we will shift toward a 10% testing set to make sure
    that we have enough data to learn from. Conversely, if we have a lot of data,
    we may move toward 30% testing, since, not only do we not want to overfit, but
    we want to give our model a good amount of data to prove its worth. Note that
    there is a big caveat with this rule of thumb: there are diminishing returns on
    the amount of training data we use. If we have a ton of data, we will most likely
    use much less than 70% of it for training because our computational costs may
    rise significantly for possibly minuscule improvements and an increased risk of
    overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: When building models that require tuning, we split the data into training, validation,
    and testing sets. We will introduce validation sets in [*Chapter 10*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217),
    *Making Better Predictions – Optimizing Models*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the dimensions of our training and testing sets now.
    Since we are using three features (`eccentricity`, `semimajoraxis`, and `mass`),
    `X_train` and `X_test` have three columns. The `y_train` and `y_test` sets will
    be a single column each. The number of observations in the `X` and `y` data for
    training will be equal, as will be the case for the testing set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`X_train` and `X_test` are returned to us as dataframes since that is the format
    we passed them in as. If we are working with data in NumPy directly, we will get
    NumPy arrays or `ndarrays` back instead. We are going to work with this data for
    other examples in the *Preprocessing data* section, so let''s take a look at the
    first five rows of the `X_train` dataframe. Don''t worry about the `NaN` values
    for now; we will discuss different ways of handling them in the *Imputing* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Both `y_train` and `y_test` are series since that is what we passed into the
    `train_test_split()` function. If we had passed in a NumPy array, that is what
    we would have gotten back instead. The rows in `y_train` and `y_test` must line
    up with the rows in `X_train` and `X_test`, respectively. Let''s confirm this
    by looking at the first five rows of `y_train`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Indeed, everything matches up, as expected. Note that for our wine models, we
    need to use stratified sampling, which can also be done with `train_test_split()`
    by passing the values to stratify on in the `stratify` argument. We will see this
    in the *Classification* section. For now, let's move on to the rest of our preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling and centering data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've seen that our dataframes had columns with very different scales; if we
    want to use any model that calculates a distance metric (such as k-means, which
    we will discuss in this chapter, or `preprocessing` module for standardizing (scaling
    by calculating Z-scores) and min-max scaling (to normalize data to be in the range
    [0, 1]), among others.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: We should check the requirements of the model we are building to see if the
    data needs to be scaled.
  prefs: []
  type: TYPE_NORMAL
- en: 'For standard scaling, we use the `StandardScaler` class. The `fit_transform()`
    method combines `fit()`, which figures out the mean and standard deviation needed
    to center and scale, and `transform()`, which applies the transformation to the
    data. Note that, when instantiating a `StandardScaler` object, we can choose to
    not subtract the mean or not divide by the standard deviation by passing `False`
    to `with_mean` or `with_std`, respectively. Both are `True` by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: After this transformation, the data is in `e` tells us where the decimal point
    got moved to. For a `+` sign, we move the decimal point to the right by the number
    of places indicated; we move to the left for a `-` sign. Therefore, `1.00e+00`
    is equivalent to `1`, `2.89e-02` is equivalent to `0.0289`, and `2.89e+02` is
    equivalent to `289`. The transformed planets data is mostly between -3 and 3 because
    everything is now a Z-score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other scalers can be used with the same syntax. Let''s use the `MinMaxScaler`
    class to transform the planets data into the range [0, 1]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Another option is the `RobustScaler` class, which uses the median and IQR for
    robust to outliers scaling. There is an example of this in the notebook. More
    preprocessing classes can be found at [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing).
  prefs: []
  type: TYPE_NORMAL
- en: Encoding data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All of the scalers discussed so far address the preprocessing of our numeric
    data, but how can we deal with categorical data? We need to encode the categories
    into integer values. There are a few options here, depending on what the categories
    represent. If our category is binary (such as `0`/`1`, `True`/`False`, or `yes`/`no`),
    then we will `0` is one option and `1` is the other. We can easily do this with
    the `np.where()` function. Let''s encode the wine data''s `kind` field as `1`
    for red and `0` for white:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This is effectively a column that tells us whether or not the wine is red. Remember,
    we concatenated the red wines to the bottom of the white wines when we created
    our `wine` dataframe, so `np.where()` will return zeros for the top rows and ones
    for the bottom rows, just like we saw in the previous result.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: We can also use the `LabelBinarizer` class from `scikit-learn` to encode the
    `kind` field. Note that if our data is actually continuous, but we want to treat
    it as a binary categorical value, we could use the `Binarizer` class and provide
    a threshold or `pd.cut()`/`pd.qcut()`. There are examples of these in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'If our categories are ordered, we may want to use `0`, `1`, and `2`, respectively.
    The advantages of this are that we can use regression techniques to predict the
    quality, or we can use this as a feature in the model to predict something else;
    this model would be able to use the fact that high is better than medium, which
    is better than low quality. We can achieve this with the `LabelEncoder` class.
    Note that the labels will be created according to alphabetical order, so the first
    category alphabetically will be `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn provides the `OrdinalEncoder` class, but our data is not in the
    correct format—it expects 2D data (such as a `DataFrame` or `ndarray` object),
    instead of the 1D `Series` object we are working with here. We still need to ensure
    that the categories are in the proper order beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: However, note that the ordinal encoding may create a potential data issue. In
    our example, if high-quality wines are now `2` and medium-quality wines are `1`,
    the model may interpret that `2 * med = high`. This is implicitly creating an
    association between the levels of quality that we may not agree with.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, a safer approach would be to perform `is_low` and `is_med`, which
    take only `0` or `1`. Using those two, we automatically know whether the wine
    quality was high (when `is_low` = `is_med` = `0`). These are called `1`, that
    row is a member of that group; in our example of wine quality categories, if `is_low`
    is `1`, then that row is a member of the low-quality group. This can be achieved
    with the `pd.get_dummies()` function and the `drop_first` argument, which will
    remove the redundant column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use one-hot encoding to encode the `list` column in the planets data,
    since the categories have no inherent order. Before we do any transformations,
    let''s take a look at the lists we have in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `pd.get_dummies()` function to create dummy variables if we
    want to include the planet list in our models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This turns our single series into the following dataframe, where the dummy
    variables were created in the order they appeared in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.15 – One-hot encoding'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.15_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.15 – One-hot encoding
  prefs: []
  type: TYPE_NORMAL
- en: 'As we discussed previously, one of these columns is redundant because the values
    in the remaining ones can be used to determine the value for the redundant one.
    Some models may be significantly affected by the high correlation between these
    columns (referred to as `drop_first` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the first column from the previous result has been removed, but we
    can still determine that all but the last row were in the `Confirmed Planets`
    list:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.16 – Dropping redundant columns after one-hot encoding'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.16_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.16 – Dropping redundant columns after one-hot encoding
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we can obtain a similar result by using the `LabelBinarizer` class
    and its `fit_transform()` method on our planets list. This won''t drop a redundant
    feature, so we once again have the first feature belonging to the confirmed planets
    list, which can be seen in bold in the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn provides the `OneHotEncoder` class, but our data is not in the
    correct format—it expects the data to come in a 2D array, and our series is just
    1D. We will see an example of how to use this in the *Additional transformers*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Imputing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We already know that we have some missing values in our planet data, so let''s
    discuss a few of the options `scikit-learn` offers for handling them, which can
    be found in the `impute` module: imputing with a value (using constants or summary
    statistics), imputing based on similar observations, and indicating what is missing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Back in the *Exploratory data analysis* section, we ran `dropna()` on the planets
    data we planned to model with. Let''s say we don''t want to get rid of it, and
    we want to try imputing it instead. The last few rows of our data have some missing
    values for `semimajoraxis`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `SimpleImputer` class to impute with a value, which will be
    the mean by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The mean hardly seems like a good strategy here since the planets we know about
    may share something in common, and surely things like what system a planet is
    a part of and its orbit can be good indicators of some of the missing data points.
    We have the option to provide the `strategy` parameter with a method other than
    the mean; currently, it can be `median`, `most_frequent`, or `constant` (specify
    the value with `fill_value`). None of these is really appropriate for us; however,
    `scikit-learn` also provides the `KNNImputer` class for imputing missing values
    based on similar observations. By default, it uses the five nearest neighbors
    and runs k-NN, which we will discuss in [*Chapter 10*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217),
    *Making Better Predictions – Optimizing Models*, using the features that aren''t
    missing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Notice that each of the bottom three rows has a unique value imputed for the
    semi-major axis now. This is because the mass and eccentricity were used to find
    similar planets from which to impute the semi-major axis. While this is certainly
    better than using the `SimpleImputer` class for the planets data, imputing can
    be dangerous.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than imputing the data, in some cases, we may be more interested in
    noting where we have missing data and using that as a feature in our model. This
    can be achieved with the `MissingIndicator` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: As we turn our attention to the final set of preprocessors that we will discuss,
    notice that all of them have a `fit_transform()` method, along with `fit()` and
    `transform()` methods. This API design decision makes it very easy to figure out
    how to use new classes and is one of the reasons why `scikit-learn` is so easy
    to learn and use—it's very consistent.
  prefs: []
  type: TYPE_NORMAL
- en: Additional transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What if, rather than scaling our data or encoding it, we want to run a mathematical
    operation, such as taking the square root or the logarithm? The `preprocessing`
    module also has some classes for this. While there are a few that perform a specific
    transformation, such as the `QuantileTransformer` class, we will focus our attention
    on the `FunctionTransformer` class, which lets us provide an arbitrary function
    to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Here, we took the absolute value of every number. Take note of the `validate=True`
    argument; the `FunctionTransformer` class knows that `scikit-learn` models won't
    accept `NaN` values, infinite values, or missing ones, so it will throw an error
    if we get those back. For this reason, we run `dropna()` here as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that for scaling, encoding, imputing, and transforming data, everything
    we passed was transformed. If we have features of different data types, we can
    use the `ColumnTransformer` class to map transformations to a column (or group
    of columns) in a single call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'There is also the `make_column_transformer()` function, which will name the
    transformers for us. Let''s make a `ColumnTransformer` object that will treat
    categorical data and numerical data differently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: We are passing `sparse=False` upon instantiating our `OneHotEncoder` object
    so that we can see our result. In practice, we don't need to do this since `scikit-learn`
    models know how to handle NumPy sparse matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Building data pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It sure seems like there are a lot of steps involved in preprocessing our data,
    and they need to be applied in the correct order for both training and testing
    data—quite tedious. Thankfully, `scikit-learn` offers the ability to create pipelines
    to streamline the preprocessing and ensure that the training and testing sets
    are treated the same. This prevents issues, such as calculating the mean using
    all the data in order to standardize it and then splitting it into training and
    testing sets, which will create a model that looks like it will perform better
    than it actually will.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: When information from outside the training set (such as using the full dataset
    to calculate the mean for standardization) is used to train the model, it is referred
    to as **data leakage**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are learning about pipelines before we build our first models because they
    ensure that the models are built properly. Pipelines can contain all the preprocessing
    steps and the model itself. Making a pipeline is as simple as defining the steps
    and naming them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We aren''t limited to using pipelines with models—they can also be used inside
    other `scikit-learn` objects, for example, `ColumnTransformer` objects. This makes
    it possible for us to first use k-NN imputing on the semi-major axis data (the
    column at index `0`) and then standardize the result. We can then include this
    as part of a pipeline, which gives us tremendous flexibility in how we build our
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like with the `ColumnTransformer` class, we have a function that can make
    pipelines for us without having to name the steps. Let''s make another pipeline,
    but this time we will use the `make_pipeline()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Note that the steps have been automatically named the lowercase version of the
    class name. As we will see in the next chapter, naming the steps will make it
    easier to optimize model parameters by name. The consistency of the `scikit-learn`
    API will also allow us to use this pipeline to fit our model and make predictions
    using the same object, which we will see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use clustering to divide our data points into groups of similar points. The
    points in each group are more like their fellow group members than those of other
    groups. Clustering is commonly used for tasks such as recommendation systems (think
    of how Netflix recommends what to watch based on what other people who've watched
    similar things are watching) and market segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, say we work at an online retailer and want to segment our website
    users for more targeted marketing efforts; we can gather data on time spent on
    the site, page visits, products viewed, products purchased, and much more. Then,
    we can have an unsupervised clustering algorithm find groups of users with similar
    behavior; if we make three groups, we can come up with labels for each group according
    to its behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.17 – Clustering website users into three groups'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.17_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.17 – Clustering website users into three groups
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we can use clustering for unsupervised learning, we will need to interpret
    the groups that are created and then try to derive a meaningful name for each
    group. If our clustering algorithm identified the three clusters in the preceding
    scatter plot, we may be able to make the following behavioral observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Frequent customers (group 0)**: Purchase a lot and look at many products.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Occasional customers (group 1)**: Have made some purchases, but less than
    the most frequent customers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Browsers (group 2)**: Visit the website, but haven''t bought anything.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once these groups have been identified, the marketing team can focus on marketing
    to each of these groups differently; it's clear that the frequent customers will
    do more for the bottom line, but if they are already buying a lot, perhaps the
    marketing budget is better utilized trying to increase the purchases of the occasional
    customers or converting browsers into occasional customers.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Deciding on the number of groups to create can clearly influence how the groups
    are later interpreted, meaning that this is not a trivial decision. We should
    at least visualize our data and obtain some domain knowledge on it before attempting
    to guess the number of groups to split it into.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, clustering can be used in a supervised fashion if we know the
    group labels for some of the data for training purposes. Say we collected data
    on login activity, like in [*Chapter 8*](B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172),
    *Rule-Based Anomaly Detection*, but we had some examples of what attacker activity
    looks like; we could gather those data points for all activity and then use a
    clustering algorithm to assign to the valid users group or to the attacker group.
    Since we have the labels, we can tweak our input variables and/or the clustering
    algorithm we use to best align these groups to their true group.
  prefs: []
  type: TYPE_NORMAL
- en: k-means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The clustering algorithms offered by `scikit-learn` can be found in the `cluster`
    module's documentation at [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.cluster).
    We will take a look at **k-means**, which iteratively assigns points to the nearest
    group using the distance from the **centroid** of the group (center point), making
    *k* groups. Since this model uses distance calculations, it is imperative that
    we understand the effect scale will have on our results beforehand; we can then
    decide which columns, if any, to scale.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to measure the distance between points in space. Often,
    Euclidean distance, or straight-line distance, is the default; however, another
    common one is Manhattan distance, which can be thought of as city-block distance.
  prefs: []
  type: TYPE_NORMAL
- en: When we plotted out the period versus the semi-major axis for all the planets
    using a log scale for the period, we saw a nice separation of the planets along
    an arc. We are going to use k-means to find groups of planets with similar orbits
    along that arc.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping planets by orbit characteristics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we discussed in the *Preprocessing data* section, we can build a pipeline
    to scale and then model our data. Here, our model will be a `KMeans` object that
    makes eight clusters (for the number of planets in our solar system—sorry, Pluto).
    Since the k-means algorithm randomly picks its starting centroids, it''s possible
    to get different cluster results unless we specify the seed. Therefore, we also
    provide `random_state=0` for reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have our pipeline, we fit it on all the data since we aren''t trying
    to predict anything (in this case)—we just want to find similar planets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model is fit to our data, we can use the `predict()` method to get
    the cluster labels for each point (on the same data that we used previously).
    Let''s take a look at the clusters that k-means identified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Mercury and Venus landed in the same cluster, as did Earth and Mars. Jupiter,
    Saturn, and Uranus each belong to separate clusters, while Neptune and Pluto share
    a cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.18 – Eight clusters of planets identified by k-means'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.18_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.18 – Eight clusters of planets identified by k-means
  prefs: []
  type: TYPE_NORMAL
- en: We picked eight clusters arbitrarily here, since this is the number of planets
    in our solar system. Ideally, we would have some domain knowledge about the true
    groupings or need to pick a specific number. For example, say we want to fit wedding
    guests at five tables so that they all get along, then our *k* is 5; if we can
    run three marketing campaigns on user groups, we have a *k* of 3\. If we have
    no intuition as to the number of groups there will be in the data, a rule of thumb
    is to try the square root of our observations, but this can yield an unmanageable
    amount of clusters. Therefore, if it doesn't take too long to create many k-means
    models on our data, we can use the elbow point method.
  prefs: []
  type: TYPE_NORMAL
- en: The elbow point method for determining k
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **elbow point method** involves creating multiple models with many values
    of *k* and plotting each model's **inertia** (**within-cluster sum of squares**)
    versus the number of clusters. We want to minimize the sum of squared distances
    from points to their cluster's center while not creating too many clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ml_utils.elbow_point` module contains our `elbow_point()` function, which
    has been reproduced here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s use the elbow point method to find an appropriate value for *k*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The point at which we see diminishing returns is an appropriate *k*, which
    may be around two or three here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.19 – Interpreting an elbow point plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.19_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.19 – Interpreting an elbow point plot
  prefs: []
  type: TYPE_NORMAL
- en: 'If we create just two clusters, we divide the planets into a group with most
    of the planets (orange) and a second group with only a few in the upper-right
    (blue), which are likely to be outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.20 – Two clusters of planets identified by k-means'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.20_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.20 – Two clusters of planets identified by k-means
  prefs: []
  type: TYPE_NORMAL
- en: Note that while this may have been an appropriate amount of clusters, it doesn't
    tell us as much as the previous attempt. If we wanted to know about planets that
    are similar to each of the planets in our solar system, we would want to use a
    larger *k*.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting centroids and visualizing the cluster space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since we standardized our data before clustering, we can look at the `cluster_centers_`
    attribute of the model. The centroid of the blue cluster is located at (18.9,
    20.9), which is in the (semi-major axis, period) format; remember, these are Z-scores,
    so these are quite far from the rest of the data. The orange cluster, on the other
    hand, is centered at (-0.035, -0.038).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build a visualization that shows us the location of the centroids with
    the scaled input data and the cluster distance space (where points are represented
    as the distance to their cluster''s centroid). First, we will set up our layout
    for a smaller plot inside of a larger one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we grab the scaled version of the input data and the distances between
    those data points and the centroid of the cluster they belong to. We can use the
    `transform()` and `fit_transform()` (`fit()` followed by `transform()`) methods
    to convert the input data into cluster distance space. We get NumPy `ndarrays`
    back, where each value in the outer array represents the coordinates of a point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we know that each array in the outer array will have the semi-major axis
    as the first entry and the period as the second, we use `[:,0]` to select all
    the semi-major axis values and `[:,1]` to select all the period values. These
    will be the *x* and *y* for our scatter plot. Note that we actually don''t need
    to call `predict()` to get the cluster labels for the data because we want the
    labels for the data we trained the model on; this means that we can use the `labels_`
    attribute of the `KMeans` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we annotate the location of the centroids on the outer plot, which
    shows the scaled data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'In the resulting plot, we can easily see that the three blue points are quite
    different from the rest and that they are the only members of the second cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.21 – Visualizing the planets in the cluster distance space'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.21_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.21 – Visualizing the planets in the cluster distance space
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have been using `transform()` or combination methods, such as `fit_predict()`
    or `fit_transform()`, but not all models will support these methods. We will see
    a slightly different workflow in the *Regression* and *Classification* sections.
    In general, most `scikit-learn` objects will support the following, based on what
    they are used for:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.22 – General reference for the scikit-learn model API'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.22_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.22 – General reference for the scikit-learn model API
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have built a few models, we are ready for the next step: quantifying
    their performance. The `metrics` module in `scikit-learn` contains various metrics
    for evaluating model performance across clustering, regression, and classification
    tasks; the API lists the functions at [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics).
    Let''s discuss how to evaluate an unsupervised clustering model next.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating clustering results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most important criterion for evaluating our clustering results is that they
    are useful for what we set out to do; we used the elbow point method to pick an
    appropriate value for *k*, but that wasn't as useful to us as the original model
    with eight clusters. That being said, when looking to quantify the performance,
    we need to pick metrics that match the type of learning we performed.
  prefs: []
  type: TYPE_NORMAL
- en: When we know the true clusters for our data, we can check that our clustering
    model places the points together in a cluster as they are in the true cluster.
    The cluster label given by our model can be different than the true one—all that
    matters is that the points in the same true cluster are also together in the predicted
    clusters. One such metric is the **Fowlkes-Mallows Index**, which we will see
    in the end-of-chapter exercises.
  prefs: []
  type: TYPE_NORMAL
- en: With the planets data, we performed unsupervised clustering because we don't
    have labels for each data point, and therefore, we can't measure how well we did
    against those. This means that we have to use metrics that evaluate aspects of
    the clusters themselves, such as how far apart they are and how close the points
    in a cluster are together. We can compare multiple metrics to get a more well-rounded
    evaluation of the performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'One such method is called the **silhouette coefficient**, which helps quantify
    cluster separation. It is calculated by subtracting the mean of the distances
    between every two points in a cluster (*a*) from the mean of distances between
    points in a given cluster and the closest different cluster (*b*) and dividing
    by the maximum of the two:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This metric returns values in the range [-1, 1], where -1 is the worst (clusters
    are wrongly assigned) and 1 is the best; values near 0 indicate overlapping clusters.
    The higher this number is, the better defined (more separated) the clusters are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Another score we could use to evaluate our clustering result is the ratio of
    **within-cluster distances** (distances between points in a cluster) to the **between-cluster
    distances** (distances between points in different clusters), called the **Davies-Bouldin
    score**. Values closer to zero indicate better partitions between clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'One last metric for unsupervised clustering that we will discuss here is the
    **Calinski and Harabasz score**, or **Variance Ratio Criterion**, which is the
    ratio of dispersion within a cluster to dispersion between clusters. Higher values
    indicate better defined (more separated) clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: For a complete list of clustering evaluation metrics offered by `scikit-learn`
    (including supervised clustering) and when to use them, check out the *Clustering
    performance evaluation* section of their guide at [https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation](https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation).
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the planets dataset, we want to predict the length of the year, which is
    a numeric value, so we will turn to regression. As mentioned at the beginning
    of this chapter, regression is a technique for modeling the strength and magnitude
    of the relationship between independent variables (our `X` data)—often called
    `y` data) that we want to predict.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scikit-learn provides many algorithms that can handle regression tasks, ranging
    from decision trees to linear regression, spread across modules according to the
    various algorithm classes. However, typically, the best starting point is a linear
    regression, which can be found in the `linear_model` module. In **simple linear
    regression**, we fit our data to a line of the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, epsilon (*ε*) is the error term and betas (*β*) are coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The coefficients we get from our model are those that minimize the **cost function**,
    or error between the observed values (*y*) and those predicted with the model
    (*ŷ*, pronounced *y-hat*). Our model gives us estimates of these coefficients,
    and we write them as ![](img/Formula_09_004.png) (pronounced *beta-hat*).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we want to model additional relationships, we need to use **multiple
    linear regression**, which contains multiple regressors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Linear regression in `scikit-learn` uses **ordinary least squares** (**OLS**),
    which yields the coefficients that minimize the sum of squared errors (measured
    as the distance between *y* and *ŷ*). The coefficients can be found using the
    closed-form solution, or estimated with optimization methods, such as **gradient
    descent**, which uses the negative gradient (direction of steepest ascent calculated
    with partial derivatives) to determine which coefficients to try next (see the
    link in the *Further reading* section for more information). We will use gradient
    descent in [*Chapter 11*](B16834_11_Final_SK_ePub.xhtml#_idTextAnchor237), *Machine
    Learning Anomaly Detection*.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression makes some assumptions about the data, which we must keep
    in mind when choosing to use this technique. It assumes that the residuals are
    normally distributed and homoskedastic and that there is no multicollinearity
    (high correlations between the regressors).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a little background on how linear regression works, let's build
    a model to predict the orbit period of a planet.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the length of a year on a planet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we can build our model, we must isolate the columns that are used to
    predict (`semimajoraxis`, `mass`, and `eccentricity`) from the column that will
    be predicted (`period`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a supervised task. We want to be able to predict the length of a year
    on a planet using its semi-major axis, mass, and eccentricity of orbit, and we
    have the period lengths for most of the planets in the data. Let''s create a 75/25
    split of training to testing data so that we can assess how well this model predicts
    year length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have separated the data into the training and testing sets, we can
    create and fit the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: This fitted model can be used to examine the estimated coefficients and also
    to predict the value of the dependent variable for a given set of independent
    variables. We will cover both of these use cases in the next two sections.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the linear regression equation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The equation derived from a linear regression model gives coefficients to quantify
    the relationships between the variables. Care must be exercised when attempting
    to interpret these coefficients if we are dealing with more than a single regressor.
    In the case of multicollinearity, we can't interpret them because we are unable
    to hold all other regressors constant to examine the effect of a single one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thankfully, the regressors we are using for the planets data aren''t correlated,
    as we saw from the correlation matrix heatmap we made in the *Exploratory data
    analysis* section (*Figure 9.8*). So, let''s get the intercept and coefficients
    from the fitted linear model object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields the following equation for our linear regression model of planet
    year length:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to interpret this more completely, we need to understand the units
    everything is in:'
  prefs: []
  type: TYPE_NORMAL
- en: '`period` (length of year): Earth days'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`semimajoraxis`: **Astronomical units** (**AUs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mass`: Jupiter masses (planet mass divided by Jupiter''s mass)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eccentricity`: N/A'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An astronomical unit is the average distance between the Earth and the Sun,
    which is equivalent to 149,597,870,700 meters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The intercept in this particular model doesn''t have any meaning: if the planet
    had a semi-major axis of zero, no mass, and a perfect circle eccentricity, its
    year would be -623 Earth days long. A planet must have a non-negative, non-zero
    period, semi-major axis, and mass, so this clearly makes no sense. We can, however,
    interpret the other coefficients. The equation says that, holding mass and eccentricity
    constant, adding one additional AU to the semi-major axis distance increases the
    year length by 1,880 Earth days. Holding the semi-major axis and eccentricity
    constant, each additional Jupiter mass decreases the year length by 90.2 Earth
    days.'
  prefs: []
  type: TYPE_NORMAL
- en: Going from a perfectly circular orbit (`eccentricity=0`) to a nearly parabolic
    escape orbit (`eccentricity=1`) will decrease the year length by 3,201 Earth days;
    note that these are approximate for this term because, with a parabolic escape
    orbit, the planet will never return, and consequently, this equation wouldn't
    make sense. In fact, if we tried to use this equation for eccentricities greater
    than or equal to 1, we would be extrapolating because we have no such values in
    the training data. This is a clear example of when extrapolation doesn't work.
    The equation tells us that the larger the eccentricity, the shorter the year,
    but once we get to eccentricities of one and beyond, the planets never come back
    (they have reached escape orbits), so the year is infinite.
  prefs: []
  type: TYPE_NORMAL
- en: All of the eccentricity values in the training data are in the range [0, 1),
    so we are interpolating (predicting period values using data in the ranges we
    trained on). This means that as long as the eccentricity of the planet we want
    to predict is also in the range [0, 1), we can use this model to make the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Making predictions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have an idea of the effect each of our regressors has, let''s use
    our model to make predictions of year length for the planets in the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s visualize how well we did by plotting the actual and predicted values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The predicted values seem pretty close to the actual values and follow a similar
    pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.23 – Predictions versus actual values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.23_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.23 – Predictions versus actual values
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Try running this regression with just the `semimajoraxis` regressor. Some reshaping
    of the data will be necessary, but this will show how much better this performs
    as we add in `eccentricity` and `mass`. In practice, we often have to build many
    versions of our model to find one we are happy with.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check their correlation to see how well our model tracks the true relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Our predictions are very strongly positively correlated with the actual values
    (0.97 correlation coefficient). Note that the correlation coefficient will tell
    us whether our model moves with the actual data; however, it will not tell us
    whether we are off magnitude-wise. For that, we will use the metrics discussed
    in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating regression results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When looking to evaluate a regression model, we are interested in how much of
    the variance in the data our model is able to capture, as well as how accurate
    the predictions are. We can use a combination of metrics and visuals to assess
    the model for each of these aspects.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing residuals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whenever we work with linear regression, we should visualize our **residuals**,
    or the discrepancies between the actual values and the model's predictions; as
    we learned in [*Chapter 7*](B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146), *Financial
    Analysis – Bitcoin and the Stock Market*, they should be centered around zero
    and homoskedastic (similar variance throughout). We can use a kernel density estimate
    to assess whether the residuals are centered around zero and a scatter plot to
    see if they are homoskedastic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the utility function in `ml_utils.regression`, which will create
    these subplots for checking the residuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s look at the residuals for this linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'It looks like our predictions don''t have a pattern (left subplot), which is
    good; however, they aren''t quite centered around zero and the distribution has
    negative skew (right subplot). These negative residuals occur when the predicted
    year was longer than the actual year:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.24 – Examining the residuals'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.24_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.24 – Examining the residuals
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: If we find patterns in the residuals, our data isn't linear and the chances
    are that visualizing the residuals could help us plan our next move. This may
    mean employing strategies such as polynomial regression or log transformations
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to examining the residuals, we should calculate metrics to evaluate
    our regression model. Perhaps the most common is **R****2** (pronounced *R-squared*),
    or the **coefficient of determination**, which quantifies the proportion of variance
    in the dependent variable that we can predict from our independent variables.
    It is calculated by subtracting the ratio of the sum of squared residuals to the
    total sum of squares from 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Sigma (*Σ*) represents the sum. The average of the *y* values is denoted as
    *ȳ* (pronounced *y-bar*). The predictions are denoted with *ŷ* (pronounced *y-hat*).
  prefs: []
  type: TYPE_NORMAL
- en: 'This value will be in the range [0, 1], where higher values are better. Objects
    of the `LinearRegression` class in `scikit-learn` use R2 as their scoring method.
    Therefore, we can simply use the `score()` method to calculate it for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also get R2 from the `metrics` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: This model has a very good R2; however, keep in mind that there are many factors
    that affect the period, such as the stars and other planets, which exert a gravitational
    force on the planet in question. Despite this abstraction, our simplification
    does pretty well because the orbital period of a planet is determined in large
    part by the distance that must be traveled, which we account for by using the
    semi-major axis data.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a problem with R2, though; we can keep adding regressors, which would
    make our model more and more complex while at the same time increasing R2\. We
    need a metric that penalizes model complexity. For that, we have **adjusted R**2,
    which will only increase if the added regressor improves the model more than what
    would be expected by chance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Unfortunately, `scikit-learn` doesn''t offer this metric; however, it is very
    easy to implement ourselves. The `ml_utils.regression` module contains a function
    for calculating the adjusted R2 for us. Let''s take a look at it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Adjusted R2 will always be lower than R2\. By using the `adjusted_r2()` function,
    we can see that our adjusted R2 is slightly lower than the R2 value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, R2 (and adjusted R2) values don''t tell us anything about our
    prediction error or even whether we specified our model correctly. Think back
    to when we discussed Anscombe''s quartet in [*Chapter 1*](B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015),
    *Introduction to Data Analysis*. These four different datasets have the same summary
    statistics. They also have the same R2 when fit with a linear regression line
    (0.67), despite some of them not indicating a linear relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.25 – R2 can be misleading'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.25_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.25 – R2 can be misleading
  prefs: []
  type: TYPE_NORMAL
- en: 'Another metric offered by `scikit-learn` is the **explained variance score**,
    which tells us the percentage of the variance that is explained by our model.
    We want this as close to 1 as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that our model explains 92% of the variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: We aren't limited to looking at variance when evaluating our regression models;
    we can also look at the magnitude of the errors themselves. The remaining metrics
    we will discuss in this section all yield errors in the same unit of measurement
    we are using for prediction (Earth days here), so we can understand the meaning
    of the size of the error.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean absolute error** (**MAE**) tells us the average error our model made
    in either direction. Values range from 0 to ∞ (infinity), with smaller values
    being better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'By using the `scikit-learn` function, we can see that our MAE was 1,369 Earth
    days:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '**Root mean squared error** (**RMSE**) allows for further penalization of poor
    predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Scikit-learn provides a function for the **mean squared error** (**MSE**),
    which is the portion of the preceding equation inside the square root; therefore,
    we simply have to take the square root of the result. We would use this metric
    when large errors are undesirable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'An alternative to all these mean-based measures is the **median absolute error**,
    which is the median of the residuals. This can be used in cases where we have
    a few outliers in our residuals, and we want a more accurate description of the
    bulk of the errors. Note that this is smaller than the MAE for our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: There is also a `mean_squared_log_error()` function, which can only be used
    for non-negative values. Some of the predictions are negative, which prevents
    us from using this. Negative predictions happen when the semi-major axis is very
    small (less than 1) since that is the only portion of the regression equation
    with a positive coefficient. If the semi-major axis isn't large enough to balance
    out the rest of our equation, the prediction will be negative and, thus, automatically
    incorrect. For a complete list of regression metrics offered by `scikit-learn`,
    check out [https://scikit-learn.org/stable/modules/classes.html#regression-metrics](https://scikit-learn.org/stable/modules/classes.html#regression-metrics).
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of classification is to determine how to label data using a set of
    discrete labels. This probably sounds similar to supervised clustering; however,
    in this case, we don't care how close members of the groups are spatially. Instead,
    we concern ourselves with classifying them with the correct class label. Remember,
    in [*Chapter 8*](B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172), *Rule-Based
    Anomaly Detection*, when we classified the IP addresses as valid user or attacker?
    We didn't care how well-defined clusters of IP addresses were—we just wanted to
    find the attackers.
  prefs: []
  type: TYPE_NORMAL
- en: Just as with regression, `scikit-learn` provides many algorithms for classification
    tasks. These are spread across modules, but will usually say *Classifier* at the
    end for classification tasks, as opposed to *Regressor* for regression tasks.
    Some common methods are logistic regression, **support vector machines** (**SVMs**),
    k-NN, decision trees, and random forests; here, we will discuss logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logistic regression is a way to use linear regression to solve classification
    tasks. However, it uses the logistic sigmoid function to return probabilities
    in the range [0, 1] that can be mapped to class labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.26 – The logistic sigmoid function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.26_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.26 – The logistic sigmoid function
  prefs: []
  type: TYPE_NORMAL
- en: Let's use logistic regression to classify red wines as high or low quality and
    to classify wines as red or white based on their chemical properties. We can treat
    logistic regression as we did the linear regression in the previous section, using
    the `linear_model` module in `scikit-learn`. Just like the linear regression problem,
    we will be using a supervised method, so we have to split our data into testing
    and training sets.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: While the examples discussed in this section are both binary classification
    problems (two classes), `scikit-learn` provides support for multiclass problems
    as well. The process of building multiclass models will be nearly identical to
    the binary case but may require passing an additional parameter to let the model
    know that there are more than two classes. You will have a chance to build a multiclass
    classification model in the exercises at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting red wine quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We made the `high_quality` column back at the beginning of this chapter, but
    remember that there was a large imbalance in the number of red wines that were
    high quality. So, when we split our data, we will stratify by that column for
    a stratified random sample to make sure that both the training and testing sets
    preserve the ratio of high-quality to low-quality wines in the data (roughly 14%
    are high quality):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s make a pipeline that will first standardize all of our data and then
    build a logistic regression. We will provide the seed (`random_state=0`) for reproducibility
    and `class_weight=''balanced''` to have `scikit-learn` compute the weights of
    the classes, since we have an imbalance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: The class weights determine how much the model will be penalized for wrong predictions
    for each class. By selecting balanced weights, wrong predictions on smaller classes
    will carry more weight, where the weight will be inversely proportional to the
    frequency of the class in the data. These weights are used for regularization,
    which we will discuss more in [*Chapter 10*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217),
    *Making Better Predictions – Optimizing Models*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have our pipeline, we can fit it to the data with the `fit()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we can use our model fit on the training data to predict the red wine
    quality for the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn makes it easy to switch between models because we can count on
    them to have the same methods, such as `score()`, `fit()`, and `predict()`. In
    some cases, we also can use `predict_proba()` for probabilities or `decision_function()`
    to evaluate a point with the equation derived by the model instead of `predict()`.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on to evaluating the performance of this model, let's build another
    classification model using the full wine dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Determining wine type by chemical properties
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We want to know whether it is possible to tell red and white wine apart based
    solely on their chemical properties. To test this, we will build a second logistic
    regression model, which will predict whether a wine is red or white. First, let''s
    split our data into testing and training sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'We will once again use logistic regression in a pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will save our predictions of which kind of wine each observation
    in the test set was:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have predictions for both of our logistic regression models using
    their respective testing sets, we are ready to evaluate their performance.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating classification results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We evaluate the performance of classification models by looking at how well
    each class in the data was predicted by the model. The **positive class** is the
    class of interest to us; all other classes are considered **negative classes**.
    In our red wine classification, the positive class is high quality, while the
    negative class is low quality. Despite our problem only being a binary classification
    problem, the metrics that are discussed in this section extend to multiclass classification
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we discussed in [*Chapter 8*](B16834_08_Final_SK_ePub.xhtml#_idTextAnchor172),
    *Rule-Based Anomaly Detection*, a classification problem can be evaluated by comparing
    the predicted labels to the actual labels using a **confusion matrix**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.27 – Evaluating classification results with a confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.27_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.27 – Evaluating classification results with a confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'Each prediction can be one of four outcomes, based on how it matches up to
    the actual value:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positive (TP)**: Correctly predicted to be the positive class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive (FP)**: Incorrectly predicted to be the positive class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negative (TN)**: Correctly predicted to not be the positive class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negative (FN)**: Incorrectly predicted to not be the positive class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: False positives are also referred to as **type I errors**, while false negatives
    are **type II errors**. Given a certain classifier, an effort to reduce one will
    cause an increase in the other.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Scikit-learn provides the `confusion_matrix()` function, which we can pair
    with the `heatmap()` function from `seaborn` to visualize our confusion matrix.
    In the `ml_utils.classification` module, the `confusion_matrix_visual()` function
    handles this for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s call our confusion matrix visualization function to see how we did for
    each of our classification models. First, we will look at how well the model identified
    high-quality red wines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the confusion matrix, we can see that the model had trouble finding the
    high-quality red wines consistently (bottom row):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.28 – Results for the red wine quality model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.28_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.28 – Results for the red wine quality model
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at how well the `white_or_red` model predicted the wine type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'It looks like this model had a much easier time, with very few incorrect predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.29 – Results for the white or red wine model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.29_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.29 – Results for the white or red wine model
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the composition of the confusion matrix, we can use it
    to calculate additional performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Classification metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using the values in the confusion matrix, we can calculate metrics to help evaluate
    the performance of a classifier. The best metrics will depend on the goal for
    which we are building the model and whether our classes are balanced. The formulas
    in this section are derived from the data we get from the confusion matrix, where
    *TP* is the number of true positives, *TN* is the number of true negatives, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and error rate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When our classes are roughly equal in size, we can use **accuracy**, which
    will give us the percentage of correctly classified values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `accuracy_score()` function in `sklearn.metrics` will calculate the accuracy
    as per the formula; however, the `score()` method of our model will also give
    us the accuracy (this isn''t always the case, as we will see with grid search
    in [*Chapter 10*](B16834_10_Final_SK_ePub.xhtml#_idTextAnchor217), *Making Better
    Predictions – Optimizing Models*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Since accuracy is the percentage we correctly classified (our **success rate**),
    it follows that our **error rate** (the percentage we got wrong) can be calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Our accuracy score tells us that we got 77.5% of the red wines correctly classified
    according to their quality. Conversely, the `zero_one_loss()` function gives us
    the percentage of values that were misclassified, which is 22.5% for the red wine
    quality model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Note that while both of these are easy to compute and understand, they require
    a threshold. By default, this is 50%, but we can use any probability we wish as
    a cutoff when predicting the class using the `predict_proba()` method in `scikit-learn`.
    In addition, accuracy and error rate can be misleading in cases of class imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: Precision and recall
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When we have a **class imbalance**, accuracy can become an unreliable metric
    for measuring our performance. For instance, if we had a 99/1 split between two
    classes, A and B, where the rare event, B, is our positive class, we could build
    a model that was 99% accurate by just saying everything belonged to class A. This
    problem stems from the fact that true negatives will be very large, and being
    in the numerator (in addition to the denominator), they will make the results
    look better than they are. Clearly, we shouldn''t bother building a model if it
    doesn''t do anything to identify class B; thus, we need different metrics that
    will discourage this behavior. For this, we use precision and recall instead of
    accuracy. **Precision** is the ratio of true positives to everything flagged positive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Recall** gives us the **true positive rate** (**TPR**), which is the ratio
    of true positives to everything that was actually positive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the case of the 99/1 split between classes A and B, the model that classifies
    everything as A would have a recall of 0% for the positive class, B (precision
    would be undefined—0/0). Precision and recall provide a better way of evaluating
    model performance in the face of a class imbalance. They will correctly tell us
    that the model has little value for our use case.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn provides the `classification_report()` function, which will calculate
    precision and recall for us. In addition to calculating these metrics per class
    label, it also calculates the **macro** average (unweighted average between classes)
    and the **weighted** average (average between classes weighted by the number of
    observations in each class). The **support** column indicates the count of observations
    that belong to each class using the labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The classification report indicates that our model does well at finding the
    low-quality red wines, but not so great with the high-quality red wines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Given that the quality scores are very subjective and not necessarily related
    to the chemical properties, it is no surprise that this simple model doesn''t
    perform too well. On the other hand, chemical properties are different between
    red and white wines, so this information is more useful for the `white_or_red`
    model. As we can imagine, based on the confusion matrix for the `white_or_red`
    model, the metrics are good:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like accuracy, both precision and recall are easy to compute and understand,
    but require thresholds. In addition, precision and recall each only consider half
    of the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.30 – Confusion matrix coverage for precision and recall'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.30_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.30 – Confusion matrix coverage for precision and recall
  prefs: []
  type: TYPE_NORMAL
- en: There is typically a trade-off between maximizing recall and maximizing precision,
    and we have to decide which is more important to us. This preference can be quantified
    using the F score.
  prefs: []
  type: TYPE_NORMAL
- en: F score
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The classification report also includes the **F**1 **score**, which helps us
    balance precision and recall using the **harmonic mean** of the two:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The harmonic mean is the reciprocal of the arithmetic mean, and is used with
    rates to get a more accurate average (compared to the arithmetic mean of the rates).
    Both precision and recall are proportions in the range [0, 1], which we can treat
    as rates.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **F**β **score**, pronounced *F-beta*, is the more general formulation
    for the F score. By varying β, we can put more weight on precision (β between
    0 and 1) or on recall (β greater than 1), where β is how many more times recall
    is valued over precision:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Some commonly used values for β are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**F**0.5 **score**: Precision twice as important as recall'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F**1 **score**: Harmonic mean (equal importance)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F**2 **score**: Recall twice as important as precision'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The F score is also easy to compute and relies on thresholds. However, it doesn't
    consider true negatives and is hard to optimize due to the trade-offs between
    precision and recall. Note that when working with large class imbalances, we are
    typically more concerned with predicting the positive class correctly, meaning
    that we may be less interested in true negatives, so using a metric that ignores
    them isn't necessarily an issue.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Functions for precision, recall, F1 score, and Fβ score can be found in the
    `sklearn.metrics` module.
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity and specificity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Along the lines of the precision and recall trade-off, we have another pair
    of metrics that can be used to illustrate the delicate balance we strive to achieve
    with classification problems: sensitivity and specificity.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensitivity** is the true positive rate, or recall, which we saw previously.
    **Specificity**, however, is the **true negative rate**, or the proportion of
    true negatives to everything that should have been classified as negative:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that, together, specificity and sensitivity consider the full confusion
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.31 – Confusion matrix coverage for sensitivity and specificity'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.31_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.31 – Confusion matrix coverage for sensitivity and specificity
  prefs: []
  type: TYPE_NORMAL
- en: We would like to maximize both sensitivity and specificity; however, we could
    easily maximize specificity by decreasing the number of times we classify something
    as the positive class, which would decrease sensitivity. Scikit-learn doesn't
    offer specificity as a metric—preferring precision and recall—however, we can
    easily make our own by writing a function or using the `make_scorer()` function
    from `scikit-learn`. We are discussing them here because they form the basis of
    the sensitivity-specificity plot, or ROC curve, which is the topic of the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: ROC curve
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to using metrics to evaluate classification problems, we can turn
    to visualizations. By plotting the true positive rate (*sensitivity*) versus the
    false positive rate (*1 - specificity*), we get the `predict_proba()` method in
    `scikit-learn`. Say that we find the threshold to be 60%—we would require `predict_proba()`
    to return a value greater than or equal to 0.6 to predict the positive class (`predict()`
    uses 0.5 as the cutoff).
  prefs: []
  type: TYPE_NORMAL
- en: The `roc_curve()` function from `scikit-learn` calculates the false and true
    positive rates at thresholds from 0 to 100% using the probabilities of an observation
    belonging to a given class, as determined by the model. We can then plot this,
    with the goal being to maximize the **area under the curve** (**AUC**), which
    is in the range [0, 1]; values below 0.5 are worse than guessing and good scores
    are above 0.8\. Note that when referring to the area under a ROC curve, the AUC
    may also be written as **AUROC**. The AUROC summarizes the model's performance
    across thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are examples of good ROC curves. The dashed line would be random
    guessing (no predictive value) and is used as a baseline; anything below that
    is considered worse than guessing. We want to be toward the top-left corner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.32 – Comparing ROC curves'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.32_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.32 – Comparing ROC curves
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ml_utils.classification` module contains a function for plotting our ROC
    curve. Let''s take a look at it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: As we can imagine, our `white_or_red` model will have a very good ROC curve.
    Let's see what that looks like by calling the `plot_roc()` function. Since we
    need to pass the probabilities of each entry belonging to the positive class,
    we need to use the `predict_proba()` method instead of `predict()`. This gives
    us the probabilities that each observation belongs to each class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, for every row in `w_X_test`, we have a NumPy array of `[P(white), P(red)]`.
    Therefore, we use slicing to select the probabilities that the wine is red for
    the ROC curve (`[:,1]`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Just as we expected, the ROC curve for the `white_or_red` model is very good,
    with an AUC of nearly 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.33 – ROC curve for the white or red wine model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.33_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.33 – ROC curve for the white or red wine model
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the other metrics we have looked at, we don''t expect the red wine quality
    prediction model to have a great ROC curve. Let''s call our function to see what
    the ROC curve for the red wine quality model looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'This ROC curve isn''t as good as the previous one, as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.34 – ROC curve for the red wine quality model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.34_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.34 – ROC curve for the red wine quality model
  prefs: []
  type: TYPE_NORMAL
- en: Our AUROC is 0.85; however, note that the AUROC provides optimistic estimates
    under class imbalance (since it considers true negatives). For this reason, we
    should also look at the precision-recall curve.
  prefs: []
  type: TYPE_NORMAL
- en: Precision-recall curve
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When faced with a class imbalance, we use **precision-recall curves** instead
    of ROC curves. This curve shows precision versus recall at various probability
    thresholds. The baseline is a horizontal line at the percentage of the data that
    belongs to the positive class. We want our curve above this line, with an **area
    under the precision-recall curve** (**AUPR**) greater than that percentage (the
    higher the better).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ml_utils.classification` module contains the `plot_pr_curve()` function
    for drawing precision-recall curves and providing the AUPR:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the implementation of the AUC calculation in `scikit-learn` uses interpolation,
    it may give an optimistic result, so our function also calculates **average precision**
    (**AP**), which summarizes the precision-recall curve as the weighted mean of
    the precision scores (*P*n) achieved at various thresholds. The weights are derived
    from the change in recall (*R*n) between one threshold and the next. Values are
    between 0 and 1, with higher values being better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_09_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a look at the precision-recall curve for the red wine quality model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'This still shows that our model is better than the baseline of random guessing;
    however, the performance reading we get here seems more in line with the lackluster
    performance we saw in the classification report. We can also see that the model
    loses lots of precision when going from a recall of 0.2 to 0.4\. Here, the trade-off
    between precision and recall is evident, and we will likely choose to optimize
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.35 – Precision-recall curve for the red wine quality model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.35_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.35 – Precision-recall curve for the red wine quality model
  prefs: []
  type: TYPE_NORMAL
- en: Since we have a class imbalance between the high-quality and low-quality red
    wines (less than 14% are high quality), we must make a choice as to whether we
    optimize precision or recall. Our choice would depend on who we work for in the
    wine industry. If we are renowned for producing high-quality wine, and we are
    choosing which wines to provide to critics for reviews, we want to make sure we
    pick the best ones and would rather miss out on good ones (false negatives) than
    tarnish our name with low-quality ones that the model classifies as high quality
    (false positives). However, if we are trying to make the best profit from selling
    the wines, we wouldn't want to sell such a high-quality wine for the same price
    as a low-quality wine (false negative), so we would rather overprice some low-quality
    wines (false positives).
  prefs: []
  type: TYPE_NORMAL
- en: Note that we could easily have classified everything as low quality to never
    disappoint or as high quality to maximize our profit selling them; however, this
    isn't too practical. It's clear that we need to strike an acceptable balance between
    false positives and false negatives. To do so, we need to quantify this trade-off
    between the two extremes in terms of what matters to us more. Then, we can use
    the precision-recall curve to find a threshold that meets our precision and recall
    targets. In [*Chapter 11*](B16834_11_Final_SK_ePub.xhtml#_idTextAnchor237), *Machine
    Learning Anomaly Detection*, we will work through an example of this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now take a look at the precision-recall curve for our white or red wine
    classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that this curve is in the upper right-hand corner. With this model, we
    can achieve high precision and high recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.36 – Precision-recall curve for the white or red wine model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_9.36_B16834.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.36 – Precision-recall curve for the white or red wine model
  prefs: []
  type: TYPE_NORMAL
- en: As we saw with the red wine quality model, AUPR works very well with class imbalance.
    However, it can't be compared across datasets, is expensive to compute, and is
    hard to optimize. Note that this was just a subset of the metrics we can use to
    evaluate classification problems. All the classification metrics offered by `scikit-learn`
    can be found at [https://scikit-learn.org/stable/modules/classes.html#classification-metrics](https://scikit-learn.org/stable/modules/classes.html#classification-metrics).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter served as an introduction to machine learning in Python. We discussed
    the terminology that's commonly used to describe learning types and tasks. Then,
    we practiced EDA using the skills we learned throughout this book to get a feel
    for the wine and planet datasets. This gave us some ideas about what kinds of
    models we would want to build. A thorough exploration of the data is essential
    before attempting to build a model.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we learned how to prepare our data for use in machine learning models
    and the importance of splitting the data into training and testing sets before
    modeling. In order to prepare our data efficiently, we used pipelines in `scikit-learn`
    to package up everything from our preprocessing through our model.
  prefs: []
  type: TYPE_NORMAL
- en: We used unsupervised k-means to cluster the planets using their semi-major axis
    and period; we also discussed how to use the elbow point method to find a good
    value for *k*. Then, we moved on to supervised learning and made a linear regression
    model to predict the period of a planet using its semi-major axis, eccentricity
    of orbit, and mass. We learned how to interpret the model coefficients and how
    to evaluate the model's predictions. Finally, we turned to classification to identify
    high-quality red wines (which had a class imbalance) and distinguish between red
    and white wine by their chemical properties. Using precision, recall, F1 score,
    confusion matrices, ROC curves, and precision-recall curves, we discussed how
    to evaluate classification models.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to remember that machine learning models make assumptions about
    the underlying data, and while this wasn't a chapter on the mathematics of machine
    learning, we should make sure that we understand that there are consequences for
    violating these assumptions. In practice, when looking to build models, it's crucial
    that we have a solid understanding of statistics and domain-level expertise. We
    saw that there is a multitude of metrics for evaluating our models. Each metric
    has its strengths and weaknesses, and, depending on the problem, some are better
    than others; we must take care to choose the appropriate metrics for the task
    at hand.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to tune our models to improve their performance,
    so make sure to complete the exercises to practice this chapter's material before
    moving on.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Practice building and evaluating machine learning models in `scikit-learn`
    with the following exercises:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a clustering model to distinguish between red and white wine by their
    chemical properties:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Combine the red and white wine datasets (`data/winequality-red.csv` and `data/winequality-white.csv`,
    respectively) and add a column for the kind of wine (red or white).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Perform some initial EDA.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Build and fit a pipeline that scales the data and then uses k-means clustering
    to make two clusters. Be sure not to use the `quality` column.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Use the Fowlkes-Mallows Index (the `fowlkes_mallows_score()` function is
    in `sklearn.metrics`) to evaluate how well k-means is able to make the distinction
    between red and white wine.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e) Find the center of each cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Predict star temperature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Using the `data/stars.csv` file, perform some initial EDA and then build
    a linear regression model of all the numeric columns to predict the temperature
    of the star.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Train the model on 75% of the initial data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Calculate the R2 and RMSE of the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Find the coefficients for each regressor and the intercept of the linear
    regression equation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e) Visualize the residuals using the `plot_residuals()` function from the `ml_utils.regression`
    module.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Classify planets that have shorter years than Earth:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Using the `data/planets.csv` file, build a logistic regression model with
    the `eccentricity`, `semimajoraxis`, and `mass` columns as regressors. You will
    need to make a new column to use for the *y* (year shorter than Earth).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Find the accuracy score.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Use the `classification_report()` function from `scikit-learn` to see the
    precision, recall, and F1 score for each class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) With the `plot_roc()` function from the `ml_utils.classification` module,
    plot the ROC curve.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e) Create a confusion matrix using the `confusion_matrix_visual()` function
    from the `ml_utils.classification` module.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Multiclass classification of white wine quality:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) Using the `data/winequality-white.csv` file, perform some initial EDA on
    the white wine data. Be sure to look at how many wines had a given quality score.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Build a pipeline to standardize the data and fit a multiclass logistic regression
    model. Pass `multi_class='multinomial'` and `max_iter=1000` to the `LogisticRegression`
    constructor.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Look at the classification report for your model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Create a confusion matrix using the `confusion_matrix_visual()` function
    from the `ml_utils.classification` module. This will work as is for multiclass
    classification problems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e) Extend the `plot_roc()` function to work for multiple class labels. To do
    so, you will need to create a ROC curve for each class label (which are quality
    scores here), where a true positive is correctly predicting that quality score
    and a false positive is predicting any other quality score. Note that `ml_utils`
    has a function for this, but try to build your own implementation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: f) Extend the `plot_pr_curve()` function to work for multiple class labels by
    following a similar method to part *e)*. However, give each class its own subplot.
    Note that `ml_utils` has a function for this, but try to build your own implementation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We have seen how easy the `scikit-learn` API is to navigate, making it a cinch
    to change which algorithm we are using for our model. Rebuild the red wine quality
    model that we created in this chapter using an SVM instead of logistic regression.
    We haven''t discussed this model, but you should still be able to use it in `scikit-learn`.
    Check out the link in the *Further reading* section to learn more about the algorithm.
    Some guidance for this exercise is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) You will need to use the `SVC` (support vector classifier) class from `scikit-learn`,
    which can be found at [https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Use `C=5` as an argument to the `SVC` constructor.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Pass `probability=True` to the `SVC` constructor to be able to use the `predict_proba()`
    method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Build a pipeline first using the `StandardScaler` class and then the `SVC`
    class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e) Be sure to look at the classification report, precision-recall curve, and
    confusion matrix for the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check out the following resources for more information on the topics that were
    covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A Beginner''s Guide to Deep Reinforcement Learning*: [https://pathmind.com/wiki/deep-reinforcement-learning](https://pathmind.com/wiki/deep-reinforcement-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*An Introduction to Gradient Descent and Linear Regression*: [https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/](https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Assumptions of Multiple Linear Regression*: [https://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/](https://www.statisticssolutions.com/assumptions-of-multiple-linear-regression/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Clustering*: [https://scikit-learn.org/stable/modules/clustering.html](https://scikit-learn.org/stable/modules/clustering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Generalized Linear Models*: [https://scikit-learn.org/stable/modules/linear_model.html](https://scikit-learn.org/stable/modules/linear_model.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Guide to Interpretable Machine Learning – Techniques to dispel the black box
    myth of deep learning*: [https://towardsdatascience.com/guide-to-interpretable-machine-learning-d40e8a64b6cf](https://towardsdatascience.com/guide-to-interpretable-machine-learning-d40e8a64b6cf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*In Depth: k-Means*: [https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html](https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Interpretable Machine Learning – A Guide for Making Black Box Models Explainable*:
    [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Interpretable Machine Learning – Extracting human understandable insights
    from any Machine Learning model*: [https://towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b](https://towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MAE and RMSE – Which Metric is Better?*: [https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model evaluation: quantifying the quality of predictions*: [https://scikit-learn.org/stable/modules/model_evaluation.html](https://scikit-learn.org/stable/modules/model_evaluation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Preprocessing data*: [https://scikit-learn.org/stable/modules/preprocessing.html](https://scikit-learn.org/stable/modules/preprocessing.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scikit-learn Glossary of Common Terms and API Elements*: [https://scikit-learn.org/stable/glossary.html#glossary](https://scikit-learn.org/stable/glossary.html#glossary)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Scikit-learn User Guide*: [https://scikit-learn.org/stable/user_guide.html](https://scikit-learn.org/stable/user_guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Seeing Theory* [*Chapter 6*](B16834_06_Final_SK_ePub.xhtml#_idTextAnchor125)*:
    Regression Analysis*: [https://seeing-theory.brown.edu/index.html#secondPage/chapter6](https://seeing-theory.brown.edu/index.html#secondPage/chapter6)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Simple Beginner''s Guide to Reinforcement Learning & its implementation*:
    [https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/](https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Support Vector Machine – Introduction to Machine Learning Algorithms*: [https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The 5 Clustering Algorithms Data Scientists Need to Know*: [https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
