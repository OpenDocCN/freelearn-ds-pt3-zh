<html><head></head><body>
  <div id="_idContainer057" class="Basic-Text-Frame">
    <h1 class="chapterNumber">2</h1>
    <h1 id="_idParaDest-39" class="chapterTitle">Acquiring and Processing Time Series Data</h1>
    <p class="normal">In the previous chapter, we learned what a time series is and established some standard notation and terminology. Now, let’s switch tracks from theory to practice. In this chapter, we are going to get our hands dirty and start working with data. Although we said time series data is everywhere, we are yet to start working with a few time series datasets. We are going to start working on the dataset we will use throughout this book, process it in the right way, and learn about a few techniques to deal with missing values.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Understanding the time series dataset</li>
      <li class="bulletList">pandas datetime operations, indexing, and slicing—a refresher</li>
      <li class="bulletList">Handling missing data</li>
      <li class="bulletList">Mapping additional information</li>
      <li class="bulletList">Saving and loading files to disk</li>
      <li class="bulletList">Handling longer periods of missing data</li>
    </ul>
    <h1 id="_idParaDest-40" class="heading-1">Technical requirements</h1>
    <p class="normal">You will need to set up the <strong class="keyWord">Anaconda</strong> environment, following the instructions in the <em class="italic">Preface</em> of the book, to get a working environment with all the libraries and datasets required for the code in this book. Any additional library will be installed while running the notebooks.</p>
    <p class="normal">The code for this chapter can be found at <a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter02"><span class="url">https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python-2E/tree/main/notebooks/Chapter02</span></a>.</p>
    <p class="normal">Handling time series data is like handling other tabular datasets, only with a focus on the temporal dimension. As with any tabular dataset, <code class="inlineCode">pandas</code> is perfectly equipped to handle time series data as well.</p>
    <p class="normal">Let’s start getting our hands dirty and work through a dataset from the beginning. We are going to use the <em class="italic">London Smart Meters</em> dataset throughout this book. If you have not downloaded the data already as part of the environment setup, go to the <em class="italic">Preface</em> and do that now.</p>
    <h1 id="_idParaDest-41" class="heading-1">Understanding the time series dataset</h1>
    <p class="normal">This is the key first step in any new<a id="_idIndexMarker059"/> dataset you come across, even before <strong class="keyWord">Exploratory Data Analysis</strong> (<strong class="keyWord">EDA</strong>), which we will <a id="_idIndexMarker060"/>cover in <em class="chapterRef">Chapter 3</em>, <em class="italic">Analyzing and Visualizing Time Series Data</em>. Understanding where the data comes from, the data generating process behind it, and the source domain is essential to having a good understanding of the dataset.</p>
    <p class="normal">London Data Store, a free and open data-sharing portal, provided this dataset, which was collected and enriched by Jean-Michel D and uploaded on Kaggle.</p>
    <p class="normal">The dataset contains energy consumption readings for a sample of 5,567 London households that took part in the UK Power Networks-led Low Carbon London project between November 2011 and February 2014. Readings were taken at half-hourly intervals. Some metadata about the households is also available as part of the dataset. Let’s look at what metadata is available as part of the dataset:</p>
    <ul>
      <li class="bulletList">CACI UK segmented the UK’s population into demographic types, called Acorn. For each household in the data, we have the corresponding Acorn classification. The Acorn classes (Lavish Lifestyles, City Sophisticates, Student Life, and so on) are grouped into parent classes (Affluent Achievers, Rising Prosperity, Financially Stretched, and so on). A full list of Acorn classes can be found in <em class="italic">Table 2.1</em>. The complete documentation detailing each class is available at <a href="https://acorn.caci.co.uk/downloads/Acorn-User-guide.pdf"><span class="url">https://acorn.caci.co.uk/downloads/Acorn-User-guide.pdf</span></a>.</li>
      <li class="bulletList">The dataset contains two groups <a id="_idIndexMarker061"/>of customers—one group who was subjected to <strong class="keyWord">dynamic time-of-use</strong> (<strong class="keyWord">dToU</strong>) energy prices throughout 2013, and another group who were on flat-rate tariffs. The tariff prices for the dToU were given a day ahead, via the smart meter IHD or text message.</li>
      <li class="bulletList">Jean-Michel D also enriched<a id="_idIndexMarker062"/> the dataset with weather and UK bank holiday data.</li>
    </ul>
    <p class="normal">The following table shows the Acorn classes:</p>
    <table id="table001" class="table-container">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Acorn Group</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Acorn Class</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell" rowspan="3">
            <p class="normal">Affluent Achievers</p>
          </td>
          <td class="table-cell">
            <p class="normal">A-Lavish Lifestyles</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">B-Executive Wealth</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">C-Mature <a id="_idIndexMarker063"/>Money</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell" rowspan="2">
            <p class="normal">Rising Prosperity</p>
          </td>
          <td class="table-cell">
            <p class="normal">D-City Sophisticates</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">E-Career Climbers</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell" rowspan="5">
            <p class="normal">Comfortable Communities</p>
          </td>
          <td class="table-cell">
            <p class="normal">F-Countryside Communities</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">G-Successful Suburbs</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">H-Steady Neighborhoods</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">I-Comfortable Seniors</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">J-Starting Out</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell" rowspan="4">
            <p class="normal">Financially Stretched</p>
          </td>
          <td class="table-cell">
            <p class="normal">K-Student Life</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">L-Modest Means</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">M-Striving Families</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">N-Poorer Pensioners</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell" rowspan="3">
            <p class="normal">Urban Adversity</p>
          </td>
          <td class="table-cell">
            <p class="normal">O-Young Hardship</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">P-Struggling Estates</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Q-Difficult Circumstances</p>
          </td>
        </tr>
      </tbody>
    </table>
    <figure class="mediaobject">Table 2.1: Acorn classification</figure>
    <div class="note">
      <p class="normal">The Kaggle dataset <a id="_idIndexMarker064"/>also preprocesses the time series data daily and combines all the separate files. Here, we will ignore those files and start with the raw files, which can be found in the <code class="inlineCode">hhblock_dataset</code> folder. Learning to work with raw files is an integral part of working with real-world datasets in the industry.</p>
    </div>
    <h1 id="_idParaDest-42" class="heading-1">Preparing a data model</h1>
    <p class="normal">Once we understand where the data comes from, we can look at it, understand the information present in the different files, and figure out a mental model of how to relate the different files. You <a id="_idIndexMarker065"/>may call it old-school, but Microsoft Excel is an excellent tool for gaining this first-level understanding. If the file is too big to open in Excel, we can also read it in Python, save a sample of the data to an Excel file, and open it. However, keep in mind that Excel sometimes messes with the format of the data, especially dates, so we need to take care to not save the file and write back the formatting changes Excel made. If you are allergic to Excel, you can do it in Python as well, albeit with a lot more keystrokes. The purpose of this exercise is to see what the different data files contain, explore the relationship between the different files, and so on. </p>
    <p class="normal">We can make this more formal and explicit by drawing a data model, similar to the one shown in the following diagram:</p>
    <figure class="mediaobject"><img src="../Images/B22389_02_01.png" alt="Figure 2.1 – Data model of the London Smart Meters dataset "/></figure>
    <p class="packt_figref">Figure 2.1: Data model of the London Smart Meters dataset</p>
    <p class="normal">The data <a id="_idIndexMarker066"/>model is more for us to understand the data rather than any data engineering purpose. Therefore, it only contains bare-minimum information, such as the key columns on the left and the sample data on the right. We also have arrows connecting different files, with keys used to link the files.</p>
    <p class="normal">Let’s look at a few key column names and their meanings:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">LCLid</code>: The unique consumer ID for a household</li>
      <li class="bulletList"><code class="inlineCode">stdorTou</code>: Whether the household has dToU or standard tarriff</li>
      <li class="bulletList"><code class="inlineCode">Acorn</code>: The ACORN class</li>
      <li class="bulletList"><code class="inlineCode">Acorn_grouped</code>: The ACORN group</li>
      <li class="bulletList"><code class="inlineCode">file</code>: The block number</li>
    </ul>
    <p class="normal">Each <code class="inlineCode">LCLid</code> has a unique time series attached to it. The time series file is formatted in a slightly tricky format—each day, there will be 48 observations at a half-hourly frequency in the columns of the file.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">To follow along with the complete code, use the <code class="inlineCode">01-Pandas_Refresher_&amp;_Missing_Values_Treatment.ipynb</code> notebook in the <code class="inlineCode">Chapter01</code> folder.</p>
    </div>
    <p class="normal">Before we start working with our dataset, there are a few concepts we need to establish. One of them is a concept in pandas DataFrames, which is of utmost importance—the pandas <a id="_idIndexMarker067"/>datetime properties and index. Let’s quickly look at a few <code class="inlineCode">pandas</code> concepts that will be useful.</p>
    <div class="note">
      <p class="normal">If you are familiar with the datetime manipulations in pandas, feel free to skip ahead to the next section.</p>
    </div>
    <h1 id="_idParaDest-43" class="heading-1">pandas datetime operations, indexing, and slicing—a refresher</h1>
    <p class="normal">Instead of using our dataset, which is slightly complex, let’s pick an easy, well-formatted stock exchange price dataset from the UCI Machine Learning Repository and look at the functionality of pandas:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Skipping first row cause it doesn't have any data</span>
df = pd.read_excel(<span class="hljs-string">"https://archive.ics.uci.edu/ml/machine-learning-databases/00247/data_akbilgic.xlsx"</span>, skiprows=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">The DataFrame that we read<a id="_idIndexMarker068"/> looks as follows:</p>
    <figure class="mediaobject"><img src="../Images/B22389_02_02.png" alt="Figure 2.2 – The DataFrame with stock exchange prices "/></figure>
    <p class="packt_figref">Figure 2.2: The DataFrame with stock exchange prices</p>
    <p class="normal">Now that we have read the <code class="inlineCode">DataFrame</code>, let’s start manipulating it.</p>
    <h2 id="_idParaDest-44" class="heading-2">Converting the date columns into pd.Timestamp/DatetimeIndex</h2>
    <p class="normal">First, we must convert the date column (which may not always be parsed as dates automatically by pandas) into <code class="inlineCode">pandas</code> datetime format. For that, <code class="inlineCode">pandas</code> has a handy function called <code class="inlineCode">pd.to_datetime</code>. It infers the<a id="_idIndexMarker069"/> datetime format automatically and converts the input into a <code class="inlineCode">pd.Timestamp</code>, if the input is a <code class="inlineCode">string</code>, or into a <code class="inlineCode">DatetimeIndex</code>, if the input is a <code class="inlineCode">list</code> of <a id="_idIndexMarker070"/>strings. So if we pass a single date as a string, <code class="inlineCode">pd.to_datetime</code> converts it into <code class="inlineCode">pd.Timestamp</code>, while if we pass a list of dates, it converts it into <code class="inlineCode">DatetimeIndex</code>. Let’s also use a handy function, <code class="inlineCode">strftime</code>, which formats the date representation into a format we specify. It uses <code class="inlineCode">strftime</code> conventions to specify the format of the data. For instance, <code class="inlineCode">%d</code> means a zero-padded date, <code class="inlineCode">%B</code> means a month’s full name, and <code class="inlineCode">%Y</code> means a <a id="_idIndexMarker071"/>year in four digits. A full list of <code class="inlineCode">strftime</code> conventions can be found at <a href="https://strftime.org/"><span class="url">https://strftime.org/</span></a>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;</span>&gt;&gt; pd.to_datetime(<span class="hljs-con-string">"13-4-1987"</span>).strftime(<span class="hljs-con-string">"%d, %B %Y"</span>)
'13, April 1987'
</code></pre>
    <p class="normal">Now, let’s look at a case where the automatic parsing fails. The date is January 4, 1987. Let’s see what happens when we pass the string to the function:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;</span>&gt;&gt; pd.to_datetime(<span class="hljs-con-string">"4-1-1987"</span>).strftime(<span class="hljs-con-string">"%d, %B %Y"</span>)
'01, April 1987'
</code></pre>
    <p class="normal">Well, that wasn’t expected, right? But if you think about it, anyone can make that mistake because we are not telling the computer whether the month or the day comes first, and pandas assumes the month comes first. Let’s rectify that:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;</span>&gt;&gt; pd.to_datetime(<span class="hljs-con-string">"4-1-1987"</span>, dayfirst=True).strftime(<span class="hljs-con-string">"%d, %B %Y"</span>)
'04, January 1987'
</code></pre>
    <p class="normal">Another case where automatic date parsing fails is when the date string is in a non-standard form. In that case, we can provide a <code class="inlineCode">strftime</code>-formatted string to help pandas parse the dates correctly:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;</span>&gt;&gt; pd.to_datetime(<span class="hljs-con-string">"4|1|1987"</span>, format=<span class="hljs-con-string">"%d|%m|%Y"</span>).strftime(<span class="hljs-con-string">"%d, %B %Y"</span>)
'04, January 1987'
</code></pre>
    <p class="normal">A full list of <code class="inlineCode">strftime</code> conventions<a id="_idIndexMarker072"/> can be found at <a href="https://strftime.org/"><span class="url">https://strftime.org/</span></a>.</p>
    <div class="packt_tip">
      <p class="normal"><strong class="keyWord">Practioner’s tip</strong>:</p>
      <p class="normal">Because of the wide variety of data formats, pandas may infer the time incorrectly. While reading a file, pandas will try to parse dates automatically and create an error. There are many ways we can control this behavior: we can use the <code class="inlineCode">parse_dates</code> flag to turn off date parsing, the <code class="inlineCode">date_parser</code> argument to pass in a custom date parser, and <code class="inlineCode">year_first</code> and <code class="inlineCode">day_first</code> to easily denote two popular formats of dates. From version 2.0, pandas supports <code class="inlineCode">date_format</code>, which can be used to pass in the exact format of the date as a Python dictionary, with the column name as the key.</p>
      <p class="normal">Out of all these options, I prefer to use <code class="inlineCode">date_format</code>, if using <code class="inlineCode">pandas</code> &gt;=2.0. We can keep <code class="inlineCode">parse_dates=True</code> and then pass in the exact date format, using <code class="inlineCode">strftime</code> conventions. This ensures that the date is parsed in the way we want it to be.</p>
      <p class="normal">If working with <code class="inlineCode">pandas</code> &lt;2.0, then I prefer to keep <code class="inlineCode">parse_dates=False</code> in both <code class="inlineCode">pd.read_csv</code> and <code class="inlineCode">pd.read_excel</code> to make sure that pandas does not parse the data automatically. After that, you can convert the date using the <code class="inlineCode">format</code> parameter, which lets you explicitly set the date format of the column using <code class="inlineCode">strftime</code> conventions. There are two other parameters in <code class="inlineCode">pd.to_datetime</code> that will also make inferring dates less error-prone—<code class="inlineCode">yearfirst</code> and <code class="inlineCode">dayfirst</code>. If you don’t provide an explicit date format, at least provide one of these.</p>
    </div>
    <p class="normal">Now, let’s convert the date<a id="_idIndexMarker073"/> column in our stock prices dataset into datetime:</p>
    <pre class="programlisting code"><code class="hljs-code">df[<span class="hljs-string">'date'</span>] = pd.to_datetime(df[<span class="hljs-string">'date'</span>], yearfirst=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">Now, the <code class="inlineCode">'date'</code> column, <code class="inlineCode">dtype</code>, should be either <code class="inlineCode">datetime64[ns]</code> or <code class="inlineCode">&lt;M8[ns]</code>, which are both pandas/NumPy-native <a id="_idIndexMarker074"/>datetime formats. But why do we need to do this?</p>
    <p class="normal">It’s because of the wide range of additional functionalities this unlocks. The traditional <code class="inlineCode">min()</code> and <code class="inlineCode">max()</code> functions will start working because pandas knows it is a datetime column:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;</span>&gt;&gt; df.date.min(),df.date.max()
(Timestamp('2009-01-05 00:00:00'), Timestamp('2011-02-22 00:00:00'))
</code></pre>
    <p class="normal">Let’s look at a few cool features that the datetime format gives us.</p>
    <h2 id="_idParaDest-45" class="heading-2">Using the .dt accessor and datetime properties</h2>
    <p class="normal">Since the column is now in date <a id="_idIndexMarker075"/>format, all the semantic information that is encoded in the date can be used through pandas datetime<a id="_idIndexMarker076"/> properties. We can <a id="_idIndexMarker077"/>access many <a id="_idIndexMarker078"/>datetime properties, such as <code class="inlineCode">month</code>, <code class="inlineCode">day_of_week</code>, <code class="inlineCode">day_of_year</code>, and so on, using the <code class="inlineCode">.dt</code> accessor:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;</span>&gt;&gt; <span class="hljs-con-built_in">print</span>(f<span class="hljs-con-string">"""</span>
     Date: {df.date.iloc[0]}
     Day of year: {df.date.dt.day_of_year.iloc[0]}
     Day of week: {df.date.dt.dayofweek.iloc[0]}
     Month: {df.date.dt.month.iloc[0]}
     Month Name: {df.date.dt.month_name().iloc[0]}
     Quarter: {df.date.dt.quarter.iloc[0]}
     Year: {df.date.dt.year.iloc[0]}
     ISO Week: {df.date.dt.isocalendar().week.iloc[0]}
     """)
Date: 2009-01-05 00:00:00
Day of year: 5
Day of week: 0
Month: 1
Month Name: January
Quarter: 1
Year: 2009
ISO Week: 2
</code></pre>
    <p class="normal">As of pandas 1.1.0, <code class="inlineCode">week_of_year</code> has been deprecated because of the inconsistencies it produces at the end/start of the year. Instead, the ISO calendar standards (which are commonly used in government and business) have been adopted, and we can access the ISO calendar to get the ISO weeks.</p>
    <h2 id="_idParaDest-46" class="heading-2">Indexing and slicing</h2>
    <p class="normal">The real fun <a id="_idIndexMarker079"/>starts when we make the<a id="_idIndexMarker080"/> date column the index of the DataFrame. By doing this, you can use all<a id="_idIndexMarker081"/> the fancy slicing operations that pandas <a id="_idIndexMarker082"/>supports but on the datetime axis. Let’s take a look at a few of them:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Setting the index as the datetime column</span>
df.set_index(<span class="hljs-string">"date"</span>, inplace=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># Select all data after 2010-01-04(inclusive)</span>
df[<span class="hljs-string">"2010-01-04"</span>:]
<span class="hljs-comment"># Select all data between 2010-01-04 and 2010-02-06(exclusive)</span>
df[<span class="hljs-string">"2010-01-04"</span>: <span class="hljs-string">"2010-02-06"</span>]
<span class="hljs-comment"># Select data 2010 and before</span>
df[: <span class="hljs-string">"2010"</span>]
<span class="hljs-comment"># Select data between 2010-01 and 2010-06(both including)</span>
df[<span class="hljs-string">"2010-01"</span>: <span class="hljs-string">"2010-06"</span>]
</code></pre>
    <p class="normal">In addition to the semantic<a id="_idIndexMarker083"/> information and intelligent indexing and <a id="_idIndexMarker084"/>slicing, <code class="inlineCode">pandas</code> also<a id="_idIndexMarker085"/> provide tools to create <a id="_idIndexMarker086"/>and manipulate date sequences.</p>
    <h2 id="_idParaDest-47" class="heading-2">Creating date sequences and managing date offsets</h2>
    <p class="normal">If you are familiar with <code class="inlineCode">range</code> in Python<a id="_idIndexMarker087"/> and <code class="inlineCode">np.arange</code> in NumPy, then you will know they help us create <code class="inlineCode">integer/float</code> sequences by providing a start point and an end point. pandas has something<a id="_idIndexMarker088"/> similar for datetime—<code class="inlineCode">pd.date_range</code>. The function accepts start and end dates, along with a frequency (daily, monthly, and so on), and creates the sequence of dates in <a id="_idIndexMarker089"/>between. Let’s look <a id="_idIndexMarker090"/>at a couple of ways to create a sequence of dates:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Specifying start and end dates with frequency</span>
pd.date_range(start=<span class="hljs-string">"2018-01-20"</span>, end=<span class="hljs-string">"2018-01-23"</span>, freq=<span class="hljs-string">"D"</span>).astype(<span class="hljs-built_in">str</span>).tolist()
<span class="hljs-comment"># Output: ['2018-01-20', '2018-01-21', '2018-01-22', '2018-01-23']</span>
<span class="hljs-comment"># Specifying start and number of periods to generate in the given frequency</span>
pd.date_range(start=<span class="hljs-string">"2018-01-20"</span>, periods=<span class="hljs-number">4</span>, freq=<span class="hljs-string">"D"</span>).astype(<span class="hljs-built_in">str</span>).tolist()
<span class="hljs-comment"># Output: ['2018-01-20', '2018-01-21', '2018-01-22', '2018-01-23']</span>
<span class="hljs-comment"># Generating a date sequence with every 2 days</span>
pd.date_range(start=<span class="hljs-string">"2018-01-20"</span>, periods=<span class="hljs-number">4</span>, freq=<span class="hljs-string">"2D"</span>).astype(<span class="hljs-built_in">str</span>).tolist()
<span class="hljs-comment"># Output: ['2018-01-20', '2018-01-22', '2018-01-24', '2018-01-26']</span>
<span class="hljs-comment"># Generating a date sequence every month. By default it starts with Month end</span>
pd.date_range(start=<span class="hljs-string">"2018-01-20"</span>, periods=<span class="hljs-number">4</span>, freq=<span class="hljs-string">"M"</span>).astype(<span class="hljs-built_in">str</span>).tolist()
<span class="hljs-comment"># Output: ['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30']</span>
<span class="hljs-comment"># Generating a date sequence every month, but month start</span>
pd.date_range(start=<span class="hljs-string">"2018-01-20"</span>, periods=<span class="hljs-number">4</span>, freq=<span class="hljs-string">"MS"</span>).astype(<span class="hljs-built_in">str</span>).tolist()
<span class="hljs-comment"># Output: ['2018-02-01', '2018-03-01', '2018-04-01', '2018-05-01']</span>
</code></pre>
    <p class="normal">We can also add or subtract <a id="_idIndexMarker091"/>days, months, and other values to/from dates using <code class="inlineCode">pd.TimeDelta</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Add four days to the date range</span>
(pd.date_range(start=<span class="hljs-string">"2018-01-20"</span>, end=<span class="hljs-string">"2018-01-23"</span>, freq=<span class="hljs-string">"D"</span>) + pd.Timedelta(<span class="hljs-number">4</span>, unit=<span class="hljs-string">"D"</span>)).astype(<span class="hljs-built_in">str</span>).tolist()
<span class="hljs-comment"># Output: ['2018-01-24', '2018-01-25', '2018-01-26', '2018-01-27']</span>
<span class="hljs-comment"># Add four weeks to the date range</span>
(pd.date_range(start=<span class="hljs-string">"2018-01-20"</span>, end=<span class="hljs-string">"2018-01-23"</span>, freq=<span class="hljs-string">"D"</span>) + pd.Timedelta(<span class="hljs-number">4</span>, unit=<span class="hljs-string">"W"</span>)).astype(<span class="hljs-built_in">str</span>).tolist()
<span class="hljs-comment"># Output: ['2018-02-17', '2018-02-18', '2018-02-19', '2018-02-20']</span>
</code></pre>
    <p class="normal">There are a lot of these<a id="_idIndexMarker092"/> aliases in <code class="inlineCode">pandas</code>, including <code class="inlineCode">W</code>, <code class="inlineCode">W-MON</code>, <code class="inlineCode">MS</code>, and others. The full list can be found at <a href="https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases"><span class="url">https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases</span></a>.</p>
    <p class="normal">In this section, we looked at a <a id="_idIndexMarker093"/>few useful features<a id="_idIndexMarker094"/> and operations we can perform on datetime indices and know how to manipulate DataFrames with datetime columns. Now, let’s review a few techniques we can use to deal with missing data.</p>
    <h1 id="_idParaDest-48" class="heading-1">Handling missing data</h1>
    <p class="normal">While dealing with large datasets in the wild, you are bound to encounter missing data. If it is not part of the time series, it may be part of the additional information you collect and map. Before we<a id="_idIndexMarker095"/> jump the gun and fill it with a mean value or drop those rows, let’s consider a few aspects:</p>
    <ul>
      <li class="bulletList">The first consideration should be whether the missing data we are worried about is missing or not. For that, we need to think<a id="_idIndexMarker096"/> about the <strong class="keyWord">Data Generating Process</strong> (<strong class="keyWord">DGP</strong>) (the process that generates the time series). As an example, let’s look at sales at a local supermarket. You have been given <a id="_idIndexMarker097"/>the <strong class="keyWord">point-of-sale</strong> (<strong class="keyWord">POS</strong>) transactions for the last 2 years, and you are processing the data into a time series. While analyzing the data, you found that there are a few products where there aren’t any transactions for a few days. Now, what you need to think about is whether the missing data is missing or whether there is some information that this missingness gives you. If you don’t have any transactions for a particular product for a day, it will appear as missing data while you are processing it, even though it is not missing. What that tells us is that there were no sales for that item and that you should fill such missing data with zeros.</li>
      <li class="bulletList">Now, what if you see that, every Sunday, the data is missing—that is, there is a pattern to the missingness? This becomes tricky because how you fill in such gaps depends on the model that you intend to use. If you fill in such gaps with zeros, a model that looks at the immediate past to predict the future might be<a id="_idIndexMarker098"/> thrown off, especially for Monday. However, if you tell the model that the previous day was Sunday, then the model still can learn to tell the difference.</li>
      <li class="bulletList">Lastly, what if you see zero sales on one of the best-selling products that always gets sold? This can happen because of something such as a POS machine malfunction, a data entry mistake, or an out-of-stock situation. These types of missing values can be imputed with a few techniques.</li>
    </ul>
    <p class="normal">Let’s look at an Air Quality dataset published by the ACT Government, Canberra, Australia, under the CC by Attribution 4.0 International License (<a href="https://www.data.act.gov.au/Environment/Air-Quality-Monitoring-Data/94a5-zqnn"><span class="url">https://www.data.act.gov.au/Environment/Air-Quality-Monitoring-Data/94a5-zqnn</span></a>) and see how we can impute such values using pandas (there are more sophisticated techniques available, all of which will be covered later in this chapter).</p>
    <div class="packt_tip">
      <p class="normal"><strong class="keyWord">Practitioner’s tip</strong>:</p>
      <p class="normal">When reading data using a method such as <code class="inlineCode">read_csv</code>, pandas provides a few handy ways to handle missing values. pandas treats values such as <code class="inlineCode">#N/A</code>, <code class="inlineCode">null</code>, and so on as <code class="inlineCode">NaN </code>by default. We can control this list of allowable <code class="inlineCode">NaN</code> values using the <code class="inlineCode">na_values</code> and <code class="inlineCode">keep_default_na</code> parameters.</p>
    </div>
    <p class="normal">We have chosen region <strong class="keyWord">Monash</strong> and <strong class="keyWord">PM2.5</strong> readings, and artificially introduced some missing values, as shown in the following diagram:</p>
    <figure class="mediaobject"><img src="../Images/B22389_02_03.png" alt="Figure 2.3 – Missing values in the Air Quality dataset "/></figure>
    <p class="packt_figref">Figure 2.3: Missing values in the Air Quality dataset</p>
    <p class="normal">Now, let’s look at a few<a id="_idIndexMarker099"/> simple techniques we can use to fill in the missing values:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Last Observation Carried Forward or Forward Fill</strong>: This imputation technique takes the last observed value, using that to fill in all the missing values until it finds the next observation. This is also<a id="_idIndexMarker100"/> called forward fill. We can do this like so:
        <pre class="programlisting code-one"><code class="hljs-code">df[<span class="hljs-string">'pm2_5_1_hr'</span>].ffill()
</code></pre>
      </li>
      <li class="bulletList"><strong class="keyWord">Next Observation Carried Backward of Backward Fill</strong>: This imputation technique takes the next observation and backtracks to fill in all the missing values with this value. This is <a id="_idIndexMarker101"/>also called backward fill. Let’s see how we can do this in pandas:
        <pre class="programlisting code-one"><code class="hljs-code">df[<span class="hljs-string">'pm2_5_1_hr'</span>].bfill()
</code></pre>
      </li>
      <li class="bulletList"><strong class="keyWord">Mean Value Fill</strong>: This imputation technique is also pretty simple. We calculate the mean of the entire series, and wherever we find missing values, we fill it with the mean value:
        <pre class="programlisting code-one"><code class="hljs-code">df[<span class="hljs-string">'pm2_5_1_hr'</span>].fillna(df[<span class="hljs-string">'pm2_5_1_hr'</span>].mean())
</code></pre>
      </li>
    </ul>
    <p class="normal">Let’s plot the imputed lines we get from using these three techniques:</p>
    <figure class="mediaobject"><img src="../Images/B22389_02_04.png" alt="Figure 2.4 – Imputed missing values using forward, backward, and mean value fill "/></figure>
    <p class="packt_figref">Figure 2.4: Imputed missing values using forward, backward, and mean value fill</p>
    <p class="normal">Another family of imputation techniques covers interpolation:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Linear Interpolation</strong>: Linear interpolation is just like drawing a line between the two observed points and filling in the <a id="_idIndexMarker102"/>missing values so that they lie on this line. This is how we do it:
        <pre class="programlisting code-one"><code class="hljs-code">df[<span class="hljs-string">'pm2_5_1_hr'</span>].interpolate(method=<span class="hljs-string">"</span><span class="hljs-string">linear"</span>)
</code></pre>
      </li>
      <li class="bulletList"><strong class="keyWord">Nearest Interpolation</strong>: This is intuitively like a combination of the forward and backward fill. For each missing<a id="_idIndexMarker103"/> value, the closest observed value is found and used to fill in the missing value:
        <pre class="programlisting code-one"><code class="hljs-code">df[<span class="hljs-string">'pm2_5_1_hr'</span>].interpolate(method=<span class="hljs-string">"nearest"</span>)
</code></pre>
      </li>
    </ul>
    <p class="normal">Let’s plot the two interpolated lines:</p>
    <figure class="mediaobject"><img src="../Images/B22389_02_05.png" alt="Figure 2.5 – Imputed missing values using linear and nearest interpolation "/></figure>
    <p class="packt_figref">Figure 2.5: Imputed missing values using linear and nearest interpolation</p>
    <p class="normal">There are a few non-linear interpolation techniques as well:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Spline, Polynomial, and Other Interpolations</strong>: In addition to linear interpolation, pandas also supports non-linear interpolation techniques that call a SciPy routine at the<a id="_idIndexMarker104"/> backend. Spline and polynomial interpolations are similar. They fit a spline/polynomial of a given order to the data and use that to fill in missing values. While using <code class="inlineCode">spline</code> or <code class="inlineCode">polynomial</code> as the method in <code class="inlineCode">interpolate</code>, we should always provide <code class="inlineCode">order</code> as well. The higher the order, the more flexible the function that is used to fit the observed points will be. Let’s see how we can use spline and polynomial interpolation:
        <pre class="programlisting code-one"><code class="hljs-code">df[<span class="hljs-string">'pm2_5_1_hr'</span>].interpolate(method=<span class="hljs-string">"spline"</span>, order=<span class="hljs-number">2</span>)
df[<span class="hljs-string">'pm2_5_1_hr'</span>].interpolate(method=<span class="hljs-string">"polynomial"</span>, order=<span class="hljs-number">5</span>)
</code></pre>
      </li>
    </ul>
    <p class="normal">Let’s plot these two non-linear interpolation techniques:</p>
    <figure class="mediaobject"><img src="../Images/B22389_02_06.png" alt="Figure 2.6 – Imputed missing values using spline and polynomial interpolation "/></figure>
    <p class="packt_figref">Figure 2.6: Imputed missing values using spline and polynomial interpolation</p>
    <p class="normal">For a complete list <a id="_idIndexMarker105"/>of interpolation techniques supported by <code class="inlineCode">interpolate</code>, go to <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html"><span class="url">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html</span></a> and <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html#scipy.interpolate.interp1d"><span class="url">https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html#scipy.interpolate.interp1d</span></a>.</p>
    <p class="normal">Now that we are more comfortable with the way pandas manages datetime, let’s go back to our dataset and convert the data into a more manageable form.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">To follow along with the complete code for pre-processing, use the <code class="inlineCode">02-Preprocessing_London_Smart_Meter_Dataset.ipynb</code> notebook in the <code class="inlineCode">Chapter02</code> folder.</p>
    </div>
    <h2 id="_idParaDest-49" class="heading-2">Converting the half-hourly block-level data (hhblock) into time series data</h2>
    <p class="normal">Before we start processing, let’s understand a <a id="_idIndexMarker106"/>few general categories of information we will find in a time series dataset:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Time Series Identifiers</strong>: These are identifiers <a id="_idIndexMarker107"/>for a particular time series. It can be a name, an ID, or any other unique feature—for example, the<a id="_idIndexMarker108"/> SKU name or the ID of a retail sales dataset or the consumer ID in the energy dataset that we are working with are all time series identifiers.</li>
      <li class="bulletList"><strong class="keyWord">Metadata or Static Features</strong>: This information does not vary with time. An example of this is the ACORN classification of the household in our dataset.</li>
      <li class="bulletList"><strong class="keyWord">Time-Varying Features</strong>: This information varies with time—for example, the weather information. For each point in time, we have a different value for weather, unlike the Acorn classification.</li>
    </ul>
    <p class="normal">Next, let’s discuss formatting of a dataset.</p>
    <h2 id="_idParaDest-50" class="heading-2">Compact, expanded, and wide forms of data</h2>
    <p class="normal">There are many ways to format a time series dataset, especially a dataset with many related time series, like the one we<a id="_idIndexMarker109"/> have now. A standard way <a id="_idIndexMarker110"/>of doing this is <strong class="keyWord">wide</strong> data. This is where the date column becomes sort of an index and each time series occupies a different column. And if there are a million time series, it will have a million and one columns (hence the term wide). Apart from the standard <strong class="keyWord">wide</strong> data, we can also look at two non-standard ways to format time series <a id="_idIndexMarker111"/>data. Although there is no standard nomenclature for <a id="_idIndexMarker112"/>them, we will refer to them as <strong class="keyWord">compact</strong> and <strong class="keyWord">expanded</strong> in this book. The <a id="_idIndexMarker113"/>expanded form is also referred to as <strong class="keyWord">long</strong> in some literature.</p>
    <p class="normal">Compact-form data is when any<a id="_idIndexMarker114"/> particular time series occupies only a single row in the pandas DataFrame—that is, the time dimension is managed as an array within a DataFrame row. The time series identifiers and the metadata occupy the columns with scalar values and then the time series values; other time-varying features occupy the columns with an array. Two additional columns are included to extrapolate time—<code class="inlineCode">start_datetime</code> and <code class="inlineCode">frequency</code>. If we know the start datetime and the frequency of the time series, we can easily construct the time and recover the time series from the DataFrame. This only works for regularly sampled time series. The <a id="_idIndexMarker115"/>advantage is that the DataFrames take up much less memory and are easy and faster to work with:</p>
    <figure class="mediaobject"><img src="../Images/B22389_02_07.png" alt="Figure 2.7 – Compact form data "/></figure>
    <p class="packt_figref">Figure 2.7: Compact-form data</p>
    <p class="normal">The expanded form is when the<a id="_idIndexMarker116"/> time series is expanded along the<a id="_idIndexMarker117"/> rows of a DataFrame. If there are <em class="italic">n</em> steps in the time series, it occupies <em class="italic">n</em> rows in the DataFrame. The time series identifiers and the metadata get repeated along all the rows. The time-varying features also get expanded along the rows. Also, instead of the start date and frequency, we have the timestamp as a column:</p>
    <figure class="mediaobject"><img src="../Images/B22389_02_08.png" alt="Figure 2.8 – Expanded form data "/></figure>
    <p class="packt_figref">Figure 2.8: Expanded-form data</p>
    <p class="normal">If the compact form had a time series identifier as the key, the time series identifier and the datetime column would be combined and become the key.</p>
    <p class="normal">Wide-format data is more common in traditional time series literature. It can be considered a legacy format, which is limiting in many ways. Do you remember the stock data we saw earlier (<em class="italic">Figure 2.2</em>)? We have the date as an index or one of the columns, and the different time series as different columns of the DataFrame. As the number of time series increases, they become wider <a id="_idIndexMarker118"/>and wider, hence the name. This data format does not allow us to include any<a id="_idIndexMarker119"/> metadata about the time series. For instance, in our data, we have information about whether a particular household is under standard or dynamic pricing. There is <a id="_idIndexMarker120"/>no way for us to include such metadata in the wide format. From an operational perspective, the wide format also does not play well with relational databases because we have to keep adding columns to a table when we get new time series. We won’t be using this format in this book.</p>
    <h2 id="_idParaDest-51" class="heading-2">Enforcing regular intervals in time series</h2>
    <p class="normal">One of the first things you should check and correct is whether the regularly sampled time series data that you have has equal intervals of time. In practice, even regularly sampled time series have some<a id="_idIndexMarker121"/> samples missing in between, due to some data collection error or some other peculiar way data is collected. So while working with the data, we will make sure we enforce regular intervals in the time series.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Best practice</strong>:</p>
      <p class="normal">While working with datasets with multiple time series, it is best practice to check the end dates of all the time series. If they are not uniform, we can align them with the latest date across all the time series in the dataset.</p>
    </div>
    <p class="normal">In our smart meters dataset, some <code class="inlineCode">LCLid</code> columns end much earlier than the rest. Maybe the household opted out of the program, or they moved out and left the house empty; the reason could be anything. However, we need to handle that while we enforce regular intervals.</p>
    <p class="normal">We will learn how to convert the dataset into a time series format in the next section. The code for this process can <a id="_idIndexMarker122"/>be found in the <code class="inlineCode">02-Preprocessing_London_Smart_Meter_Dataset.ipynb</code> notebook.</p>
    <h2 id="_idParaDest-52" class="heading-2">Converting the London Smart Meters dataset into a time series format</h2>
    <p class="normal">For each dataset that you come across, the steps you would have to take to convert it into either a compact or<a id="_idIndexMarker123"/> expanded form would be different. It depends on how the original data is structured. Here, we will look at how the London Smart Meters dataset can be transformed so that we can<a id="_idIndexMarker124"/> transfer those learnings to other datasets.</p>
    <p class="normal">There are two steps we need to do before we can start processing the data into either compact or expanded form:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">Find the Global End Date</strong>: We must find the maximum date across all the block files so that we know the global end date of the time series.</li>
      <li class="numberedList"><strong class="keyWord">Basic Preprocessing</strong>: If you remember how <code class="inlineCode">hhblock_dataset</code> is structured, you will remember that each row had a date and that, along the columns, we have half-hourly blocks. We need to reshape that into a long form, where each row has a date and a single half-hourly block. It’s easier to handle that way.</li>
    </ol>
    <p class="normal">Now, let’s define separate functions to convert data into compact and expanded forms and <code class="inlineCode">apply</code> those functions to each of the <code class="inlineCode">LCLid</code> columns. We will do this for each <code class="inlineCode">LCLid</code> separately, since the start date for each <code class="inlineCode">LCLid</code> is different.</p>
    <h3 id="_idParaDest-53" class="heading-3">Expanded form</h3>
    <p class="normal">The function to convert data into the expanded form does the following:</p>
    <ol>
      <li class="numberedList" value="1">Finds the start date.</li>
      <li class="numberedList">Create a standard DataFrame using the start date and the global end date.</li>
      <li class="numberedList">Left-merges the DataFrame for <code class="inlineCode">LCLid</code> with the standard DataFrame, leaving the missing data as <code class="inlineCode">np.nan</code>.</li>
      <li class="numberedList">Returns the merged DataFrame.</li>
    </ol>
    <p class="normal">Once we have all the <code class="inlineCode">LCLid</code> DataFrames, we must perform a couple of additional steps to complete the expanded form processing:</p>
    <ol>
      <li class="numberedList" value="1">Concatenate all the DataFrames into a single DataFrame.</li>
      <li class="numberedList">Create a column called offset, which is the numerical representation of the half-hour blocks; for example, <code class="inlineCode">hh_3</code> → <code class="inlineCode">3</code>.</li>
      <li class="numberedList">Create a timestamp by adding a 30-minute offset to the day and dropping the unnecessary columns.</li>
    </ol>
    <p class="normal">For one block, this<a id="_idIndexMarker125"/> representation takes up ~47 MB of memory.</p>
    <h3 id="_idParaDest-54" class="heading-3">Compact form</h3>
    <p class="normal">The function for converting into compact form does the following:</p>
    <ol>
      <li class="numberedList" value="1">Finds the start date and<a id="_idIndexMarker126"/> time series identifiers.</li>
      <li class="numberedList">Creates a standard DataFrame using the start date and the global end date.</li>
      <li class="numberedList">Left merges the DataFrame for <code class="inlineCode">LCLid</code> to the standard DataFrame, leaving the missing data as <code class="inlineCode">np.nan</code>.</li>
      <li class="numberedList">Sorts the values on the date.</li>
      <li class="numberedList">Returns the time series array, along with the time series identifier, start date, and the length of the time series.</li>
    </ol>
    <p class="normal">Once we have this information for each <code class="inlineCode">LCLid</code>, we can compile it into a DataFrame and add 30min as the frequency.</p>
    <p class="normal">For one block, this representation takes up only ~0.002 MB of memory.</p>
    <p class="normal">We are going to use the compact form because it is easy to work with and much less resource-hungry.</p>
    <h1 id="_idParaDest-55" class="heading-1">Mapping additional information</h1>
    <p class="normal">From the data model that we prepared earlier, we know that there are three key files that we have to map: <em class="italic">Household Information</em>, <em class="italic">Weather</em>, and <em class="italic">Bank Holidays</em>.</p>
    <p class="normal">The <code class="inlineCode">informations_households.csv</code> file contains <a id="_idIndexMarker127"/>metadata about the household. There are static features that are not dependent on time. For this, we just need to left-merge <code class="inlineCode">informations_households.csv</code> with the compact form based on <code class="inlineCode">LCLid</code>, which is the time series identifier.</p>
    <div class="packt_tip">
      <p class="normal"><strong class="keyWord">Best practice</strong>:</p>
      <p class="normal">While doing a pandas <code class="inlineCode">merge</code>, one of the most common and unexpected outcomes is that the number of rows before and after the operation is not the same (even if you are doing a left merge). This typically happens because there are duplicates in the keys on which you are merging. As a best practice, you can use the <code class="inlineCode">validate</code> parameter in the pandas merge, which takes in inputs such as <code class="inlineCode">one_to_one</code> and <code class="inlineCode">many_to_one</code> so that this check is done while merging and will throw an error if the assumption is not met. For more information, go to <a href="https://pandas.pydata.org/docs/reference/api/pandas.merge.html"><span class="url">https://pandas.pydata.org/docs/reference/api/pandas.merge.html</span></a>.</p>
    </div>
    <p class="normal">Bank Holidays and Weather, on the other hand, are time-varying features and should be dealt with accordingly. The most important aspect to keep in mind is that while we map this information, it should <a id="_idIndexMarker128"/>perfectly align with the time series that we have already stored as an array.</p>
    <p class="normal"><code class="inlineCode">uk_bank_holidays.csv</code> is a file that contains the dates of the holidays and the kind of holiday. The holiday information is quite important here because the energy consumption patterns would be different on a holiday when the family members are at home spending time with each other, watching television, and so on. Follow these steps to process this file:</p>
    <ol>
      <li class="numberedList" value="1">Convert the date column into the datetime format and set it as the index of the DataFrame.</li>
      <li class="numberedList">Using the <code class="inlineCode">resample</code> function we saw earlier, we must ensure that the index is resampled every 30 minutes, which is the frequency of the times series.</li>
      <li class="numberedList">Forward fill the holidays within a day and fill in the rest of the <code class="inlineCode">NaN</code> values with <code class="inlineCode">NO_HOLIDAY</code>.</li>
    </ol>
    <p class="normal">Now, we have converted the holiday file into a DataFrame that has a row for each 30-minute interval. On each row, we have a column that specifies whether that day was a holiday or not.</p>
    <p class="normal"><code class="inlineCode">weather_hourly_darksky.csv</code> is a file that is, once again, at the daily frequency. We need to downsample it to a 30-minute frequency because the data that we need to map to this is at a half-hourly frequency. If we don’t do this, the weather will only be mapped to the hourly timestamps, leaving the half-hourly timestamps empty.</p>
    <p class="normal">The steps we must follow to process this file are also similar to the way we processed holidays:</p>
    <ol>
      <li class="numberedList" value="1">Convert the date column into the datetime format and set it as the index of the DataFrame.</li>
      <li class="numberedList">Using the <code class="inlineCode">resample</code> function, we must ensure that the index is resampled every 30 minutes, which is the frequency of the times series.</li>
      <li class="numberedList">Forward fill the weather features to fill in the missing values that were created while resampling.</li>
    </ol>
    <p class="normal">Now that you have made sure the alignment between the time series and the time-varying features is ensured, you <a id="_idIndexMarker129"/>can loop over each of the time series and extract the weather and bank holiday array before storing it in the corresponding row of the DataFrame.</p>
    <h1 id="_idParaDest-56" class="heading-1">Saving and loading files to disk</h1>
    <p class="normal">The fully merged DataFrame in its compact form takes up only ~10 MB. However, saving this file requires a little bit of engineering. If we try to save the file in the CSV format, it will not work because of the way we<a id="_idIndexMarker130"/> have stored arrays in pandas columns (since the data is in its compact form). We can save it in <code class="inlineCode">pickle</code> or <code class="inlineCode">parquet</code> format, or any of the binary forms of file storage. This can work, depending on the size of the RAM available on our machines. Although the fully merged DataFrame is just ~10 MB, saving it in <code class="inlineCode">pickle</code> format will make the size explode to ~15 GB.</p>
    <p class="normal">What we can do is save this as a text file while making a few tweaks to accommodate the column names, column types, and other metadata that is required to read the file back into memory. The resulting file size on disk still comes out to ~15 GB, but since we are doing it as an I/O operation, we do not keep all that data in our memory. We call this the time series (<code class="inlineCode">.ts</code>) format. The functions to save a compact form in the<code class="inlineCode">.ts</code> format, read the <code class="inlineCode">.ts</code> format, and convert the compact form into the expanded form are available in this book’s GitHub repository under <code class="inlineCode">src/data_utils.py</code>.</p>
    <p class="normal">If you don’t need to store all of the DataFrame in a single file, you can split it into multiple chunks and save them individually in a binary format, such as <code class="inlineCode">parquet</code>. For our datasets, let’s follow this route and split the whole DataFrame into chunks of blocks and save them as <code class="inlineCode">parquet</code> files. This is the best route for us for a few reasons:</p>
    <ul>
      <li class="bulletList">It leverages the compression that comes with the format</li>
      <li class="bulletList">It reads in parts of the whole data for quick iteration and experimentation</li>
      <li class="bulletList">The data types are retained between the read and write operations, leading to less ambiguity</li>
    </ul>
    <div class="note">
      <p class="normal">For very large datasets, we can use some pandas alternatives, which makes it easier to process datasets that are out of memory. Polars is a great library that has lazy loading and is very fast. And for truly huge datasets, PySpark with a distributed cluster might be the right choice.</p>
    </div>
    <p class="normal">Now that we have processed the dataset and stored it on disk, let’s read it back into memory and look at a few <a id="_idIndexMarker131"/>more techniques to handle missing data.</p>
    <h1 id="_idParaDest-57" class="heading-1">Handling longer periods of missing data</h1>
    <p class="normal">We saw some techniques to <a id="_idIndexMarker132"/>handle missing data earlier—forward and <a id="_idIndexMarker133"/>backward filling, interpolation, and so on. Those techniques usually work if there are one or two missing data points. But if a large section of data is missing, then these simple techniques fall short.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">To follow along with the complete code for missing data imputation, use the <code class="inlineCode">03-Handling_Missing_Data_(Long_Gaps).ipynb</code> notebook in the <code class="inlineCode">Chapter02</code> folder.</p>
    </div>
    <p class="normal">Let’s read blocks <code class="inlineCode">0–7 parquet</code> from memory:</p>
    <pre class="programlisting code"><code class="hljs-code">block_df = pd.read_parquet(<span class="hljs-string">"data/london_smart_meters/preprocessed/london_smart_meters_merged_block_0-7.parquet"</span>)
</code></pre>
    <p class="normal">The data that we have saved is in the compact form. We need to convert it into the expanded form because it is easier to work with time series data in that form. Since we only need a subset of the time series (for faster demonstration purposes), we will just extract one block from these seven blocks. To convert the compact form into the expanded form, we can use a helpful function in <code class="inlineCode">src/utils/data_utils.py</code> called <code class="inlineCode">compact_to_expanded</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#Converting to expanded form</span>
exp_block_df = compact_to_expanded(block_df[block_df.file==<span class="hljs-string">"block_7"</span>], timeseries_col = <span class="hljs-string">'energy_consumption'</span>,
static_cols = [<span class="hljs-string">"frequency"</span>, <span class="hljs-string">"series_length"</span>, <span class="hljs-string">"stdorToU"</span>, <span class="hljs-string">"Acorn"</span>, <span class="hljs-string">"</span><span class="hljs-string">Acorn_grouped"</span>, <span class="hljs-string">"file"</span>],
time_varying_cols = [<span class="hljs-string">'holidays'</span>, <span class="hljs-string">'visibility'</span>, <span class="hljs-string">'windBearing'</span>, <span class="hljs-string">'temperature'</span>, <span class="hljs-string">'dewPoint'</span>,
       <span class="hljs-string">'pressure'</span>, <span class="hljs-string">'apparentTemperature'</span>, <span class="hljs-string">'windSpeed'</span>, <span class="hljs-string">'</span><span class="hljs-string">precipType'</span>, <span class="hljs-string">'icon'</span>,
       <span class="hljs-string">'humidity'</span>, <span class="hljs-string">'summary'</span>],
ts_identifier = <span class="hljs-string">"LCLid"</span>)
</code></pre>
    <p class="normal">One of the best ways to visualize the missing data in a group of related time series is by using a very helpful package called <code class="inlineCode">missingno</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Pivot the data to set the index as the datetime and the different time series along the columns</span>
plot_df = pd.pivot_table(exp_block_df, index=<span class="hljs-string">"timestamp"</span>, columns=<span class="hljs-string">"LCLid"</span>, values=<span class="hljs-string">"energy_consumption"</span>)
<span class="hljs-comment"># Generate Plot. Since we have a datetime index, we can mention the frequency to decide what do we want on the X axis</span>
msno.matrix(plot_df, freq=<span class="hljs-string">"M"</span>)
</code></pre>
    <p class="normal">The preceding code<a id="_idIndexMarker134"/> produces the following output:</p>
    <figure class="mediaobject"><img src="../Images/B22389_02_09.png" alt="Figure 2.9 – Visualization of the missing data in block 7 "/></figure>
    <p class="packt_figref">Figure 2.9: Visualization of the missing data in block 7</p>
    <div class="note">
      <p class="normal">Only attempt the <code class="inlineCode">missingno</code> visualization on related time series where there are less than 25 time series. If you have a dataset that contains thousands of time series (such as in our full dataset), applying this visualization will give us an illegible plot and a frozen computer.</p>
    </div>
    <p class="normal">This visualization tells us <a id="_idIndexMarker135"/>a lot of things at a single glance. The <em class="italic">Y</em>-axis contains the dates that we are plotting the visualization for, while the <em class="italic">X</em>-axis contains the columns, which in this case are the different households. We know that all the time series are not perfectly aligned—that is, not all of them start at the same time and end at the same time. The big white gaps we can see at the beginning of many of the time series show that data collection for those consumers started later than the others. We can also see that a few time series finish earlier than the rest, which means either they stopped being consumers or the measurement phase stopped. There are also a few smaller white lines in many time series, which are real missing values. We can also notice a sparkline to the right, which is a compact representation of the number of missing columns for each row. If there are no <a id="_idIndexMarker136"/>missing values (all time series have some value), then the sparkline would be at the far right. Finally, if there are a lot of missing values, the line will be to the left.</p>
    <p class="normal">Just because there are<a id="_idIndexMarker137"/> missing values, we are not going to fill/impute them because the decision of whether to impute missing data or not comes later in the workflow. For some models, we do not need to do the imputation, while for others, we do. There are multiple ways of imputing missing data, and which one to choose is another decision we cannot make beforehand.</p>
    <p class="normal">So for now, let’s pick one <code class="inlineCode">LCLid</code> and<a id="_idIndexMarker138"/> dig deeper. We already know that there are some missing values between <code class="inlineCode">2012-09-30</code> and <code class="inlineCode">2012-10-31</code>. Let’s visualize that period:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Taking a single time series from the block</span>
ts_df = exp_block_df[exp_block_df.LCLid==<span class="hljs-string">"MAC000193"</span>].set_index(<span class="hljs-string">"timestamp"</span>)
msno.matrix(ts_df[<span class="hljs-string">"2012-09-30"</span>: <span class="hljs-string">"2012-10-31"</span>], freq=<span class="hljs-string">"D"</span>)
</code></pre>
    <p class="normal">The preceding code produces the following output:</p>
    <figure class="mediaobject"><img src="../Images/B22389_02_10.png" alt="Figure 2.10 – Visualization of missing data of MAC000193 between 2012-09-30 and 2012-10-31 "/></figure>
    <p class="packt_figref">Figure 2.10: Visualization of missing data of MAC000193 between 2012-09-30 and 2012-10-31</p>
    <p class="normal">Here, we can see that the missing data is between <code class="inlineCode">2012-10-18</code> and <code class="inlineCode">2012-10-19</code>. Normally, we would go ahead and impute the missing data in this period, but since we are looking at this with an academic lens, we will take a slightly different route. </p>
    <p class="normal">Let’s introduce an artificial missing data section, see how the different techniques we are going to look at impute the missing data, and compute a metric to see how close we are to the real time series (We <a id="_idIndexMarker139"/>are going to use a metric called <strong class="keyWord">Mean Absolute Error</strong> (<strong class="keyWord">MAE</strong>) to do the <a id="_idIndexMarker140"/>comparison, and it’s nothing but the average of the absolute <a id="_idIndexMarker141"/>error across the time steps. Just understand that it is a lower-the-better metric that we will talk about in detail later in the book.):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># The dates between which we are nulling out the time series</span>
window = <span class="hljs-built_in">slice</span>(<span class="hljs-string">"2012-10-07"</span>, <span class="hljs-string">"2012-10-08"</span>)
<span class="hljs-comment"># Creating a new column and artificially creating missing values</span>
ts_df[<span class="hljs-string">'energy_consumption_missing'</span>] = ts_df.energy_consumption
ts_df.loc[window, <span class="hljs-string">"energy_consumption_missing"</span>] = np.nan
</code></pre>
    <p class="normal">Now, let’s plot the missing area in the time series:</p>
    <figure class="mediaobject"><img src="../Images/B22389_02_11.png" alt="Figure 2.11 – The energy consumption of MAC000193 between 2012-10-05 and 2012-10-10 "/></figure>
    <p class="packt_figref">Figure 2.11: The energy consumption of MAC000193 between 2012-10-05 and 2012-10-10</p>
    <p class="normal">We are missing 2 whole days of energy consumption readings, which means there are 96 missing data points (half-hourly). If we use one of the techniques we saw earlier, such as interpolation, we will see that it will mostly be a straight line because none of the methods are complex <a id="_idIndexMarker142"/>enough to capture the pattern over a long time.</p>
    <p class="normal">There are a few <a id="_idIndexMarker143"/>techniques that we can use to fill in such large missing gaps in data. We will cover these now.</p>
    <h2 id="_idParaDest-58" class="heading-2">Imputing with the previous day</h2>
    <p class="normal">Since this is a half-hourly time series of energy consumption, it stands to reason that there might be a pattern that repeats<a id="_idIndexMarker144"/> day after day. The energy consumption between 9:00 A.M. and 10:00 A.M. might be higher as everybody gets ready to go to the office, slumping during the day when most houses may be empty. </p>
    <p class="normal">So the simplest way to fill in the missing data would be to use the previous day’s energy readings so that the energy reading at 10:00 A.M, 2012-10-18, can be filled with the energy reading at 10:00 A.M, 2012-10-17:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#Shifting 48 steps to get previous day</span>
ts_df[<span class="hljs-string">"prev_day"</span>] = ts_df[<span class="hljs-string">'energy_consumption'</span>].shift(<span class="hljs-number">48</span>)
<span class="hljs-comment">#Using the shifted column to fill missing</span>
ts_df[<span class="hljs-string">'prev_day_imputed'</span>] =  ts_df[<span class="hljs-string">'energy_consumption_missing'</span>]
ts_df.loc[null_mask,<span class="hljs-string">"prev_day_imputed"</span>] = ts_df.loc[null_mask,<span class="hljs-string">"prev_day"</span>]
mae = mean_absolute_error(ts_df.loc[window, <span class="hljs-string">"</span><span class="hljs-string">prev_day_imputed"</span>], ts_df.loc[window, <span class="hljs-string">"energy_consumption"</span>])
</code></pre>
    <p class="normal">Let’s see what the imputation looks like:</p>
    <figure class="mediaobject"><img src="../Images/B22389_02_12.png" alt="Figure 2.12 – Imputing with the previous day "/></figure>
    <p class="packt_figref">Figure 2.12: Imputing with the previous day</p>
    <p class="normal">While this looks better, this is <a id="_idIndexMarker145"/>also very brittle. When we copy the previous day, we also assume that any kind of variation or anomalous behavior is also repeated. We can already see that the patterns for the day before and the day after are not the same.</p>
    <h2 id="_idParaDest-59" class="heading-2">Hourly average profile</h2>
    <p class="normal">A better approach would be<a id="_idIndexMarker146"/> to calculate an hourly profile from the data—the mean consumption for every hour—and use the average to fill in the missing data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#Create a column with the Hour from timestamp</span>
ts_df[<span class="hljs-string">"hour"</span>] = ts_df.index.hour
<span class="hljs-comment">#Calculate hourly average consumption</span>
hourly_profile = ts_df.groupby([<span class="hljs-string">'hour'</span>])[<span class="hljs-string">'energy_consumption'</span>].mean().reset_index()
hourly_profile.rename(columns={<span class="hljs-string">"energy_consumption"</span>: <span class="hljs-string">"hourly_profile"</span>}, inplace=<span class="hljs-literal">True</span>)
<span class="hljs-comment">#Saving the index because it gets lost in merge</span>
idx = ts_df.index
<span class="hljs-comment">#Merge the hourly profile dataframe to ts dataframe</span>
ts_df = ts_df.merge(hourly_profile, on=[<span class="hljs-string">'hour'</span>], how=<span class="hljs-string">'left'</span>, validate=<span class="hljs-string">"many_to_one"</span>)
ts_df.index = idx
<span class="hljs-comment">#Using the hourly profile to fill missing</span>
ts_df[<span class="hljs-string">'hourly_profile_imputed'</span>] = ts_df[<span class="hljs-string">'energy_consumption_missing'</span>]
ts_df.loc[null_mask,<span class="hljs-string">"hourly_profile_imputed"</span>] = ts_df.loc[null_mask,<span class="hljs-string">"hourly_profile"</span>]
mae = mean_absolute_error(ts_df.loc[window, <span class="hljs-string">"hourly_profile_imputed"</span>], ts_df.loc[window, <span class="hljs-string">"energy_consumption"</span>])
</code></pre>
    <p class="normal">Let’s see if this is better:</p>
    <figure class="mediaobject"><img src="../Images/B22389_02_13.png" alt="Figure 2.13 – Imputing with an hourly profile "/></figure>
    <p class="packt_figref">Figure 2.13: Imputing with an hourly profile</p>
    <p class="normal">This gives us a much <a id="_idIndexMarker147"/>more generalized curve that does not have the spikes that we saw for the individual days. The hourly ups and downs have also been captured as per our expectations. The MAE is also lower than before.</p>
    <h2 id="_idParaDest-60" class="heading-2">The hourly average for each weekday</h2>
    <p class="normal">We can further refine this rule by introducing a specific profile for each weekday. It stands to reason that the usage<a id="_idIndexMarker148"/> pattern on a weekday is not going to be the same on a weekend. Hence, we can calculate the average hourly consumption for each weekday separately so that we have one profile for Monday, another for Tuesday, and so on:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#Create a column with the weekday from timestamp</span>
ts_df[<span class="hljs-string">"weekday"</span>] = ts_df.index.weekday
<span class="hljs-comment">#Calculate weekday-hourly average consumption</span>
day_hourly_profile = ts_df.groupby([<span class="hljs-string">'weekday'</span>,<span class="hljs-string">'hour'</span>])[<span class="hljs-string">'energy_consumption'</span>].mean().reset_index()
day_hourly_profile.rename(columns={<span class="hljs-string">"energy_consumption"</span>: <span class="hljs-string">"day_hourly_profile"</span>}, inplace=<span class="hljs-literal">True</span>)
<span class="hljs-comment">#Saving the index because it gets lost in merge</span>
idx = ts_df.index
<span class="hljs-comment">#Merge the day-hourly profile dataframe to ts dataframe</span>
ts_df = ts_df.merge(day_hourly_profile, on=[<span class="hljs-string">'weekday'</span>, <span class="hljs-string">'hour'</span>], how=<span class="hljs-string">'left'</span>, validate=<span class="hljs-string">"many_to_one"</span>)
ts_df.index = idx
<span class="hljs-comment">#Using the day-hourly profile to fill missing</span>
ts_df[<span class="hljs-string">'day_hourly_profile_imputed'</span>] = ts_df[<span class="hljs-string">'energy_consumption_missing'</span>]
ts_df.loc[null_mask,<span class="hljs-string">"day_hourly_profile_imputed"</span>] = ts_df.loc[null_mask,<span class="hljs-string">"day_hourly_profile"</span>]
mae = mean_absolute_error(ts_df.loc[window, <span class="hljs-string">"day_hourly_profile_imputed"</span>], ts_df.loc[window, <span class="hljs-string">"energy_consumption"</span>])
</code></pre>
    <p class="normal">Let’s see what this looks like:</p>
    <figure class="mediaobject"><img src="../Images/B22389_02_14.png" alt="Figure 2.14 – Imputing the hourly average for each weekday "/></figure>
    <figure class="mediaobject">Figure 2.14: Imputing the hourly average for each weekday</figure>
    <p class="normal">This looks very similar to <a id="_idIndexMarker149"/>the other one, but this is because the day we are imputing is a weekday and the weekday profiles are similar. The MAE is also lower than the day profile. The weekend profile is slightly different, which you can see in the associated Jupyter notebook.</p>
    <h2 id="_idParaDest-61" class="heading-2">Seasonal interpolation</h2>
    <p class="normal">Although calculating seasonal profiles and using them to impute works well, there are instances, especially when there is <a id="_idIndexMarker150"/>a trend in the time series, where such a simple technique falls short. The simple seasonal profile doesn’t capture the trend at all and ignores it completely. For such cases, we can do the following:</p>
    <ol>
      <li class="numberedList" value="1">Calculate the seasonal profile, similar to how we calculated the averages earlier.</li>
      <li class="numberedList">Subtract the seasonal profile and apply any of the interpolation techniques we saw earlier.</li>
      <li class="numberedList">Return the seasonal profile to the interpolated series.</li>
    </ol>
    <p class="normal">This process has been implemented in this book’s GitHub repository in the <code class="inlineCode">src/imputation/interpolation.py</code> file. We can use it as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> src.imputation.interpolation <span class="hljs-keyword">import</span> SeasonalInterpolation
<span class="hljs-comment"># Seasonal interpolation using 48*7 as the seasonal period.</span>
recovered_matrix_seas_interp_weekday_half_hour = SeasonalInterpolation(seasonal_period=<span class="hljs-number">48</span>*<span class="hljs-number">7</span>,decomposition_strategy=<span class="hljs-string">"additive"</span>, interpolation_strategy=<span class="hljs-string">"spline"</span>, interpolation_args={<span class="hljs-string">"order"</span>:<span class="hljs-number">3</span>}, min_value=<span class="hljs-number">0</span>).fit_transform(ts_df.energy_consumption_missing.values.reshape(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>))
ts_df[<span class="hljs-string">'seas_interp_weekday_half_hour_imputed'</span>] = recovered_matrix_seas_interp_weekday_half_hour
</code></pre>
    <p class="normal">The key parameter here is <code class="inlineCode">seasonal_period</code>, which tells the algorithm to look for patterns that repeat every <code class="inlineCode">seasonal_period</code>. If we mention <code class="inlineCode">seasonal_period=48</code>, it will look for patterns that repeat every 48 data points. In our case, they are after each day (because we have 48<a id="_idIndexMarker151"/> half-hour timesteps in a day). In addition to this, we need to specify what kind of interpolation we need to perform.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Additional information</strong>:</p>
      <p class="normal">Internally, we use something called seasonal decomposition (<code class="inlineCode">statsmodels.tsa.seasonal.seasonal_decompose</code>), which will be covered in <em class="chapterRef">Chapter 3</em>, <em class="italic">Analyzing and Visualizing Time Series Data</em>, to isolate the seasonality component.</p>
    </div>
    <p class="normal">Here, we have done seasonal interpolation using 48 (half-hourly) and 48*7 (weekday to half-hourly) and plotted the resulting imputation:</p>
    <figure class="mediaobject"><img src="../Images/B22389_02_15.png" alt="Figure 2.15 – Imputing with seasonal interpolation "/></figure>
    <p class="packt_figref">Figure 2.15: Imputing with seasonal interpolation</p>
    <p class="normal">Here, we can see that both have captured the seasonality patterns, but the half-hourly profile every weekday has captured the peaks of the first day better, so they have a lower MAE. There is <a id="_idIndexMarker152"/>no improvement in terms of hourly averages, mostly because there are no strong increasing or decreasing patterns in the time series.</p>
    <p class="normal">With this, we have come to the end of this chapter. We are now officially into the nitty-gritty of juggling, cleaning, and processing time series data. Congratulations on finishing this chapter!</p>
    <h1 id="_idParaDest-62" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, after a short refresher on pandas DataFrames, especially on the datetime manipulations and simple techniques to handle missing data, we learned about the two forms of storing and working with time series data—compact and expanded. With all this knowledge, we took our raw dataset and built a pipeline to convert it into the compact form. If you have run the accompanying notebook, you should have the preprocessed dataset saved on disk. We also had an in-depth look at some techniques to handle long gaps of missing data.</p>
    <p class="normal">Now that we have the processed datasets, in the next chapter, we will learn how to visualize and analyze a time series dataset.</p>
    <h1 class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/mts"><span class="url">https://packt.link/mts</span></a></p>
    <p class="normal"><img src="../Images/QR_Code15080603222089750.png" alt=""/></p>
  </div>
</body></html>