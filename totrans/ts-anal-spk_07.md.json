["```py\nfrom statsmodels.tsa.stattools import adfuller\n# Perform Augmented Dickey-Fuller test\nresult = adfuller(data_hr[-300:]['Global_active_power'])\n# if Test statistic < Critical Value and p-value < 0.05\n#   reject the Null hypothesis, time series does not have a unit root\n#   series is stationary\n# Extract and print the ADF test results\nprint('ADF Statistic:', result[0])\nprint('p-value:', result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print(f'   {key}: {value}')\n```", "```py\nADF Statistic: -6.615237252003429\np-value: 6.231223531550648e-09\nCritical Values:\n 1%: -3.4524113009049935\n 5%: -2.8712554127251764\n 10%: -2.571946570731871\n```", "```py\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n# Decompose the time series data into seasonal, trend, and residual \n# components\nresults = seasonal_decompose(data_hr)\n# Plot the last 300 data points of the seasonal component\nresults.seasonal[-300:].plot(figsize = (12,8));\n```", "```py\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n# Plot ACF to identify autocorrelation in 'data_hr' DataFrame\nplot_acf(data_hr['Global_active_power'])\n# Plot PACF to identify partial autocorrelation in 'data_hr' DataFrame\nplot_pacf(data_hr['Global_active_power'])\n# Display the ACF and PACF plots\nplt.show()\n```", "```py\n# Split the data into training and testing sets\n# The last 48 observations are used for testing,\n# the rest for training\ntrain = data_hr[:-48]\ntest = data_hr[-48:]\n```", "```py\nimport pmdarima as pm\n# Create auto_arima model to automatically select the best ARIMA parameters\nmodel = pm.auto_arima(\n    # Use the last 300 observations of the series for modeling:\n    train[-300:][\"Global_active_power\"],\n    # Enable seasonal differencing:\n    seasonal=True,\n    # Set the seasonal period to 24\n    # (e.g., 24 hours for daily data):\n    m=24,\n    # Set the degree of non-seasonal differencing to 0\n    # (assumes data is already stationary):\n    d=0,\n    # Set the degree of seasonal differencing to 1:\n    D=1,\n    # Set the maximum value of AR (p) terms to consider:\n    max_p=3,\n    # Set the maximum value of MA (q) terms to consider:\n    max_q=3,\n    # Set the maximum value of seasonal AR (P) terms to consider:\n    max_P=3,\n    # Set the maximum value of seasonal MA (Q) terms to consider:\n    max_Q=3,\n    # Use AIC (Akaike Information Criterion) to select the best model:\n    information_criterion='aic',\n    # Print fit information to see the progression of\n    # the model fitting:\n    trace=True,\n    # Ignore models that fail to converge:\n    error_action='ignore',\n    # Use stepwise algorithm for efficient search of the model space:\n    stepwise=True,\n    # Suppress convergence warnings:\n    suppress_warnings=True\n)\n# Print the summary of the fitted model\nprint(model.summary())\n```", "```py\nPerforming stepwise search to minimize aic\n…\nARIMA(1,0,1)(2,1,0)[24] intercept : AIC=688.757, Time=9.37 sec\n…\nARIMA(2,0,2)(2,1,0)[24] : AIC=681.750, Time=6.83 sec\n…\nARIMA(1,0,1)(2,1,0)[24] : AIC=686.763, Time=6.02 sec\nBest model: ARIMA(2,0,2)(2,1,0)[24]\n```", "```py\n# Define parameter grid for SARIMAX model configuration\nparam_grid = {\n    'order': [(0, 0, 0), (1, 0, 1), (2, 0, 0)],\n    # Non-seasonal ARIMA orders\n    'seasonal_order': [\n        (0, 0, 0, 24),\n        (2, 0, 1, 24),\n        (2, 1, 1, 24)\n    ],  # Seasonal ARIMA orders with period of 24\n}\n# Initialize variables to store the best AIC and\n# corresponding parameters\nbest_aic = float(\"inf\")\nbest_params = [\"\",\"\"]\n# Iterate over all combinations of parameters in the grid\nfor params in ParameterGrid(param_grid):\n    print(\n        f\"order: {params['order']}, seasonal_order: {params['seasonal_order']}\"\n    )\n    try:\n        # Initialize and fit SARIMAX model with current parameters\n        model = SARIMAX(\n            train['Global_active_power'],\n            order=params['order'],\n            seasonal_order=params['seasonal_order'])\n        model_fit = model.fit(disp=False)\n        print(f\"aic: {model_fit.aic}\")\n        # Update best parameters if current model has lower AIC\n        if model_fit.aic < best_aic:\n            best_aic = model_fit.aic\n            best_params = params\n    except Exception as error:\n        print(\"An error occurred:\", error)\n        continue\n```", "```py\ndef forecast_step():\n    # Predicts the next period with confidence intervals\n    forecast, conf_int = model.predict(\n        n_periods=1, return_conf_int=True)\n…\n# Iterate over each observation in the test dataset\nfor obs in test['Global_active_power']:\n    forecast, conf_int = forecast_step()  # Forecast next step\n    forecasts.append(forecast)  # Append forecast to list\n…\n    # Update the model with the new observation\n    model.update(obs)\n```", "```py\nfrom sklearn.metrics import mean_squared_error\nfrom pmdarima.metrics import smape\n# Calculate and print the mean squared error of the forecasts\nprint(f\"Mean squared error: {mean_squared_error(test['Global_active_power'], forecasts)}\")\n# Calculate and print the Symmetric Mean Absolute Percentage Error \n# (SMAPE)\nprint(f\"SMAPE: {smape(test['Global_active_power'], forecasts)}\")\n```", "```py\nMean squared error: 0.6131968222566936\nSMAPE: 43.775868579535334\n```", "```py\nfrom SeqMetrics import RegressionMetrics, plot_metrics\n# Initialize the RegressionMetrics object with actual and\n# predicted values\ner = RegressionMetrics(\n    test['Global_active_power'], forecasts)\n# Calculate all available regression metrics\nmetrics = er.calculate_all()\n# Plot the calculated metrics using a color scheme\nplot_metrics(metrics, color=\"Blues\")\n# Display the Symmetric Mean Absolute Percentage Error (SMAPE)\nprint(f\"Test SMAPE: {metrics['smape']}\")\n# Display the Weighted Absolute Percentage Error (WAPE)\nprint(f\"Test WAPE: {metrics['wape']}\")\n```", "```py\nTest SMAPE: 43.775868579535334\nTest WAPE: 0.4202224470299464\n```", "```py\n# Split the data into training and testing sets\n# The last 48 observations are used for testing, the rest for training\ntrain = data_hr[:-48]\ntest = data_hr[-48:]\n```", "```py\n# Define the parameter grid for LightGBM\nparam_grid = {\n    'num_leaves': [30, 50, 100],\n    'learning_rate': [0.1, 0.01, 0.001],\n    'n_estimators': [50, 100, 200]\n}\n# Initialize LightGBM regressor\nlgbm = lgb.LGBMRegressor()\n# Setup TimeSeriesSplit for cross-validation\ntscv = TimeSeriesSplit(n_splits=10)\n# Configure and run GridSearchCV\ngsearch = GridSearchCV(\n    estimator=lgbm,\n    param_grid=param_grid,\n    cv=tscv\n)\ngsearch.fit(X_train, y_train)\n# Output the best parameters from Grid Search\nprint(f\"Best Parameters: {gsearch.best_params_}\")\n```", "```py\nBest Parameters: {'learning_rate': 0.1, 'n_estimators': 50, 'num_leaves': 30}\n```", "```py\nfinal_model = lgb.LGBMRegressor(**best_params)\nfinal_model.fit(X_train, y_train)\n```", "```py\n# Predict on the test set\ny_pred = final_model.predict(X_test)\n```", "```py\nTest SMAPE: 41.457989848314384\nTest WAPE: 0.38978585281926825\n```", "```py\n# Initialize and fit the Prophet model\n# model = NeuralProphet()\nmodel = NeuralProphet(n_lags=24, quantiles=[0.05, 0.95])\nmetrics = model.fit(train_df)\n```", "```py\n# Convert the DataFrame index to datetime,\n# removing timezone information\ntest_df['ds'] = test_df.index.to_pydatetime()\ntest_df['ds'] = test_df['ds'].apply(\n    lambda x: x.replace(tzinfo=None))\n# Rename the target variable for Prophet compatibility\ntest_df = test_df.rename(columns={'Global_active_power': 'y'})\n# Use the trained model to make predictions on the test set\npredictions_48h = model.predict(test_df)\n```", "```py\nTest SMAPE: 41.193985580947896\nTest WAPE: 0.35355667972102317\n```", "```py\nimport shap\n# Initialize a SHAP TreeExplainer with the trained model\nexplainer = shap.TreeExplainer(final_model)\n# Select features for SHAP analysis\nX = data_hr[[\n    'Global_active_power_lag1', 'Global_active_power_lag2',\n    'Global_active_power_lag3', 'Global_active_power_lag4',\n    'Global_active_power_lag5', 'Global_active_power_lag12',\n    'Global_active_power_lag24', 'Global_active_power_lag24x7'\n]]\n# Compute SHAP values for the selected features\nshap_values = explainer(X)\n# Generate and display a summary plot of the SHAP values\nshap.summary_plot(shap_values, X)\n```", "```py\n# Plot a SHAP waterfall plot for the first observation's SHAP values # to visualize the contribution of each feature\nshap.plots.waterfall(shap_values[0])\n```"]