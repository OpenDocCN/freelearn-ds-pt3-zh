<html><head></head><body>
		<div id="_idContainer051">
			<h1 id="_idParaDest-171" class="chapter-number"><a id="_idTextAnchor170"/>8</h1>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor171"/>Data Quality</h1>
			<p>The quality of decisions is tightly linked to the quality of data that supports them. As a result, the quality of data impacts every facet of the organization. It’s an (appropriately) bold statement that is often under-appreciated. From day-to-day operations to long-term strategic direction, having high-quality, reliable, and trustworthy information is not a “nice to have;” it’s a must-have for any company. Having quality data is critical for the success of any company of any size. How you operationalize ensuring you have quality data in your organization should vary based on the size and complexity of your organization, but the need for data quality is static. It’s at the very core of running <span class="No-Break">any business.</span></p>
			<p>In the ever-evolving landscape of data-driven decision-making, there exists a profound truth that we, as data custodians and stewards, must humbly acknowledge: the quality of our data is the cornerstone upon which all our analytical endeavors stand. In this chapter, I will delve deep into the vital realm of data quality, for it is here that we will confront the very essence of our responsibilities as guardians of the information our companies depend on. With a direct and unwavering focus, I aim to illuminate the significance of data quality in the context of data governance. In this chapter, I will uncover not only the intrinsic value of high-quality data but also the transformative potential it holds for organizations. I can’t emphasize enough the undeniable importance of ensuring that our data is not just data, but reliable, insightful, and, above <span class="No-Break">all, trustworthy.</span></p>
			<p>It’s quite easy to suggest that data quality is “everyone’s job” (and it is); however, ultimately, it is up to you to define what is needed, define and provide the implementation of data quality capabilities, and drive the importance of data quality for the company you work for. You will be held accountable for the quality of data enterprise-wide, regardless of how much of it is under your direct control. This may feel unfair, but as the data leader, the duty is yours. When a major data error is uncovered, whether you are directly involved or not, you will be brought in to help solve it. The burden will rest on your shoulders. Therefore, this chapter and the guidance herein are critical for <span class="No-Break">your success.</span></p>
			<p>We will cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Data <span class="No-Break">quality defined</span></li>
				<li><span class="No-Break">Core capabilities</span></li>
				<li>Building an optimal data <span class="No-Break">quality capability</span></li>
				<li>Setting up data quality <span class="No-Break">for success</span></li>
			</ul>
			<p>One of the hallmarks of the success of a Chief Data &amp; Analytics Officer is defining and implementing a trusted data capability. This will show the company not just that data is trustworthy, but provides transparency into <em class="italic">why</em> they can trust the data. When it comes to designing optimal capabilities for data <a id="_idIndexMarker473"/>quality, <strong class="bold">Trusted Data</strong> is one we will explore later in this chapter. First, let’s start with some <span class="No-Break">key definitions.</span></p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor172"/>Data quality defined</h1>
			<p>Data Quality is the data governance capability that refers to the degree to which data is accurate, reliable, and fit for its intended purpose in a given context. There are several dimensions by which data<a id="_idIndexMarker474"/> quality is assessed; these include completeness, accuracy, timeliness, consistency, and relevance. As mentioned previously, data quality is an essential capability for all organizations. Overall data quality across the company is critical, as is data quality in individual data elements, on key reports, as a part of operations, and for the overall functioning of the business. Data quality is the core of building trust in our information. Next, each data quality dimension is defined, along with a few examples to help contextualize what data quality is and how it may show up in your company. We will move quickly into the core capabilities needed to apply these core dimensions after <a id="_idIndexMarker475"/>we ground ourselves on <span class="No-Break">the basics:</span></p>
			<table id="table001-6" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Data </strong><span class="No-Break"><strong class="bold">Quality Dimension</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Definition</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Example(s)</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Accuracy</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Accuracy refers to the correctness of data. Accurate data is free from errors, and it reflects the real-world entities and events it is supposed to represent. Inaccurate data can lead to incorrect conclusions <span class="No-Break">and decisions.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>In a customer database, a person’s birthdate is recorded as January 15, 1980, instead of January <span class="No-Break">25, 1980.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Completeness</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Completeness refers to whether all the required data elements are present. Incomplete <a id="_idIndexMarker476"/>data can hinder analysis and lead to gaps <span class="No-Break">in understanding.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>An inventory database lacks records for certain products, leaving gaps in the list of <span class="No-Break">available items.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Consistency</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Consistency ensures that data is uniform and follows established standards. Inconsistent data can arise from variations in data entry, formatting, or terminology, leading to confusion and data <span class="No-Break">integration challenges.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>In a sales dataset, the currency is inconsistently recorded as “USD,” “US Dollars,” or “$,” making it difficult to aggregate sales <span class="No-Break">figures accurately.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Timeliness</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Timeliness pertains to how up-to-date the data is. Timely data is relevant for decision-making and analysis, while outdated data can lead to missed opportunities or <span class="No-Break">misinformed decisions.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>A financial report for the first quarter of the year is not updated until several months into the second quarter, making it less relevant <span class="No-Break">for decision-making.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Relevance</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Relevance is about whether the data is suitable for the specific task or analysis at hand. Irrelevant data can clutter datasets and make it harder to extract <span class="No-Break">meaningful insights.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>In a marketing campaign analysis, data on customer shoe sizes is included, even though it has no bearing on the <span class="No-Break">campaign’s effectiveness.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Validity</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Validity checks whether data conforms to predefined rules, constraints, or business logic. Valid <a id="_idIndexMarker477"/>data adheres to the defined criteria and ensures <span class="No-Break">data integrity.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>An email address field contains entries that do not follow a valid email format, such as <strong class="source-inline">user(at)example(dot)com</strong> instead <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">user@example.com</strong></span><span class="No-Break">.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Integrity</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Data integrity ensures that data remains accurate and consistent throughout its life cycle, preventing unauthorized changes <span class="No-Break">or corruption.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>A database administrator accidentally deletes or modifies records without proper authorization, resulting in data <span class="No-Break">integrity issues.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Trustworthiness</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Trustworthiness reflects the reliability and credibility of the data source. Data from reputable <a id="_idIndexMarker478"/>sources is more likely to be of <span class="No-Break">higher quality.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Data obtained from a well-established, government-regulated financial institution is considered more trustworthy compared to data from an anonymous <span class="No-Break">online source.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.1 – Data quality dimensions</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor173"/>Data Quality Strategy</h2>
			<p>A Data Quality Strategy is a<a id="_idIndexMarker479"/> foundational capability of any data quality program. The data quality strategy defines the company’s integrated, company-wide approach to achieve and maintain the quality level<a id="_idIndexMarker480"/> required to meet business goals. Set by the company’s Chief Data &amp; Analytics Office, the strategy should include the goals, key objectives, plans, and measures to improve and maintain data quality for the organization. While the strategy is set centrally, the strategy should clearly articulate the remit and responsibilities of the central team, as compared to business data stewards, technical data stewards, architects, data analysts, and <span class="No-Break">so on.</span></p>
			<p>The Data Quality Strategy should contain the following <span class="No-Break">core components:</span></p>
			<ul>
				<li><strong class="bold">Data quality objectives</strong>: Start by <a id="_idIndexMarker481"/>defining what the purpose of the data quality strategy is, what outcomes you will deliver through the strategy, and what success looks like, specifically for your company (for example, how data quality supports data strategy, including data <span class="No-Break">governance outcomes).</span></li>
				<li><strong class="bold">Assessment of current state</strong>: Tell the reader exactly how bad (or good) the company’s data really is. Be as specific as possible, and provide powerful examples that explain what the business impact is of current data quality issues <span class="No-Break">wherever possible.</span></li>
				<li><strong class="bold">Data quality standards</strong>: Articulate the criteria for trustworthy data. Your standards may include minimum rules for data quality dimensions, as well as what is expected to be<a id="_idIndexMarker482"/> done for each critical data asset. The <em class="italic">Core capabilities</em> section is a great place to start if you aren’t sure what <span class="No-Break">to include.</span></li>
				<li><strong class="bold">Implementation plan for data quality enablement</strong>: Define what processes and tooling are required to enable data quality for the company and how you and your team will implement the required <span class="No-Break">enablement mechanisms.</span></li>
				<li><strong class="bold">Remediation and issue management approach</strong>: Define how data quality will be measured, reported, <span class="No-Break">and remediated.</span></li>
				<li><strong class="bold">Data quality ownership and accountability</strong>: Clear articulation of which roles are necessary for effective data quality management and what is expected of <span class="No-Break">each role.</span></li>
			</ul>
			<p>For example, your objective might be “Establish an enterprise data quality framework that supports prioritization, governance, and oversight of data quality to improve transparency into critical data and improve trust in key reports,” whereas an example assessment approach may be something like “You may benchmark a system, a report, or a process to show the current quality level for the data.” The output of this strategy should be actionable and translated into a roadmap <span class="No-Break">for implementation.</span></p>
			<h3>Purpose</h3>
			<p>The purpose of a <strong class="bold">Data Quality Strategy</strong> is to drive clarity <a id="_idIndexMarker483"/>and alignment across your company regarding what data quality is, why it matters, and how the company will know it has been successful in driving trust into its information. The strategy is a great forcing function to drive conversation about what the gaps are today and how you will be able to measure success in the future. Most executives struggle to <a id="_idIndexMarker484"/>understand the <strong class="bold">return on investment</strong> (<strong class="bold">ROI</strong>) of a strategy. I recommend focusing on the business outcomes that you will drive as a <em class="italic">result</em> of the strategy, not the strategy itself. The strategy sets the North Star for this work, but it will not deliver the <span class="No-Break">results alone.</span></p>
			<p>You may wish to implement a <strong class="bold">Data Quality Standard</strong> as a follow-on to your Enterprise Data Governance Policy. The Data <a id="_idIndexMarker485"/>Quality Standard should include specific information regarding the Data Quality Strategy, codifying the roles and responsibilities for managing data quality as outlined in this chapter (and curated for the needs of your specific business), and provide<a id="_idIndexMarker486"/> further detail about how to comply with policy, through adoption of the capabilities set forth in the <em class="italic">Data quality </em><span class="No-Break"><em class="italic">enablement</em></span><span class="No-Break"> section.</span></p>
			<h3>Accountability</h3>
			<p>The Chief Data &amp; Analytics Officer’s organization owns the Data Quality Strategy for the company. Depending on your team’s size and scale, you may have someone who is fully dedicated to running data quality for<a id="_idIndexMarker487"/> the company, or you may have someone who runs all Data Governance capabilities; either is a great candidate to own and drive this for the Chief Data &amp; Analytics Office. If possible, establish a head of data quality who will be ultimately responsible for the delivery of the strategy and the implementation of the capabilities needed to drive the strategy for the company. Other interested parties should include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Business <span class="No-Break">data stewards</span></li>
				<li>Technical <span class="No-Break">data stewards</span></li>
				<li><span class="No-Break">Architects</span></li>
				<li><span class="No-Break">Data engineers</span></li>
				<li>Data analysts and <span class="No-Break">business analysts</span></li>
				<li>Executives who use and rely on quality data <span class="No-Break">for decision-making</span></li>
			</ul>
			<p class="callout-heading">“Are we there yet?”</p>
			<p class="callout">Inevitably, when you are setting out to implement a data quality program at your company, you will run into a stakeholder who will ask something along the lines of “How long until we <span class="No-Break">are done?”</span></p>
			<p class="callout">Data quality is like exercise. When you’re out of shape, you need to pick a plan and do the work to get into shape, but then you also have to maintain it. Data quality is the exercise plan for your company. You will need to define a plan, create a get-well plan (implementation), and support data quality on an ongoing basis. The work is never “finished,” and by failing to<a id="_idIndexMarker488"/> maintain it, your company will quickly <span class="No-Break">regress backward.</span></p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor174"/>Data quality enablement</h2>
			<p>Data quality enablement is<a id="_idIndexMarker489"/> the core of any data quality program. Data quality enablement provides the company with centralized and standardized capabilities that serve the company’s data quality needs. Enablement usually includes people to support the enterprise needs, processes to power the data quality needs of the company, and technology, usually in the form of tooling, to support managing data <span class="No-Break">quality effectively.</span></p>
			<h3>Purpose</h3>
			<p>By providing central capabilities for<a id="_idIndexMarker490"/> the company, you can ensure the company has consistent views into the quality of the data, a common process to remediate issues defined, a way to prioritize, and appropriate communication so that individuals who need to know about data quality issues know about them when they <span class="No-Break">matter most.</span></p>
			<p>It boils down to transparent information about the quality of data, common tooling, which focuses budget and costs on shared tooling (versus competing tooling), and standardized processes to enable people to maximize their time when it comes to <span class="No-Break">data quality.</span></p>
			<p>There’s also the key vector of reusability. For example, data quality enablement can ensure we are measuring critical data assets once and using them over and over again versus measuring data quality every time a data asset is used. The enablement team or function can provide a certification process to critical data assets so that users can see which data assets have been evaluated for data quality, passed expectations, and are reliable for use. This builds trust and reduces time spent searching for and measuring the reliability of common-use data assets across an organization. The bigger the company is, the more likely we are to be using common data assets without realizing it, creating the risk of mass redundancy. Imagine the time that can be saved when we can transparently see what critical assets we have (see <em class="italic">data marketplace</em> in the chapters on metadata) and know that the data is reliable (that’s <span class="No-Break">data quality!).</span></p>
			<h3>Accountability</h3>
			<p>As with a Data Quality Strategy and data quality standards, data quality enablement should be a function driven by the Chief <a id="_idIndexMarker491"/>Data &amp; Analytics Office. The central function is best positioned to create the capabilities necessary for the company’s business and technical data stewards to drive data quality efforts for the data they are responsible for. Usually, this enablement team sits either within the data governance sub-team or with a tooling team. Given the close marriage of metadata management and data quality, it’s often <span class="No-Break">bucketed together.</span></p>
			<p>I have seen very effective programs where the data governance team leads the data quality strategy, data quality standards, and the data quality program enterprise-wide, while the data solutions or data tooling team provides the technology solutions to enable these capabilities. This can work very well if you have a large company, as it aligns the business and technical skills with the solutions necessary to deliver excellence from your <span class="No-Break">data organization.</span></p>
			<p class="callout-heading">Spreadsheets don’t scale</p>
			<p class="callout">When it comes to deploying great data quality solutions, perfect can’t be the enemy of good. However, there is a balance of scalability that needs to <span class="No-Break">be considered.</span></p>
			<p class="callout">At one of my previous companies, we were enabling an end-to-end view of several key reports to manage the business at the executive level. Our objective was seemingly simple: Is this report trustworthy for our executives to make business decisions? Operationalizing this question, however, was much harder than <span class="No-Break">we expected.</span></p>
			<p class="callout">We had to deconstruct the key metrics being used. We did not have the technical solutions to make this efficient or repeatable, but we moved forward anyway. The problem? The content of the reports changed frequently. Instead of looking at the sources and measuring the totality of the source at a feed or system level, we measured the individual metrics. However, as the metrics evolved or new calculations were introduced, we had to go back to square one to validate the data. If we had taken a more holistic approach and used technology to enable this work, we wouldn’t have been documenting data quality in spreadsheets (defining critical data elements, mapping to source systems, and manually measuring quality). Each change required a significant amount of manual work. We <span class="No-Break">couldn’t scale.</span></p>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor175"/>The value of measuring data quality</h1>
			<p>I have no doubt you’ve experienced this if you’ve been in a data role for more than a day: Someone reaches out to you, convinced <a id="_idIndexMarker492"/>that their data is “wrong.” They want it fixed. Ultimately, what they are saying to you is: I don’t trust <span class="No-Break">this data.</span></p>
			<p>When we think about measuring the value of data quality, we have to challenge ourselves to think more broadly than one might expect to. When we are thinking about how to measure the value of data quality, we are really asking: How do I measure the value of trust? What does it mean to my company to be able to trust the information we use for decision-making, every single day? What is it worth to us to know we can trust the information being used to run our business? To drive value for our customers? To operate effectively? If you could place a dollar value on that, what would <span class="No-Break">it be?</span></p>
			<p>The value of trust in our data is highly dependent on your business. Let’s start with a <span class="No-Break">real-life example.</span></p>
			<p class="callout-heading">Building trust in data quality</p>
			<p class="callout">Marketing is a function in a company that many do not necessarily correlate with data, but in my experience, the best marketing functions are highly data-driven. I worked with a marketing department that was<a id="_idIndexMarker493"/> struggling to understand which individuals they could market to. They had a plethora of lead data, but they were having a hard time determining <span class="No-Break">the following:</span></p>
			<p class="callout">Which leads had accurate email addresses and <span class="No-Break">quality data</span></p>
			<p class="callout">If the leads had consented to marketing holistically or for only specific products (further complicated <span class="No-Break">by M&amp;A)</span></p>
			<p class="callout">If the most recent consent was valid for their country’s regulations (for example, how long is the consent <span class="No-Break">valid for?)</span></p>
			<p class="callout">Ultimately, the Chief Marketing Officer needed to measure how much the marketing team was contributing to sales, but without quality lead information that they could trust, measuring the impact of marketing on sales was very difficult. They believed their team was contributing more than they were able to measure, but they couldn’t prove that <span class="No-Break">with data.</span></p>
			<p class="callout">At the beginning of the process to improve the trustworthiness of the marketing lead data, we baselined the quality of the data by completing basic data profiling based on what was determined to be “key fields.” We defined key fields as the fields associated with a contact record that were required to be able to market to that contact, which included: first name, last name, email address, company, and valid consent. Our initial profiling put the quality of these key fields at about 43%, meaning only about 43% of the contacts had reliable data in these <span class="No-Break">five fields.</span></p>
			<p class="callout">We deployed an enrichment service to be able to improve the reliability of the contact data. It allowed us to overwrite<a id="_idIndexMarker494"/> low-quality data (such as initials in fields for first name and last name, and fill in blanks), and to validate email addresses with a third-party service. This improved leads where the address had been typed incorrectly. Therefore, we were able to take instances where we had consent but an invalid email address and use the email address. Overall, this effort increased our marketable contacts from 43% to over 85%. As a result, the CMO was able to demonstrate that their contact data was of higher value and was directly tracible to sales at a higher rate. It was also a measure of their success as a CMO, which led to their ultimate success as a leader. This individual became a champion of our data and analytics team and supported further transformation work <span class="No-Break">we led.</span></p>
			<p>Ultimately, the value of having trustworthy data wasn’t just a simple calculation. But for the Marketing division, it was a combination of cost savings from being sure they could market to real leads, the confidence that they had consent to market to the quality leads they had, <em class="italic">AND</em> that they <a id="_idIndexMarker495"/>were compliant with laws. They also needed to measure the value of revenue contribution, generated from the quality leads. In short, it was a combination of cost savings + risk avoidance + fine and penalty avoidance + contribution to revenue. This calculation will vary based on the use case, based on division, and based on your company. The important takeaway here is that you measure the value in <span class="No-Break">business terms.</span></p>
			<p>Most data professionals measure the improvement in data quality based on the percentage of improvement or reduction in errors but don’t take it back to the business context. That’s where you, as a data leader, can demonstrate your value to your company, by showing them how data investments have a real business impact in dollars (increase in revenue and increase in savings), time, and risk avoidance. That’s how you demonstrate trust in data and return on your data investments. If you take nothing else away from this book, it should <span class="No-Break">be this.</span></p>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor176"/>Core capabilities</h1>
			<p>Implementing data quality capabilities can significantly enhance the integrity, accuracy, and reliability of data, making it a trustworthy asset for decision-making, analytics, and business operations. It’s worth noting<a id="_idIndexMarker496"/> that roles can vary based on the size and structure of the organization. In smaller companies, a single individual might handle multiple responsibilities, while larger organizations might have entire teams dedicated to specific tasks. Collaboration and clear communication among these roles are crucial to ensure cohesive and effective data <span class="No-Break">quality management.</span></p>
			<p>Achieving maturity in data quality management indicates that an organization has not only implemented the core capabilities but also refined, optimized, and integrated them into <span class="No-Break">daily operations.</span></p>
			<p>Measuring the value of each data quality capability ensures that an organization can justify its investments in data quality management and recognize areas of improvement. For each capability, the value can further be translated into tangible benefits such as monetary savings and increased revenue or intangible benefits such as enhanced stakeholder trust, improved brand reputation, and organizational agility. Converting the value of data quality capabilities into dollars or time requires understanding the specific financial and operational context of an organization. To truly get an accurate dollar or time value for each capability, an organization would need to conduct detailed assessments, considering its operational costs, business context, and the consequences of data issues. The framework for calculating value should be defined by the data office; the inputs should be provided by the business. We won’t go into specifics here, but use the framework defined previously in <em class="italic">The value of measuring data quality</em> section to apply it to <span class="No-Break">your organization.</span></p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor177"/>Data profiling</h2>
			<p>Building an understanding of the <a id="_idIndexMarker497"/>existing state of data, including inconsistencies, anomalies, and patterns is delivered through a foundational capability called <strong class="bold">data profiling</strong>. Data profiling is<a id="_idIndexMarker498"/> usually performed by business data stewards and technical data stewards together. The business data steward can provide expertise on the business meaning and usage of the data, while the technical data steward can provide expertise on technical aspects of <span class="No-Break">the data:</span></p>
			<ul>
				<li><strong class="bold">Business Data Stewards</strong> generally have a deep understanding of the business data they are stewarding/responsible for, which includes understanding (or defining) the <a id="_idIndexMarker499"/>meaning, how it should be used, and the quality requirements. They are best <a id="_idIndexMarker500"/>positioned to identify data quality issues and define the business impact of the issues identified. Business data stewards should do <span class="No-Break">the following:</span><ul><li>Identify business requirements for data quality for their <span class="No-Break">data elements</span></li><li>Ensure data quality profiling is implemented for their <span class="No-Break">data elements</span></li><li>Define business rules that govern <span class="No-Break">the data</span></li><li>Determine the frequency of data profiling (consider how frequently the data is changing, the volume of data, and the volume of issues in the data elements they <span class="No-Break">are stewarding)</span></li><li>Review and analyze data profiling reports to identify and understand data quality issues, and report relevant issues to users of <span class="No-Break">the data</span></li><li>Work with data stewards (other business data stewards and technical data stewards) to resolve data <span class="No-Break">quality issues</span></li><li>Ensure profiling is complete, measured appropriately, monitored, and published transparently <span class="No-Break">for users</span></li></ul></li>
				<li><strong class="bold">Technical Data Stewards</strong> generally have a technical understanding of the data, such as structures, formats, and systems. Technical data stewards are responsible for assisting the business data steward in identifying the physical locations of physical data elements that must be measured in support of the business data stewards’ needs. They use this technical understanding to identify and troubleshoot data issues at the technical level in support of business data stewards. Technical data stewards should do <span class="No-Break">the following:</span><ul><li>Design and implement data <span class="No-Break">profiling jobs</span></li><li>Configure and use data <span class="No-Break">profiling tools</span></li><li>Analyze data profiling reports to identify and troubleshoot data quality issues in partnership with business <span class="No-Break">data stewards</span></li><li>Work with other technical data stewards to identify up and downstream impacts of data quality issues and <span class="No-Break">remediation required</span></li></ul></li>
				<li><strong class="bold">Data Engineers</strong> are responsible for the technical implementation of profiling requirements within systems to provide business data stewards, technical data stewards, and ultimately, users of the data with transparent information regarding the state of <span class="No-Break">the data.</span></li>
				<li><strong class="bold">Users</strong> (data analysts, business <a id="_idIndexMarker501"/>analysts, executives) are responsible for understanding the results of the data profiling efforts and taking into consideration any issues identified through profiling when using the <a id="_idIndexMarker502"/>data for <span class="No-Break">business needs.</span></li>
			</ul>
			<p>The figure below illustrates the process flow of business data and technical data <span class="No-Break">stewards’ partnership:</span></p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B18846_08_01.jpg" alt="Figure 8.1 – Simple process flow of business data and technical data stewards’ partnership"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Simple process flow of business data and technical data stewards’ partnership</p>
			<p>There are a few simple measures you can deploy within your team to show the progress your team is making in improving the quality of your company’s data. Basic measures include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Percentage of datasets profiled per domain <span class="No-Break">or system</span></li>
				<li>Reduction in surprises or issues when using data for analytics or operations (for example, how many issues you started with compared to how many you <span class="No-Break">currently have)</span></li>
				<li>Cost savings from avoiding<a id="_idIndexMarker503"/> incorrect data-driven decisions or regulatory reporting errors (for example, did you catch a material <a id="_idIndexMarker504"/>error that would cause a <span class="No-Break">financial misstatement?)</span></li>
				<li>Reduction in hours spent identifying and addressing data issues manually by adopting automated data <span class="No-Break">profiling solutions</span></li>
			</ul>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor178"/>Data cleansing</h2>
			<p>The process of cleaning your data is<a id="_idIndexMarker505"/> referred to as <strong class="bold">data cleansing</strong>. Data cleansing identifies<a id="_idIndexMarker506"/> and rectifies errors, inconsistencies, and redundancies in datasets. In short, data cleansing corrects your data to meet the expectations of your users. Data cleansing should be conducted as close to the source as possible so that correct and trustworthy data flows through the organization already corrected. Wherever data is corrected, the business data steward and technical data steward have a shared responsibility to ensure users are aware of corrections (business data steward) and the corrected data is sent up and downstream (technical <span class="No-Break">data steward).</span></p>
			<p class="callout-heading">When corrected data looks wrong</p>
			<p class="callout">During a system implementation, my team improved the quality of the data for a specific dataset. The users of the data reported data quality issues. Why did <span class="No-Break">this happen?</span></p>
			<p class="callout">We hadn’t appropriately notified the users of the data that they should expect changes and what the changes would look like. Thus, when the data changed, although improved, it appeared different from what was expected by the user, thus leading them to believe it <span class="No-Break">was wrong.</span></p>
			<p>Much like with data profiling, the partnership between the business data steward and technical data steward is critical to the success of this capability. The business data steward should identify which data elements should be cleansed (as every data element is not of equal importance, and there is a cost to cleansing) and work with the users of the data to determine the importance and priority of cleansing capabilities. The technical data steward is responsible for enabling cleansing activities in the system(s) and ensuring up- and downstream system(s) are aware of the cleansing being performed to preemptively alert consumers that the data is expected <span class="No-Break">to change.</span></p>
			<p>Because not every data element in<a id="_idIndexMarker507"/> a company can be cleansed, it is important to define what the strategy will be for data cleansing and which data <a id="_idIndexMarker508"/>elements are most critical for cleansing and to develop a prioritization mechanism to determine which data elements should be prioritized over others, and why. You may want to use your enterprise data committee or data governance council to support and approve your <span class="No-Break">prioritization methodology.</span></p>
			<p>Advanced data cleansing functions may offer a cleansing service centrally to support the company. One such example would be address standardization for contacts. Contact data often sits across an organization and may include marketing, sales, and customer service data. A central data cleansing service could be created to provide address standardization and enrichment for these divisions as a shared capability so that the company is paying for these services once and providing them out to multiple departments <span class="No-Break">for value.</span></p>
			<p>Much like data profiling, data cleansing value is measured in very similar ways: in terms of business value, dollars generated or saved, and time reduction. Some examples include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Percentage of reduction in data errors post-cleansing (or count <span class="No-Break">versus percentage)</span></li>
				<li>Increased confidence in the analysis used in decision-making (<em class="italic">note</em>: it is important to align on how to measure confidence in terms of time <span class="No-Break">or money)</span></li>
				<li>Money saved from preventing decisions based on <span class="No-Break">erroneous data</span></li>
				<li>Reduction in hours spent manually <span class="No-Break">correcting data</span></li>
				<li>Number of automated corrections made/number of <span class="No-Break">errors identified</span></li>
				<li>Number of manual interventions required/number of <span class="No-Break">errors identified</span></li>
			</ul>
			<p>Over time, manual interventions should reduce, and the quality of data should uplift. You should be able to identify issues more easily as the overall quality of data improves. This should free up resources to <a id="_idIndexMarker509"/>dig into more critical issues and<a id="_idIndexMarker510"/> focus on business impact versus manually <span class="No-Break">correcting data.</span></p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor179"/>Data validation and standardization</h2>
			<p>The process to determine if the data is <a id="_idIndexMarker511"/>valid is called <strong class="bold">data validation</strong>. Data validation determines what values are appropriate for a given field. For example, it may be a range of values (0-99) or a specific list of values (for example, <strong class="bold">North American Industry Classification System</strong> (<strong class="bold">NAICS</strong>) codes) that the field is permitted to contain. Validation may also include<a id="_idIndexMarker512"/> whether a field can have characters or numerical values. Validation requires fields to conform to <a id="_idIndexMarker513"/>specific <span class="No-Break">expected values.</span></p>
			<p>Similarly, <strong class="bold">Data Standardization</strong> harmonizes data formats, units, and definitions so that there is consistency across the <a id="_idIndexMarker514"/>organization. Often defined alongside validation, these two capabilities ensure the data is formatted and consistent as expected. Standardization efforts ensure that the same data shows up consistently across systems and, at its most mature state, at the <span class="No-Break">reporting level.</span></p>
			<p>Much like data profiling and data cleansing, business data stewards and technical data stewards should work together to ensure these capabilities are <span class="No-Break">adopted holistically:</span></p>
			<ul>
				<li><strong class="bold">Business Data Stewards</strong> should define values that are appropriate for a field that meets the needs of the users of the data and are enabled across <span class="No-Break">the company</span></li>
				<li><strong class="bold">Technical Data Stewards</strong> should work with engineers to implement validation rules in the system(s) and ensure that the validation is consistent (<span class="No-Break">standardized) enterprise-wide</span></li>
			</ul>
			<p>Measuring the value of these two capabilities can be kept rather simple. A few easy metrics of success include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Rate of data entry errors (should be near zero <span class="No-Break">over time)</span></li>
				<li>Number/rate of <span class="No-Break">validation failures</span></li>
				<li>Reduction in operational disruptions and reduced data <span class="No-Break">remediation/correction costs</span></li>
				<li>Savings from reduced data correction errors post integrations <span class="No-Break">or ingestions</span></li>
				<li>Faster data processing <a id="_idIndexMarker515"/>speeds due to a reduction in <span class="No-Break">validation failures</span></li>
				<li>Automated validation checks at all data <span class="No-Break">ingestion points</span></li>
			</ul>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor180"/>Data enrichment</h2>
			<p><strong class="bold">Data Enrichment</strong> is the process of <a id="_idIndexMarker516"/>adding new and supplemental<a id="_idIndexMarker517"/> information to existing datasets. This can be done by combining first-party data (your company’s data) from internal sources with disparate data from other internal systems or third-party data (data from external sources). There are several ways to enrich data. Some common methods include adding <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Demographic data</strong>: This could include information such as age, gender, income, location, <span class="No-Break">and education.</span></li>
				<li><strong class="bold">Firmographic data</strong>: This could include information such as company size, industry, <span class="No-Break">and revenue.</span></li>
				<li><strong class="bold">Behavioral data</strong>: This could include information such as website visits, product purchases, and social <span class="No-Break">media engagement.</span></li>
				<li><strong class="bold">Contextual data</strong>: This could include information such as time of day, location, and <span class="No-Break">device type.</span></li>
			</ul>
			<p>Data enrichment can be used to improve the accuracy, completeness, and relevance of data. This can be beneficial for a variety of purposes, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Improving customer segmentation</strong>: Enriched data can be used to create more accurate and detailed customer segments. This can help businesses to better understand their customers’ needs and preferences and target them with more relevant marketing and <span class="No-Break">sales messages.</span></li>
				<li><strong class="bold">Personalizing customer experiences</strong>: Enriched data can be used to personalize the customer experience across all channels. For example, businesses can use enriched data to recommend relevant products or services to customers or to provide them<a id="_idIndexMarker518"/> with more <span class="No-Break">personalized support.</span></li>
				<li><strong class="bold">Improving fraud detection</strong>: Enriched data can be used to identify and prevent fraudulent activity. For <a id="_idIndexMarker519"/>example, businesses can use enriched data to verify the identities of new customers or to detect <span class="No-Break">fraudulent transactions.</span></li>
				<li><strong class="bold">Making better business decisions</strong>: Enriched data can be used to make better business decisions across all areas of the organization. For example, businesses can use enriched data to identify new market opportunities, optimize their pricing strategies, and improve their product <span class="No-Break">development process.</span></li>
			</ul>
			<p>Data enrichment can be a complex and time-consuming process, but it can be a valuable investment for businesses of all sizes. By enriching their data, businesses can gain a deeper understanding of their customers, improve their marketing and sales efforts, and make better business decisions. Simple value measures include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Increase in data attributes or <span class="No-Break">features post-enrichment</span></li>
				<li>Enhanced insights and better predictive <span class="No-Break">modeling capabilities</span></li>
				<li>Increased revenue from better-targeted marketing or <span class="No-Break">analytics insights</span></li>
				<li>Reduction in manual <span class="No-Break">data-gathering processes</span></li>
			</ul>
			<p>More specifically, for the aforementioned examples, you can measure the value of these enrichment services with the following <span class="No-Break">business metrics:</span></p>
			<ul>
				<li><strong class="bold">Improved customer segmentation</strong>: You can track the accuracy of your customer segmentation by comparing the predicted segment membership to the actual segment membership. You can also track the performance of your marketing campaigns that are targeted to specific <span class="No-Break">customer segments.</span></li>
				<li><strong class="bold">Personalized customer experiences</strong>: You can track customer satisfaction and retention rates to measure the impact of personalized customer experiences. You can also track<a id="_idIndexMarker520"/> metrics such as <strong class="bold">click-through rate</strong> (<strong class="bold">CTR</strong>) and conversion rate to measure the effectiveness of your personalized <span class="No-Break">marketing campaigns.</span></li>
				<li><strong class="bold">Improved fraud detection</strong>: You can track the number of fraudulent transactions that are detected and prevented as a result of data enrichment. You can also track cost savings that are achieved by preventing <span class="No-Break">fraudulent transactions.</span></li>
				<li><strong class="bold">Better business decisions</strong>: You <a id="_idIndexMarker521"/>can track the number of business decisions that are made using enriched data. You can also <a id="_idIndexMarker522"/>track the financial impact of these decisions, such as increased revenue or <span class="No-Break">reduced costs.</span></li>
			</ul>
			<p>These can be difficult to measure concretely. Here’s a specific example to show how you can break down the measure <span class="No-Break">more specifically:</span></p>
			<ul>
				<li><strong class="bold">Business goal</strong>: Improve the accuracy of <span class="No-Break">customer segmentation.</span></li>
				<li><strong class="bold">Metric</strong>: Percentage of customers that are correctly assigned to their <span class="No-Break">customer segment.</span></li>
				<li><strong class="bold">Baseline</strong>: 70% of customers are correctly assigned to their <span class="No-Break">customer segment.</span></li>
				<li><strong class="bold">After data enrichment</strong>: 85% of customers are correctly assigned to their <span class="No-Break">customer segment.</span></li>
				<li><strong class="bold">Result</strong>: Customer segmentation accuracy has improved <span class="No-Break">by 15%.</span></li>
			</ul>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor181"/>Feedback loops, exception handling, and issue remediation</h2>
			<p>While users of data are engaged throughout the development of your data quality Program, the phase they are most engaged is once the program is up and running. As you operationalize business rules by configuring them into the systems through profiling, validation, enrichment, and so on, the results of the data quality program start to shine. The first set of results published is usually a bit jarring for business users to process. If data quality has never been an area of focus, the results are often alarming. As the data professional, your job is to help users understand that the first baseline of results is just that—a baseline. The initial results are used to show where we <span class="No-Break">are today.</span></p>
			<h3>Feedback loops</h3>
			<p>As you start to work with users of the data and the various stewards, your role is to ensure the data quality program has appropriate <strong class="bold">feedback loops</strong> established for data stewards and your team so that<a id="_idIndexMarker523"/> you can improve the enablement of the data quality program and support the stewards as they work to improve the trust in their data. Feedback<a id="_idIndexMarker524"/> loops are required in a few different places in your program, but first, between the users and the business data stewards. You may want to help guide your business data stewards on how to set up appropriate forums for feedback about their data quality. They may find value in starting a <strong class="bold">stewardship council</strong> by data domain to help provide feedback and prioritization for where data quality remediation <a id="_idIndexMarker525"/>needs to be <span class="No-Break">focused first.</span></p>
			<p>You should also provide a forum for business data stewards and data domain executives to bring forward significant and cross-functional data quality issues for enterprise prioritization, where needed. The enterprise data committee and data governance council are great options for the escalation of these types of complex data issues that require enterprise funding, prioritization, or awareness. Let’s start by exploring how a data issue would be escalated to such <span class="No-Break">a forum.</span></p>
			<p>Conversely, the business data steward may have identified a data quality issue that the user ultimately decides is not significant and isn’t worthy of remediation efforts. It’s important to document the process by which data quality issues are prioritized and dispositioned. You don’t want to be in a situation where we’re not sure if a data quality issue is a true issue or perhaps just a data defect that is not worth the time or energy to remediate. I recommend creating a disposition log so that you can track issues you identified and who signed off on the disposition to have a record to back your <span class="No-Break">decision-making process.</span></p>
			<h3>Exception handing</h3>
			<p>There may be times in the data life cycle <a id="_idIndexMarker526"/>when exceptions are made regarding the data quality process. For example, a data lineage process may be defined for a specific data metric, and financial reporting purposes, the data must be at a specific quality level. However, for ad hoc analytical purposes, we<a id="_idIndexMarker527"/> may accept a lower level of quality to allow for speed to that insight. It’s important to take the use of data into account when defining exceptions. In this example, the user may need a one-time pull of revenue data for a reasonableness check on progress to quarterly revenue targets. This extraction may not need to be perfect. The user should be informed that the data extract has not been through all the normal control processes, but if they understand this, an exception process may be very appropriate, especially <span class="No-Break">if time-sensitive.</span></p>
			<p>The importance of exception<a id="_idIndexMarker528"/> handling is to ensure that exceptions do not become the rule. You should define appropriate times when exceptions<a id="_idIndexMarker529"/> may be made. If you find that a specific request is being made somewhat regularly, consider whether that is truly an exception or is simply circumventing the expected flow <span class="No-Break">of information.</span></p>
			<table id="table002-4" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Helpful Hint</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Let’s take a lesson from our IT counterparts and develop a severity methodology. In the IT world, IT issues are classified as <em class="italic">Sev1</em>, <em class="italic">Sev2</em>, and so on, based on a set of criteria. Based on that severity rating, a different process is followed to drive the sense <span class="No-Break">of urgency.</span></p>
							<p>For data issues, we could follow the same methodology. If the issue is material or causes a system outage, perhaps that should be a <em class="italic">Sev1</em> and follow the highest level of escalation and remediation, versus a minor data issue, which can be remediated in weeks or months without much impact on the business and maybe <span class="No-Break">a </span><span class="No-Break"><em class="italic">Sev3</em></span><span class="No-Break">.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>At a data element level, you might find that you have anomalies in the data, and that may also be considered an exception. You should work with the business data steward to determine if the exception is truly an exception, and why it is an exception. Did a control fail? Did the data get corrupted in transit? Did you receive a bad file from a third party? There are a number of reasons that data could come through with some kind of exception. The important thing is that you follow a standardized process to handle exceptions and follow them to closure. Here are some examples of how to manage specific types of data <span class="No-Break">quality exceptions:</span></p>
			<ul>
				<li><strong class="bold">Missing data</strong>: If you have a large number<a id="_idIndexMarker530"/> of missing values in a dataset, you may want to consider imputing the missing values. This can be done using a variety of methods, such as using the average or median value of the <a id="_idIndexMarker531"/>column or using a <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) model to predict <span class="No-Break">missing values.</span></li>
				<li><strong class="bold">Inconsistent data</strong>: If you have inconsistent data, you may need to manually correct the data or update the data quality rules to allow for inconsistencies. For example, if you have a dataset with different formats for the date column, you may need to normalize the date format before using <span class="No-Break">the data.</span></li>
				<li><strong class="bold">Duplicate data</strong>: If you have duplicate <a id="_idIndexMarker532"/>data, you may need to remove duplicates or merge them into a single record. For example, if you have a dataset with two customer records with the same name and address, you may need to merge the two records into a <span class="No-Break">single record.</span></li>
			</ul>
			<p>Through this process, you might<a id="_idIndexMarker533"/> determine that the exception isn’t an exception at all and that it is actually a data issue and needs to follow the issue <span class="No-Break">remediation process.</span></p>
			<h3>Issue remediation</h3>
			<p>When a material data issue is identified, the business data steward should review and develop a comprehensive <a id="_idIndexMarker534"/>understanding of the issue and the source <a id="_idIndexMarker535"/>of the issue and identify options for remediating the issue. If the issue is significant, spans multiple data domains, or has a material impact on the company, it should get escalated to the appropriate forum. A simplified issue remediation process may look like <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B18846_08_02.jpg" alt="Figure 8.2 – Sample issue remediation process"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Sample issue remediation process</p>
			<p>What’s most important is to define an issue remediation process. This will also enable you to define exceptions and feedback<a id="_idIndexMarker536"/> loops that make sense for <span class="No-Break">your company.</span></p>
			<p>In my opinion, the person closest to the data is the one most likely to understand the data the best and should be the<a id="_idIndexMarker537"/> one presenting the issue in these forums. That may mean that someone at an analyst level is presenting to senior executives. As the data leader, you should support the person with the most information and help them to shape the presentation for the right audience, which may result in a variance from the aforementioned <span class="No-Break">standardized process.</span></p>
			<p>There are a number of great ways to measure the value of feedback loops, exception handling, and issue remediation. One consideration: At the earliest stages of data quality management, you will have a surge in issues. I would expect that to normalize over 12-18 months, but it may be shorter or longer depending on the amount of time and resources applied to data quality measurement and remediation efforts. Either way, the following measures are a great way to show the value of your feedback loops, exception-handling processes, and issue <span class="No-Break">remediation efforts:</span></p>
			<ul>
				<li>Number of feedback instances leading <span class="No-Break">to improvements</span></li>
				<li>Increase in user <a id="_idIndexMarker538"/>satisfaction (similar to <strong class="bold">net promoter </strong><span class="No-Break"><strong class="bold">score</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">NPS</strong></span><span class="No-Break">))</span></li>
				<li>Efficiency gains from remediation efforts (<span class="No-Break">time, money)</span></li>
				<li>Reduced iterative cycles in <span class="No-Break">data processing</span></li>
				<li>Average time to resolve a <span class="No-Break">data exception</span></li>
				<li>Average time to remediate <span class="No-Break">an issue</span></li>
				<li>Number of <span class="No-Break">open issues</span></li>
			</ul>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor182"/>Building an optimal data quality capability</h1>
			<p>A mature data-driven company not only <a id="_idIndexMarker539"/>implements the individual capabilities from the preceding section but also integrates them in a holistic manner, ensuring that data quality is embedded in the organization’s operations and overall culture—a culture where trusted information is valued and expected. This state, where trusted information is the baseline and anything else is rejected, includes the expectation that the quality of the data used in reporting, operations, and analytics is measured, monitored, and reported for <em class="italic">all</em> in-scope data elements, assets, <span class="No-Break">and integrations.</span></p>
			<p>What does it mean to be in scope? It does <a id="_idIndexMarker540"/>not mean that every single data element, every single report or dashboard, or every single integration is measured, monitored, and reported. That would be just as irrational as measuring, monitoring, or reporting nothing at all (for the opposite reasons). Rather, optimal data quality requires thoughtful, intentional scoping of the right data elements, the right reports, and the right integrations to ensure that the company’s assets (money, resources, time) are applied judiciously across the company’s most important and <span class="No-Break">relevant data.</span></p>
			<p>Understanding the quality of data and being able to have a defendable stance when it comes to “Can I trust it?” is key for any user of the data, or information, that’s leveraged for decision-making. Establishing a data quality capability enables the CDO, and their teams, to stand behind their data and be able to defend the quality of <span class="No-Break">the information.</span></p>
			<p>When we consider what an optimal data quality capability looks like, it comes down to a few <span class="No-Break">key steps:</span></p>
			<ol>
				<li>Identifying what data is critical, leveraging a <span class="No-Break">prioritization framework</span></li>
				<li>Profiling the data to deliver insights about the <span class="No-Break">underlying data</span></li>
				<li>Partnering with users to define appropriate business rules for <span class="No-Break">the data</span></li>
				<li>Writing and testing rules on <span class="No-Break">data elements</span></li>
				<li>Promoting rules to production to generate insights into the quality of <span class="No-Break">the data</span></li>
				<li>Monitoring and reporting on the quality of the data to business data stewards and users of <span class="No-Break">the data</span></li>
				<li>Remediating appropriate issues, leveraging feedback loops, exception management, and issue management processes, leveraging councils, stewardship groups, and data committees <span class="No-Break">where appropriate</span></li>
			</ol>
			<p>The flow of this process could look something <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B18846_08_03.jpg" alt="Figure 8.3 – Optimal DQ Monitoring from onboarding to reporting"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Optimal DQ Monitoring from onboarding to reporting</p>
			<p>The role of data stewards (both business and technical) cannot be overstated. They are the critical success factor in an optimal <a id="_idIndexMarker541"/>data quality capability. Without high-quality stewards who interact with users of information and have an intimate understanding of the data, its use, and the challenges, data quality will be a mere check-the-box activity. When stewardship is strong, the company’s data will be highly trusted because it will be cared for exceptionally well. Therefore, ensuring your data stewards have accepted responsibility for data quality is of the utmost importance for the success of your <span class="No-Break">data program.</span></p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor183"/>Certified data</h2>
			<p>As you are standing up a strong data quality program, you may want to consider enabling a <strong class="bold">certified data</strong> process. When <a id="_idIndexMarker542"/>coupled with metadata and data lineage, data quality can lead you to deploy a data certification<a id="_idIndexMarker543"/> process that not only enables the reusability of common data elements or curated datasets but also increases the value of the data <span class="No-Break">quality work.</span></p>
			<p>To start, identify what data element or dataset is used often across the organization. You might want to start with a metric that is used for operational and reporting purposes (such as Customer Count). Average Customer Count is a good example because it is seemingly simple but often controversial. To certify Customer Count for the company, you first need to define who is the business data steward for Average Customer Count. This individual will work with relevant stakeholders to align on the definition of Customer Count. For the purposes of this example, let’s assume the definition of a customer is: “An individual or company that has purchased one or more products in the last 12 months.” To calculate the average, we will need to pull information from <span class="No-Break">several systems.</span></p>
			<p>The next step is to align on the calculation of the term “Average Customer Count” and the source(s) for the data. For this example, let’s assume the calculation is as follows (as <span class="No-Break">of 1/1/2023):</span></p>
			<p><em class="italic">Number of customers = Total number of transactions in the last 12 months / Average number of transactions </em><span class="No-Break"><em class="italic">per customer</em></span></p>
			<p>The average number of transactions per customer can be calculated by dividing the total number of transactions in a given period by the number of customers in that period. For example, if a company has 100,000 transactions in the last 12 months and an average of 2 transactions per customer, then the number of customers would be <span class="No-Break">the following:</span></p>
			<p><em class="italic">Number of customers = 100,000 transactions / 2 transactions per customer = </em><span class="No-Break"><em class="italic">50,000 customers</em></span></p>
			<p>This formula can be used for any company, regardless of industry or size. However, it is important to note that the accuracy of the calculation will depend on the quality of the data used. For example, if the company does not have a good system for tracking customer transactions, then the results may not <span class="No-Break">be accurate.</span></p>
			<p>Here is a step-by-step guide on how to calculate the number of customers that have transacted with a company in the last <span class="No-Break">12 months:</span></p>
			<ol>
				<li>Gather data on the total number of transactions in the last 12 months. This data can be found in the<a id="_idIndexMarker544"/> company’s sales records or <strong class="bold">customer relationship management</strong> (<span class="No-Break"><strong class="bold">CRM</strong></span><span class="No-Break">) system.</span></li>
				<li>Calculate the average number <a id="_idIndexMarker545"/>of transactions per customer. This can be done by dividing the total number of transactions in a given period by the number of customers in <span class="No-Break">that period.</span></li>
				<li>Divide the total number of transactions in the last 12 months by the average number of transactions per customer to get the number <span class="No-Break">of customers.</span></li>
			</ol>
			<p><em class="italic">Note</em>: <em class="italic">Once you have calculated the number of customers that have transacted with the company in the last 12 months, you can use this information to track customer churn, identify customer segments, and develop targeted </em><span class="No-Break"><em class="italic">marketing campaigns.</em></span></p>
			<p>From here, you should identify the optimal sourcing for this data. For example, you may pull customer information from your <strong class="bold">Customer Data Platform</strong> (<strong class="bold">CDP</strong>), and your sales information might come from your<a id="_idIndexMarker546"/> company’s CRM platform. The business data steward would be responsible for determining the authorized source for <span class="No-Break">this data.</span></p>
			<p>Once a definition, calculation, and sourcing of the data are determined, the business data steward would then work with the data office to evaluate the lineage of the data, validate the lineage (see <a href="B18846_07.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> for details on lineage capabilities), and measure the quality of the data. Any issues with lineage and quality should be remediated (or disclosed to the users). You will need to use your data lineage capability to trace the data through the systems and identify where to measure data quality. Apply data profiling, enrichment, and de-duplication efforts where necessary to strengthen and improve your data quality. Once the metric is validated as trustworthy, the data office can mark the metric <span class="No-Break">as certified.</span></p>
			<p>If you’ve deployed a data marketplace, this should be the place where the metric is published for consumption and marked as certified. This will show any user of the data marketplace that the Average Customer Count metric is trustworthy and reliable for use. It also shows the user that any other metrics are <em class="italic">not</em> the authorized and certified metric and should not be as trusted <a id="_idIndexMarker547"/>as the certified metric. This process can be repeated for all key metrics in your organization and applied to common-use datasets, reports, and <span class="No-Break">so on.</span></p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor184"/>Transparency</h2>
			<p>The best way to build trust is through transparency. When the quality of data is low, the best way to lose trust is to not share the<a id="_idIndexMarker548"/> results openly. When the results of your data quality work show that data should not be relied upon, you should publish these results and leverage the response from users to drive remediation efforts. If important data is low quality, providing visibility into that deficiency will garner support for your remediation needs and, oftentimes, can lead to increases in funding and resourcing to help drive your data quality program. Don’t be shy about reporting your deficiencies. It will <span class="No-Break">help you.</span></p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor185"/>Setting up data quality for success</h1>
			<p>My best advice for setting up your data quality program is to start small, be specific, be transparent, and scale rapidly. Pick a handful of <a id="_idIndexMarker549"/>metrics or a couple of key reports, and show the value of understanding the data, tracing the lineage, and measuring the quality of the data. Remediate issues quickly and visibly and show improvement in the data for these key metrics <span class="No-Break">and/or reports.</span></p>
			<p>Be very clear with your stakeholders, both users and executives alike: the goal is not to create perfect data for the enterprise. The goal is to deploy resources (time, money, people) for the most effective return on those resources. You could spend unlimited time and money remediating every data element, but it would be like trying to plug every hole in every boat in every ocean. Sometimes, you just need to build a better boat. You may run into instances where a report is incredibly low quality. It might be easier to completely recreate the report with different data sources or different metrics than trying to fix every single error. You’ll have to use your professional judgment to determine the best course of action for your key metrics and reports. Choose wisely and deliver measurable <span class="No-Break">improvements quickly.</span></p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor186"/>The real-time request</h2>
			<p>You may run into instances when business users<a id="_idIndexMarker550"/> request “real-time” insight into the quality of their data. It’s your job as a data professional to credibly challenge this request. From a position of curiosity, seek to understand the reasons why the business wants real-time measurement of the quality of their data. For starters, measuring real-time data quality requires you to test data in production, which can impact the performance of the production system. Oftentimes, data is profiled in a lower environment with a copy of production data, not to impact performance. Secondly, most “real-time” requests are satisfied with intra-day or hourly reporting instead of “real-time” reporting. Work to determine the business need driving the request, and then react accordingly. Sometimes, data professionals are eager to please and say yes without assessing the true business need against the cost<a id="_idIndexMarker551"/> to deliver. Real-time data often comes with a significant cost over even slight delays (a few hours) and requires different configurations of underlying system performance and integrations, often at a significant cost in infrastructure, processing, and people to monitor <span class="No-Break">the delivery.</span></p>
			<p class="callout-heading">Real-time can backfire</p>
			<p class="callout">Sometimes, real-time can backfire on you. When you have systems integrated in real time, in the spirit of ensuring high-quality data available for operations, that produces quick access to quality data. However, when things go wrong, you are propagating data problems in real <span class="No-Break">time too.</span></p>
			<p class="callout">It’s not uncommon to design a CDP to push cleansed and enriched customer data into a CRM platform. Customer data is usually captured in the CRM, pushed to the CDP for enrichment, and then pushed back to the CRM. In one instance, a bug was introduced into the CDP, such that the hierarchy information (what connects individual companies to their parent company, which is used for transactions, sales territory mapping, and revenue reporting by geolocation, by account executive, and so on), was removed. When the hierarchy data was deleted in the CDP, it was replicated in the CRM. In the CRM, the now-orphaned customer records lost their parent company, which removed the territory assignment. The data was then sent back to the CDP without the territory assigned. This replicated blanks in the customer data records in both systems. We went from having a few orphaned accounts to thousands in a matter of minutes, to hundreds of thousands in a matter of hours, before we had to turn the synchronization process off. It took weeks to remediate and restore the full dataset in both systems <span class="No-Break">to accuracy.</span></p>
			<p class="callout">If the real-time requirement had been a few hours instead of real time, we would have been able to catch the replication issue without such a huge blast radius. Sometimes, intentional friction in the process can prevent these sorts of issues without such a big impact. A simple validation check for blank fields may have also stopped the replication job and alerted the team to the<a id="_idIndexMarker552"/> problem before causing such a <span class="No-Break">widespread impact.</span></p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor187"/>Integrations with other systems</h2>
			<p>Another facet of a strong data quality program is <a id="_idIndexMarker553"/>evaluating data pipelines. Often, data quality profiling and testing are performed at a system level or an integration level, versus an element level. There are reasons to do both, and the right application at the right time is the best way to allocate resources appropriately. Integration data quality can be measured in a few ways to determine the value of the <span class="No-Break">integration measurement:</span></p>
			<ul>
				<li>Latency between the ingestion and its validation and <span class="No-Break">subsequent correction</span></li>
				<li><strong class="bold">Time to decision</strong> (<strong class="bold">TTD</strong>) from <a id="_idIndexMarker554"/>data ingestion from the source system <span class="No-Break">to report</span></li>
				<li>Savings from immediate or real-time error detection and avoidance of downstream <span class="No-Break">system implications</span></li>
				<li>Number of real-time corrections without <span class="No-Break">manual intervention</span></li>
				<li>Number of successful integrations without data <span class="No-Break">quality issues</span></li>
				<li>Speed of data flow <span class="No-Break">between systems</span></li>
			</ul>
			<p>Ultimate success with data quality comes down to understanding the business needs, what quality is required, and producing transparent results that business data stewards can take action to fix in a timeline that meets their needs. Start small, scale rapidly, and share results broadly. This is how you drive trust in <span class="No-Break">your data.</span></p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor188"/>Conclusion</h1>
			<p>In this chapter, we explored the fundamental concept of data quality and its critical role in driving informed decision-making. We established a framework for understanding the various dimensions of data quality, including accuracy, completeness, consistency, and timeliness. We further delved into different sources of data errors and imperfections, highlighting the importance of proactive data quality management practices. You should now have a firm understanding of the <span class="No-Break">following areas:</span></p>
			<ul>
				<li>The importance of data quality, including why it is required for building trust <span class="No-Break">in data.</span></li>
				<li>How to define a data quality strategy <span class="No-Break">and framework</span></li>
				<li>Specific needs when it comes <span class="No-Break">to implementation</span></li>
				<li>How to design a data quality solution <span class="No-Break">with impact</span></li>
				<li>How to set data quality up for success in <span class="No-Break">your company</span></li>
			</ul>
			<p>By prioritizing data quality, organizations can unlock the true potential of their information assets. Clean, consistent, and reliable data empowers effective analytics, fosters trust in insights, and ultimately, fuels better business outcomes. Moving forward, the following chapters will delve deeper into specific data capabilities but keep data quality in mind. Data quality is key in the implementation of all data management capabilities and should be considered as you implement primary data and data operations, and will come to light in the use case in <a href="B18846_17.xhtml#_idTextAnchor351"><span class="No-Break"><em class="italic">Chapter 17</em></span></a>. You are now equipped with practical knowledge to assess, improve, and maintain the quality of your data for <span class="No-Break">optimal results.</span></p>
		</div>
	</body></html>