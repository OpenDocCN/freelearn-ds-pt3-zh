- en: A Tour of Statistics with pandas and NumPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll take a brief tour of classical statistics (also called
    the frequentist approach) and show you how we can use pandas together with the `numpy`
    and `stats` packages, such as `scipy.stats` and `statsmodels`, to conduct statistical
    analysis. We will also learn how to write the calculations behind these statistics
    from scratch in Python. This chapter and the following ones are not intended to
    be primers on statistics; they just serve as an illustration of using pandas along
    with the `stats` and `numpy` packages. In the next chapter, we will examine an
    alternative approach to the classical view—that is, **Bayesian statistics**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Descriptive statistics versus inferential statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measures of central tendency and variability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hypothesis testing – the null and alternative hypotheses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The z-test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The t-test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The chi-square test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of variance (ANOVA) test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confidence intervals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlation and linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Descriptive statistics versus inferential statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In descriptive or summary statistics, we attempt to describe the features of
    a collection of data in a quantitative way. This is different from inferential
    or inductive statistics because its aim is to summarize a sample rather than use
    the data to infer or draw conclusions about the population from which the sample
    is drawn.
  prefs: []
  type: TYPE_NORMAL
- en: Measures of central tendency and variability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some of the measures that are used in descriptive statistics include the measures
    of central tendency and measures of variability.
  prefs: []
  type: TYPE_NORMAL
- en: A measure of central tendency is a single value that attempts to describe a
    dataset by specifying a central position within the data. The three most common
    measures of central tendency are the **mean**, **median**, and **mode**.
  prefs: []
  type: TYPE_NORMAL
- en: A measure of variability is used to describe the variability in a dataset. Measures
    of variability include variance and standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Measures of central tendency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a look at the measures of central tendency, along with illustrations
    of them, in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: The mean
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The mean or sample is the most popular measure of central tendency. It is equal
    to the sum of all the values in the dataset, divided by the number of values in
    the dataset. Thus, in a dataset of *n* values, the mean is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d09d2a35-9c6b-4ddc-985f-eb213e2bb47f.png)'
  prefs: []
  type: TYPE_IMG
- en: We use ![](img/7d4b5155-fd91-4540-9c10-1e0efbfc1e52.png) if the data values
    are from a sample and **µ** if the data values are from a population.
  prefs: []
  type: TYPE_NORMAL
- en: The sample mean and population mean are different. The sample mean is what is
    known as an unbiased estimator of the true population mean. By repeatedly randomly
    sampling the population to calculate the sample mean, we can obtain a mean of
    sample means. We can then invoke the law of large numbers and the **central limit
    theorem** (**CLT**) and denote the mean of the sample means as an estimate of
    the true population mean.
  prefs: []
  type: TYPE_NORMAL
- en: The population mean is also referred to as the expected value of the population.
  prefs: []
  type: TYPE_NORMAL
- en: The mean, as a calculated value, is often not one of the values observed in
    the dataset. The main drawback of using the mean is that it is very susceptible
    to outlier values, or if the dataset is very skewed.
  prefs: []
  type: TYPE_NORMAL
- en: The median
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The median is the data value that divides the set of sorted data values into
    two halves. It has exactly half of the population to its left and the other half
    to its right. In the case when the number of values in the dataset is even, the
    median is the average of the two middle values. It is less affected by outliers
    and skewed data.
  prefs: []
  type: TYPE_NORMAL
- en: The mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The mode is the most frequently occurring value in the dataset. It is more
    commonly used for categorical data so that we can find out which category is the
    most common. One downside to using the mode is that it is not unique. A distribution
    with two modes is described as bimodal, while one with many modes is described
    as multimodal. The following code is an illustration of a bimodal distribution
    with modes at two and seven, since they both occur four times in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The generated bimodal distribution appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c81a26c-5b65-4cdd-b97d-8a49dc914a8a.png)'
  prefs: []
  type: TYPE_IMG
- en: Computing the measures of central tendency of a dataset in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To illustrate this, let''s consider the following dataset, which consists of
    marks that were obtained by 15 pupils for a test scored out of 20:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The mean, median, and mode can be obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5abb54e-0726-47c0-aaa6-0c26479aa388.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To illustrate how the skewness of data or an outlier value can drastically
    affect the usefulness of the mean as a measure of central tendency, consider the
    following dataset, which shows the wages (in thousands of dollars) of the staff
    at a factory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the mean value, we may make the assumption that the data is centered
    around the mean value of `31.82`. However, we would be wrong. To explain why,
    let''s display an empirical distribution of the data using a bar plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53ef5bca-b623-464b-9de9-bf30315fd731.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding bar plot, we can see that most of the salaries are far below
    30K and that no one is close to the mean of 32K. Now, if we take a look at the
    median, we will see that it is a better measure of central tendency in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also take a look at a histogram of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34f8f7b2-ba9e-4d66-8183-6a037dbf9a70.png)'
  prefs: []
  type: TYPE_IMG
- en: The histogram is actually a better representation of the data as bar plots are
    generally used to represent categorical data, while histograms are preferred for
    quantitative data, which is the case for the salaries data. For more information
    on when to use histograms versus bar plots, refer to [http://onforb.es/1Dru2gv](http://onforb.es/1Dru2gv).
  prefs: []
  type: TYPE_NORMAL
- en: 'If the distribution is symmetrical and unimodal (that is, has only one mode),
    the three measures—mean, median, and mode—will be equal. This is not the case
    if the distribution is skewed. In that case, the mean and median will differ from
    each other. With a negatively skewed distribution, the mean will be lower than
    the median and vice versa for a positively skewed distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7c9061d-d758-492c-94a9-b4ac0e186e3b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Diagram sourced from http://www.southalabama.edu/coe/bset/johnson/lectures/lec15_files/iage014.jpg.
  prefs: []
  type: TYPE_NORMAL
- en: Measures of variability, dispersion, or spread
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another characteristic of distribution that we measure in descriptive statistics
    is variability.
  prefs: []
  type: TYPE_NORMAL
- en: Variability specifies how much the data points are different from each other
    or dispersed. Measures of variability are important because they provide an insight
    into the nature of the data that is not provided by the measures of central tendency.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, suppose we conduct a study to examine how effective a pre-K education
    program is in lifting test scores of economically disadvantaged children. We can
    measure the effectiveness not only in terms of the average value of the test scores
    of the entire sample, but also in terms of the dispersion of the scores. Is it
    useful for some students and not so much for others? The variability of the data
    may help us identify some steps to be taken to improve the usefulness of the program.
  prefs: []
  type: TYPE_NORMAL
- en: Range
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest measure of dispersion is the range. The range is the difference
    between the lowest and highest scores in a dataset. This is the simplest measure
    of spread, and can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Range = highest value - lowest value*'
  prefs: []
  type: TYPE_NORMAL
- en: Quartile
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A more significant measure of dispersion is the quartile and related interquartile
    ranges. It also stands for *quarterly percentile*, which means that it is the
    value on the measurement scale below which 25, 50, 75, and 100 percent of the
    scores in the sorted dataset fall. The quartiles are three points that split the
    dataset into four groups, with each one containing one-fourth of the data. To
    illustrate this, suppose we have a dataset of 20 test scores that are ranked,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The first quartile (Q1) lies between the fifth and sixth score, the second
    quartile (Q2) lies between the tenth and eleventh score, and the third quartile
    (Q3) lies between the fifteenth and sixteenth score. Thus, we get the following
    results by using linear interpolation and calculating the midpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To see this in IPython, we can use the `scipy.stats` or `numpy.percentile`
    packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The reason why the values don't match exactly with our previous calculations
    is due to the different interpolation methods. The interquartile range is the
    first quartile subtracted from the third quartile (*Q3 - Q1*). It represents the
    middle 50 in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on statistical measures, refer to [https://statistics.laerd.com/statistical-guides/measures-central-tendency-mean-mode-median.php](https://statistics.laerd.com/statistical-guides/measures-central-tendency-mean-mode-median.php).
  prefs: []
  type: TYPE_NORMAL
- en: For more details on the `scipy.stats` and `numpy.percentile` functions, see
    the documents at [http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.mquantiles.html](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.mquantiles.html)
    and [http://docs.scipy.org/doc/nu](http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html)[mpy-dev/reference/generated/numpy.percentile.html](http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html).
  prefs: []
  type: TYPE_NORMAL
- en: Deviation and variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A fundamental idea in the discussion of variability is the concept of deviation.
    Simply put, a deviation measure tells us how far away a given value is from the
    mean of the distribution—that is, ![](img/54f39392-c434-4f81-993e-69f75ec09d5b.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the deviation of a set of values, we define the variance as the sum
    of the squared deviations and normalize it by dividing it by the size of the dataset.
    This is referred to as the variance. We need to use the sum of the squared deviations.
    By taking the sum of the deviations around the mean results in 0, since the negative
    and positive deviations cancel each other out. The sum of the squared deviations
    is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52004cdb-f669-458f-aad4-7e365575b42e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding expression is equivalent to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4159a0da-e491-4843-b0b4-cc1db7c9f9d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Formally, the variance is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For sample variance, use the following formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/727d8bb9-6145-4127-a593-786249844eca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For population variance, use the following formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/8c80298c-4ecc-408d-bc81-ebe9251b0b15.png)'
  prefs: []
  type: TYPE_IMG
- en: The reason why the denominator is *N-1*for the sample variance instead of *N* is
    that, for sample variance, we want to use an unbiased estimator. For more details
    on this, take a look at [http://en.wikipedia.org/wiki/Bias_of_an_estimator](http://en.wikipedia.org/wiki/Bias_of_an_estimator).
  prefs: []
  type: TYPE_NORMAL
- en: 'The values of this measure are in squared units. This emphasizes the fact that
    what we have calculated as the variance is the squared deviation. Therefore, to
    obtain the deviation in the same units as the original points of the dataset,
    we must take the square root, and this gives us what we call the standard deviation.
    Thus, the standard deviation of a sample is given by using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec150e4f-1fc1-44a3-9569-f97a2a76185e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, for a population, the standard deviation is given by the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c1cc04a-1347-4a1b-81f7-d0c51febc169.png)'
  prefs: []
  type: TYPE_IMG
- en: Hypothesis testing – the null and alternative hypotheses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding section, we had a brief discussion of what is referred to as
    descriptive statistics. In this section, we will discuss what is known as inferential
    statistics, whereby we try to use characteristics of the sample dataset to draw
    conclusions about the wider population as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most important methods in inferential statistics is hypothesis testing.
    In hypothesis testing, we try to determine whether a certain hypothesis or research
    question is true to a certain degree. One example of a hypothesis would be this:
    eating spinach improves long-term memory.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to investigate this statement using hypothesis testing, we can select
    a group of people as subjects for our study and divide them into two groups, or
    samples. The first group will be the experimental group, and it will eat spinach
    over a predefined period of time. The second group, which does not receive spinach,
    will be the control group. Over selected periods of time, the memory of The individuals
    in the two groups will be measured and tallied.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal at the end of our experiment will be to be able to make a statement
    such as "Eating spinach results in an improvement in long-term memory, which is
    not due to chance". This is also known as significance.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding scenario, the collection of subjects in the study is referred
    to as the sample, and the general set of people about whom we would like to draw
    conclusions is the population.
  prefs: []
  type: TYPE_NORMAL
- en: The ultimate goal of our study will be to determine whether any effects that
    we observed in the sample can be generalized to the population as a whole. In
    order to carry out hypothesis testing, we need to come up with what's known as
    the null and alternative hypotheses.
  prefs: []
  type: TYPE_NORMAL
- en: The null and alternative hypotheses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By referring to the preceding spinach example, the null hypothesis would be
    "Eating spinach has no effect on long-term memory performance".
  prefs: []
  type: TYPE_NORMAL
- en: The null hypothesis is just that—it nullifies what we're trying to *prove* by
    running our experiment. It does so by asserting that a statistical metric (to
    be explained later) is zero.
  prefs: []
  type: TYPE_NORMAL
- en: The alternative hypothesis is what we hope to support. It is the opposite of
    the null hypothesis and we assume it to be true until the data provides sufficient
    evidence that indicates otherwise. Thus, our alternative hypothesis, in this case,
    is "Eating spinach results in an improvement in long-term memory".
  prefs: []
  type: TYPE_NORMAL
- en: 'Symbolically, the null hypothesis is referred to as ***H0*** and the alternative
    hypothesis is referred to as ***H1***. You may wish to restate the preceding null
    and alternative hypotheses as something more concrete and measurable for our study.
    For example, we could recast ***H0*** as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '"The mean memory score for a sample of 1,000 subjects who ate 40 grams of spinach
    daily for a period of 90 days would not differ from the control group of 1,000
    subjects who consume no spinach within the same time period."'
  prefs: []
  type: TYPE_NORMAL
- en: In conducting our experiment/study, we focus on trying to prove or disprove
    the null hypothesis. This is because we can calculate the probability that our
    results are due to chance. However, there is no easy way to calculate the probability
    of the alternative hypothesis since any improvement in long-term memory could
    be due to factors other than just eating spinach.
  prefs: []
  type: TYPE_NORMAL
- en: We test out the null hypothesis by assuming that it is true and calculate the
    probability of the results we gather arising by chance alone. We set a threshold
    level—alpha (*α)*—for which we can reject the null hypothesis if the calculated
    probability is smaller or accept it if it is greater. Rejecting the null hypothesis
    is tantamount to accepting the alternative hypothesis and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: The alpha and p-values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to conduct an experiment to support or disprove our null hypothesis,
    we need to come up with an approach that will allow us to make the decision in
    a concrete and measurable way. To do this test of significance, we have to consider
    two numbers—the p-value of the test statistic and the threshold level of significance,
    which is also known as **alpha**.
  prefs: []
  type: TYPE_NORMAL
- en: The p-value is the probability that the result we observe by assuming that the
    null hypothesis is true occurred by chance alone.
  prefs: []
  type: TYPE_NORMAL
- en: The p-value can also be thought of as the probability of obtaining a test statistic
    as extreme as or more extreme than the obtained test statistic, given that the
    null hypothesis is true.
  prefs: []
  type: TYPE_NORMAL
- en: The alpha value is the threshold value against which we compare p-values. This
    gives us a cut-off point at which we can accept or reject the null hypothesis.
    It is a measure of how extreme the results we observe must be in order to reject
    the null hypothesis of our experiment. The most commonly used values of alpha
    are 0.05 or 0.01.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, the rule is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If the p-value is less than or equal to alpha (p< .05), then we reject the null
    hypothesis and state that the result is statistically significant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the p-value is greater than alpha (p > .05), then we have failed to reject
    the null hypothesis, and we say that the result is not statistically significant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In other words, the rule is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If the test statistic value is greater than or smaller than the two critical
    test statistic values (for two-tailed tests), then we reject the null hypothesis
    and state that the (alternative) result is statistically significant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the test statistic value lies within the two critical test statistic values,
    then we have failed to reject the null hypothesis, and we say that the (alternative)
    result is not statistically significant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The seemingly arbitrary values of alpha in usage is one of the shortcomings
    of the frequentist methodology, and there are many questions concerning this approach.
    An article in the *Nature* journal highlights some of these problems; you can
    find it at: [http://www.nature.com/news/scientific-method-statistical-errors-1.14700](http://www.nature.com/news/scientific-method-statistical-errors-1.14700).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For more details on this topic, please refer to the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://statistics.about.com/od/Inferential-Statistics/a/What-Is-The-Difference-Between-Alpha-And-P-Values.htm](http://statistics.about.com/od/Inferential-Statistics/a/What-Is-The-Difference-Between-Alpha-And-P-Values.htm)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://courses.washington.edu/p209s07/lecturenotes/Week%205_Monday%20overheads.pdf](http://courses.washington.edu/p209s07/lecturenotes/Week%205_Monday%20overheads.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Type I and Type II errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two types of errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Type I Error**: In this type of error, we reject *H0* when, in fact, *H0*
    is true. An example of this would be a jury convicting an innocent person for
    a crime that the person did not commit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type II Error**: In this type of error, we fail to reject *H0* when, in fact, *H1*
    is true. This is equivalent to a guilty person escaping conviction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s a table showing null hypothesis conditions leading to an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1b2f5f3-7529-438a-8e35-21cba3a272ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Statistical hypothesis tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A statistical hypothesis test is a method that we use to make a decision. We
    do this using data from a statistical study or experiment. In statistics, a result
    is termed statistically significant if it is unlikely to have occurred only by
    chance based on a predetermined threshold probability or significance level. There
    are two classes of statistical tests: 1-tailed and 2-tailed tests.'
  prefs: []
  type: TYPE_NORMAL
- en: In a 2-tailed test, we allot half of our alpha to testing the statistical significance
    in one direction and the other half to testing the statistical significance in
    the other direction.
  prefs: []
  type: TYPE_NORMAL
- en: In a 1-tailed test, the test is performed in one direction only.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on this topic, please refer to [http://www.ats.ucla.edu/stat/mult_pkg/faq/general/tail_tests.htm](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/tail_tests.htm).
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To apply statistical inference, it is important to understand the concept of
    what is known as a sampling distribution. A sampling distribution is the set of
    all possible values of a statistic, along with their probabilities, assuming that
    we sample at random from a population where the null hypothesis holds true.
  prefs: []
  type: TYPE_NORMAL
- en: 'A more simplistic definition is this: a sampling distribution is the set of
    values that the statistic can assume (distribution) if we were to repeatedly draw
    samples from the population, along with their associated probabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: The value of a statistic is a random sample from the statistic's sampling distribution.
    The sampling distribution of the mean is calculated by obtaining many samples
    of various sizes and taking their mean.
  prefs: []
  type: TYPE_NORMAL
- en: The central limit theorem states that the sampling distribution is normally
    distributed if the original or raw-score population is normally distributed, or
    if the sample size is large enough. Conventionally, statisticians define large
    enough sample sizes as N ≥ 30—that is, a sample size of 30 or more. This is still
    a topic of debate, though.
  prefs: []
  type: TYPE_NORMAL
- en: For more details on this topic, refer to [http://stattrek.com/sampling/sampling-distribution.aspx](http://stattrek.com/sampling/sampling-distribution.aspx).
  prefs: []
  type: TYPE_NORMAL
- en: The standard deviation of the sampling distribution is often referred to as
    the standard error of the mean, or just the standard error.
  prefs: []
  type: TYPE_NORMAL
- en: The z-test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The z-test is appropriate under the following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: The study involves a single sample mean and the parameters *µ* and* ![](img/4a9cadd5-c79b-48e1-a397-f5e519b8e881.png)*
    of the null hypothesis population are known
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sampling distribution of the mean is normally distributed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of the sample is *N ≥ 30*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We use the z-test when the mean of the population is *known*. In the z-test,
    we ask the question of whether the population mean, µ, is different from a hypothesized
    value. The null hypothesis in the case of the z-test is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b143bcc-02bd-4cf4-a015-53840a2beb17.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, µ is the population mean and ![](img/5350818f-46d3-4c74-a8ff-72260e49754c.png) is
    the hypothesized value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The alternative hypothesis, ![](img/6aea0dc2-32a7-42d7-8ab2-decdefd929ae.png),
    can be one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3494f6c-5151-4f4e-8334-59f22b1af759.png)![](img/7caa782d-7852-47a6-9721-5ad3775c6c4e.png)![](img/2328f214-aee4-45d3-b96a-adf6a2fd4740.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first two are 1-tailed tests, while the last one is a 2-tailed test. In
    concrete terms, to test µ, we calculate the test statistic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6919e0ad-b75c-4f18-84fa-abfe3d45b082.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/701bb134-02a3-4ed1-b105-5c645eb8d6fc.png) is the true standard
    deviation of the sampling distribution of ![](img/f8293b68-5991-4916-a9b7-144b1c0c43ec.png).
    If ![](img/4d1c4354-6b7c-4726-873b-0269a20bdc78.png)is true, the z-test statistics
    will have the standard normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go through a quick illustration of the z-test.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have a fictional company, Intelligenza, that claims that they have
    come up with a radical new method for improved memory retention and study. They
    claim that their technique can improve grades over traditional study techniques.
    Suppose the improvement in grades is 40 percent with a standard deviation of 10
    percent compared with results obtained using traditional study techniques.
  prefs: []
  type: TYPE_NORMAL
- en: A random test was run on 100 students using the Intelligenza method, and this
    resulted in a mean improvement of 44 percent. Does Intelligenza's claim hold true?
  prefs: []
  type: TYPE_NORMAL
- en: The null hypothesis for this study states that there is no improvement in grades
    using Intelligenza's method over traditional study techniques. The alternative
    hypothesis is that there is an improvement in using Intelligenza's method over
    traditional study techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'The null hypothesis is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b143bcc-02bd-4cf4-a015-53840a2beb17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The alternative hypothesis is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7caa782d-7852-47a6-9721-5ad3775c6c4e.png)'
  prefs: []
  type: TYPE_IMG
- en: std error = 10/sqrt(100) = 1
  prefs: []
  type: TYPE_NORMAL
- en: z = (43.75-40)/(10/10) = 3.75 std errors
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that if the null hypothesis is true, then the test statistic, z, will
    have a standard normal distribution that would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/596ca817-4bc9-48f5-b765-cbd24e45744f.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram sourced from http://mathisfun.com/data/images/normal-distrubution-large.gif.
  prefs: []
  type: TYPE_NORMAL
- en: This value of z would be a random sample from the standard normal distribution,
    which is the distribution of z if the null hypothesis is true.
  prefs: []
  type: TYPE_NORMAL
- en: The observed value of *z=43.75* corresponds to an extreme outlier p-value on
    the standard normal distribution curve, which is much less than 0.1 percent.
  prefs: []
  type: TYPE_NORMAL
- en: The p-value is the area under the curve to the right of the value of *3.75*
    on the preceding normal distribution curve.
  prefs: []
  type: TYPE_NORMAL
- en: This suggests that it would be highly unlikely for us to obtain the observed
    value of the test statistic if we were sampling from a standard normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can look up the actual p-value using Python by using the `scipy.stats` package,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, *P(z ≥ 3.75 = 8.8e-05)*—that is, if the test statistic was normally
    distributed, then the probability of obtaining the observed value is *8.8e-05*,
    which is close to zero. So it would be almost impossible to obtain the value that
    we observed if the null hypothesis was actually true.
  prefs: []
  type: TYPE_NORMAL
- en: In more formal terms, we would normally define a threshold or alpha value and
    reject the null hypothesis if the p-value was ≤ α or fail to reject it otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The typical values for α are 0.05 or 0.01\. The following list explains the
    different values of alpha:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p-value <0.01*: There is strong evidence against *H0*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*0.01 < p-value < 0.05*: There is strong evidence against *H0*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*0.05 < p-value < 0.1*: There is weak evidence against *H0*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p-value > 0.1*: There is little or no evidence against *H0*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Therefore, in this case, we would reject the null hypothesis and give credence
    to Intelligenza''s claim and state that their claim is highly significant. The
    evidence against the null hypothesis, in this case, is significant. There are
    two methods that we use to determine whether to reject the null hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: The p-value approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rejection region approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The approach that we used in the preceding example was the latter one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The smaller the p-value, the less likely it is that the null hypothesis is
    true. In the rejection region approach, we have the following rule:'
  prefs: []
  type: TYPE_NORMAL
- en: If ![](img/8e4fae50-e9dc-455d-a787-1a4a29c74196.png), reject the null hypothesis;
    otherwise, retain it.
  prefs: []
  type: TYPE_NORMAL
- en: The t-test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The z-test is useful when the standard deviation of the population is known.
    However, in most real-world cases, this is an unknown quantity. For these cases,
    we turn to the t-test of significance.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the t-test, given that the standard deviation of the population is unknown,
    we replace it with the standard deviation, *s*, of the sample. The standard error
    of the mean is now as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40588631-f2e0-4c27-b03a-527547a2f027.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The standard deviation of the sample, *s*, is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0f83fd9-a389-485b-91c7-e1cc6c79f509.png)'
  prefs: []
  type: TYPE_IMG
- en: The denominator is *N-1* and not *N*. This value is known as the number of degrees
    of freedom. We will now state (without an explanation) that, by the CLT, the t-distribution
    approximates the normal, Guassian, or z-distribution as *N,* and so *N-1* increases—that
    is, with increasing **degrees of freedom** (**df**). When *df* *=* ∞, the t-distribution
    is identical to the normal or z-distribution. This is intuitive since, as *df*
    increases, the sample size increases and *s* approaches ![](img/03309792-82d0-44d4-9393-cf4ba0feac4c.png),
    which is the true standard deviation of the population. There are an infinite
    number of t-distributions, each corresponding to a different value of *df*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2e48643-784e-4df8-a029-c1a924ed347e.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram sourced from http://zoonek2.free.fr/UNIX/48_R/g593.png
  prefs: []
  type: TYPE_NORMAL
- en: A more detailed technical explanation on the relationship between t-distribution,
    z-distribution, and the degrees of freedom can be found at [http://en.wikipedia.org/wiki/Student's_t-distribution](http://en.wikipedia.org/wiki/Student's_t-distribution).
  prefs: []
  type: TYPE_NORMAL
- en: Types of t-tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are various types of t-tests. The following are the most common one.
    They typically formulate a null hypothesis that makes a claim about the mean of
    a distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**One-sample independent t-test**: This is used to compare the mean of a sample
    with that of a known population mean or known value. Let''s assume that we''re
    health researchers in Australia who are concerned with the health of the aboriginal
    population and wish to ascertain whether babies born to low-income aboriginal
    mothers have lower birth weight than normal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An example of a null hypothesis test for a one-sample t-test would be this:
    the mean birth weight for our sample of 150 deliveries of full-term, live babies
    from low-income aboriginal mothers is no different from the mean birth weight
    of babies in the general Australian population—that is, 3,367 grams.'
  prefs: []
  type: TYPE_NORMAL
- en: The reference for this information can be found at [http://www.healthinfonet.ecu.edu.au/health-facts/overviews/births-and-pregnancy-outcome](http://www.healthinfonet.ecu.edu.au/health-facts/overviews/births-and-pregnancy-outcome).
  prefs: []
  type: TYPE_NORMAL
- en: '**Independent samples t-tests**: This is used to compare means from independent
    samples with each other. An example of an independent sample t-test would be a
    comparison of the fuel economy of automatic transmission versus manual transmission
    vehicles. This is what our real-world example will focus on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The null hypothesis for the t-test would be this: there is no difference between
    the average fuel efficiency of cars with manual and automatic transmissions in
    terms of their average combined city/highway mileage.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Paired samples t-test**: In a paired/dependent samples t-test, we take each
    data point in one sample and pair it with a data point in the other sample in
    a meaningful way. One way to do this would be to measure against the same sample
    at different points in time. An example of this would be to examine the efficacy
    of a slimming diet by comparing the weight of a sample of participants before
    and after the diet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The null hypothesis, in this case, would be this: there is no difference between
    the mean weights of participants before and after going on the slimming diet,
    or more succinctly, the mean difference between paired observations is zero.'
  prefs: []
  type: TYPE_NORMAL
- en: This information can be found at [http://en.wikiversity.org/wiki/T-test](http://en.wikiversity.org/wiki/T-test).
  prefs: []
  type: TYPE_NORMAL
- en: A t-test example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In simplified terms, to do **Null Hypothesis Significance Testing** (**NHST**),
    we need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Formulate our null hypothesis. The null hypothesis is our model of the system,
    assuming that the effect we wish to verify was actually due to chance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate our p-value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the calculated p-value with that of our alpha, or threshold value, and
    decide whether to reject or accept the null hypothesis. If the p-value is low
    enough (lower than the alpha), we will draw the conclusion that the null hypothesis
    is likely to be false.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For our real-world illustration, we want to investigate whether manual transmission
    vehicles are more fuel efficient than automatic transmission vehicles. In order
    to do this, we will make use of the Fuel Economy data that was published by the
    US government for 2014 at [http://www.fueleconomy.gov](http://www.fueleconomy.gov):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we wish to modify the preceding series so that the values just contain
    the `Auto` and `Manual` strings. We can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create a final modified DataFrame from Series that consists of
    the transmission type and the combined fuel economy figures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now separate the data for vehicles with automatic transmission from
    those with manual transmission, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows that there were 987 vehicles with automatic transmission versus
    211 with manual transmission:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: chi-square test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to implement a chi-square test from scratch
    in Python and run it on an example dataset.
  prefs: []
  type: TYPE_NORMAL
- en: A chi-square test is conducted to determine the statistical significance of
    a causal relationship of two categorical variables with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the following dataset, a chi-square test can be used to determine
    whether color preferences affect a personality type (introvert and extrovert)
    or not, and vice versa:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c3fc09e-f64b-4e58-90dc-5a5a283d1b40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The two hypotheses for chi-square tests are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**H0**: Color preferences are not associated with a personality type'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ha**: Color preferences are associated with a personality type'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To calculate the chi-square statistic, we assume that the null hypothesis is
    true. If there is no relationship between the two variables, we could just take
    the contribution (proportion) of that column as the total and multiply that with
    the row total for that cell; that would give us the expected cell. In other words,
    the absence of a specific relationship implies a simple proportional relationship
    and distribution. Therefore, we calculate the expected number in each subcategory
    (assuming the null hypothesis is true) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Expected Frequency = (Row Total X Column Total )/ Total:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6904394e-51d6-40bc-b8e4-057b5c1b2f24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the expected frequency has been calculated, the ratio of the square of
    the difference between the expected and observed frequency, divided by the expected
    frequency, is calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chi_Square_Stat =Sum( (Expected Frequency-Observed Frequency)**2/Expected
    Frequency)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'These statistics follow a chi-square distribution with a parameter called the **degree
    of freedom (DOF)**. The degree of freedom is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*DOF = (Number of Rows -1)*(Number of Column-1)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a different distribution for each degree of freedom. This is shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/076c226d-e9b8-4387-9db7-d60c01556359.png)'
  prefs: []
  type: TYPE_IMG
- en: Chi-square distributions at different degrees of freedoms
  prefs: []
  type: TYPE_NORMAL
- en: Like any other test we've looked at, we need to decide a significance level
    and find the p-value associated with the chi-square statistics for that degree
    of freedom.
  prefs: []
  type: TYPE_NORMAL
- en: If the p-value is less than the alpha value, the null hypothesis can be rejected.
  prefs: []
  type: TYPE_NORMAL
- en: 'This whole calculation can be done by writing some Python code. The following
    two functions calculate the chi-square statistic and the degrees of freedom:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The chi-square statistic is 71.99, while the degrees of freedom is 3\. The p-values
    can be calculated using the table found here: [https://people.smp.uq.edu.au/YoniNazarathy/stat_models_B_course_spring_07/distributions/chisqtab.pdf](https://people.smp.uq.edu.au/YoniNazarathy/stat_models_B_course_spring_07/distributions/chisqtab.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: From the tables, the p-value for 71.99 is very close to 0\. Even if we choose
    alpha to be a small number, such as 0.01, the p-value is still smaller. With this,
    we can say that the null hypothesis can be rejected with a good degree of statistical
    confidence.
  prefs: []
  type: TYPE_NORMAL
- en: ANOVA test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let's talk about another popular hypothesis test, called ANOVA. It is used
    to test whether similar data points coming from different groups or under a different
    set of experiments are statistically similar to each other or different—for example,
    the average height of different sections of a class in a school or the peptide
    length of a certain protein found in humans across different ethnicities.
  prefs: []
  type: TYPE_NORMAL
- en: 'ANOVA calculates two metrics to conduct this test:'
  prefs: []
  type: TYPE_NORMAL
- en: Variance among different groups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variance within each group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on these metrics, a statistic is calculated with variance among different
    groups as a numerator. If it is a statistically large enough number, it means
    that the variance among different groups is larger than the variance within the
    group, implying that the data points coming from the different groups are different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at how variance among different groups and variance within each
    group can be calculated. Suppose we have *k* groups that data points are coming
    from:'
  prefs: []
  type: TYPE_NORMAL
- en: The data points from group 1 are *X[11]*, *X[12]*, ......., *X*[*1n*.]
  prefs: []
  type: TYPE_NORMAL
- en: The data points from group 2 are *X[21]*, *X[22]*, ......., *X*[*2n*.]
  prefs: []
  type: TYPE_NORMAL
- en: This means that the data points from group *k* are *X[k1]*, *X[k2]*, .......,
    *X*[*kn*.]
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the following abbreviations and symbols to describe some features
    of this data:'
  prefs: []
  type: TYPE_NORMAL
- en: Variance among different groups is represented by SSAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variance within each group is represented by SSWG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of elements in group k is represented by *n[k]*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean of data points in a group is represented by *µ[k]*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean of all data points across groups is represented by *µ*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of groups is represented by *k*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s define the two hypotheses for our statistical test:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ho**: µ[1] = µ[2] = .....= µ[k]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ho**: µ[1] != µ[2] != .....= µ[k]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, the null hypothesis states that the mean of the data points
    across all the groups is the same, while the alternative hypothesis says that
    the mean of at least one group is different from the others.
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads to the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: SSAG = *(∑ n[k] * (X[k] - µ)**2) / k-1*
  prefs: []
  type: TYPE_NORMAL
- en: SSWG = *(∑∑(X[ki]-µ[k])**2) / n*k-k-1*
  prefs: []
  type: TYPE_NORMAL
- en: In SSAG, the summation is over all the groups.
  prefs: []
  type: TYPE_NORMAL
- en: In SSWG, the first summation is across the data points from a particular group
    and the second summation is across the groups.
  prefs: []
  type: TYPE_NORMAL
- en: The denominators in both cases denote the degree of freedom. For SSAG, we are
    dealing with k groups and the last value can be derived from the other *k-1* values.
    Therefore, DOF is *k-1*. For SSWG, there are *n*k* data points, but the k-1 mean
    values are restricted (or fixed) by those choices, and so the DOF is *n*k-k-1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once these numbers have been calculated, the test statistic is calculated as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Test Statistic = SSAG/SSWG*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This ratio of SSAG and SSWG follows a new distribution called F-distribution,
    and so the statistic is called an F statistic. It is defined by the two different
    degrees of freedom, and there is a separate distribution for each combination,
    as shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72aaa7fc-4367-45ce-9a20-d118c6523055.png)'
  prefs: []
  type: TYPE_IMG
- en: F-distribution based on the two DOFs, d1 =k-1 and d2=n*k-k-1
  prefs: []
  type: TYPE_NORMAL
- en: Like any of the other tests we've looked at, we need to decide a significance
    level and need to find the p-value associated with the F statistics for those
    degrees of freedom. If the p-value is less than the alpha value, the null hypothesis
    can be rejected. This whole calculation can be done by writing some Python code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at some example data and see how ANOVA can be applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4fd41cb4-54c5-4fc8-8239-0d42555201ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Head of the OD data by Lot and Run
  prefs: []
  type: TYPE_NORMAL
- en: 'We are interested in finding out whether the mean OD is the same for different
    lots and runs. We will apply ANOVA for that purpose, but before that, we can draw
    a boxplot to get an intuitive sense of the differences in the distributions for
    different lots and runs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/650a5a5f-161c-4b1d-a879-439bb8edb592.png)'
  prefs: []
  type: TYPE_IMG
- en: Boxplot of OD by Lot
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, a boxplot of OD grouped by Run can also be plotted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89d1e6a6-7c1f-48be-9a6c-2c12249df343.png)'
  prefs: []
  type: TYPE_IMG
- en: Boxplot of OD by Run
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s write the Python code to perform the calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The value of the F statistic comes out to be 3.84, while the degrees of freedom
    are 4 and 69, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: At a significance level—that is, alpha—of 0.05, the critical value of the F
    statistic lies between 2.44 and 2.52 (from the F-distribution table found at: [http://socr.ucla.edu/Applets.dir/F_Table.html](http://socr.ucla.edu/Applets.dir/F_Table.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Since the value of the F statistic (3.84) is larger than the critical value
    of 2.52, the F statistic lies in the rejection region, and so the null hypothesis
    can be rejected. Therefore, it can be concluded that the mean OD values are different
    for different lot groups. At a significance level of 0.001, the F statistic becomes
    smaller than the critical value, and so the null hypothesis can't be rejected.
    We would have to accept that the OD means from the different groups are statistically
    the same. The same test can be performed for different run groups. This has been
    left as an exercise for you to practice with.
  prefs: []
  type: TYPE_NORMAL
- en: Confidence intervals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will address the issue of confidence intervals. A confidence
    interval allows us to make a probabilistic estimate of the value of the mean of
    a population's given sample data.
  prefs: []
  type: TYPE_NORMAL
- en: This estimate, called an interval estimate, consists of a range of values (intervals)
    that act as good estimates of the unknown population parameter.
  prefs: []
  type: TYPE_NORMAL
- en: The confidence interval is bounded by confidence limits. A 95 percent confidence
    interval is defined as an interval in which the interval contains the population
    mean with a 95 percent probability. So how do we construct a confidence interval?
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a 2-tailed t-test and we want to construct a 95 percent confidence
    interval. In this case, we want the sample t-value, ![](img/bea7daa2-f67c-4259-bfde-cf790f51841e.png),
    corresponding to the mean to satisfy the following inequality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e98d2c60-01e2-458c-952f-a3950a64daaf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Given that ![](img/a7d87347-ae59-479c-a894-add67b396748.png) , we can substitute
    this in the preceding inequality relation to obtain the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b70a50cf-7705-4a25-8a69-f3b000639f74.png)'
  prefs: []
  type: TYPE_IMG
- en: The ![](img/ce778062-d045-4f19-9187-985877ed0186.png) interval is our 95 percent
    confidence interval.
  prefs: []
  type: TYPE_NORMAL
- en: Generalizing any confidence interval for any percentage, *y*, can be expressed
    as ![](img/a4721c72-a296-4dbb-8350-430f4ef60c70.png), where ![](img/736ba19b-2281-4848-ad52-13b923adb857.png) is
    the t-tailed value of *t—t*hat is, ![](img/1640980b-71b8-4e3e-bef0-b50584e5bb79.png)
    correlation to the desired confidence interval for *y*.
  prefs: []
  type: TYPE_NORMAL
- en: We will now take the opportunity to illustrate how we can calculate the confidence
    interval using a dataset from the popular statistical environment known as R.
    The `stats` models' module provides access to the datasets that are available
    in the core datasets package of R through the `get_rdataset` function.
  prefs: []
  type: TYPE_NORMAL
- en: An illustrative example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will consider the dataset known as faithful, which consists of data that
    was obtained by observing the eruptions of the Old Faithful geyser in the Yellowstone
    National Park in the US. The two variables in the dataset are eruptions, which
    are the length of time the geyser erupts, and waiting, which is the time until
    the next eruption. There were 272 observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s calculate a 95 percent confidence interval for the mean waiting time
    of the geyser. To do this, we must obtain the sample mean and standard deviation
    of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we''ll make use of the `scipy.stats` package to calculate the confidence
    interval:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Thus, we can state with 95 percent confidence that the [69.28, 72.51] interval
    contains the actual mean waiting time of the geyser.
  prefs: []
  type: TYPE_NORMAL
- en: This information can be found at [http://statsmodels.sourceforge.net/devel/datasets/index.html](http://statsmodels.sourceforge.net/devel/datasets/index.html)
    and [http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.norm.html](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.norm.html).
  prefs: []
  type: TYPE_NORMAL
- en: Correlation and linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most common tasks in statistics when determining the relationship
    between two variables is whether there is dependence between them. Correlation
    is the general term we use in statistics for variables that express dependence
    with each other.
  prefs: []
  type: TYPE_NORMAL
- en: We can then use this relationship to try and predict the value of one set of
    variables from the other. This is known as regression.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The statistical dependence that's expressed in a correlation relationship does
    not imply a causal relationship between the two variables; the famous line regarding
    this is c*orrelation does not imply causation*. Thus, the correlation between
    two variables or datasets implies just a casual rather than a causal relationship
    or dependence. For example, there is a correlation between the amount of ice cream
    purchased on a given day and the weather.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on correlation and dependency, refer to [http://en.wikipedia.org/wiki/Correlation_and_dependence](http://en.wikipedia.org/wiki/Correlation_and_dependence).
  prefs: []
  type: TYPE_NORMAL
- en: The correlation measure, known as the correlation coefficient, is a number that
    describes the size and direction of the relationship between the two variables.
    It can vary from -1 to +1 in direction and 0 to 1 in magnitude. The direction
    of the relationship is expressed through the sign, with a *+* sign expressing
    a positive correlation and a *-* sign expressing a negative correlation. The higher
    the magnitude, the greater the correlation, with a 1 being termed as the perfect
    correlation.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular and widely used correlation coefficient is the Pearson product-moment
    correlation coefficient, known as *r*. It measures the linear correlation or dependence
    between two *x* and *y* variables and takes values between -1 and +1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sample correlation coefficient, *r*, is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b0bc723b-d636-416a-ac94-c27380de54b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This can also be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07d2c249-b46c-4baf-8b1f-d913123ea43b.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we have omitted the summation limits.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned previously, regression focuses on using the relationship between
    two variables for prediction. In order to make predictions using linear regression,
    the best-fitting straight line must be computed.
  prefs: []
  type: TYPE_NORMAL
- en: If all the points (values for the variables) lie on a straight line, then the
    relationship is deemed perfect. This rarely happens in practice and the points
    do not all fit neatly on a straight line. Because of this, the relationship is
    imperfect. In some cases, a linear relationship only occurs among log-transformed
    variables. This is a log-log model. An example of such a relationship would be
    a power law distribution in physics, where one variable varies as a power of another.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, an expression such as this results in the linear relationship.
  prefs: []
  type: TYPE_NORMAL
- en: For more information, refer to [http://en.wikipedia.org/wiki/Power_law.](http://en.wikipedia.org/wiki/Power_law)
  prefs: []
  type: TYPE_NORMAL
- en: To construct the best-fit line, the method of least squares is used. In this
    method, the best-fit line is the optimal line that is constructed between the
    points for which the sum of the squared distance from each point to the line is
    the minimum. This is deemed to be the best linear approximation of the relationship
    between the variables we are trying to model using linear regression. The best-fit
    line in this case is called the least squares regression line.
  prefs: []
  type: TYPE_NORMAL
- en: More formally, the least squares regression line is the line that has the minimum
    possible value for the sum of squares of the vertical distance from the data points
    to the line. These vertical distances are also known as residuals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, by constructing the least squares regression line, we''re trying to minimize
    the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f90155b-138a-486d-9346-dcf72ac89226.png)'
  prefs: []
  type: TYPE_IMG
- en: An illustrative example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now illustrate all the preceding points with an example. Suppose we're
    doing a study in which we would like to illustrate the effect of temperature on
    how often crickets chirp. The data for this example was obtained from the book
    *The Song of Insects*, by George W Pierce, which was written in 1948\. George
    Pierce measured the frequency of chirps made by a ground cricket at various temperatures.
  prefs: []
  type: TYPE_NORMAL
- en: We want to investigate the frequency of cricket chirps and the temperature,
    as we suspect that there is a relationship between them. The data consists of
    16 data points, and we will read it into a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data is sourced from [http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/slr/frames/slr02.html](http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/slr/frames/slr02.html).
    Let''s take a look at it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let''s make a scatter plot of the data, along with a regression line,
    or line of best fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the following diagram, there seems to be a linear relationship
    between temperature and the chirp frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/746594a9-2159-4625-bc28-fba78e040a63.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can now proceed to investigate further by using the `statsmodels.ols` (ordinary
    least squares) method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We will ignore most of the preceding results, except for the `R-squared`, `Intercept`,
    and `chirpFrequency` values.
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding result, we can conclude that the slope of the regression
    line is `3.29` and that the intercept on the temperature axis is `25.23`. Thus,
    the regression line equation looks like `temperature = 25.23 + 3.29 * chirpFrequency`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that as the chirp frequency increases by 1, the temperature increases
    by about 3.29 degrees Fahrenheit. However, note that the intercept value is not
    really meaningful as it is outside the bounds of the data. We can also only make
    predictions for values within the bounds of the data. For example, we cannot predict
    what `chirpFrequency` is at 32 degrees Fahrenheit as it is outside the bounds
    of the data; moreover, at 32 degrees Fahrenheit, the crickets would have frozen
    to death. The value of R—that is, the correlation coefficient—is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Thus, our correlation coefficient is `R = 0.835`. This would indicate that about
    84 percent of the chirp frequency can be explained by the changes in temperature.
  prefs: []
  type: TYPE_NORMAL
- en: The book containing this data, *The Song of Insects*, can be found at[http://www.hup.harvard.edu/catalog.php?isbn=9780674420663](http://www.hup.harvard.edu/catalog.php?isbn=9780674420663).
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more in-depth treatment of single and multivariable regression, refer
    to the following websites:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression (Part I)**: [http://bit.ly/1Eq5kSx](http://bit.ly/1Eq5kSx)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression (Part II)**: [http://bit.ly/1OmuFTV](http://bit.ly/1OmuFTV)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a brief tour of the classical or frequentist approach
    to statistics and saw how to combine pandas with the `numpy` and `stats` packages—`scipy.stats`
    and `statsmodels`—to calculate, interpret, and make inferences from statistical
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will examine an alternative approach to statistics called
    the Bayesian approach. For a deeper look at the statistics topics that we touched
    on, take a look at *Understanding Statistics in the Behavioral Sciences*, which
    can be found at [http://www.amazon.com/Understanding-Statistics-Behavioral-Sciences-Robert/dp/0495596523](http://www.amazon.com/Understanding-Statistics-Behavioral-Sciences-Robert/dp/0495596523).
  prefs: []
  type: TYPE_NORMAL
