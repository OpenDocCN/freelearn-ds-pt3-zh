<html><head></head><body>
		<div id="_idContainer219">
			<h1 id="_idParaDest-83"><em class="italic"><a id="_idTextAnchor082"/>Chapter 4</em>: Aggregating Pandas DataFrames</h1>
			<p>In this chapter, we will continue our discussion of data wrangling from <a href="B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Wrangling with Pandas</em>, by addressing the enrichment and aggregation of data. This includes essential skills, such as merging dataframes, creating new columns, performing window calculations, and aggregating by group membership. Calculating aggregations and summaries will help us draw conclusions about our data.</p>
			<p>We will also take a look at the additional functionality <strong class="source-inline">pandas</strong> has for working with time series data, beyond the time series slicing we introduced in previous chapters, including how we can roll up the data with aggregation and select it based on the time of day. Much of the data we will encounter is time series data, so being able to effectively work with time series is paramount. Of course, performing these operations efficiently is important, so we will also review how to write efficient <strong class="source-inline">pandas</strong> code.</p>
			<p>This chapter will get us comfortable with performing analyses using <strong class="source-inline">DataFrame</strong> objects. Consequently, these topics are more advanced compared to the prior content and may require a few rereads, so be sure to follow along with the notebooks, which contain additional examples.</p>
			<p>The following topics will be covered in this chapter:</p>
			<ul>
				<li>Performing database-style operations on DataFrames</li>
				<li>Using DataFrame operations to enrich data</li>
				<li>Aggregating data</li>
				<li>Working with time series data</li>
			</ul>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor083"/>Chapter materials</h1>
			<p>The materials for this chapter can be found on GitHub at <a href="https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_04">https://github.com/stefmolin/Hands-On-Data-Analysis-with-Pandas-2nd-edition/tree/master/ch_04</a>. There are four notebooks that we will work through, each numbered according to when they will be used. The text will prompt you to switch. We will begin with the <strong class="source-inline">1-querying_and_merging.ipynb</strong> notebook to learn about querying and merging dataframes. Then, we will move on to the <strong class="source-inline">2-dataframe_operations.ipynb</strong> notebook to discuss data enrichment through operations such as binning, window functions, and pipes. For this section, we will also use the <strong class="source-inline">window_calc.py</strong> Python file, which contains a function for performing window calculations using pipes.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The <strong class="source-inline">understanding_window_calculations.ipynb</strong> notebook contains some interactive visualizations for understanding window functions. This may require some additional setup, but the instructions are in the notebook.</p>
			<p>Next, in the <strong class="source-inline">3-aggregations.ipynb</strong> notebook, we will discuss aggregations, pivot tables, and crosstabs. Finally, we will focus on additional capabilities <strong class="source-inline">pandas</strong> provides when working with time series data in the <strong class="source-inline">4-time_series.ipynb</strong> notebook. Note that we will not go over the <strong class="source-inline">0-weather_data_collection.ipynb</strong> notebook; however, for those interested, it contains the code that was used to collect the data from the <strong class="bold">National Centers for Environmental Information</strong> (<strong class="bold">NCEI</strong>) API, which can be found at <a href="https://www.ncdc.noaa.gov/cdo-web/webservices/v2">https://www.ncdc.noaa.gov/cdo-web/webservices/v2</a>.</p>
			<p>Throughout this chapter, we will use a variety of datasets, which can be found in the <strong class="source-inline">data/</strong> directory:</p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/Figure_4.1_B16834.jpg" alt="Figure 4.1 – Datasets used in this chapter&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – Datasets used in this chapter</p>
			<p>Note that the <strong class="source-inline">exercises/</strong> directory contains the CSV files that are required to complete the end-of-chapter exercises. More information on these datasets can be found in the <strong class="source-inline">exercises/README.md</strong> file.</p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor084"/>Performing database-style operations on DataFrames</h1>
			<p><strong class="source-inline">DataFrame</strong> objects are analogous to<a id="_idIndexMarker497"/> tables in a <a id="_idIndexMarker498"/>database: each has a name we refer to it by, is composed of rows, and contains columns of specific data types. Consequently, <strong class="source-inline">pandas</strong> allows us to carry out database-style operations on them. Traditionally, databases support a minimum of four<a id="_idIndexMarker499"/> operations, called <strong class="bold">CRUD</strong>: <strong class="bold">C</strong>reate, <strong class="bold">R</strong>ead, <strong class="bold">U</strong>pdate, and <strong class="bold">D</strong>elete.</p>
			<p>A database query language—most commonly <strong class="bold">SQL</strong> (pronounced <em class="italic">sequel</em> or <em class="italic">S-Q-L</em>), which stands for <strong class="bold">Structured Query Language</strong>—is used to ask the database to perform these operations. Knowledge of SQL is not required for this book; however, we will look at the <a id="_idIndexMarker500"/>SQL equivalent for the <strong class="source-inline">pandas</strong> operations that will be discussed in this section since it may aid the understanding of those familiar with SQL. Many data professionals have some familiarity with basic SQL, so consult the <em class="italic">Further reading</em> section for resources<a id="_idIndexMarker501"/> that provide a more formal introduction.</p>
			<p>For this section, we will be working in the <strong class="source-inline">1-querying_and_merging.ipynb</strong> notebook. We will begin with our imports and read in the NYC weather data CSV file:</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; weather = pd.read_csv('data/nyc_weather_2018.csv')</p>
			<p class="source-code">&gt;&gt;&gt; weather.head()</p>
			<p>This is long format data—we have <a id="_idIndexMarker502"/>several different weather observations per day for various stations covering NYC in 2018:</p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/Figure_4.2_B16834.jpg" alt="Figure 4.2 – NYC weather data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – NYC weather data</p>
			<p>In <a href="B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Working with Pandas DataFrames</em>, we covered how to create dataframes; this was the <strong class="source-inline">pandas</strong> equivalent of a <strong class="source-inline">"CREATE TABLE ..."</strong> SQL statement. When we discussed selection and filtering in <a href="B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Working with Pandas DataFrames</em>, and <a href="B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Wrangling with Pandas</em>, we were focusing on reading from dataframes, which equated to the <strong class="source-inline">SELECT</strong> (picking columns) and <strong class="source-inline">WHERE</strong> (filtering by Boolean criteria) SQL clauses. We carried out update (<strong class="source-inline">UPDATE</strong> in SQL) and delete (<strong class="source-inline">DELETE FROM</strong> in SQL) operations when we discussed working with missing data in <a href="B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Wrangling with Pandas</em>. In addition to those basic CRUD operations, the concept of a <strong class="bold">join</strong> or <strong class="bold">merge</strong> of tables exists. We will discuss the <strong class="source-inline">pandas</strong> implementation in this<a id="_idIndexMarker503"/> section, along with the idea <a id="_idIndexMarker504"/>of querying a <strong class="source-inline">DataFrame</strong> object.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor085"/>Querying DataFrames</h2>
			<p>Pandas provides the <strong class="source-inline">query()</strong> method so that <a id="_idIndexMarker505"/>we can easily write complicated filters instead of using a Boolean mask. The syntax is similar to the <strong class="source-inline">WHERE</strong> clause in a SQL statement. To illustrate this, let's query the weather data for all the rows where the value of the <strong class="source-inline">SNOW</strong> column was greater than zero for stations with <strong class="source-inline">US1NY</strong> in their station ID:</p>
			<p class="source-code">&gt;&gt;&gt; snow_data = weather.<strong class="bold">query(</strong></p>
			<p class="source-code">...     <strong class="bold">'datatype == "SNOW" and value &gt; 0 '</strong></p>
			<p class="source-code">...     <strong class="bold">'and station.str.contains("US1NY")'</strong></p>
			<p class="source-code">... <strong class="bold">) </strong></p>
			<p class="source-code">&gt;&gt;&gt; snow_data.head()</p>
			<p>Each row is a snow observation for a given combination of date and station. Notice that the values vary quite a bit for January 4th—some stations received more snow than others:</p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/Figure_4.3_B16834.jpg" alt="Figure 4.3 – Querying the weather data for observations of snow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – Querying the weather data for observations of snow</p>
			<p>This query is equivalent to<a id="_idIndexMarker506"/> the following in SQL. Note that <strong class="source-inline">SELECT *</strong> selects all the columns in the table (our dataframe, in this case):</p>
			<p class="source-code">SELECT * FROM weather</p>
			<p class="source-code">WHERE</p>
			<p class="source-code">  datatype == "SNOW" AND value &gt; 0 AND station LIKE "%US1NY%";</p>
			<p>In <a href="B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Working with Pandas DataFrames</em>, we learned how to use a Boolean mask to get the same result:</p>
			<p class="source-code">&gt;&gt;&gt; weather[</p>
			<p class="source-code">...     <strong class="bold">(weather.datatype == 'SNOW')</strong> <strong class="bold">&amp; (weather.value &gt; 0)</strong></p>
			<p class="source-code">...     <strong class="bold">&amp; weather.station.str.contains('US1NY')</strong></p>
			<p class="source-code">... ].equals(snow_data)</p>
			<p class="source-code">True</p>
			<p>For the most part, which one we use is a matter of preference; however, if we have a long name for our dataframe, we will probably prefer the <strong class="source-inline">query()</strong> method. In the previous example, we had to type the dataframe's name an additional three times in order to use the mask.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">When using Boolean logic with the <strong class="source-inline">query()</strong> method, we can use both logical operators (<strong class="source-inline">and</strong>, <strong class="source-inline">or</strong>, <strong class="source-inline">not</strong>) and<a id="_idIndexMarker507"/> bitwise operators (<strong class="source-inline">&amp;</strong>, <strong class="source-inline">|</strong>, <strong class="source-inline">~</strong>).</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor086"/>Merging DataFrames</h2>
			<p>When we discussed stacking dataframes one on top of the other with the <strong class="source-inline">pd.concat()</strong> function and the <strong class="source-inline">append()</strong> method in <a href="B16834_02_Final_SK_ePub.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Working with Pandas DataFrames</em>, we were performing<a id="_idIndexMarker508"/> the equivalent of the SQL <strong class="source-inline">UNION ALL</strong> statement (or just <strong class="source-inline">UNION</strong>, if we also removed the duplicates, as we saw in <a href="B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Wrangling with Pandas</em>). Merging dataframes deals with how to line them up by row.</p>
			<p>When referring to <a id="_idIndexMarker509"/>databases, merging is traditionally called a <strong class="bold">join</strong>. There are four types of joins: full (outer), left, right, and inner. These join types let us know how the result will be affected by values that are only present on one side of the join. This is a concept that's much more easily understood visually, so let's look at some Venn diagrams and then do some sample joins on the weather data. Here, the darker regions represent the data we are left with after performing the join: </p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/Figure_4.4_B16834.jpg" alt="Figure 4.4 – Understanding join types&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4 – Understanding join types</p>
			<p>We have been working with data from numerous weather stations, but we don't know anything about them besides their IDs. It would be helpful to know exactly where each of the stations is located to better understand discrepancies between weather readings for the same day in NYC. When we queried for the snow data, we saw quite a bit of variation in the readings<a id="_idIndexMarker510"/> for January 4th (see <em class="italic">Figure 4.3</em>). This is most likely due to the location of the station. Stations at higher elevations or farther north may record more snow. Depending on how far they actually are from NYC, they may have been experiencing a snowstorm that was heavier somewhere else, such as Connecticut or Northern New Jersey.</p>
			<p>The NCEI API's <strong class="source-inline">stations</strong> endpoint gives us all the information we need for the stations. This is in the <strong class="source-inline">weather_stations.csv</strong> file, as well as in the <strong class="source-inline">stations</strong> table in the SQLite database. Let's read this data into a dataframe:</p>
			<p class="source-code">&gt;&gt;&gt; station_info = pd.read_csv('data/weather_stations.csv')</p>
			<p class="source-code">&gt;&gt;&gt; station_info.head()</p>
			<p>For reference, Central Park in NYC is at 40.7829° N, 73.9654° W (latitude 40.7829 and longitude -73.9654), and NYC has an elevation of 10 meters. The first five stations that record NYC data are not in New York. The ones in New Jersey are southwest of NYC, while the ones in Connecticut are northeast of NYC:</p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/Figure_4.5_B16834.jpg" alt="Figure 4.5 – Weather stations dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5 – Weather stations dataset</p>
			<p>Joins require us to specify how to match the data up. The only data the <strong class="source-inline">weather</strong> dataframe has in common with the <strong class="source-inline">station_info</strong> dataframe is the station ID. However, the columns containing this information are not named the same: in the <strong class="source-inline">weather</strong> dataframe, this column is called <strong class="source-inline">station</strong>, while in the <strong class="source-inline">station_info</strong> dataframe, it is called <strong class="source-inline">id</strong>. Before we join the data, let's get some information on how many distinct stations we have and how many entries are in each dataframe:</p>
			<p class="source-code">&gt;&gt;&gt; station_info.id.describe()</p>
			<p class="source-code"><strong class="bold">count                   279</strong></p>
			<p class="source-code"><strong class="bold">unique                  279</strong></p>
			<p class="source-code">top       GHCND:US1NJBG0029</p>
			<p class="source-code">freq                      1</p>
			<p class="source-code">Name: id, dtype: object</p>
			<p class="source-code">&gt;&gt;&gt; weather.station.describe()</p>
			<p class="source-code"><strong class="bold">count                 78780</strong></p>
			<p class="source-code"><strong class="bold">unique                  110</strong></p>
			<p class="source-code">top       GHCND:USW00094789</p>
			<p class="source-code">freq                   4270</p>
			<p class="source-code">Name: station, dtype: object</p>
			<p>The difference in the number of unique stations across the dataframes tells us they don't contain all the <a id="_idIndexMarker511"/>same stations. Depending on the type of join we pick, we may lose some data. Therefore, it's important to look at the row count before and after the join. We can see this in the <strong class="bold">count</strong> entry from the output of <strong class="source-inline">describe()</strong>, but we don't need to run that just to get the row count. Instead, we can use the <strong class="source-inline">shape</strong> attribute, which gives us a tuple of the form <strong class="source-inline">(number of rows, number of columns)</strong>. To select the rows, we just grab the value at index <strong class="source-inline">0</strong> (<strong class="source-inline">1</strong> for columns):</p>
			<p class="source-code">&gt;&gt;&gt; station_info.shape[0], weather.shape[0] # 0=rows, 1=cols</p>
			<p class="source-code">(279, 78780)</p>
			<p>Since we will be checking the row count often, it makes more sense to write a function that will give us the row count for any number of dataframes. The <strong class="source-inline">*dfs</strong> argument collects all the input to this function in a tuple, which we can iterate over in a list comprehension to get the row count:</p>
			<p class="source-code">&gt;&gt;&gt; def get_row_count(*dfs):</p>
			<p class="source-code">...     return [df.shape[0] for df in dfs]</p>
			<p class="source-code">&gt;&gt;&gt; get_row_count(station_info, weather)</p>
			<p class="source-code">[279, 78780]</p>
			<p>Now that we know that we have 78,780 rows of weather data and 279 rows of station information data, we can begin looking at the types of joins. We'll begin with the inner join, which will result in the least amount of rows (unless the two dataframes have all the same values for the column being joined on, in which case all the joins will be equivalent). The <strong class="bold">inner join</strong> will return the <a id="_idIndexMarker512"/>columns from both dataframes where they have a match on the specified key column. Since we will be joining on the <strong class="source-inline">weather.station</strong> column and the <strong class="source-inline">station_info.id</strong> column, we will only get weather data for stations that are in <strong class="source-inline">station_info</strong>.</p>
			<p>We will use the <strong class="source-inline">merge()</strong> method to <a id="_idIndexMarker513"/>perform the join (which is an inner join by default) by providing the left and right dataframes, along with specifying which columns to join on. Since the station ID column is named differently across dataframes, we must specify the names with <strong class="source-inline">left_on</strong> and <strong class="source-inline">right_on</strong>. The left dataframe is the one we call <strong class="source-inline">merge()</strong> on, while the right one is the dataframe that gets passed in as an argument:</p>
			<p class="source-code">&gt;&gt;&gt; inner_join = <strong class="bold">weather.merge(</strong></p>
			<p class="source-code">...     <strong class="bold">station_info, left_on='station', right_on='id'</strong></p>
			<p class="source-code">... <strong class="bold">)</strong></p>
			<p class="source-code">&gt;&gt;&gt; inner_join.sample(5, random_state=0)</p>
			<p>Notice that we have five additional columns, which have been added to the right. These came from the <strong class="source-inline">station_info</strong> dataframe. This operation also kept both the <strong class="source-inline">station</strong> and <strong class="source-inline">id</strong> columns, which are identical:</p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/Figure_4.6_B16834.jpg" alt="Figure 4.6 – Results of an inner join between the weather and stations datasets&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6 – Results of an inner join between the weather and stations datasets</p>
			<p>In order to remove the <a id="_idIndexMarker514"/>duplicate information in the <strong class="source-inline">station</strong> and <strong class="source-inline">id</strong> columns, we can rename one of them before the join. Consequently, we will only have to supply a value for the <strong class="source-inline">on</strong> parameter because the columns will share the same name:</p>
			<p class="source-code">&gt;&gt;&gt; weather.merge(</p>
			<p class="source-code">...     <strong class="bold">station_info.rename(dict(id='station'), axis=1),</strong> </p>
			<p class="source-code">...     <strong class="bold">on='station'</strong></p>
			<p class="source-code">... ).sample(5, random_state=0)</p>
			<p>Since the columns shared the name, we only get one back after joining on them:</p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/Figure_4.7_B16834.jpg" alt="Figure 4.7 – Matching the names of the joining column to prevent duplicate data in the result&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7 – Matching the names of the joining column to prevent duplicate data in the result</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We can join on multiple columns by passing the list of column names to the <strong class="source-inline">on</strong> parameter or to the <strong class="source-inline">left_on</strong> and <strong class="source-inline">right_on</strong> parameters.</p>
			<p>Remember that we had 279 unique stations in the <strong class="source-inline">station_info</strong> dataframe, but only 110 unique stations for the weather data. When we performed the inner join, we lost all the stations that didn't have weather observations associated with them. If we don't want to lose rows on <a id="_idIndexMarker515"/>a particular side of the join, we can perform a left or<a id="_idIndexMarker516"/> right join instead. A <strong class="bold">left join</strong> requires us to list the dataframe with the rows that we want to keep (even if they don't exist in the other dataframe) on the left and the<a id="_idIndexMarker517"/> other dataframe on the right; a <strong class="bold">right join</strong> is the inverse:</p>
			<p class="source-code">&gt;&gt;&gt; left_join = station_info.merge(</p>
			<p class="source-code">...     weather, left_on='id', right_on='station', <strong class="bold">how='left'</strong></p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; right_join = weather.merge(</p>
			<p class="source-code">...     station_info, left_on='station', right_on='id',</p>
			<p class="source-code">...     <strong class="bold">how='right'</strong></p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; right_join[right_join.datatype.isna()].head() # see nulls</p>
			<p>Wherever the other dataframe contains no data, we will get null values. We may want to investigate why we don't have any weather data associated with these stations. Alternatively, our analysis may involve determining the availability of data per station, so getting null values isn't necessarily an issue:</p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/Figure_4.8_B16834.jpg" alt="Figure 4.8 – Null values may be introduced when not using an inner join&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8 – Null values may be introduced when not using an inner join</p>
			<p>Since we placed the <strong class="source-inline">station_info</strong> dataframe on the left for the left join and on the right for the right join, the results here are equivalent. In both cases, we chose to keep all the stations <a id="_idIndexMarker518"/>present in the <strong class="source-inline">station_info</strong> dataframe, accepting null values for the weather observations. To prove they are equivalent, we need to put the columns in the same order, reset the index, and sort the data:</p>
			<p class="source-code">&gt;&gt;&gt; left_join.sort_index(axis=1)\</p>
			<p class="source-code">...     .sort_values(['date', 'station'], ignore_index=True)\</p>
			<p class="source-code">...     .equals(right_join.sort_index(axis=1).sort_values(</p>
			<p class="source-code">...         ['date', 'station'], ignore_index=True</p>
			<p class="source-code">...     ))</p>
			<p class="source-code">True</p>
			<p>Note that we have additional rows in the left and right joins because we kept all the stations that didn't have weather observations:</p>
			<p class="source-code">&gt;&gt;&gt; get_row_count(inner_join, left_join, right_join)</p>
			<p class="source-code">[78780, 78949, 78949]</p>
			<p>The final type of join is a <strong class="bold">full outer join</strong>, which will <a id="_idIndexMarker519"/>keep all the values, regardless of whether or not they exist in both dataframes. For instance, say we queried for stations with <strong class="source-inline">US1NY</strong> in their station ID because we believed that stations measuring NYC weather would have to be labeled as such. This means that an inner join would result in losing observations from the stations in Connecticut and New Jersey, while a left/right join would result in either lost station information or lost weather data. The outer join will preserve all the <a id="_idIndexMarker520"/>data. We will also pass in <strong class="source-inline">indicator=True</strong> to add an additional column to the resulting dataframe, which will indicate which dataframe each row came from:</p>
			<p class="source-code">&gt;&gt;&gt; outer_join = weather.merge(</p>
			<p class="source-code">...     station_info[station_info.id.str.contains('US1NY')], </p>
			<p class="source-code">...     left_on='station', right_on='id',</p>
			<p class="source-code">...     <strong class="bold">how='outer', indicator=True</strong></p>
			<p class="source-code">... )</p>
			<p class="source-code"># view effect of outer join</p>
			<p class="source-code">&gt;&gt;&gt; pd.concat([</p>
			<p class="source-code">...     outer_join.query(f'_merge == "{kind}"')\</p>
			<p class="source-code">...         .sample(2, random_state=0)</p>
			<p class="source-code">...     for kind in outer_join._merge.unique()</p>
			<p class="source-code">... ]).sort_index()</p>
			<p>Indices <strong class="bold">23634</strong> and <strong class="bold">25742</strong> come from stations located in New York, and the match gives us information about the station. Indices <strong class="bold">60645</strong> and <strong class="bold">70764</strong> are for stations that don't have <strong class="source-inline">US1NY</strong> in their station ID, causing nulls for the station information columns. The bottom two rows are stations in New York that aren't providing weather observations for NYC. This join keeps all the data and will often introduce null values, unlike inner joins, which won't:</p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="image/Figure_4.9_B16834.jpg" alt="Figure 4.9 – An outer join keeps all the data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9 – An outer join keeps all the data</p>
			<p>The aforementioned joins <a id="_idIndexMarker521"/>are equivalent to SQL statements of the following form, where we simply change <strong class="source-inline">&lt;JOIN_TYPE&gt;</strong> to <strong class="source-inline">(INNER) JOIN</strong>, <strong class="source-inline">LEFT JOIN</strong>, <strong class="source-inline">RIGHT JOIN</strong>, or <strong class="source-inline">FULL OUTER JOIN</strong> for the appropriate join:</p>
			<p class="source-code">SELECT *</p>
			<p class="source-code">FROM left_table</p>
			<p class="source-code">&lt;JOIN_TYPE&gt; right_table</p>
			<p class="source-code">ON left_table.&lt;col&gt; == right_table.&lt;col&gt;;</p>
			<p>Joining dataframes makes working with the dirty data in <a href="B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Wrangling with Pandas</em>, easier. Remember, we had data from two distinct stations: one had a valid station ID and the other was <strong class="source-inline">?</strong>. The <strong class="source-inline">?</strong> station was the only one recording the water equivalent of snow (<strong class="source-inline">WESF</strong>). Now that we know about joining dataframes, we can join the data from the valid station ID to the data from the <strong class="source-inline">?</strong> station that we are missing by date. First, we will need to read in the CSV file, setting the <strong class="source-inline">date</strong> column as the index. We will drop the duplicates and the <strong class="source-inline">SNWD</strong> column (snow depth), which we found to be uninformative since most of the values were infinite (both in the presence and absence of snow): </p>
			<p class="source-code">&gt;&gt;&gt; dirty_data = pd.read_csv(</p>
			<p class="source-code">...     'data/dirty_data.csv', index_col='date'</p>
			<p class="source-code">... ).drop_duplicates().drop(columns='SNWD')</p>
			<p class="source-code">&gt;&gt;&gt; dirty_data.head()</p>
			<p>Our starting data looks like this:</p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/Figure_4.10_B16834.jpg" alt="Figure 4.10 – Dirty data from the previous chapter&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10 – Dirty data from the previous chapter</p>
			<p>Now, we need to<a id="_idIndexMarker522"/> create a dataframe for each station. To reduce output, we will drop some additional columns:</p>
			<p class="source-code">&gt;&gt;&gt; valid_station = dirty_data.query('station != "?"')\</p>
			<p class="source-code">...     .drop(columns=['WESF', 'station'])</p>
			<p class="source-code">&gt;&gt;&gt; station_with_wesf = dirty_data.query('station == "?"')\</p>
			<p class="source-code">...     .drop(columns=['station', 'TOBS', 'TMIN', 'TMAX'])</p>
			<p>This time, the column we want to join on (the date) is actually the index, so we will pass in <strong class="source-inline">left_index</strong> to indicate that the column to use from the left dataframe is the index, and then <strong class="source-inline">right_index</strong> to indicate the same for the right dataframe. We will perform a left join to make sure we don't lose any rows from our valid station, and, where possible, augment them with the observations from the <strong class="source-inline">?</strong> station:</p>
			<p class="source-code">&gt;&gt;&gt; valid_station.merge(</p>
			<p class="source-code">...     station_with_wesf, how='left',</p>
			<p class="source-code">...     <strong class="bold">left_index=True, right_index=True</strong></p>
			<p class="source-code">... ).query('WESF &gt; 0').head()</p>
			<p>For all the columns that the dataframes had in common, but weren't part of the join, we have two versions now. The versions coming from the left dataframe have the <strong class="source-inline">_x</strong> suffix appended to the column names, and those coming from the right dataframe have <strong class="source-inline">_y</strong> as the suffix:</p>
			<div>
				<div id="_idContainer164" class="IMG---Figure">
					<img src="image/Figure_4.11_B16834.jpg" alt="Figure 4.11 – Merging weather data from different stations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.11 – Merging weather data from different stations</p>
			<p>We can provide our<a id="_idIndexMarker523"/> own suffixes with the <strong class="source-inline">suffixes</strong> parameter. Let's use a suffix for the <strong class="source-inline">?</strong> station only:</p>
			<p class="source-code">&gt;&gt;&gt; valid_station.merge(</p>
			<p class="source-code">...     station_with_wesf, how='left',</p>
			<p class="source-code">...     left_index=True, right_index=True, </p>
			<p class="source-code">...     <strong class="bold">suffixes=('', '_?')</strong></p>
			<p class="source-code">... ).query('WESF &gt; 0').head()</p>
			<p>Since we specified an empty string for the left suffix, the columns coming from the left dataframe have their original names. However, the right suffix of <strong class="source-inline">_?</strong> was added to the names of the columns that came from the right dataframe:</p>
			<div>
				<div id="_idContainer165" class="IMG---Figure">
					<img src="image/Figure_4.12_B16834.jpg" alt="Figure 4.12 – Specifying the suffix for shared columns not being used in the join&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.12 – Specifying the suffix for shared columns not being used in the join</p>
			<p>When we are joining on the index, an easier way to do this is to use the <strong class="source-inline">join()</strong> method instead of <strong class="source-inline">merge()</strong>. It also defaults to an inner join, but this behavior can be changed with the <strong class="source-inline">how</strong> parameter, just like with <strong class="source-inline">merge()</strong>. The <strong class="source-inline">join()</strong> method will always use the index of the left dataframe to join, but it can use a column in the right dataframe if its name is passed to the <strong class="source-inline">on</strong> parameter. Note that suffixes are now specified using <strong class="source-inline">lsuffix</strong> for the left dataframe's suffix and <strong class="source-inline">rsuffix</strong> for the right one. This yields the same result as the previous example (<em class="italic">Figure 4.12</em>):</p>
			<p class="source-code">&gt;&gt;&gt; valid_station.join(</p>
			<p class="source-code">...     station_with_wesf, how='left', rsuffix='_?'</p>
			<p class="source-code">... ).query('WESF &gt; 0').head()</p>
			<p>One important thing to<a id="_idIndexMarker524"/> keep in mind is that joins can be rather resource-intensive, so it is often beneficial to figure out what will happen to the rows before going through with it. If we don't already know what type of join we want, this can help give us an idea. We <a id="_idIndexMarker525"/>can use <strong class="bold">set operations</strong> on the index we plan to join on to figure this out.</p>
			<p>Remember that the mathematical definition of a <strong class="bold">set</strong> is a collection of distinct objects. By definition, the index is a set. Set operations are often explained with Venn diagrams: </p>
			<div>
				<div id="_idContainer166" class="IMG---Figure">
					<img src="image/Figure_4.13_B16834.jpg" alt="Figure 4.13 – Set operations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.13 – Set operations</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Note that <strong class="source-inline">set</strong> is also a Python type that's available in the standard library. A common use of sets is to remove duplicates from a list. More information on sets in Python can be found in the documentation at <a href="https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset">https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset</a>.</p>
			<p>Let's use the <strong class="source-inline">weather</strong> and <strong class="source-inline">station_info</strong> dataframes to illustrate set operations. First, we must set the index to the column(s) that will be used for the join operation:</p>
			<p class="source-code">&gt;&gt;&gt; weather.set_index('station', inplace=True)</p>
			<p class="source-code">&gt;&gt;&gt; station_info.set_index('id', inplace=True)</p>
			<p>To see what will remain <a id="_idIndexMarker526"/>with an inner join, we can take the <strong class="bold">intersection</strong> of the indices, which shows us the overlapping stations:</p>
			<p class="source-code">&gt;&gt;&gt; weather.index.<strong class="bold">intersection</strong>(station_info.index)</p>
			<p class="source-code">Index(['GHCND:US1CTFR0039', ..., 'GHCND:USW1NYQN0029'],</p>
			<p class="source-code">      dtype='object', <strong class="bold">length=110</strong>)</p>
			<p>As we saw when we ran the inner join, we only got station information for the stations with weather observations. This doesn't tell us what we lost, though; for this, we need to find the <strong class="bold">set difference</strong>, which will <a id="_idIndexMarker527"/>subtract the sets and give us the values of the first index that aren't in the second. With the set difference, we can easily see that, when performing an inner join, we don't lose any rows from the weather data, but we lose 169 stations that don't have weather observations:</p>
			<p class="source-code">&gt;&gt;&gt; weather.index.<strong class="bold">difference</strong>(station_info.index)</p>
			<p class="source-code">Index(<strong class="bold">[]</strong>, dtype='object')</p>
			<p class="source-code">&gt;&gt;&gt; station_info.index.<strong class="bold">difference</strong>(weather.index)</p>
			<p class="source-code">Index(['GHCND:US1CTFR0022', ..., 'GHCND:USW00014786'],</p>
			<p class="source-code">      dtype='object', <strong class="bold">length=169</strong>)</p>
			<p>Note that this output also tells us how left and right joins will turn out. To avoid losing rows, we want to put the <strong class="source-inline">station_info</strong> dataframe on the same side as the join (on the left for a left join and on the right for a right join).</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We can use the <strong class="source-inline">symmetric_difference()</strong> method on the indices of the dataframes involved in the join to see what will be lost from both sides: <strong class="source-inline">index_1.symmetric_difference(index_2)</strong>. The result will be the values that are only in one of the indices. An example is in the notebook.</p>
			<p>Lastly, we can use the <strong class="bold">union</strong> to view all the values we <a id="_idIndexMarker528"/>will keep if we run a full outer join. Remember, the <strong class="source-inline">weather</strong> dataframe contains the stations repeated throughout because they provide daily<a id="_idIndexMarker529"/> measurements, so we call the <strong class="source-inline">unique()</strong> method before taking the union to see the number of stations we will keep:</p>
			<p class="source-code">&gt;&gt;&gt; weather.index.unique().<strong class="bold">union</strong>(station_info.index)</p>
			<p class="source-code">Index(['GHCND:US1CTFR0022', ..., 'GHCND:USW00094789'],</p>
			<p class="source-code">      dtype='object', <strong class="bold">length=279</strong>)</p>
			<p>The <em class="italic">Further reading</em> section at the end of this chapter contains some resources on set operations and how <strong class="source-inline">pandas</strong> compares to SQL. For now, let's move on to data enrichment.</p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor087"/>Using DataFrame operations to enrich data</h1>
			<p>Now that we've discussed how <a id="_idIndexMarker530"/>to query and merge <strong class="source-inline">DataFrame</strong> objects, let's learn how to perform complex operations on them to create and modify columns and rows. For this section, we will be working in the <strong class="source-inline">2-dataframe_operations.ipynb</strong> notebook using the weather data, along with Facebook stock's volume traded and opening, high, low, and closing prices daily for 2018. Let's import what we will need and read in the data:</p>
			<p class="source-code">&gt;&gt;&gt; import numpy as np</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; weather = pd.read_csv(</p>
			<p class="source-code">...     'data/nyc_weather_2018.csv', parse_dates=['date']</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; fb = pd.read_csv(</p>
			<p class="source-code">...     'data/fb_2018.csv', index_col='date', parse_dates=True</p>
			<p class="source-code">... )</p>
			<p>We will begin by<a id="_idIndexMarker531"/> reviewing operations that summarize entire rows and columns before moving on to binning, applying functions across rows and columns, and window calculations, which summarize data along a certain number of observations at a time (such as moving averages).</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor088"/>Arithmetic and statistics</h2>
			<p>Pandas has several<a id="_idIndexMarker532"/> methods for calculating statistics and performing mathematical operations, including comparisons, floor division, and the modulo operation. These methods give us more flexibility in how we define the calculation by allowing us to specify the axis to perform the calculation on (when performing it on a <strong class="source-inline">DataFrame</strong> object). By default, the calculation will be performed along the columns (<strong class="source-inline">axis=1</strong> or <strong class="source-inline">axis='columns'</strong>), which generally contain observations of a single variable of a single data type; however, we can pass in <strong class="source-inline">axis=0</strong> or <strong class="source-inline">axis='index'</strong> to perform the calculation along the rows instead.</p>
			<p>In this section, we are going to use a few of these methods to create new columns and modify our data to see how we can use new data to draw some initial conclusions. Note that the complete list can be found at <a href="https://pandas.pydata.org/pandas-docs/stable/reference/series.html#binary-operator-functions">https://pandas.pydata.org/pandas-docs/stable/reference/series.html#binary-operator-functions</a>.</p>
			<p>To start off, let's create a column with the Z-score for the volume traded in Facebook stock and use it to find the days where the Z-score is greater than three in absolute value. These values are more than three standard deviations from the mean, which may be abnormal (depending on the data). Remember from our discussion of Z-scores in <a href="B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to Data Analysis</em>, that we calculate them by subtracting the mean and dividing by the standard deviation. Rather than using mathematical operators for subtraction and division, we will use the <strong class="source-inline">sub()</strong> and <strong class="source-inline">div()</strong> methods, respectively:</p>
			<p class="source-code">&gt;&gt;&gt; fb.assign(</p>
			<p class="source-code">...     abs_z_score_volume=lambda x: <strong class="bold">x.volume</strong> \</p>
			<p class="source-code">...         <strong class="bold">.sub(x.volume.mean()).div(x.volume.std()).abs()</strong></p>
			<p class="source-code">... ).query('abs_z_score_volume &gt; 3')</p>
			<p>Five days in 2018 had Z-scores for volume traded greater than three in absolute value. These dates in particular will come up often in the rest of this chapter as they mark some trouble points for Facebook's stock price:</p>
			<div>
				<div id="_idContainer167" class="IMG---Figure">
					<img src="image/Figure_4.14_B16834.jpg" alt="Figure 4.14 – Adding a Z-score column&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.14 – Adding a Z-score column</p>
			<p>Two other very useful methods are <strong class="source-inline">rank()</strong> and <strong class="source-inline">pct_change()</strong>, which let us rank the values of a column (and store them in a new column) and calculate the percentage change between<a id="_idIndexMarker533"/> periods, respectively. By combining these, we can see which five days had the largest percentage change of volume traded in Facebook stock from the day prior:</p>
			<p class="source-code">&gt;&gt;&gt; fb.assign(</p>
			<p class="source-code">...     volume_pct_change=fb.volume.<strong class="bold">pct_change()</strong>,</p>
			<p class="source-code">...     pct_change_rank=lambda x: \</p>
			<p class="source-code">...         x.volume_pct_change.abs().<strong class="bold">rank(ascending=False)</strong></p>
			<p class="source-code">... ).nsmallest(5, 'pct_change_rank')</p>
			<p>The day with the largest percentage change in volume traded was January 12, 2018, which happens to coincide with one of the many Facebook scandals that shook the stock in 2018 (<a href="https://www.cnbc.com/2018/11/20/facebooks-scandals-in-2018-effect-on-stock.html">https://www.cnbc.com/2018/11/20/facebooks-scandals-in-2018-effect-on-stock.html</a>). This was when they announced changes to the news feed to prioritize content from a user's friends over brands they follow. Given that a large component of Facebook's revenue comes from advertising (nearly 89% in 2017, <em class="italic">source</em>: <a href="https://www.investopedia.com/ask/answers/120114/how-does-facebook-fb-make-money.asp">https://www.investopedia.com/ask/answers/120114/how-does-facebook-fb-make-money.asp</a>), this caused panic as <a id="_idIndexMarker534"/>many sold the stock, driving up the volume traded drastically and dropping the stock price:</p>
			<div>
				<div id="_idContainer168" class="IMG---Figure">
					<img src="image/Figure_4.15_B16834.jpg" alt="Figure 4.15 – Ranking trading days by percentage change in volume traded&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.15 – Ranking trading days by percentage change in volume traded</p>
			<p>We can use slicing to look at the change around this announcement:</p>
			<p class="source-code">&gt;&gt;&gt; fb['2018-01-11':'2018-01-12']</p>
			<p>Notice how we are able to combine everything we learned in the last few chapters to get interesting insights from our data. We were able to sift through a year's worth of stock data and find some days that had large effects on Facebook stock (good or bad):</p>
			<div>
				<div id="_idContainer169" class="IMG---Figure">
					<img src="image/Figure_4.16_B16834.jpg" alt="Figure 4.16 – Facebook stock data before and after announcing changes to the news feed&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.16 – Facebook stock data before and after announcing changes to the news feed</p>
			<p>Lastly, we can inspect the dataframe with aggregated Boolean operations. For example, we can see that Facebook stock never had a daily low price greater than $215 in 2018 with the <strong class="source-inline">any()</strong> method:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">(fb &gt; 215).any()</strong></p>
			<p class="source-code">open          True</p>
			<p class="source-code">high          True</p>
			<p class="source-code"><strong class="bold">low          False</strong></p>
			<p class="source-code">close         True</p>
			<p class="source-code">volume        True</p>
			<p class="source-code">dtype: bool</p>
			<p>If we want to see if all the rows in a <a id="_idIndexMarker535"/>column meet the criteria, we can use the <strong class="source-inline">all()</strong> method. This tells us that Facebook has at least one day for the opening, high, low, and closing prices with a value less than or equal to $215:</p>
			<p class="source-code">&gt;&gt;&gt; (fb &gt; 215).<strong class="bold">all()</strong></p>
			<p class="source-code">open      False</p>
			<p class="source-code">high      False</p>
			<p class="source-code">low       False</p>
			<p class="source-code">close     False</p>
			<p class="source-code">volume     True</p>
			<p class="source-code">dtype: bool</p>
			<p>Now, let's take a look at how we can use binning to divide up our data rather than a specific value, such as $215 in the <strong class="source-inline">any()</strong> and <strong class="source-inline">all()</strong> examples.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor089"/>Binning</h2>
			<p>Sometimes, it's more <a id="_idIndexMarker536"/>convenient to work with categories rather than the specific values. A common example is working with ages—most likely, we don't want to look at the data for each age, such as 25 compared to 26; however, we may very well be interested in how the group of individuals aged 25-34 compares to the group of those aged 35-44. This is called <strong class="bold">binning</strong> or <strong class="bold">discretizing</strong> (going from continuous to discrete); we <a id="_idIndexMarker537"/>take our data and place<a id="_idIndexMarker538"/> the observations into bins (or buckets) matching the range they<a id="_idIndexMarker539"/> fall into. By doing so, we can drastically reduce the number of distinct values our data can take on and make it easier to analyze.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">While binning our data can make certain parts of the analysis easier, keep in mind that it will reduce the information in that field since the granularity is reduced.</p>
			<p>One interesting thing we could do with the volume traded would be to see which days had high trade volume and look for news about Facebook on those days or large swings in price. Unfortunately, it is highly unlikely that the volume will be the same any two days; in fact, we can confirm that, in the data, no two days have the same volume traded:</p>
			<p class="source-code">&gt;&gt;&gt; (fb.volume.value_counts() &gt; 1).sum()</p>
			<p class="source-code">0</p>
			<p>Remember that <strong class="source-inline">fb.volume.value_counts()</strong> gives us the number of occurrences for each unique value for <strong class="source-inline">volume</strong>. We can then create a Boolean mask for whether the count is greater than 1 and sum it up (<strong class="source-inline">True</strong> evaluates to <strong class="source-inline">1</strong> and <strong class="source-inline">False</strong> evaluates to <strong class="source-inline">0</strong>). Alternatively, we can use <strong class="source-inline">any()</strong> instead of <strong class="source-inline">sum()</strong>, which, rather than telling us the number of unique values of <strong class="source-inline">volume</strong> that had more than one occurrence, would give us <strong class="source-inline">True</strong> if at least one volume traded amount occurred more than once and <strong class="source-inline">False</strong> otherwise.</p>
			<p>Clearly, we will need to create some ranges for the volume traded in order to look at the days of high trading volume, but how do we decide which range is a good range? One way is to use the <strong class="source-inline">pd.cut()</strong> function for binning based on value. First, we should decide how many bins we want to create—three seems like a good split, since we can label the bins low, medium, and high. Next, we need to determine the width of each bin; <strong class="source-inline">pandas</strong> tries to make this process as painless as possible, so if we want equally-sized bins, all we have to do is specify the number of bins we want (otherwise, we must specify the upper bound for each bin as a list):</p>
			<p class="source-code">&gt;&gt;&gt; volume_binned = <strong class="bold">pd.cut(</strong></p>
			<p class="source-code">...     <strong class="bold">fb.volume, bins=3, labels=['low', 'med', 'high']</strong></p>
			<p class="source-code">... <strong class="bold">)</strong></p>
			<p class="source-code">&gt;&gt;&gt; volume_binned.value_counts()</p>
			<p class="source-code">low     240</p>
			<p class="source-code">med       8</p>
			<p class="source-code">high      3</p>
			<p class="source-code">Name: volume, dtype: int64</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Note that we provided labels for each bin here; if we don't do this, each bin will be labeled by the interval of values it includes, which may or may not be helpful for us, depending on our application. If we want to both label the values and see the bins afterward, we can pass in <strong class="source-inline">retbins=True</strong> when we call <strong class="source-inline">pd.cut()</strong>. Then, we can access the binned data as the first element of the tuple that is returned, and the bin ranges themselves as the second element.</p>
			<p>It looks like an overwhelming majority of the trading days were in the low-volume bin; keep in mind that this is all relative because we evenly divided the range between the minimum and maximum<a id="_idIndexMarker540"/> trading volumes. Let's look at the data for the three days of high volume:</p>
			<p class="source-code">&gt;&gt;&gt; fb[volume_binned == 'high']\</p>
			<p class="source-code">...     .sort_values('volume', ascending=False)</p>
			<p>Even among the high-volume days, we can see that July 26, 2018 had a much higher trade volume compared to the other two dates in March (nearly 40 million additional shares were traded):</p>
			<div>
				<div id="_idContainer170" class="IMG---Figure">
					<img src="image/Figure_4.17_B16834.jpg" alt="Figure 4.17 – Facebook stock data on days in the high-volume traded bucket&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.17 – Facebook stock data on days in the high-volume traded bucket</p>
			<p>In fact, querying a search engine for <em class="italic">Facebook stock price July 26, 2018</em> reveals that Facebook had announced their earnings and disappointing user growth after market close on July 25th, which was followed by lots of after-hours selling. When the market opened the next morning, the <a id="_idIndexMarker541"/>stock had dropped from $217.50 at close on the 25th to $174.89 at market open on the 26th. Let's pull out this data:</p>
			<p class="source-code">&gt;&gt;&gt; fb['2018-07-25':'2018-07-26']</p>
			<p>Not only was there a huge drop in stock price, but the volume traded also skyrocketed, increasing by more than 100 million. All of this resulted in a loss of about $120 billion in Facebook's market capitalization (<a href="https://www.marketwatch.com/story/facebook-stock-crushed-after-revenue-user-growth-miss-2018-07-25">https://www.marketwatch.com/story/facebook-stock-crushed-after-revenue-user-growth-miss-2018-07-25</a>):</p>
			<div>
				<div id="_idContainer171" class="IMG---Figure">
					<img src="image/Figure_4.18_B16834.jpg" alt="Figure 4.18 – Facebook stock data leading up to the day with the highest volume traded in 2018&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.18 – Facebook stock data leading up to the day with the highest volume traded in 2018</p>
			<p>If we look at the other two days marked as high-volume trading days, we will find a plethora of information as to why. Both of these days were marked by scandal for Facebook. The Cambridge Analytica political data privacy scandal broke on Saturday, March 17, 2018, so trading with this information didn't commence until Monday the 19th (<a href="https://www.nytimes.com/2018/03/19/technology/facebook-cambridge-analytica-explained.html">https://www.nytimes.com/2018/03/19/technology/facebook-cambridge-analytica-explained.html</a>):</p>
			<p class="source-code">&gt;&gt;&gt; fb['2018-03-16':'2018-03-20']</p>
			<p>Things only got worse once more information was revealed in the following days with regards to the severity of the incident:</p>
			<div>
				<div id="_idContainer172" class="IMG---Figure">
					<img src="image/Figure_4.19_B16834.jpg" alt="Figure 4.19 – Facebook stock data when the Cambridge Analytica scandal broke&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.19 – Facebook stock data when the Cambridge Analytica scandal broke</p>
			<p>As for the third day of high trading volume (March 26, 2018), the FTC launched an investigation into the Cambridge Analytica scandal, so Facebook's woes continued (<a href="https://www.cnbc.com/2018/03/26/ftc-confirms-facebook-data-breach-investigation.html">https://www.cnbc.com/2018/03/26/ftc-confirms-facebook-data-breach-investigation.html</a>).</p>
			<p>If we look at some of the dates<a id="_idIndexMarker542"/> within the medium trading volume group, we can see that many are part of the three trading events we just discussed. This forces us to reexamine how we created the bins in the first place. Perhaps equal-width bins wasn't the answer? Most days were pretty close in volume traded; however, a few days caused the bin width to be rather large, which left us with a large imbalance of days per bin: </p>
			<div>
				<div id="_idContainer173" class="IMG---Figure">
					<img src="image/Figure_4.20_B16834.jpg" alt="Figure 4.20 – Visualizing the equal-width bins&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.20 – Visualizing the equal-width bins</p>
			<p>If we want each bin to have an equal number of observations, we can split the bins based on evenly-spaced quantiles using the <strong class="source-inline">pd.qcut()</strong> function. We can bin the volumes into quartiles to evenly bucket the observations into bins of varying width, giving us the 63 highest trading volume days in the <strong class="bold">q4</strong> bin:</p>
			<p class="source-code">&gt;&gt;&gt; volume_qbinned = <strong class="bold">pd.qcut(</strong></p>
			<p class="source-code">...     <strong class="bold">fb.volume, q=4, labels=['q1', 'q2', 'q3', 'q4']</strong></p>
			<p class="source-code">... <strong class="bold">)</strong></p>
			<p class="source-code">&gt;&gt;&gt; volume_qbinned.value_counts()</p>
			<p class="source-code">q1    63</p>
			<p class="source-code">q2    63</p>
			<p class="source-code">q4    63</p>
			<p class="source-code">q3    62</p>
			<p class="source-code">Name: volume, dtype: int64</p>
			<p>Notice that the<a id="_idIndexMarker543"/> bins don't cover the same range of volume traded anymore: </p>
			<div>
				<div id="_idContainer174" class="IMG---Figure">
					<img src="image/Figure_4.21_B16834.jpg" alt="Figure 4.21 – Visualizing the bins based on quartiles&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.21 – Visualizing the bins based on quartiles</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">In both of these examples, we let <strong class="source-inline">pandas</strong> calculate the bin ranges; however, both <strong class="source-inline">pd.cut()</strong> and <strong class="source-inline">pd.qcut()</strong> allow us to specify the upper bounds for each bin as a list.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor090"/>Applying functions</h2>
			<p>So far, most of the actions we have taken on our data have been column-specific. When we want to run the same code on <a id="_idIndexMarker544"/>all the columns in our dataframe, we can use the <strong class="source-inline">apply()</strong> method for more succinct code. Note that this will not be done in-place. </p>
			<p>Before we get started, let's isolate the weather observations from the Central Park station and pivot the data:</p>
			<p class="source-code">&gt;&gt;&gt; central_park_weather = weather.query(</p>
			<p class="source-code">...     'station == "GHCND:USW00094728"'</p>
			<p class="source-code">... ).pivot(index='date', columns='datatype', values='value')</p>
			<p>Let's calculate the Z-scores of the <strong class="source-inline">TMIN</strong> (minimum temperature), <strong class="source-inline">TMAX</strong> (maximum temperature), and <strong class="source-inline">PRCP</strong> (precipitation) observations in Central Park in October 2018. It's important that we don't try to take the Z-scores across the full year. NYC has four seasons, and what is considered normal weather will depend on which season we are looking at. By isolating our calculation to October, we can see if October had any days with very different weather:</p>
			<p class="source-code">&gt;&gt;&gt; oct_weather_z_scores = central_park_weather\</p>
			<p class="source-code">...     .loc['2018-10', ['TMIN', 'TMAX', 'PRCP']]\</p>
			<p class="source-code">...     .<strong class="bold">apply(lambda x: x.sub(x.mean()).div(x.std()))</strong></p>
			<p class="source-code">&gt;&gt;&gt; oct_weather_z_scores.describe().T</p>
			<p><strong class="source-inline">TMIN</strong> and <strong class="source-inline">TMAX</strong> don't appear to have any values that differ much from the rest of October, but <strong class="source-inline">PRCP</strong> does:</p>
			<div>
				<div id="_idContainer175" class="IMG---Figure">
					<img src="image/Figure_4.22_B16834.jpg" alt="Figure 4.22 – Calculating Z-scores for multiple columns at once&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.22 – Calculating Z-scores for multiple columns at once</p>
			<p>We can use <strong class="source-inline">query()</strong> to extract the<a id="_idIndexMarker545"/> value for this date:</p>
			<p class="source-code">&gt;&gt;&gt; oct_weather_z_scores.query('PRCP &gt; 3').PRCP</p>
			<p class="source-code">date</p>
			<p class="source-code">2018-10-27    3.936167</p>
			<p class="source-code">Name: PRCP, dtype: float64</p>
			<p>If we look at the summary statistics for precipitation in October, we can see that this day had much more precipitation than the rest:</p>
			<p class="source-code">&gt;&gt;&gt; central_park_weather.loc['2018-10', 'PRCP'].describe()</p>
			<p class="source-code">count    31.000000</p>
			<p class="source-code">mean      2.941935</p>
			<p class="source-code">std       7.458542</p>
			<p class="source-code">min       0.000000</p>
			<p class="source-code">25%       0.000000</p>
			<p class="source-code">50%       0.000000</p>
			<p class="source-code">75%       1.150000</p>
			<p class="source-code"><strong class="bold">max      32.300000</strong></p>
			<p class="source-code">Name: PRCP, dtype: float64</p>
			<p>The <strong class="source-inline">apply()</strong> method lets us run vectorized operations on entire columns or rows at once. We can apply pretty much any function we can think of as long as those operations are valid on all the columns (or rows) in our data. For example, we can use the <strong class="source-inline">pd.cut()</strong> and <strong class="source-inline">pd.qcut()</strong> binning functions<a id="_idIndexMarker546"/> we discussed in the previous section to divide each column into bins (provided we want the same number of bins or value ranges). Note that there is also an <strong class="source-inline">applymap()</strong> method if the function we want to apply isn't vectorized. Alternatively, we can use <strong class="source-inline">np.vectorize()</strong> to vectorize our functions for use with <strong class="source-inline">apply()</strong>. Consult the notebook for an example.</p>
			<p>Pandas does provide some functionality for iterating over the dataframe, including the <strong class="source-inline">iteritems()</strong>, <strong class="source-inline">itertuples()</strong>, and <strong class="source-inline">iterrows()</strong> methods; however, we should avoid using these unless we absolutely can't find another solution. Pandas and NumPy are designed for vectorized operations, which are much faster because they are written in efficient C code; by writing a loop to iterate one element at a time, we are making it more computationally intensive due to the way Python implements integers and floats. For instance, look at how the time to complete the simple operation of adding the number <strong class="source-inline">10</strong> to each value in a series of floats grows linearly with the number of rows when using <strong class="source-inline">iteritems()</strong>, but stays near zero, regardless of size, when using a vectorized operation: </p>
			<div>
				<div id="_idContainer176" class="IMG---Figure">
					<img src="image/Figure_4.23_B16834.jpg" alt="Figure 4.23 – Vectorized versus iterative operations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.23 – Vectorized versus iterative operations</p>
			<p>All the functions and methods we have used so far have involved the full row or column; however, sometimes, we are more<a id="_idIndexMarker547"/> interested in performing window calculations, which use a section of the data.</p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor091"/>Window calculations</h2>
			<p>Pandas makes it possible to perform<a id="_idIndexMarker548"/> calculations over a window or range of rows/columns. In this section, we will discuss a few ways of constructing these windows. Depending on the type of window, we get a different look at our data.</p>
			<h3>Rolling windows</h3>
			<p>When our index is of type <strong class="source-inline">DatetimeIndex</strong>, we can <a id="_idIndexMarker549"/>specify the window in day parts (such as <strong class="source-inline">2H</strong> for two hours or <strong class="source-inline">3D</strong> for three days); otherwise, we can specify the number of periods as an integer. Say we are interested in the amount of rain that has fallen in a rolling 3-day window; it would be quite tedious (and probably inefficient) to implement this with what we have learned so far. Fortunately, we can use the <strong class="source-inline">rolling()</strong> method to get this information easily:</p>
			<p class="source-code">&gt;&gt;&gt; central_park_weather.loc['2018-10'].assign(</p>
			<p class="source-code">...     rolling_PRCP=lambda x: x.PRCP.<strong class="bold">rolling('3D').sum()</strong></p>
			<p class="source-code">... )[['PRCP', 'rolling_PRCP']].head(7).T</p>
			<p>After performing the<a id="_idIndexMarker550"/> rolling 3-day sum, each date will show the sum of that day's and the previous two days' precipitation:</p>
			<div>
				<div id="_idContainer177" class="IMG---Figure">
					<img src="image/Figure_4.24_B16834.jpg" alt="Figure 4.24 – Rolling 3-day total precipitation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.24 – Rolling 3-day total precipitation</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">If we want to use dates for the rolling calculation, but don't have dates in the index, we can pass the name of our date column to the <strong class="source-inline">on</strong> parameter in the call to <strong class="source-inline">rolling()</strong>. Conversely, if we want to use an integer index of row numbers, we can simply pass in an integer as the window; for example, <strong class="source-inline">rolling(3)</strong> for a 3-row window. </p>
			<p>To change the aggregation, all we have to do is call a different method on the result of <strong class="source-inline">rolling()</strong>; for example, <strong class="source-inline">mean()</strong> for the average and <strong class="source-inline">max()</strong> for the maximum. The rolling calculation can also be applied to all the columns at once:</p>
			<p class="source-code">&gt;&gt;&gt; central_park_weather.loc['2018-10']\</p>
			<p class="source-code">...     .rolling('3D').<strong class="bold">mean()</strong>.head(7).iloc[:,:6]</p>
			<p>This gives us the 3-day rolling average for all the weather observations from Central Park:</p>
			<div>
				<div id="_idContainer178" class="IMG---Figure">
					<img src="image/Figure_4.25_B16834.jpg" alt="Figure 4.25 – Rolling 3-day average for all weather observations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.25 – Rolling 3-day average for all weather observations</p>
			<p>To apply different aggregations across columns, we can use the <strong class="source-inline">agg()</strong> method instead; it allows us to specify the aggregations to perform per column as a predefined or <a id="_idIndexMarker551"/>custom function. We simply pass in a dictionary mapping the columns to the aggregation to perform on them. Let's find the rolling 3-day maximum temperature (<strong class="source-inline">TMAX</strong>), minimum temperature (<strong class="source-inline">TMIN</strong>), average wind speed (<strong class="source-inline">AWND</strong>), and total precipitation (<strong class="source-inline">PRCP</strong>). Then, we will join it to the original data so that we can compare the results:</p>
			<p class="source-code">&gt;&gt;&gt; central_park_weather\</p>
			<p class="source-code">...     ['2018-10-01':'2018-10-07'].rolling('3D').<strong class="bold">agg({</strong></p>
			<p class="source-code">...     <strong class="bold">'TMAX': 'max', 'TMIN': 'min',</strong></p>
			<p class="source-code">...     <strong class="bold">'AWND': 'mean', 'PRCP': 'sum'</strong></p>
			<p class="source-code">... <strong class="bold">})</strong>.join( # join with original data for comparison</p>
			<p class="source-code">...     central_park_weather[['TMAX', 'TMIN', 'AWND', 'PRCP']], </p>
			<p class="source-code">...     lsuffix='_rolling'</p>
			<p class="source-code">... ).sort_index(axis=1) # put rolling calcs next to originals</p>
			<p>Using <strong class="source-inline">agg()</strong>, we were able to calculate different rolling aggregations for each column:</p>
			<div>
				<div id="_idContainer179" class="IMG---Figure">
					<img src="image/Figure_4.26_B16834.jpg" alt="Figure 4.26 – Using different rolling calculations per column&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.26 – Using different rolling calculations per column</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We can also use variable-width windows with a little extra effort: we can either create a subclass of <strong class="source-inline">BaseIndexer</strong> and provide the logic for determining the window bounds in the <strong class="source-inline">get_window_bounds()</strong> method (more information can be found at <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#custom-window-rolling">https://pandas.pydata.org/pandas-docs/stable/user_guide/computation.html#custom-window-rolling</a>), or we can use one of the predefined classes in the <strong class="source-inline">pandas.api.indexers</strong> module. The notebook we are currently working in contains an example of using the <strong class="source-inline">VariableOffsetWindowIndexer</strong> class to perform a 3-business day rolling calculation.</p>
			<p>With rolling calculations, we<a id="_idIndexMarker552"/> have a sliding window over which we calculate our functions; however, in some cases, we are more interested in the output of a function on all the data up to that point, in which case we use an expanding window.</p>
			<h3>Expanding windows</h3>
			<p>Expanding calculations will give us the<a id="_idIndexMarker553"/> cumulative value of our aggregation function. We use the <strong class="source-inline">expanding()</strong> method to perform a calculation with an expanding window; methods such as <strong class="source-inline">cumsum()</strong> and <strong class="source-inline">cummax()</strong> use expanding windows for their calculations. The advantage of using <strong class="source-inline">expanding()</strong> directly is additional flexibility: we aren't limited to predefined aggregations, and we can specify the minimum number of periods before the calculation starts with the <strong class="source-inline">min_periods</strong> parameter (defaults to 1). With the Central Park weather data, let's use the <strong class="source-inline">expanding()</strong> method to calculate the month-to-date average precipitation:</p>
			<p class="source-code">&gt;&gt;&gt; central_park_weather.loc['2018-06'].assign(</p>
			<p class="source-code">...     TOTAL_PRCP=lambda x: x.PRCP.cumsum(),</p>
			<p class="source-code">...     <strong class="bold">AVG_PRCP=lambda x: x.PRCP.expanding().mean()</strong></p>
			<p class="source-code">... ).head(10)[['PRCP', 'TOTAL_PRCP', 'AVG_PRCP']].T</p>
			<p>Note that while there is no method for the cumulative mean, we are able to use the <strong class="source-inline">expanding()</strong> method to calculate it. The values in the <strong class="source-inline">AVG_PRCP</strong> column are the values in the <strong class="source-inline">TOTAL_PRCP</strong> column divided by the number of days processed:</p>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<img src="image/Figure_4.27_B16834.jpg" alt="Figure 4.27 – Calculating the month-to-date average precipitation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.27 – Calculating the month-to-date average precipitation</p>
			<p>As we did with <strong class="source-inline">rolling()</strong>, we can provide column-specific aggregations with the <strong class="source-inline">agg()</strong> method. Let's find the expanding maximum temperature, minimum temperature, average wind speed, and total precipitation. Note that we can also pass in NumPy functions to <strong class="source-inline">agg()</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; central_park_weather\</p>
			<p class="source-code">...     ['2018-10-01':'2018-10-07'].expanding().<strong class="bold">agg({</strong></p>
			<p class="source-code">...     <strong class="bold">'TMAX': np.max, 'TMIN': np.min,</strong> </p>
			<p class="source-code">...     <strong class="bold">'AWND': np.mean, 'PRCP': np.sum</strong></p>
			<p class="source-code">... <strong class="bold">})</strong>.join(</p>
			<p class="source-code">...     central_park_weather[['TMAX', 'TMIN', 'AWND', 'PRCP']], </p>
			<p class="source-code">...     lsuffix='_expanding'</p>
			<p class="source-code">... ).sort_index(axis=1)</p>
			<p>Once again, we joined the<a id="_idIndexMarker554"/> window calculations with the original data for comparison:</p>
			<div>
				<div id="_idContainer181" class="IMG---Figure">
					<img src="image/Figure_4.28_B16834.jpg" alt="Figure 4.28 – Performing different expanding window calculations per column               &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.28 – Performing different expanding window calculations per column               </p>
			<p>Both rolling and expanding windows equally weight all the observations in the window when performing calculations, but sometimes, we want to place more emphasis on more recent values. One option is to exponentially weight the observations.                                </p>
			<h3>Exponentially weighted moving windows</h3>
			<p>Pandas also<a id="_idIndexMarker555"/> provides the <strong class="source-inline">ewm()</strong> method for exponentially weighted moving calculations. As discussed in <a href="B16834_01_Final_SK_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to Data Analysis</em>, we can use the <strong class="bold">exponentially weighted moving average</strong> (<strong class="bold">EWMA</strong>) to smooth our data. Let's compare the rolling 30-day average to the 30-day EWMA of the<a id="_idIndexMarker556"/> maximum daily temperature. Note that we use the <strong class="source-inline">span</strong> argument to specify the number of periods to use for the EWMA calculation:</p>
			<p class="source-code">&gt;&gt;&gt; central_park_weather.assign(</p>
			<p class="source-code">...     AVG=lambda x: x.TMAX.rolling('30D').mean(),</p>
			<p class="source-code">...     EWMA=lambda x: x.TMAX.ewm(span=30).mean()</p>
			<p class="source-code">... ).loc['2018-09-29':'2018-10-08', ['TMAX', 'EWMA', 'AVG']].T</p>
			<p>Unlike the<a id="_idIndexMarker557"/> rolling average, the EWMA places higher importance on more recent observations, so the jump in temperature on October 7th has a larger effect on the EWMA than the rolling average:</p>
			<div>
				<div id="_idContainer182" class="IMG---Figure">
					<img src="image/Figure_4.29_B16834.jpg" alt="Figure 4.29 – Smoothing the data with moving averages&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.29 – Smoothing the data with moving averages</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Check out the <strong class="source-inline">understanding_window_calculations.ipynb</strong> notebook, which contains some interactive visualizations for understanding window functions. This may require some additional setup, but the instructions are in the notebook.</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor092"/>Pipes</h2>
			<p>Pipes facilitate <a id="_idIndexMarker558"/>chaining together operations that expect <strong class="source-inline">pandas</strong> data structures as their first argument. By using pipes, we can build up complex workflows without needing to write highly nested and hard-to-read code. In general, pipes let us turn something like <strong class="source-inline">f(g(h(data), 20), x=True)</strong> into the following, making it much easier to read:</p>
			<p class="source-code">data.pipe(h)\ # first call h(data)</p>
			<p class="source-code">    .pipe(g, 20)\ # call g on the result with positional arg 20</p>
			<p class="source-code">    .pipe(f, x=True) # call f on result with keyword arg x=True</p>
			<p>Say we wanted to print the dimensions of a subset of the Facebook dataframe with some formatting by calling this function:</p>
			<p class="source-code">&gt;&gt;&gt; def get_info(df):</p>
			<p class="source-code">...     return '%d rows, %d cols and max closing Z-score: %d' </p>
			<p class="source-code">...             % (*df.shape, df.close.max()) </p>
			<p>Before we call the<a id="_idIndexMarker559"/> function, however, we want to calculate the Z-scores for all the columns. One approach is the following:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">get_info(</strong>fb.loc['2018-Q1']\</p>
			<p class="source-code">...            .apply(lambda x: (x - x.mean())/x.std())<strong class="bold">)</strong></p>
			<p>Alternatively, we could pipe the dataframe after calculating the Z-scores to this function:</p>
			<p class="source-code">&gt;&gt;&gt; fb.loc['2018-Q1'].apply(lambda x: (x - x.mean())/x.std())\</p>
			<p class="source-code">...     .<strong class="bold">pipe(get_info)</strong></p>
			<p>Pipes can also make it easier to write reusable code. In several of the code snippets in this book, we have seen the idea of passing a function into another function, such as when we pass a NumPy function to <strong class="source-inline">apply()</strong> and it gets executed on each column. We can use pipes to extend that functionality to methods of <strong class="source-inline">pandas</strong> data structures:</p>
			<p class="source-code">&gt;&gt;&gt; fb.pipe(pd.DataFrame.rolling, '20D').mean().equals(</p>
			<p class="source-code">...     fb.rolling('20D').mean()</p>
			<p class="source-code">... ) # the pipe is calling pd.DataFrame.rolling(fb, '20D')</p>
			<p class="source-code">True</p>
			<p>To illustrate how this can benefit us, let's look at a function that will give us the result of a window calculation of our choice. The function is in the <strong class="source-inline">window_calc.py</strong> file. We will import the function and use <strong class="source-inline">??</strong> from IPython to view the function definition:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">from window_calc import window_calc</strong></p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">window_calc??</strong></p>
			<p class="source-code">Signature: window_calc(df, func, agg_dict, *args, **kwargs)</p>
			<p class="source-code">Source:   </p>
			<p class="source-code">def window_calc(df, func, agg_dict, *args, **kwargs):</p>
			<p class="source-code">    """</p>
			<p class="source-code">    Run a window calculation of your choice on the data.</p>
			<p class="source-code">    Parameters:</p>
			<p class="source-code">        - df: The `DataFrame` object to run the calculation on.</p>
			<p class="source-code">        - func: The window calculation method that takes `df` </p>
			<p class="source-code">          as the first argument.</p>
			<p class="source-code">        - agg_dict: Information to pass to `agg()`, could be </p>
			<p class="source-code">          a dictionary mapping the columns to the aggregation </p>
			<p class="source-code">          function to use, a string name for the function, </p>
			<p class="source-code">          or the function itself.</p>
			<p class="source-code">        - args: Positional arguments to pass to `func`.</p>
			<p class="source-code">        - kwargs: Keyword arguments to pass to `func`.</p>
			<p class="source-code">    </p>
			<p class="source-code">    Returns:</p>
			<p class="source-code">        A new `DataFrame` object.</p>
			<p class="source-code">    """</p>
			<p class="source-code">    return <strong class="bold">df.pipe(func, *args, **kwargs).agg(agg_dict)</strong></p>
			<p class="source-code">File:      ~/.../ch_04/window_calc.py</p>
			<p class="source-code">Type:      function</p>
			<p>Our <strong class="source-inline">window_calc()</strong> function takes the dataframe, the function to execute (as long as it takes a dataframe as its first argument), and information on how to aggregate the result, along with any optional <a id="_idIndexMarker560"/>parameters, and gives us back a new dataframe with the results of the window calculations. Let's use this function to find the expanding median of the Facebook stock data:</p>
			<p class="source-code">&gt;&gt;&gt; window_calc(fb, <strong class="bold">pd.DataFrame.expanding</strong>, np.median).head()</p>
			<p>Note that the <strong class="source-inline">expanding()</strong> method doesn't require us to specify any parameters, so all we had to do was pass in <strong class="source-inline">pd.DataFrame.expanding</strong> (no parentheses), along with the aggregation to perform as the window calculation on the dataframe:</p>
			<div>
				<div id="_idContainer183" class="IMG---Figure">
					<img src="image/Figure_4.30_B16834.jpg" alt="Figure 4.30 – Using pipes to perform expanding window calculations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.30 – Using pipes to perform expanding window calculations</p>
			<p>The <strong class="source-inline">window_calc()</strong> function also takes <strong class="source-inline">*args</strong> and <strong class="source-inline">**kwargs</strong>; these are optional parameters that, if supplied, will be collected by Python into <strong class="source-inline">kwargs</strong> when they are passed by name (such as <strong class="source-inline">span=20</strong>) and into <strong class="source-inline">args</strong> if not (passed by position). These can then be <strong class="bold">unpacked</strong> and passed to another function or method call by using <strong class="source-inline">*</strong> for <strong class="source-inline">args</strong> and <strong class="source-inline">**</strong> for <strong class="source-inline">kwargs</strong>. We need this behavior in order to use the <strong class="source-inline">ewm()</strong> method for the EWMA of the closing price of Facebook stock:</p>
			<p class="source-code">&gt;&gt;&gt; window_calc(fb, <strong class="bold">pd.DataFrame.ewm</strong>, 'mean', <strong class="bold">span=3</strong>).head()</p>
			<p>In the previous example, we had to use <strong class="source-inline">**kwargs</strong> because the <strong class="source-inline">span</strong> argument is not the first argument that <strong class="source-inline">ewm()</strong> receives, and we didn't want to pass the ones before it:</p>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="image/Figure_4.31_B16834.jpg" alt="Figure 4.31 – Using pipes to perform exponentially weighted window calculations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.31 – Using pipes to perform exponentially weighted window calculations</p>
			<p>To calculate the rolling 3-day <a id="_idIndexMarker561"/>weather aggregations for Central Park, we take advantage of <strong class="source-inline">*args</strong> since we know that the window is the first argument to <strong class="source-inline">rolling()</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; window_calc(</p>
			<p class="source-code">...     central_park_weather.loc['2018-10'], </p>
			<p class="source-code">...     <strong class="bold">pd.DataFrame.rolling</strong>, </p>
			<p class="source-code">...     {'TMAX': 'max', 'TMIN': 'min',</p>
			<p class="source-code">...      'AWND': 'mean', 'PRCP': 'sum'},</p>
			<p class="source-code">...     <strong class="bold">'3D'</strong></p>
			<p class="source-code">... ).head()</p>
			<p>We were able to aggregate each of the columns differently since we passed in a dictionary instead of a single value:</p>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="image/Figure_4.32_B16834.jpg" alt="Figure 4.32 – Using pipes to perform rolling window calculations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.32 – Using pipes to perform rolling window calculations</p>
			<p>Notice how we were able to create a consistent API for the window calculations, without the caller needing to figure out which aggregation method to call after the window function. This hides<a id="_idIndexMarker562"/> some of the implementation details, while making it easier to use. We will be using this function as the base for some of the functionality in the <strong class="source-inline">StockVisualizer</strong> class we will build in <a href="B16834_07_Final_SK_ePub.xhtml#_idTextAnchor146"><em class="italic">Chapter 7</em></a>, <em class="italic">Financial Analysis – Bitcoin and the Stock Market</em>.</p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor093"/>Aggregating data</h1>
			<p>We already got<a id="_idIndexMarker563"/> a sneak peek at aggregation when we discussed window calculations and pipes in the previous section. Here, we will focus on summarizing the dataframe <a id="_idIndexMarker564"/>through aggregation, which will change the shape of our dataframe (often through row reduction). We also saw<a id="_idIndexMarker565"/> how easy it is to take advantage of vectorized NumPy functions on <strong class="source-inline">pandas</strong> data structures, especially to perform aggregations. This is what NumPy does best: it performs computationally efficient mathematical operations on numeric arrays.</p>
			<p>NumPy pairs well with aggregating dataframes since it gives us an easy way to summarize data with different pre-written functions; often, when aggregating, we just need the NumPy function, since most of what we would want to write ourselves has previously been built. We have already seen some NumPy functions commonly used for aggregations, such as <strong class="source-inline">np.sum()</strong>, <strong class="source-inline">np.mean()</strong>, <strong class="source-inline">np.min()</strong>, and <strong class="source-inline">np.max()</strong>; however, we aren't limited to numeric operations—we can use things such as <strong class="source-inline">np.unique()</strong> on strings. Always check whether NumPy already has a function before implementing one yourself.</p>
			<p>For this section, we will be working in the <strong class="source-inline">3-aggregations.ipynb</strong> notebook. Let's import <strong class="source-inline">pandas</strong> and <strong class="source-inline">numpy</strong> and read in the data we will be working with:</p>
			<p class="source-code">&gt;&gt;&gt; import numpy as np</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; fb = pd.read_csv(</p>
			<p class="source-code">...     'data/fb_2018.csv', index_col='date', parse_dates=True</p>
			<p class="source-code">... ).assign(trading_volume=lambda x: pd.cut(</p>
			<p class="source-code">...     x.volume, bins=3, labels=['low', 'med', 'high'] </p>
			<p class="source-code">... ))</p>
			<p class="source-code">&gt;&gt;&gt; weather = pd.read_csv(</p>
			<p class="source-code">...     'data/weather_by_station.csv', </p>
			<p class="source-code">...     index_col='date', parse_dates=True</p>
			<p class="source-code">... )</p>
			<p>Note that the weather data for this <a id="_idIndexMarker566"/>section has been merged with some of the station data:</p>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="image/Figure_4.33_B16834.jpg" alt="Figure 4.33 – Merged weather and station data for this section&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.33 – Merged weather and station data for this section</p>
			<p>Before we dive into any calculations, let's make<a id="_idIndexMarker567"/> sure that our data won't be displayed in scientific notation. We will modify how floats are formatted for displaying. The <a id="_idIndexMarker568"/>format we will apply is <strong class="source-inline">.2f</strong>, which will provide the float with two digits after the decimal point:</p>
			<p class="source-code">&gt;&gt;&gt; pd.set_option('display.float_format', lambda x: '%.2f' % x)</p>
			<p>First, we will take a look at summarizing the full dataset before moving on to summarizing by groups and building pivot tables and crosstabs.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor094"/>Summarizing DataFrames</h2>
			<p>When we discussed window calculations, we saw that we could run the <strong class="source-inline">agg()</strong> method on the result of <strong class="source-inline">rolling()</strong>, <strong class="source-inline">expanding()</strong>, or <strong class="source-inline">ewm()</strong>; however, we can also call it directly on the dataframe in the same fashion. The only difference is that the aggregations done this way will be<a id="_idIndexMarker569"/> performed on all the data, meaning <a id="_idIndexMarker570"/>that we will only get a series back that contains the overall result. Let's aggregate the Facebook stock data the same way we did with the window calculations. Note that we won't get anything back for the <strong class="source-inline">trading_volume</strong> column, which contains the volume traded bins from <strong class="source-inline">pd.cut()</strong>; this is because we aren't specifying an aggregation to run on that column:</p>
			<p class="source-code">&gt;&gt;&gt; fb.agg({</p>
			<p class="source-code">...     'open': np.mean, 'high': np.max, 'low': np.min, </p>
			<p class="source-code">...     'close': np.mean, 'volume': np.sum</p>
			<p class="source-code">... })</p>
			<p class="source-code">open            171.45</p>
			<p class="source-code">high            218.62</p>
			<p class="source-code">low             123.02</p>
			<p class="source-code">close           171.51</p>
			<p class="source-code">volume   6949682394.00</p>
			<p class="source-code">dtype: float64</p>
			<p>We can use aggregations to easily find the total snowfall and precipitation for 2018 in Central Park. In this case, since we will be performing the sum on both, we can either use <strong class="source-inline">agg('sum')</strong> or call <strong class="source-inline">sum()</strong> directly:</p>
			<p class="source-code">&gt;&gt;&gt; weather.query('station == "GHCND:USW00094728"')\</p>
			<p class="source-code">...     .pivot(columns='datatype', values='value')\</p>
			<p class="source-code">...     [['SNOW', 'PRCP']].<strong class="bold">sum()</strong></p>
			<p class="source-code">datatype</p>
			<p class="source-code">SNOW   1007.00</p>
			<p class="source-code">PRCP   1665.30</p>
			<p class="source-code">dtype: float64</p>
			<p>Additionally, we can<a id="_idIndexMarker571"/> provide multiple functions to run on each of the columns we want to aggregate. As we have already seen, we get a <strong class="source-inline">Series</strong> object when each column has a single aggregation. To distinguish between the aggregations in the case of<a id="_idIndexMarker572"/> multiple ones per column, <strong class="source-inline">pandas</strong> will return a <strong class="source-inline">DataFrame</strong> object instead. The index of this dataframe will tell us which metric is being calculated for which column:</p>
			<p class="source-code">&gt;&gt;&gt; fb.agg({</p>
			<p class="source-code">...     'open': 'mean', </p>
			<p class="source-code">...     <strong class="bold">'high': ['min', 'max'],</strong></p>
			<p class="source-code">...     <strong class="bold">'low': ['min', 'max'],</strong> </p>
			<p class="source-code">...     'close': 'mean'</p>
			<p class="source-code">... })</p>
			<p>This results in a dataframe where the rows indicate the aggregation function being applied to the data columns. Note that we get nulls for any combination of aggregation and column that we didn't explicitly ask for:</p>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/Figure_4.34_B16834.jpg" alt="Figure 4.34 – Performing multiple aggregations per column&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.34 – Performing multiple aggregations per column</p>
			<p>So far, we have learned how to aggregate over specific windows and over the entire dataframe; however, the real power comes with the ability to aggregate by group membership. This lets us calculate things such as the total precipitation per month, per station and average OHLC stock prices for each volume traded bin we've created.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor095"/>Aggregating by group</h2>
			<p>To calculate the<a id="_idIndexMarker573"/> aggregations per group, we must first call the <strong class="source-inline">groupby()</strong> method on the dataframe and provide the column(s) we want to use to<a id="_idIndexMarker574"/> determine distinct groups. Let's look at the average of our stock data points for each of the volume traded bins we created with <strong class="source-inline">pd.cut()</strong>; remember, these are three equal-width bins:</p>
			<p class="source-code">&gt;&gt;&gt; fb.<strong class="bold">groupby('trading_volume')</strong>.mean()</p>
			<p>The average OHLC prices are smaller for larger trading volumes, which was to be expected given that the three dates in the high-volume traded bin were selloffs:</p>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="image/Figure_4.35_B16834.jpg" alt="Figure 4.35 – Aggregating by group&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.35 – Aggregating by group</p>
			<p>After running <strong class="source-inline">groupby()</strong>, we can also select specific columns for aggregation:</p>
			<p class="source-code">&gt;&gt;&gt; fb.groupby('trading_volume')\</p>
			<p class="source-code">...     <strong class="bold">['close'].agg(['min', 'max', 'mean'])</strong></p>
			<p>This gives us the aggregations for the closing price in each volume traded bucket:</p>
			<div>
				<div id="_idContainer189" class="IMG---Figure">
					<img src="image/Figure_4.36_B16834.jpg" alt="Figure 4.36 – Aggregating a specific column per group&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.36 – Aggregating a specific column per group</p>
			<p>If we need more fine-tuned <a id="_idIndexMarker575"/>control over how each column gets aggregated, we use the <strong class="source-inline">agg()</strong> method again with a dictionary that maps the columns to their aggregation function. As we did previously, we can provide lists of functions per<a id="_idIndexMarker576"/> column; the result, however, will look a little different:</p>
			<p class="source-code">&gt;&gt;&gt; fb_agg = fb.groupby('trading_volume').<strong class="bold">agg({</strong></p>
			<p class="source-code">...     <strong class="bold">'open': 'mean', 'high': ['min', 'max'],</strong></p>
			<p class="source-code">...     <strong class="bold">'low': ['min', 'max'],</strong> <strong class="bold">'close': 'mean'</strong></p>
			<p class="source-code">... <strong class="bold">})</strong></p>
			<p class="source-code">&gt;&gt;&gt; fb_agg</p>
			<p>We now have a hierarchical index in the columns. Remember, this means that if we want to select the minimum low price for the medium volume traded bucket, we need to use <strong class="source-inline">fb_agg.loc['med', 'low']['min']</strong>:</p>
			<div>
				<div id="_idContainer190" class="IMG---Figure">
					<img src="image/Figure_4.37_B16834.jpg" alt="Figure 4.37 – Performing multiple aggregations per column with groups&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.37 – Performing multiple aggregations per column with groups</p>
			<p>The columns<a id="_idIndexMarker577"/> are stored in a <strong class="source-inline">MultiIndex</strong> object:</p>
			<p class="source-code">&gt;&gt;&gt; fb_agg.columns</p>
			<p class="source-code">MultiIndex([( 'open', 'mean'),</p>
			<p class="source-code">            ( 'high',  'min'),</p>
			<p class="source-code">            ( 'high',  'max'),</p>
			<p class="source-code">            (  'low',  'min'),</p>
			<p class="source-code">            (  'low',  'max'),</p>
			<p class="source-code">            ('close', 'mean')],</p>
			<p class="source-code">           )</p>
			<p>We can use a list comprehension to<a id="_idIndexMarker578"/> remove this hierarchy and instead have our column names in the form of <strong class="source-inline">&lt;column&gt;_&lt;agg&gt;</strong>. At each iteration, we will get a tuple of the levels from the <strong class="source-inline">MultiIndex</strong> object, which we can combine into a single string to remove the hierarchy:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">fb_agg.columns = ['_'.join(col_agg) </strong></p>
			<p class="source-code">...                   <strong class="bold">for col_agg in fb_agg.columns]</strong></p>
			<p class="source-code">&gt;&gt;&gt; fb_agg.head()</p>
			<p>This replaces the hierarchy in the columns with a single level:</p>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<img src="image/Figure_4.38_B16834.jpg" alt="Figure 4.38 – Flattening out the hierarchical index&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.38 – Flattening out the hierarchical index</p>
			<p>Say we want to see<a id="_idIndexMarker579"/> the average observed precipitation across <a id="_idIndexMarker580"/>all the stations per day. We would need to group by the date, but it is in the index. In this case, we have a few options:</p>
			<ul>
				<li>Resampling, which we will cover in the <em class="italic">Working with time series data</em> section, later in this chapter.</li>
				<li>Resetting the index and using the date column that gets created from the index.</li>
				<li>Passing <strong class="source-inline">level=0</strong> to <strong class="source-inline">groupby()</strong> to indicate that the grouping should be performed on the outermost level of the index.</li>
				<li>Using a <strong class="source-inline">Grouper</strong> object.</li>
			</ul>
			<p>Here, we will pass <strong class="source-inline">level=0</strong> to <strong class="source-inline">groupby()</strong>, but note that we can also pass in <strong class="source-inline">level='date'</strong> because our index is named. This gives us the average precipitation observations across the stations, which may give us a better idea of the weather than simply picking a station to look at. Since the result is a single-column <strong class="source-inline">DataFrame</strong> object, we call <strong class="source-inline">squeeze()</strong> to turn it into a <strong class="source-inline">Series</strong> object:</p>
			<p class="source-code">&gt;&gt;&gt; weather.loc['2018-10'].query('datatype == "PRCP"')\ </p>
			<p class="source-code">...     .groupby(<strong class="bold">level=0</strong>).mean().head().squeeze()</p>
			<p class="source-code">date</p>
			<p class="source-code">2018-10-01    0.01</p>
			<p class="source-code">2018-10-02    2.23</p>
			<p class="source-code">2018-10-03   19.69</p>
			<p class="source-code">2018-10-04    0.32</p>
			<p class="source-code">2018-10-05    0.96</p>
			<p class="source-code">Name: value, dtype: float64</p>
			<p>We can also group by <a id="_idIndexMarker581"/>many categories at once. Let's find the quarterly total recorded precipitation per station. Here, rather than pass in <strong class="source-inline">level=0</strong> to <strong class="source-inline">groupby()</strong>, we need to use a <strong class="source-inline">Grouper</strong> object to aggregate from daily to quarterly frequency. Since this will create a multi-level index, we will also use <strong class="source-inline">unstack()</strong> to put the inner level (the quarter) along the columns after the aggregation is performed:</p>
			<p class="source-code">&gt;&gt;&gt; weather.query('datatype == "PRCP"').groupby(</p>
			<p class="source-code">...     <strong class="bold">['station_name', pd.Grouper(freq='Q')]</strong></p>
			<p class="source-code">... ).sum().unstack().sample(5, random_state=1)</p>
			<p>There are many possible<a id="_idIndexMarker582"/> follow-ups for this result. We could look at which stations receive the most/least precipitation. We could go back to the location and elevation information we had for each station to see if that affects precipitation. We could also see which quarter has the most/least precipitation across the stations:</p>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="image/Figure_4.39_B16834.jpg" alt="Figure 4.39 – Aggregating by a column with dates in the index&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.39 – Aggregating by a column with dates in the index</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The <strong class="source-inline">DataFrameGroupBy</strong> objects returned by the <strong class="source-inline">groupby()</strong> method have a <strong class="source-inline">filter()</strong> method, which allows us to filter groups. We can use this to exclude certain groups from the aggregation. Simply pass a function that returns a Boolean for each group's subset of the dataframe (<strong class="source-inline">True</strong> to include the group and <strong class="source-inline">False</strong> to exclude it). An example is in the notebook.</p>
			<p>Let's see which<a id="_idIndexMarker583"/> months have the most precipitation. First, we need to group by <a id="_idIndexMarker584"/>day and average the precipitation across the stations. Then, we can group by month and sum the resulting precipitation. Finally, we will use <strong class="source-inline">nlargest()</strong> to get the five months with the most precipitation:</p>
			<p class="source-code">&gt;&gt;&gt; weather.query('datatype == "PRCP"')\</p>
			<p class="source-code">...     .groupby(level=0).mean()\</p>
			<p class="source-code">...     .groupby(pd.Grouper(freq='M')).sum().value.nlargest()</p>
			<p class="source-code">date</p>
			<p class="source-code">2018-11-30   210.59</p>
			<p class="source-code">2018-09-30   193.09</p>
			<p class="source-code">2018-08-31   192.45</p>
			<p class="source-code">2018-07-31   160.98</p>
			<p class="source-code">2018-02-28   158.11</p>
			<p class="source-code">Name: value, dtype: float64</p>
			<p>Perhaps the previous result was surprising. The saying goes <em class="italic">April showers bring May flowers</em>; however, April wasn't in the top five (neither was May, for that matter). Snow will count toward precipitation, but that doesn't explain why summer months are higher than April. Let's look for days that accounted for a large percentage of the precipitation in a given month to see if April shows up there.</p>
			<p>To do so, we need to <a id="_idIndexMarker585"/>calculate the average daily precipitation across stations and then find the total per month; this will be the denominator. However, in order to divide the daily values by the total for their month, we will need a <strong class="source-inline">Series</strong> object of equal dimensions. This means that we will need to use the <strong class="source-inline">transform()</strong> method, which will perform the specified calculation on the data while always returning an object of equal dimensions to<a id="_idIndexMarker586"/> what we started with. Therefore, we can call it on a <strong class="source-inline">Series</strong> object and always get a <strong class="source-inline">Series</strong> object back, regardless of what the aggregation function itself would return:</p>
			<p class="source-code">&gt;&gt;&gt; weather.query('datatype == "PRCP"')\</p>
			<p class="source-code">...     .rename(dict(value='prcp'), axis=1)\</p>
			<p class="source-code">...     .groupby(level=0).mean()\</p>
			<p class="source-code">...     .groupby(pd.Grouper(freq='M'))\</p>
			<p class="source-code">...     .<strong class="bold">transform(np.sum)</strong>['2018-01-28':'2018-02-03']</p>
			<p>Rather than getting a single sum for January and another for February, notice that we have the same value being repeated for the January entries and a different one for the February ones. Note that the value for February is the value we found in the previous result:</p>
			<div>
				<div id="_idContainer193" class="IMG---Figure">
					<img src="image/Figure_4.40_B16834.jpg" alt="Figure 4.40 – The denominator for calculating the percentage of monthly precipitation &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.40 – The denominator for calculating the percentage of monthly precipitation </p>
			<p>We can make this a <a id="_idIndexMarker587"/>column in our <a id="_idIndexMarker588"/>dataframe to easily calculate the percentage of the monthly precipitation that occurred each day. Then, we can use <strong class="source-inline">nlargest()</strong> to pull out the largest values:</p>
			<p class="source-code">&gt;&gt;&gt; weather.query('datatype == "PRCP"')\</p>
			<p class="source-code">...     .rename(dict(value='prcp'), axis=1)\</p>
			<p class="source-code">...     .groupby(level=0).mean()\</p>
			<p class="source-code">...     .<strong class="bold">assign(</strong></p>
			<p class="source-code">...         <strong class="bold">total_prcp_in_month=lambda x: x.groupby(</strong></p>
			<p class="source-code">...             <strong class="bold">pd.Grouper(freq='M')).transform(np.sum),</strong></p>
			<p class="source-code">...         <strong class="bold">pct_monthly_prcp=lambda x: \</strong></p>
			<p class="source-code">...             <strong class="bold">x.prcp.div(x.total_prcp_in_month)</strong></p>
			<p class="source-code">...     <strong class="bold">).nlargest(5, 'pct_monthly_prcp')</strong></p>
			<p>Together, the 4th- and 5th-place <a id="_idIndexMarker589"/>days in terms of the amount of monthly precipitation they accounted for make up more than 50% of the rain in April. They were also consecutive days:</p>
			<div>
				<div id="_idContainer194" class="IMG---Figure">
					<img src="image/Figure_4.41_B16834.jpg" alt="Figure 4.41 – Calculating the percentage of monthly precipitation that occurred each day&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.41 – Calculating the percentage of monthly precipitation that occurred each day</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The <strong class="source-inline">transform()</strong> method also <a id="_idIndexMarker590"/>works on <strong class="source-inline">DataFrame</strong> objects, in which case it will return a <strong class="source-inline">DataFrame</strong> object. We can use it to easily standardize all the columns at once. An example is in the notebook.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor096"/>Pivot tables and crosstabs</h2>
			<p>To wrap up this section, we will discuss some <strong class="source-inline">pandas</strong> functions that will aggregate our data into some common formats. The <a id="_idIndexMarker591"/>aggregation methods we discussed previously will give us the highest level of customization; however, <strong class="source-inline">pandas</strong> provides <a id="_idIndexMarker592"/>some functions to quickly generate a pivot table and a crosstab in a common format.</p>
			<p>In order to generate<a id="_idIndexMarker593"/> a pivot table, we must specify what to group on and, optionally, which subset of columns we want to aggregate and/or how to aggregate (average, by default). Let's create a pivot table of averaged OHLC data for Facebook per volume traded bin:</p>
			<p class="source-code">&gt;&gt;&gt; fb.pivot_table(columns='trading_volume')</p>
			<p>Since we passed in <strong class="source-inline">columns='trading_volume'</strong>, the distinct values in the <strong class="source-inline">trading_volume</strong> column were placed along <a id="_idIndexMarker594"/>the columns. The columns from the original dataframe then went to the index. Notice<a id="_idIndexMarker595"/> that the index for the columns has a name (<strong class="bold">trading_volume</strong>):</p>
			<div>
				<div id="_idContainer195" class="IMG---Figure">
					<img src="image/Figure_4.42_B16834.jpg" alt="Figure 4.42 – Pivot table of column averages per volume traded bin&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.42 – Pivot table of column averages per volume traded bin</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">If we pass <strong class="source-inline">trading_volume</strong> as the <strong class="source-inline">index</strong> argument instead, we get the transpose of <em class="italic">Figure 4.42</em>, which is also the exact same output as <em class="italic">Figure 4.35</em> when we used <strong class="source-inline">groupby()</strong>.</p>
			<p>With the <strong class="source-inline">pivot()</strong> method, we weren't able to<a id="_idIndexMarker596"/> handle multi-level indices or indices with repeated values. For this reason, we haven't been able to put the weather data in wide format. The <strong class="source-inline">pivot_table()</strong> method solves this issue. To<a id="_idIndexMarker597"/> do so, we need to put the <strong class="source-inline">date</strong> and <strong class="source-inline">station</strong> information in the index and the distinct values of the <strong class="source-inline">datatype</strong> column along the columns. The <a id="_idIndexMarker598"/>values will come from the <strong class="source-inline">value</strong> column. We will use the median to aggregate any overlapping combinations (if any):</p>
			<p class="source-code">&gt;&gt;&gt; weather.reset_index().<strong class="bold">pivot_table(</strong></p>
			<p class="source-code">...     <strong class="bold">index=['date', 'station', 'station_name'],</strong> </p>
			<p class="source-code">...     <strong class="bold">columns='datatype',</strong> </p>
			<p class="source-code">...     <strong class="bold">values='value',</strong> </p>
			<p class="source-code">...     <strong class="bold">aggfunc='median'</strong></p>
			<p class="source-code">... <strong class="bold">)</strong>.reset_index().tail()</p>
			<p>After resetting the index, we have our data in wide format. One final step would be to rename the index:</p>
			<div>
				<div id="_idContainer196" class="IMG---Figure">
					<img src="image/Figure_4.43_B16834.jpg" alt="Figure 4.43 – Pivot table with median values per datatype, station, and date&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.43 – Pivot table with median values per datatype, station, and date</p>
			<p>We can use the <strong class="source-inline">pd.crosstab()</strong> function to create a frequency table. For example, if we want to see how many low-, medium-, and<a id="_idIndexMarker599"/> high-volume trading days Facebook stock had each month, we can <a id="_idIndexMarker600"/>use a crosstab. The syntax is pretty straightforward; we pass the row <a id="_idIndexMarker601"/>and column labels to the <strong class="source-inline">index</strong> and <strong class="source-inline">columns</strong> parameters, respectively. By default, the values in the cells will be the count:</p>
			<p class="source-code">&gt;&gt;&gt; pd.crosstab(</p>
			<p class="source-code">...     index=fb.trading_volume, columns=fb.index.month,</p>
			<p class="source-code">...     colnames=['month'] # name the columns index</p>
			<p class="source-code">... )</p>
			<p>This makes it easy to see the months when high volumes of Facebook stock were traded:</p>
			<div>
				<div id="_idContainer197" class="IMG---Figure">
					<img src="image/Figure_4.44_B16834.jpg" alt="Figure 4.44 – Crosstab showing the number of trading days per month, per volume traded bin&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.44 – Crosstab showing the number of trading days per month, per volume traded bin</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We can normalize the output to percentages of the row/column totals by passing in <strong class="source-inline">normalize='rows'</strong>/<strong class="source-inline">normalize='columns'</strong>. An example is in the notebook.</p>
			<p>To change the aggregation<a id="_idIndexMarker602"/> function, we<a id="_idIndexMarker603"/> can provide an argument to <strong class="source-inline">values</strong> and then specify <strong class="source-inline">aggfunc</strong>. To illustrate this, let's find the average closing price of each<a id="_idIndexMarker604"/> trading volume bucket per month instead of the count in the previous example:</p>
			<p class="source-code">&gt;&gt;&gt; pd.crosstab(</p>
			<p class="source-code">...     index=fb.trading_volume, columns=fb.index.month,</p>
			<p class="source-code">...     colnames=['month'], <strong class="bold">values=fb.close,</strong> <strong class="bold">aggfunc=np.mean</strong></p>
			<p class="source-code">... )</p>
			<p>We now get the average closing price per month, per volume traded bin, with null values when that combination wasn't present in the data:</p>
			<div>
				<div id="_idContainer198" class="IMG---Figure">
					<img src="image/Figure_4.45_B16834.jpg" alt="Figure 4.45 – Crosstab using averages instead of counts&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.45 – Crosstab using averages instead of counts</p>
			<p>We can also get row and column subtotals with the <strong class="source-inline">margins</strong> parameter. Let's count the number of times each station recorded snow per month and include the subtotals:</p>
			<p class="source-code">&gt;&gt;&gt; snow_data = weather.query('datatype == "SNOW"')</p>
			<p class="source-code">&gt;&gt;&gt; pd.crosstab(</p>
			<p class="source-code">...     index=snow_data.station_name,</p>
			<p class="source-code">...     columns=snow_data.index.month, </p>
			<p class="source-code">...     colnames=['month'],</p>
			<p class="source-code">...     <strong class="bold">values=snow_data.value,</strong></p>
			<p class="source-code">...     <strong class="bold">aggfunc=lambda x: (x &gt; 0).sum(),</strong></p>
			<p class="source-code">...     <strong class="bold">margins=True,</strong> # show row and column subtotals</p>
			<p class="source-code">...     <strong class="bold">margins_name='total observations of snow'</strong> # subtotals</p>
			<p class="source-code">... )</p>
			<p>Along the bottom<a id="_idIndexMarker605"/> row, we have the total snow <a id="_idIndexMarker606"/>observations per month, while <a id="_idIndexMarker607"/>down the rightmost column, we have the total snow observations in 2018 per station:</p>
			<div>
				<div id="_idContainer199" class="IMG---Figure">
					<img src="image/Figure_4.46_B16834.jpg" alt="Figure 4.46 – Crosstab counting the number of days with snow per month, per station&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.46 – Crosstab counting the number of days with snow per month, per station</p>
			<p>Just by looking at a few stations, we can see that, despite all of them supplying weather information for NYC, they<a id="_idIndexMarker608"/> don't share every facet of the weather. Depending on which stations we choose to look at, we could be<a id="_idIndexMarker609"/> adding/subtracting snow from<a id="_idIndexMarker610"/> what really happened in NYC.</p>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor097"/>Working with time series data</h1>
			<p>With time<a id="_idIndexMarker611"/> series data, we have some additional operations we can use, for anything from selection and filtering to aggregation. We will be exploring some of this functionality in the <strong class="source-inline">4-time_series.ipynb</strong> notebook. Let's start off by reading in the Facebook data from the previous sections:</p>
			<p class="source-code">&gt;&gt;&gt; import numpy as np</p>
			<p class="source-code">&gt;&gt;&gt; import pandas as pd</p>
			<p class="source-code">&gt;&gt;&gt; fb = pd.read_csv(</p>
			<p class="source-code">...     'data/fb_2018.csv', index_col='date', parse_dates=True</p>
			<p class="source-code">... ).assign(trading_volume=lambda x: pd.cut( </p>
			<p class="source-code">...     x.volume, bins=3, labels=['low', 'med', 'high']     </p>
			<p class="source-code">... ))</p>
			<p>We will begin this section by discussing the selection and filtering of time series data before moving on to shifting, differencing, resampling, and finally merging based on time. Note that it's important to set the index to our date (or datetime) column, which will allow us to take advantage of the additional functionality we will be discussing. Some operations may work without doing this, but for a smooth process throughout our analysis, using an index of type <strong class="source-inline">DatetimeIndex</strong> is recommended.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor098"/>Time-based selection and filtering </h2>
			<p>Let's start with a<a id="_idIndexMarker612"/> quick recap of datetime slicing and indexing. We can easily isolate data for the year by indexing on it: <strong class="source-inline">fb.loc['2018']</strong>. In the case of our stock data, the full dataframe would be returned because we only have 2018 data; however, we can filter to a month (<strong class="source-inline">fb.loc['2018-10']</strong>) or to a range of dates. Note that using <strong class="source-inline">loc[]</strong> is optional with ranges:</p>
			<p class="source-code">&gt;&gt;&gt; fb['2018-10-11':'2018-10-15']</p>
			<p>We only get three days back because the stock market is closed on the weekends:</p>
			<div>
				<div id="_idContainer200" class="IMG---Figure">
					<img src="image/Figure_4.47_B16834.jpg" alt="Figure 4.47 – Selecting data based on a date range&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.47 – Selecting data based on a date range</p>
			<p>Keep in mind that the date range can also be supplied using other frequencies, such as month or the quarter of the year:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">fb.loc['2018-q1']</strong>.equals(<strong class="bold">fb['2018-01':'2018-03']</strong>)</p>
			<p class="source-code">True</p>
			<p>When targeting the beginning or end of a date range, <strong class="source-inline">pandas</strong> has some additional methods for selecting the first or last rows within a specified unit of time. We can select the first week of stock prices in 2018 using the <strong class="source-inline">first()</strong> method and an offset of <strong class="source-inline">1W</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; fb.<strong class="bold">first('1W')</strong></p>
			<p>January 1, 2018 was a holiday, meaning that the market was closed. It was also a Monday, so the week here is only four days long:</p>
			<div>
				<div id="_idContainer201" class="IMG---Figure">
					<img src="image/Figure_4.48_B16834.jpg" alt="Figure 4.48 – Facebook stock data during the first week of trading in 2018&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.48 – Facebook stock data during the first week of trading in 2018</p>
			<p>We can perform a similar<a id="_idIndexMarker613"/> operation for the most recent dates as well. Selecting the last week in the data is as simple as switching the <strong class="source-inline">first()</strong> method with the <strong class="source-inline">last()</strong> method:</p>
			<p class="source-code">&gt;&gt;&gt; fb.<strong class="bold">last('1W')</strong></p>
			<p>Since December 31, 2018 was a Monday, the last week only consists of one day:</p>
			<div>
				<div id="_idContainer202" class="IMG---Figure">
					<img src="image/Figure_4.49_B16834.jpg" alt="Figure 4.49 – Facebook stock data during the last week of trading in 2018&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.49 – Facebook stock data during the last week of trading in 2018</p>
			<p>When working with daily stock data, we only have data for the dates the stock market was open. Suppose that we reindexed the data to include rows for each day of the year: </p>
			<p class="source-code">&gt;&gt;&gt; fb_reindexed = fb.reindex(</p>
			<p class="source-code">...     pd.date_range('2018-01-01', '2018-12-31', freq='D')</p>
			<p class="source-code">... )</p>
			<p>The reindexed data would have all nulls for January 1st and any other days the market was closed. We can combine the <strong class="source-inline">first()</strong>, <strong class="source-inline">isna()</strong>, and <strong class="source-inline">all()</strong> methods to confirm this. Here, we will also use the <strong class="source-inline">squeeze()</strong> method to turn the 1-row <strong class="source-inline">DataFrame</strong> object resulting from the call to <strong class="source-inline">first('1D').isna()</strong> into a <strong class="source-inline">Series</strong> object so that calling <strong class="source-inline">all()</strong> yields a single value:</p>
			<p class="source-code">&gt;&gt;&gt; fb_reindexed.first('1D').isna().squeeze().all()</p>
			<p class="source-code">True</p>
			<p>We can use the <strong class="source-inline">first_valid_index()</strong> method to obtain the index of the first non-null entry in our data, which will be the first day of trading in the data. To obtain the last day of trading, we can use the <strong class="source-inline">last_valid_index()</strong> method. For the first quarter of 2018, the first<a id="_idIndexMarker614"/> day of trading was January 2nd and the last was March 29th:</p>
			<p class="source-code">&gt;&gt;&gt; fb_reindexed.loc['2018-Q1'].<strong class="bold">first_valid_index()</strong></p>
			<p class="source-code">Timestamp('2018-01-02 00:00:00', freq='D')</p>
			<p class="source-code">&gt;&gt;&gt; fb_reindexed.loc['2018-Q1'].<strong class="bold">last_valid_index()</strong></p>
			<p class="source-code">Timestamp('2018-03-29 00:00:00', freq='D')</p>
			<p>If we wanted to know what Facebook's stock price looked like as of March 31, 2018, our initial idea may be to use indexing to retrieve it. However, if we try to do so with <strong class="source-inline">loc[]</strong> (<strong class="source-inline">fb_reindexed.loc['2018-03-31']</strong>), we will get null values because the stock market wasn't open that day. If we use the <strong class="source-inline">asof()</strong> method instead, it will give us the closest non-null data that precedes the date we ask for, which in this case is March 29th. Therefore, if we wanted to see how Facebook performed on the last day in each month, we could use <strong class="source-inline">asof()</strong>, and avoid having to first check if the market was open that day:</p>
			<p class="source-code">&gt;&gt;&gt; fb_reindexed.<strong class="bold">asof('2018-03-31')</strong></p>
			<p class="source-code">open                   155.15</p>
			<p class="source-code">high                   161.42</p>
			<p class="source-code">low                    154.14</p>
			<p class="source-code">close                  159.79</p>
			<p class="source-code">volume            59434293.00</p>
			<p class="source-code">trading_volume            low</p>
			<p class="source-code">Name: 2018-03-31 00:00:00, dtype: object</p>
			<p>For the next few examples, we will need time information in addition to the date. The datasets we have been working with thus far lack a time component, so we will switch to the Facebook stock data by the minute from May 20, 2019 through May 24, 2019 from Nasdaq.com. In order to properly parse the datetimes, we need to pass in a lambda function as the <strong class="source-inline">date_parser</strong> argument since they are not in a standard format (for instance, May 20, 2019 at 9:30 AM is represented as <strong class="source-inline">2019-05-20 09-30</strong>); the lambda function <a id="_idIndexMarker615"/>will specify how to convert the data in the <strong class="source-inline">date</strong> field into datetimes:</p>
			<p class="source-code">&gt;&gt;&gt; stock_data_per_minute = pd.read_csv(</p>
			<p class="source-code">...     <strong class="bold">'data/fb_week_of_may_20_per_minute.csv',</strong> </p>
			<p class="source-code">...     index_col='date', parse_dates=True, </p>
			<p class="source-code">...     <strong class="bold">date_parser=lambda x: \</strong></p>
			<p class="source-code">...         <strong class="bold">pd.to_datetime(x, format='%Y-%m-%d %H-%M')</strong></p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; stock_data_per_minute.head()</p>
			<p>We have the OHLC data per minute, along with the volume traded per minute:</p>
			<div>
				<div id="_idContainer203" class="IMG---Figure">
					<img src="image/Figure_4.50_B16834.jpg" alt="Figure 4.50 – Facebook stock data by the minute&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.50 – Facebook stock data by the minute</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In order to properly parse datetimes in a non-standard format, we need to specify the format it is in. For a reference on the available codes, consult the Python documentation at <a href="https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior">https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior</a>.</p>
			<p>We can use <strong class="source-inline">first()</strong> and <strong class="source-inline">last()</strong> with <strong class="source-inline">agg()</strong> to bring this data to a daily granularity. To get the true open value, we need to take the first observation per day; conversely, for the true closing value, we need to take the last observation per day. The high and low will be the <a id="_idIndexMarker616"/>maximum and minimum of their respective columns per day. Volume traded will be the daily sum:</p>
			<p class="source-code">&gt;&gt;&gt; stock_data_per_minute.groupby(pd.Grouper(freq='1D')).agg({</p>
			<p class="source-code">...     <strong class="bold">'open': 'first',</strong> </p>
			<p class="source-code">...     'high': 'max', </p>
			<p class="source-code">...     'low': 'min', </p>
			<p class="source-code">...     <strong class="bold">'close': 'last',</strong> </p>
			<p class="source-code">...     'volume': 'sum'</p>
			<p class="source-code">... })</p>
			<p>This rolls the data up to a daily frequency:</p>
			<div>
				<div id="_idContainer204" class="IMG---Figure">
					<img src="image/Figure_4.51_B16834.jpg" alt="Figure 4.51 – Rolling up the data from the minute level to the daily level&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.51 – Rolling up the data from the minute level to the daily level</p>
			<p>The next two methods we will discuss help us select data based on the time part of the datetime. The <strong class="source-inline">at_time()</strong> method allows us to isolate rows where the time part of the datetime is the time we specify. By running <strong class="source-inline">at_time('9:30')</strong>, we can grab all the market open prices (the stock market opens at 9:30 AM):</p>
			<p class="source-code">&gt;&gt;&gt; stock_data_per_minute.<strong class="bold">at_time('9:30')</strong></p>
			<p>This tells us what the stock data looked like at the opening bell each day:</p>
			<div>
				<div id="_idContainer205" class="IMG---Figure">
					<img src="image/Figure_4.52_B16834.jpg" alt="Figure 4.52 – Grabbing the stock data at market open each day&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.52 – Grabbing the stock data at market open each day</p>
			<p>We can use the <strong class="source-inline">between_time()</strong> method to grab all the rows where the time portion of the datetime is between two times (inclusive of the endpoints by default). This method <a id="_idIndexMarker617"/>can be very useful if we want to look at data within a certain time range, day over day. Let's grab all the rows within the last two minutes of trading each day (15:59 - 16:00):</p>
			<p class="source-code">&gt;&gt;&gt; stock_data_per_minute.<strong class="bold">between_time('15:59', '16:00')</strong></p>
			<p>It looks like the last minute (16:00) has significantly more volume traded each day compared to the previous minute (15:59). Perhaps people rush to make trades before close:</p>
			<div>
				<div id="_idContainer206" class="IMG---Figure">
					<img src="image/Figure_4.53_B16834.jpg" alt="Figure 4.53 – Stock data in the last two minutes of trading per day&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.53 – Stock data in the last two minutes of trading per day</p>
			<p>We may wonder if this also happens in the first two minutes. Do people put their trades in the night before, and they execute when the market opens? It is trivial to change the previous code to answer<a id="_idIndexMarker618"/> that question. Instead, let's see if, on average, more shares were traded within the first 30 minutes of trading or in the last 30 minutes for the week in question. We can combine <strong class="source-inline">between_time()</strong> with <strong class="source-inline">groupby()</strong> to answer this question. In addition, we need to use <strong class="source-inline">filter()</strong> to exclude groups from the aggregation. The excluded groups are times that aren't in the time range we want:</p>
			<p class="source-code">&gt;&gt;&gt; shares_traded_in_first_30_min = stock_data_per_minute\</p>
			<p class="source-code">...     <strong class="bold">.between_time('9:30', '10:00')</strong>\</p>
			<p class="source-code">...     <strong class="bold">.groupby(pd.Grouper(freq='1D'))</strong>\</p>
			<p class="source-code">...     <strong class="bold">.filter(lambda x: (x.volume &gt; 0).all())</strong>\</p>
			<p class="source-code">...     .volume.mean()</p>
			<p class="source-code">&gt;&gt;&gt; shares_traded_in_last_30_min = stock_data_per_minute\</p>
			<p class="source-code">...     <strong class="bold">.between_time('15:30', '16:00')</strong>\</p>
			<p class="source-code">...     .groupby(pd.Grouper(freq='1D'))\</p>
			<p class="source-code">...     .filter(lambda x: (x.volume &gt; 0).all())\</p>
			<p class="source-code">...     .volume.mean()</p>
			<p>For the week in question, there were 18,593 more trades on average around the opening time than the closing time:</p>
			<p class="source-code">&gt;&gt;&gt; shares_traded_in_first_30_min \</p>
			<p class="source-code">... - shares_traded_in_last_30_min</p>
			<p class="source-code">18592.967741935485</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We can use the <strong class="source-inline">normalize()</strong> method on <strong class="source-inline">DatetimeIndex</strong> objects or after first accessing the <strong class="source-inline">dt</strong> attribute of a <strong class="source-inline">Series</strong> object to normalize all the datetimes to midnight. This is helpful when the time isn't adding value to our data. There are examples of this in the notebook.</p>
			<p>With the stock data, we <a id="_idIndexMarker619"/>have a snapshot of the price for each minute or day (depending on the granularity), but we may be interested in seeing the change between time periods as a time series rather than aggregating the data. For this, we need to learn how to create lagged data.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor099"/>Shifting for lagged data</h2>
			<p>We can use the <strong class="source-inline">shift()</strong> method to create<a id="_idIndexMarker620"/> lagged data. By default, the shift will be by one period, but this can be any integer (positive or negative). Let's use <strong class="source-inline">shift()</strong> to create a new column that indicates the previous day's closing price for the daily Facebook stock data. From this new column, we can calculate the price change due to after-hours trading (after the market close one day right up to the market open the following day):</p>
			<p class="source-code">&gt;&gt;&gt; fb.assign(</p>
			<p class="source-code">...     <strong class="bold">prior_close=lambda x: x.close.shift(),</strong></p>
			<p class="source-code">...     <strong class="bold">after_hours_change_in_price=lambda x: \</strong></p>
			<p class="source-code">...         <strong class="bold">x.open - x.prior_close,</strong></p>
			<p class="source-code">...     <strong class="bold">abs_change=lambda x: \</strong></p>
			<p class="source-code">...         <strong class="bold">x.after_hours_change_in_price.abs()</strong></p>
			<p class="source-code">... ).nlargest(5, 'abs_change')</p>
			<p>This gives us the days that were most affected by after-hours trading:</p>
			<div>
				<div id="_idContainer207" class="IMG---Figure">
					<img src="image/Figure_4.54_B16834.jpg" alt="Figure 4.54 – Using lagged data to calculate after-hours changes in stock price&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.54 – Using lagged data to calculate after-hours changes in stock price</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">To add/subtract time from the datetimes in the index, consider using <strong class="source-inline">Timedelta</strong> objects instead. There is an example of this in the notebook.</p>
			<p>In the previous example, we <a id="_idIndexMarker621"/>used the shifted data to calculate the change across columns. However, if, rather than the after-hours trading, we were interested in the change in Facebook's stock price each day, we would calculate the difference between the closing price and the shifted closing price. Pandas makes it a little easier than this, as we will see next.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor100"/>Differenced data</h2>
			<p>We've already discussed creating lagged data with the <strong class="source-inline">shift()</strong> method. However, often, we are interested in how the<a id="_idIndexMarker622"/> values change from one time period to the next. For this, <strong class="source-inline">pandas</strong> has the <strong class="source-inline">diff()</strong> method. By default, this will calculate the change from time period <em class="italic">t-1</em> to time period <em class="italic">t</em>:</p>
			<div>
				<div id="_idContainer208" class="IMG---Figure">
					<img src="image/Formula_04_001.jpg" alt=""/>
				</div>
			</div>
			<p>Note that this is equivalent to subtracting the result of <strong class="source-inline">shift()</strong> from the original data:</p>
			<p class="source-code">&gt;&gt;&gt; (fb.drop(columns='trading_volume') </p>
			<p class="source-code">...  - fb.drop(columns='trading_volume').<strong class="bold">shift()</strong></p>
			<p class="source-code">... ).equals(fb.drop(columns='trading_volume').<strong class="bold">diff()</strong>)</p>
			<p class="source-code">True</p>
			<p>We can use <strong class="source-inline">diff()</strong> to easily <a id="_idIndexMarker623"/>calculate the day-over-day change in the Facebook stock data:</p>
			<p class="source-code">&gt;&gt;&gt; fb.drop(columns='trading_volume').<strong class="bold">diff()</strong>.head()</p>
			<p>For the first few trading days of the year, we can see that the stock price increased, and that the volume traded decreased daily:</p>
			<div>
				<div id="_idContainer209" class="IMG---Figure">
					<img src="image/Figure_4.55_B16834.jpg" alt="Figure 4.55 – Calculating day-over-day changes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.55 – Calculating day-over-day changes</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">To specify the number of periods that are used for the difference, simply pass in an integer to <strong class="source-inline">diff()</strong>. Note that this number can be negative. An example of this is in the notebook.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor101"/>Resampling</h2>
			<p>Sometimes, the data is at a <a id="_idIndexMarker624"/>granularity that isn't conducive to our analysis. Consider the case where we have data per minute for the full year of 2018. The level of granularity and nature of the data may render plotting useless. Therefore, we will need to aggregate the data to a less granular frequency:</p>
			<div>
				<div id="_idContainer210" class="IMG---Figure">
					<img src="image/Figure_4.56_B16834.jpg" alt="Figure 4.56 – Resampling can be used to roll up granular data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.56 – Resampling can be used to roll up granular data</p>
			<p>Suppose we had a full year of the data in <em class="italic">Figure 4.50</em> (Facebook stock data by the minute). It's possible that this level of granularity is beyond what is useful for us, in which case we can use the <strong class="source-inline">resample()</strong> method to aggregate our time series data to a different granularity. To use <strong class="source-inline">resample()</strong>, all we have to do is say how we want to roll up the data and tack on an optional call to an aggregation method. For example, we can resample this minute-by-minute data to a daily frequency and specify how to aggregate each column:</p>
			<p class="source-code">&gt;&gt;&gt; stock_data_per_minute.<strong class="bold">resample('1D')</strong>.agg({</p>
			<p class="source-code">...     'open': 'first', </p>
			<p class="source-code">...     'high': 'max', </p>
			<p class="source-code">...     'low': 'min', </p>
			<p class="source-code">...     'close': 'last', </p>
			<p class="source-code">...     'volume': 'sum'</p>
			<p class="source-code">... })</p>
			<p>This is equivalent to the result we got back in the <em class="italic">Time-based selection and filtering</em> section (<em class="italic">Figure 4.51</em>):</p>
			<div>
				<div id="_idContainer211" class="IMG---Figure">
					<img src="image/Figure_4.57_B16834.jpg" alt="Figure 4.57 – Per-minute data resampled into daily data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.57 – Per-minute data resampled into daily data</p>
			<p>We can resample to any frequency supported by <strong class="source-inline">pandas</strong> (more information can be found in the documentation at <a href="http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html">http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html</a>). Let's resample the daily Facebook stock data to the quarterly average:</p>
			<p class="source-code">&gt;&gt;&gt; fb.resample('Q').mean()</p>
			<p>This gives us the <a id="_idIndexMarker625"/>average quarterly performance of the stock. The fourth quarter of 2018 was clearly troublesome:</p>
			<div>
				<div id="_idContainer212" class="IMG---Figure">
					<img src="image/Figure_4.58_B16834.jpg" alt="Figure 4.58 – Resampling to quarterly averages&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.58 – Resampling to quarterly averages</p>
			<p>To look further into this, we can use the <strong class="source-inline">apply()</strong> method to look at the difference between how the quarter began and how it ended. We will also need the <strong class="source-inline">first()</strong> and <strong class="source-inline">last()</strong> methods from the <em class="italic">Time-based selection and filtering</em> section:</p>
			<p class="source-code">&gt;&gt;&gt; fb.drop(columns='trading_volume').<strong class="bold">resample('Q').apply(</strong></p>
			<p class="source-code">...     <strong class="bold">lambda x: x.last('1D').values - x.first('1D').values</strong></p>
			<p class="source-code">... <strong class="bold">)</strong></p>
			<p>Facebook's stock price<a id="_idIndexMarker626"/> declined in all but the second quarter:</p>
			<div>
				<div id="_idContainer213" class="IMG---Figure">
					<img src="image/Figure_4.59_B16834.jpg" alt="Figure 4.59 – Summarizing Facebook stock's performance per quarter in 2018&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.59 – Summarizing Facebook stock's performance per quarter in 2018</p>
			<p>Consider the melted minute-by-minute stock data in <strong class="source-inline">melted_stock_data.csv</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; melted_stock_data = pd.read_csv(</p>
			<p class="source-code">...     'data/melted_stock_data.csv', </p>
			<p class="source-code">...     index_col='date', parse_dates=True</p>
			<p class="source-code">... )</p>
			<p class="source-code">&gt;&gt;&gt; melted_stock_data.head()</p>
			<p>The OHLC format makes it easy to analyze the stock data, but a single column is trickier:</p>
			<div>
				<div id="_idContainer214" class="IMG---Figure">
					<img src="image/Figure_4.60_B16834.jpg" alt="Figure 4.60 – Stock prices by the minute&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.60 – Stock prices by the minute</p>
			<p>The <strong class="source-inline">Resampler</strong> object we<a id="_idIndexMarker627"/> get back after calling <strong class="source-inline">resample()</strong> has an <strong class="source-inline">ohlc()</strong> method, which we can use to retrieve the OHLC data we are used to seeing:</p>
			<p class="source-code">&gt;&gt;&gt; melted_stock_data.resample('1D').<strong class="bold">ohlc()['price']</strong></p>
			<p>Since the column in the original data was called <strong class="source-inline">price</strong>, we select it after calling <strong class="source-inline">ohlc()</strong>, which is pivoting our data. Otherwise, we will have a hierarchical index in the columns:</p>
			<div>
				<div id="_idContainer215" class="IMG---Figure">
					<img src="image/Figure_4.61_B16834.jpg" alt="Figure 4.61 – Resampling the stock prices per minute to form daily OHLC data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.61 – Resampling the stock prices per minute to form daily OHLC data</p>
			<p>In the previous examples, we <strong class="bold">downsampled</strong> to reduce the granularity of the data; however, we can also <strong class="bold">upsample</strong> to increase the granularity of the data. We can even call <strong class="source-inline">asfreq()</strong> after to not aggregate the result:</p>
			<p class="source-code">&gt;&gt;&gt; fb.<strong class="bold">resample('6H').asfreq()</strong>.head()</p>
			<p>Note that when we resample at<a id="_idIndexMarker628"/> a granularity that's finer than the data we have, it will introduce <strong class="source-inline">NaN</strong> values:</p>
			<div>
				<div id="_idContainer216" class="IMG---Figure">
					<img src="image/Figure_4.62_B16834.jpg" alt="Figure 4.62 – Upsampling increases the granularity of the data and will introduce null values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.62 – Upsampling increases the granularity of the data and will introduce null values</p>
			<p>The following are a few ways we can handle the <strong class="source-inline">NaN</strong> values. In the interest of brevity, examples of these are in the notebook:</p>
			<ul>
				<li>Use <strong class="source-inline">pad()</strong> after <strong class="source-inline">resample()</strong> to forward fill.</li>
				<li>Call <strong class="source-inline">fillna()</strong> after <strong class="source-inline">resample()</strong>, as we saw in <a href="B16834_03_Final_SK_ePub.xhtml#_idTextAnchor061"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Wrangling with Pandas</em>, when we handled missing values.</li>
				<li>Use <strong class="source-inline">asfreq()</strong> followed by <strong class="source-inline">assign()</strong> to handle each column individually.</li>
			</ul>
			<p>So far, we have been working with time series data stored in a single <strong class="source-inline">DataFrame</strong> object, but we may want to combine time series. While the techniques discussed in the <em class="italic">Merging DataFrames</em> section will work for time series, <strong class="source-inline">pandas</strong> provides additional functionality for merging time series so that we can merge on close matches rather than requiring an exact match. We will discuss these next.</p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor102"/>Merging time series</h2>
			<p>Time series often go<a id="_idIndexMarker629"/> down to the second or are even more granular, meaning that it can be difficult to merge if the entries don't have the same datetime. Pandas solves this problem with two additional merging functions. When we want to pair up observations that are close in time, we can use <strong class="source-inline">pd.merge_asof()</strong> to match on nearby keys rather than on equal keys, like we did with joins. On the other hand, if we want to match up the equal keys and interleave the keys without matches, we can use <strong class="source-inline">pd.merge_ordered()</strong>.</p>
			<p>To illustrate how these work, we are going to use the <strong class="source-inline">fb_prices</strong> and <strong class="source-inline">aapl_prices</strong> tables in the <strong class="source-inline">stocks.db</strong> SQLite database. These contain the prices of Facebook and Apple stock, respectively, along with a timestamp of when the <a id="_idIndexMarker630"/>price was recorded. Note that the Apple data was collected before the stock split in August 2020 (<a href="https://www.marketwatch.com/story/3-things-to-know-about-apples-stock-split-2020-08-28">https://www.marketwatch.com/story/3-things-to-know-about-apples-stock-split-2020-08-28</a>). Let's read these tables from the database:</p>
			<p class="source-code">&gt;&gt;&gt; import sqlite3</p>
			<p class="source-code">&gt;&gt;&gt; with sqlite3.connect('data/stocks.db') as connection:</p>
			<p class="source-code">...     fb_prices = pd.read_sql(</p>
			<p class="source-code">...         'SELECT * FROM fb_prices', connection, </p>
			<p class="source-code">...         index_col='date', parse_dates=['date']</p>
			<p class="source-code">...     )</p>
			<p class="source-code">...     aapl_prices = pd.read_sql(</p>
			<p class="source-code">...         'SELECT * FROM aapl_prices', connection, </p>
			<p class="source-code">...         index_col='date', parse_dates=['date']</p>
			<p class="source-code">...     )</p>
			<p>The Facebook data is at the minute granularity; however, we have (fictitious) seconds for the Apple data:</p>
			<p class="source-code">&gt;&gt;&gt; fb_prices.index.second.unique()</p>
			<p class="source-code">Int64Index([0], dtype='int64', name='date')</p>
			<p class="source-code">&gt;&gt;&gt; aapl_prices.index.second.unique()</p>
			<p class="source-code">Int64Index([ 0, 52, ..., 37, 28], dtype='int64', name='date')</p>
			<p>If we use <strong class="source-inline">merge()</strong> or <strong class="source-inline">join()</strong>, we will only have values for both Apple and Facebook when the Apple price was at the top of the minute. Instead, to try and line these up, we can perform an <em class="italic">as of</em> merge. In order to handle the mismatch, we will specify to merge with the nearest minute (<strong class="source-inline">direction='nearest'</strong>) and require that a match can only occur between times that are within 30 seconds of each other (<strong class="source-inline">tolerance</strong>). This will place the Apple data <a id="_idIndexMarker631"/>with the minute that it is closest to, so <strong class="source-inline">9:31:52</strong> will go with <strong class="source-inline">9:32</strong> and <strong class="source-inline">9:37:07</strong> will go with <strong class="source-inline">9:37</strong>. Since the times are on the index, we pass in <strong class="source-inline">left_index</strong> and <strong class="source-inline">right_index</strong>, just like we did with <strong class="source-inline">merge()</strong>: </p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">pd.merge_asof(</strong></p>
			<p class="source-code">...     <strong class="bold">fb_prices, aapl_prices,</strong> </p>
			<p class="source-code">...     <strong class="bold">left_index=True, right_index=True,</strong></p>
			<p class="source-code">...     # merge with nearest minute</p>
			<p class="source-code">...     <strong class="bold">direction='nearest',</strong></p>
			<p class="source-code">...     <strong class="bold">tolerance=pd.Timedelta(30, unit='s')</strong></p>
			<p class="source-code">... <strong class="bold">)</strong>.head()</p>
			<p>This is similar to a left join; however, we are more lenient when matching the keys. Note that in the case where multiple entries in the Apple data match the same minute, this function will only keep the closest one. We get a null value for <strong class="source-inline">9:31</strong> because the entry for Apple at <strong class="source-inline">9:31</strong> was <strong class="source-inline">9:31:52</strong>, which gets placed at <strong class="source-inline">9:32</strong> when using <strong class="source-inline">nearest</strong>:</p>
			<div>
				<div id="_idContainer217" class="IMG---Figure">
					<img src="image/Figure_4.63_B16834.jpg" alt="Figure 4.63 – Merging time series data with a 30-second tolerance&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.63 – Merging time series data with a 30-second tolerance</p>
			<p>If we don't want the behavior of a left join, we can use the <strong class="source-inline">pd.merge_ordered()</strong> function instead. This will allow us to specify our join type, which will be <strong class="source-inline">'outer'</strong> by default. We will have to reset our index to be able to join on the datetimes, however:</p>
			<p class="source-code">&gt;&gt;&gt; <strong class="bold">pd.merge_ordered(</strong></p>
			<p class="source-code">...     <strong class="bold">fb_prices.reset_index(), aapl_prices.reset_index()</strong></p>
			<p class="source-code">... <strong class="bold">)</strong>.set_index('date').head()</p>
			<p>This strategy will give us <a id="_idIndexMarker632"/>null values whenever the times don't match exactly, but it will at least sort them for us:</p>
			<div>
				<div id="_idContainer218" class="IMG---Figure">
					<img src="image/Figure_4.64_B16834.jpg" alt="Figure 4.64 – Performing a strict merge on time series data and ordering it&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.64 – Performing a strict merge on time series data and ordering it</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We can pass <strong class="source-inline">fill_method='ffill'</strong> to <strong class="source-inline">pd.merge_ordered()</strong> to forward-fill the first <strong class="source-inline">NaN</strong> after a value, but it does not propagate beyond that; alternatively, we can chain a call to <strong class="source-inline">fillna()</strong>. There is an example of this in the notebook.</p>
			<p>The <strong class="source-inline">pd.merge_ordered()</strong> function also makes it possible to perform a group-wise merge, so be sure to check out the documentation for more information.</p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor103"/>Summary</h1>
			<p>In this chapter, we discussed how to join dataframes, how to determine the data we will lose for each type of join using set operations, and how to query dataframes as we would a database. We then went over some more involved transformations on our columns, such as binning and ranking, and how to do so efficiently with the <strong class="source-inline">apply()</strong> method. We also learned the importance of vectorized operations in writing efficient <strong class="source-inline">pandas</strong> code. Then, we explored window calculations and using pipes for cleaner code. Our discussion of window calculations served as a primer for aggregating across whole dataframes and by groups. We also went over how to generate pivot tables and crosstabs. Finally, we looked at some time series-specific functionality in <strong class="source-inline">pandas</strong> for everything from selection and aggregation to merging.</p>
			<p>In the next chapter, we will cover visualization, which <strong class="source-inline">pandas</strong> implements by providing a wrapper around <strong class="source-inline">matplotlib</strong>. Data wrangling will play a key role in prepping our data for visualization, so be sure to complete the exercises that are provided in the following section before moving on.</p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor104"/>Exercises</h1>
			<p>Using the CSV files in the <strong class="source-inline">exercises/</strong> folder and what we have learned so far in this book, complete the following exercises:</p>
			<ol>
				<li>With the <strong class="source-inline">earthquakes.csv</strong> file, select all the earthquakes in Japan with a magnitude of 4.9 or greater using the <strong class="source-inline">mb</strong> magnitude type.</li>
				<li>Create bins for each full number of earthquake magnitude (for instance, the first bin is (0, 1], the second is (1, 2], and so on) with the <strong class="source-inline">ml</strong> magnitude type and count how many are in each bin.</li>
				<li>Using the <strong class="source-inline">faang.csv</strong> file, group by the ticker and resample to monthly frequency. Make the following aggregations:<p>a) Mean of the opening price</p><p>b) Maximum of the high price</p><p>c) Minimum of the low price</p><p>d) Mean of the closing price</p><p>e) Sum of the volume traded</p></li>
				<li>Build a crosstab with the earthquake data between the <strong class="source-inline">tsunami</strong> column and the <strong class="source-inline">magType</strong> column. Rather than showing the frequency count, show the maximum magnitude that was observed for each combination. Put the magnitude type along the columns.</li>
				<li>Calculate the rolling 60-day aggregations of the OHLC data by ticker for the FAANG data. Use the same aggregations as exercise <em class="italic">3</em>. </li>
				<li>Create a pivot table of the FAANG data that compares the stocks. Put the ticker in the rows and show the averages of the OHLC and volume traded data.</li>
				<li>Calculate the Z-scores for each numeric column of Amazon's data (<strong class="source-inline">ticker</strong> is AMZN) in Q4 2018 using <strong class="source-inline">apply()</strong>.</li>
				<li>Add event descriptions:<p>a) Create a dataframe with the following three columns: <strong class="source-inline">ticker</strong>, <strong class="source-inline">date</strong>, and <strong class="source-inline">event</strong>. The columns should have the following values:</p><p>        i) <strong class="source-inline">ticker</strong>: <strong class="source-inline">'FB'</strong></p><p>        ii) <strong class="source-inline">date</strong>: <strong class="source-inline">['2018-07-25', '2018-03-19', '2018-03-20']</strong></p><p>        iii) <strong class="source-inline">event</strong>: <strong class="source-inline">['Disappointing user growth announced after close.', 'Cambridge Analytica story', 'FTC investigation']</strong></p><p>b) Set the index to <strong class="source-inline">['date', 'ticker']</strong>.</p><p>c) Merge this data with the FAANG data using an outer join.</p></li>
				<li>Use the <strong class="source-inline">transform()</strong> method on the FAANG data to represent all the values in terms of the first date in the data. To do so, divide all the values for each ticker by the values for the first date in the data for that ticker. This is referred to as an <strong class="bold">index</strong>, and the data for the first date is the <strong class="bold">base</strong> (<a href="https://ec.europa.eu/eurostat/statistics-explained/index.php/Beginners:Statistical_concept_-_Index_and_base_year">https://ec.europa.eu/eurostat/statistics-explained/index.php/Beginners:Statistical_concept_-_Index_and_base_year</a>). When data is in this format, we can easily see growth over time. Hint: <strong class="source-inline">transform()</strong> can take a function name.</li>
				<li>The <strong class="bold">European Centre for Disease Prevention and Control</strong> (<strong class="bold">ECDC</strong>) provides an open dataset on COVID-19 cases called <em class="italic">daily number of new reported cases of COVID-19 by country worldwide</em> (<a href="https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide">https://www.ecdc.europa.eu/en/publications-data/download-todays-data-geographic-distribution-covid-19-cases-worldwide</a>). This dataset is updated daily, but we will use a snapshot that contains data through September 18, 2020. Complete the following tasks to practice the skills you've learned up to this point in the book:<p>a) Prepare the data:</p><p>        i) Read in the data in the <strong class="source-inline">covid19_cases.csv</strong> file.</p><p>        ii) Create a <strong class="source-inline">date</strong> column by parsing the <strong class="source-inline">dateRep</strong> column into a datetime.</p><p>        iii) Set the <strong class="source-inline">date</strong> column as the index.</p><p>        iv) Use the <strong class="source-inline">replace()</strong> method to update all occurrences of <strong class="source-inline">United_States_of_America</strong> and <strong class="source-inline">United_Kingdom</strong> to <strong class="source-inline">USA</strong> and <strong class="source-inline">UK</strong>, respectively.</p><p>        v) Sort the index.</p><p>b) For the five countries with the most cases (cumulative), find the day with the largest number of cases.</p><p>c) Find the 7-day average change in COVID-19 cases for the last week in the data for the five countries with the most cases.</p><p>d) Find the first date that each country other than China had cases.</p><p>e) Rank the countries by cumulative cases using percentiles.</p></li>
			</ol>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor105"/>Further reading</h1>
			<p>Check out the following resources for more information on the topics that were covered in this chapter:</p>
			<ul>
				<li><em class="italic">Intro to SQL: Querying and managing data</em>: <a href="https://www.khanacademy.org/computing/computer-programming/sql">https://www.khanacademy.org/computing/computer-programming/sql</a> </li>
				<li><em class="italic">(Pandas) Comparison with SQL</em>: <a href="https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html">https://pandas.pydata.org/pandas-docs/stable/getting_started/comparison/comparison_with_sql.html</a></li>
				<li><em class="italic">Set Operations</em>: <a href="https://www.probabilitycourse.com/chapter1/1_2_2_set_operations.php">https://www.probabilitycourse.com/chapter1/1_2_2_set_operations.php</a></li>
				<li><em class="italic">*args and **kwargs in Python explained</em>: <a href="https://yasoob.me/2013/08/04/args-and-kwargs-in-python-explained/">https://pythontips.com/2013/08/04/args-and-kwargs-in-python-explained/</a></li>
			</ul>
		</div>
	</body></html>