["```py\n>>> import numpy as np \n>>> import pandas as pd \n\n# KNN Curse of Dimensionality \n>>> import random,math \n```", "```py\n>>> def random_point_gen(dimension): \n...     return [random.random() for _ in range(dimension)] \n```", "```py\n>>> def distance(v,w): \n...     vec_sub = [v_i-w_i for v_i,w_i in zip(v,w)] \n...     sum_of_sqrs = sum(v_i*v_i for v_i in vec_sub) \n...     return math.sqrt(sum_of_sqrs) \n```", "```py\n>>> def random_distances_comparison(dimension,number_pairs): \n...     return [distance(random_point_gen(dimension),random_point_gen(dimension)) \n            for _ in range(number_pairs)] \n\n>>> def mean(x): \n...     return sum(x) / len(x) \n```", "```py\n>>> dimensions = range(1, 201, 5)\n```", "```py\n>>> avg_distances = [] \n>>> min_distances = [] \n\n>>> dummyarray = np.empty((20,4)) \n>>> dist_vals = pd.DataFrame(dummyarray) \n>>> dist_vals.columns = [\"Dimension\",\"Min_Distance\",\"Avg_Distance\",\"Min/Avg_Distance\"] \n\n>>> random.seed(34) \n>>> i = 0 \n>>> for dims in dimensions: \n...     distances = random_distances_comparison(dims, 1000)   \n...     avg_distances.append(mean(distances))     \n...     min_distances.append(min(distances))      \n\n...     dist_vals.loc[i,\"Dimension\"] = dims \n...     dist_vals.loc[i,\"Min_Distance\"] = min(distances) \n...     dist_vals.loc[i,\"Avg_Distance\"] = mean(distances) \n...     dist_vals.loc[i,\"Min/Avg_Distance\"] = min(distances)/mean(distances) \n\n...     print(dims, min(distances), mean(distances), min(distances)*1.0 / mean( distances)) \n...     i = i+1 \n\n# Plotting Average distances for Various Dimensions \n>>> import matplotlib.pyplot as plt \n>>> plt.figure() \n>>> plt.xlabel('Dimensions') \n>>> plt.ylabel('Avg. Distance') \n>>> plt.plot(dist_vals[\"Dimension\"],dist_vals[\"Avg_Distance\"]) \n>>> plt.legend(loc='best') \n\n>>> plt.show() \n```", "```py\n# 1-Dimension Plot \n>>> import numpy as np \n>>> import pandas as pd \n>>> import matplotlib.pyplot as plt \n\n>>> one_d_data = np.random.rand(60,1) \n>>> one_d_data_df = pd.DataFrame(one_d_data) \n>>> one_d_data_df.columns = [\"1D_Data\"] \n>>> one_d_data_df[\"height\"] = 1 \n\n>>> plt.figure() \n>>> plt.scatter(one_d_data_df['1D_Data'],one_d_data_df[\"height\"]) \n>>> plt.yticks([]) \n>>> plt.xlabel(\"1-D points\") \n>>> plt.show()\n```", "```py\n# 2- Dimensions Plot \n>>> two_d_data = np.random.rand(60,2) \n>>> two_d_data_df = pd.DataFrame(two_d_data) \n>>> two_d_data_df.columns = [\"x_axis\",\"y_axis\"] \n\n>>> plt.figure() \n>>> plt.scatter(two_d_data_df['x_axis'],two_d_data_df[\"y_axis\"]) \n>>> plt.xlabel(\"x_axis\");plt.ylabel(\"y_axis\") \n>>> plt.show()  \n```", "```py\n# 3- Dimensions Plot \n>>> three_d_data = np.random.rand(60,3) \n>>> three_d_data_df = pd.DataFrame(three_d_data) \n>>> three_d_data_df.columns = [\"x_axis\",\"y_axis\",\"z_axis\"] \n\n>>> from mpl_toolkits.mplot3d import Axes3D \n>>> fig = plt.figure() \n>>> ax = fig.add_subplot(111, projection='3d') \n>>> ax.scatter(three_d_data_df['x_axis'],three_d_data_df[\"y_axis\"],three_d_data_df [\"z_axis\"]) \n>>> plt.show() \n```", "```py\n# KNN Classifier - Breast Cancer \n>>> import numpy as np \n>>> import pandas as pd \n>>> from sklearn.metrics import accuracy_score,classification_report \n>>> breast_cancer = pd.read_csv(\"Breast_Cancer_Wisconsin.csv\") \n```", "```py\n>>> breast_cancer['Bare_Nuclei'] = breast_cancer['Bare_Nuclei'].replace('?', np.NAN) \n>>> breast_cancer['Bare_Nuclei'] = breast_cancer['Bare_Nuclei'].fillna(breast_cancer[ 'Bare_Nuclei'].value_counts().index[0]) \n```", "```py\n>>> breast_cancer['Cancer_Ind'] = 0 \n>>> breast_cancer.loc[breast_cancer['Class']==4,'Cancer_Ind'] = 1\n```", "```py\n>>> x_vars = breast_cancer.drop(['ID_Number','Class','Cancer_Ind'],axis=1) \n>>> y_var = breast_cancer['Cancer_Ind'] \n>>> from sklearn.preprocessing import StandardScaler \n>>> x_vars_stdscle = StandardScaler().fit_transform(x_vars.values) \n>>> from sklearn.model_selection import train_test_split \n```", "```py\n>>> x_vars_stdscle_df = pd.DataFrame(x_vars_stdscle, index=x_vars.index, columns=x_vars.columns) \n>>> x_train,x_test,y_train,y_test = train_test_split(x_vars_stdscle_df,y_var, train_size = 0.7,random_state=42)\n```", "```py\n>>> from sklearn.neighbors import KNeighborsClassifier \n>>> knn_fit = KNeighborsClassifier(n_neighbors=3,p=2,metric='minkowski') \n>>> knn_fit.fit(x_train,y_train) \n\n>>> print (\"\\nK-Nearest Neighbors - Train Confusion Matrix\\n\\n\",pd.crosstab(y_train, knn_fit.predict(x_train),rownames = [\"Actuall\"],colnames = [\"Predicted\"]) )      \n>>> print (\"\\nK-Nearest Neighbors - Train accuracy:\",round(accuracy_score(y_train, knn_fit.predict(x_train)),3)) \n>>> print (\"\\nK-Nearest Neighbors - Train Classification Report\\n\", classification_report( y_train,knn_fit.predict(x_train))) \n\n>>> print (\"\\n\\nK-Nearest Neighbors - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test, knn_fit.predict(x_test),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))       \n>>> print (\"\\nK-Nearest Neighbors - Test accuracy:\",round(accuracy_score( y_test,knn_fit.predict(x_test)),3)) \n>>> print (\"\\nK-Nearest Neighbors - Test Classification Report\\n\", classification_report(y_test,knn_fit.predict(x_test))) \n\n```", "```py\n# KNN Classifier \nsetwd(\"D:\\\\Book writing\\\\Codes\\\\Chapter 5\") \nbreast_cancer = read.csv(\"Breast_Cancer_Wisconsin.csv\") \n\n# Column Bare_Nuclei have some missing values with \"?\" in place, we are replacing with median values \n# As Bare_Nuclei is discrete variable \nbreast_cancer$Bare_Nuclei = as.character(breast_cancer$Bare_Nuclei)\nbreast_cancer$Bare_Nuclei[breast_cancer$Bare_Nuclei==\"?\"] = median(breast_cancer$Bare_Nuclei,na.rm = TRUE)\nbreast_cancer$Bare_Nuclei = as.integer(breast_cancer$Bare_Nuclei)\n# Classes are 2 & 4 for benign & malignant respectively, we # have converted # \nto zero-one problem, as it is easy to convert to work # around with models \nbreast_cancer$Cancer_Ind = 0\nbreast_cancer$Cancer_Ind[breast_cancer$Class==4]=1\nbreast_cancer$Cancer_Ind = as.factor( breast_cancer$Cancer_Ind) \n\n# We have removed unique id number from modeling as unique # numbers does not provide value in modeling \n# In addition, original class variable also will be removed # as the same has been replaced with derived variable \n\nremove_cols = c(\"ID_Number\",\"Class\") \nbreast_cancer_new = breast_cancer[,!(names(breast_cancer) %in% remove_cols)] \n\n# Setting seed value for producing repetitive results \n# 70-30 split has been made on the data \n\nset.seed(123) \nnumrow = nrow(breast_cancer_new) \ntrnind = sample(1:numrow,size = as.integer(0.7*numrow)) \ntrain_data = breast_cancer_new[trnind,] \ntest_data = breast_cancer_new[-trnind,] \n\n# Following is classical code for computing accuracy, # precision & recall \n\nfrac_trzero = (table(train_data$Cancer_Ind)[[1]])/nrow(train_data)\nfrac_trone = (table(train_data$Cancer_Ind)[[2]])/nrow(train_data)\n\nfrac_tszero = (table(test_data$Cancer_Ind)[[1]])/nrow(test_data)\nfrac_tsone = (table(test_data$Cancer_Ind)[[2]])/nrow(test_data)\n\nprec_zero <- function(act,pred){ tble = table(act,pred)\nreturn( round( tble[1,1]/(tble[1,1]+tble[2,1]),4) ) } \n\nprec_one <- function(act,pred){ tble = table(act,pred)\nreturn( round( tble[2,2]/(tble[2,2]+tble[1,2]),4) ) } \n\nrecl_zero <- function(act,pred){tble = table(act,pred) \nreturn( round( tble[1,1]/(tble[1,1]+tble[1,2]),4) ) } \n\nrecl_one <- function(act,pred){ tble = table(act,pred) \nreturn( round( tble[2,2]/(tble[2,2]+tble[2,1]),4) ) } \n\naccrcy <- function(act,pred){ tble = table(act,pred) \nreturn( round((tble[1,1]+tble[2,2])/sum(tble),4)) } \n\n# Importing Class package in which KNN function do present library(class) \n\n# Choosing sample k-value as 3 & apply on train & test data # respectively \n\nk_value = 3 \ntr_y_pred = knn(train_data,train_data,train_data$Cancer_Ind,k=k_value)\nts_y_pred = knn(train_data,test_data,train_data$Cancer_Ind,k=k_value) \n\n# Calculating confusion matrix, accuracy, precision & # recall on train data \n\ntr_y_act = train_data$Cancer_Ind;ts_y_act = test_data$Cancer_Ind\ntr_tble = table(tr_y_act,tr_y_pred) \nprint(paste(\"Train Confusion Matrix\")) \nprint(tr_tble) \n\ntr_acc = accrcy(tr_y_act,tr_y_pred) \ntrprec_zero = prec_zero(tr_y_act,tr_y_pred); trrecl_zero = recl_zero(tr_y_act,tr_y_pred) \ntrprec_one = prec_one(tr_y_act,tr_y_pred); trrecl_one = recl_one(tr_y_act,tr_y_pred) \ntrprec_ovll = trprec_zero *frac_trzero + trprec_one*frac_trone\ntrrecl_ovll = trrecl_zero *frac_trzero + trrecl_one*frac_trone\n\nprint(paste(\"KNN Train accuracy:\",tr_acc)) \nprint(paste(\"KNN - Train Classification Report\"))\nprint(paste(\"Zero_Precision\",trprec_zero,\"Zero_Recall\",trrecl_zero))\nprint(paste(\"One_Precision\",trprec_one,\"One_Recall\",trrecl_one))\nprint(paste(\"Overall_Precision\",round(trprec_ovll,4),\"Overall_Recall\",round(trrecl_ovll,4))) \n\n# Calculating confusion matrix, accuracy, precision & # recall on test data \n\nts_tble = table(ts_y_act, ts_y_pred) \nprint(paste(\"Test Confusion Matrix\")) \nprint(ts_tble) \n\nts_acc = accrcy(ts_y_act,ts_y_pred) \ntsprec_zero = prec_zero(ts_y_act,ts_y_pred); tsrecl_zero = recl_zero(ts_y_act,ts_y_pred) \ntsprec_one = prec_one(ts_y_act,ts_y_pred); tsrecl_one = recl_one(ts_y_act,ts_y_pred) \n\ntsprec_ovll = tsprec_zero *frac_tszero + tsprec_one*frac_tsone\ntsrecl_ovll = tsrecl_zero *frac_tszero + tsrecl_one*frac_tsone\n\nprint(paste(\"KNN Test accuracy:\",ts_acc)) \nprint(paste(\"KNN - Test Classification Report\"))\nprint(paste(\"Zero_Precision\",tsprec_zero,\"Zero_Recall\",tsrecl_zero))\nprint(paste(\"One_Precision\",tsprec_one,\"One_Recall\",tsrecl_one))\nprint(paste(\"Overall_Precision\",round(tsprec_ovll,4),\"Overall_Recall\",round(tsrecl_ovll,4))) \n```", "```py\n# Tuning of K- value for Train & Test data \n>>> dummyarray = np.empty((5,3)) \n>>> k_valchart = pd.DataFrame(dummyarray) \n>>> k_valchart.columns = [\"K_value\",\"Train_acc\",\"Test_acc\"] \n\n>>> k_vals = [1,2,3,4,5] \n\n>>> for i in range(len(k_vals)): \n...     knn_fit = KNeighborsClassifier(n_neighbors=k_vals[i],p=2,metric='minkowski') \n...     knn_fit.fit(x_train,y_train) \n\n...     print (\"\\nK-value\",k_vals[i]) \n\n...     tr_accscore = round(accuracy_score(y_train,knn_fit.predict(x_train)),3) \n...     print (\"\\nK-Nearest Neighbors - Train Confusion Matrix\\n\\n\",pd.crosstab( y_train, knn_fit.predict(x_train),rownames = [\"Actuall\"],colnames = [\"Predicted\"]) )      \n...     print (\"\\nK-Nearest Neighbors - Train accuracy:\",tr_accscore) \n...     print (\"\\nK-Nearest Neighbors - Train Classification Report\\n\", classification_report(y_train,knn_fit.predict(x_train))) \n\n...     ts_accscore = round(accuracy_score(y_test,knn_fit.predict(x_test)),3)     \n...     print (\"\\n\\nK-Nearest Neighbors - Test Confusion Matrix\\n\\n\",pd.crosstab( y_test,knn_fit.predict(x_test),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))       \n...     print (\"\\nK-Nearest Neighbors - Test accuracy:\",ts_accscore) \n...     print (\"\\nK-Nearest Neighbors - Test Classification Report\\n\",classification_report(y_test,knn_fit.predict(x_test))) \n\n...     k_valchart.loc[i, 'K_value'] = k_vals[i]       \n...     k_valchart.loc[i, 'Train_acc'] = tr_accscore      \n...     k_valchart.loc[i, 'Test_acc'] = ts_accscore                \n\n# Ploting accuracies over varied K-values \n>>> import matplotlib.pyplot as plt \n>>> plt.figure() \n>>> plt.xlabel('K-value') \n>>> plt.ylabel('Accuracy') \n>>> plt.plot(k_valchart[\"K_value\"],k_valchart[\"Train_acc\"]) \n>>> plt.plot(k_valchart[\"K_value\"],k_valchart[\"Test_acc\"]) \n\n>>> plt.axis([0.9,5, 0.92, 1.005]) \n>>> plt.xticks([1,2,3,4,5]) \n\n>>> for a,b in zip(k_valchart[\"K_value\"],k_valchart[\"Train_acc\"]): \n...     plt.text(a, b, str(b),fontsize=10) \n\n>>> for a,b in zip(k_valchart[\"K_value\"],k_valchart[\"Test_acc\"]): \n...     plt.text(a, b, str(b),fontsize=10) \n\n>>> plt.legend(loc='upper right')     \n>>> plt.show() \n```", "```py\n# Tuning of K-value on Train & Test Data \nk_valchart = data.frame(matrix( nrow=5, ncol=3)) \ncolnames(k_valchart) = c(\"K_value\",\"Train_acc\",\"Test_acc\") \nk_vals = c(1,2,3,4,5) \n\ni = 1\nfor (kv in k_vals) { \n tr_y_pred = knn(train_data,train_data,train_data$Cancer_Ind,k=kv)\n ts_y_pred = knn(train_data,test_data,train_data$Cancer_Ind,k=kv)\n tr_y_act = train_data$Cancer_Ind;ts_y_act = test_data$Cancer_Ind\n tr_tble = table(tr_y_act,tr_y_pred) \n print(paste(\"Train Confusion Matrix\")) \n print(tr_tble) \n tr_acc = accrcy(tr_y_act,tr_y_pred) \n trprec_zero = prec_zero(tr_y_act,tr_y_pred); trrecl_zero = recl_zero(tr_y_act, tr_y_pred) \n trprec_one = prec_one(tr_y_act,tr_y_pred); trrecl_one = recl_one(tr_y_act,tr_y_pred) \n trprec_ovll = trprec_zero *frac_trzero + trprec_one*frac_trone\n trrecl_ovll = trrecl_zero *frac_trzero + trrecl_one*frac_trone\n print(paste(\"KNN Train accuracy:\",tr_acc)) \n print(paste(\"KNN - Train Classification Report\"))\n\nprint(paste(\"Zero_Precision\",trprec_zero,\"Zero_Recall\",trrecl_zero))\nprint(paste(\"One_Precision\",trprec_one,\"One_Recall\",trrecl_one))\nprint(paste(\"Overall_Precision\",round(trprec_ovll,4),\"Overall_Recall\",round(trrecl_ovll,4))) \n ts_tble = table(ts_y_act,ts_y_pred) \n print(paste(\"Test Confusion Matrix\")) \n print(ts_tble)\n ts_acc = accrcy(ts_y_act,ts_y_pred) \n tsprec_zero = prec_zero(ts_y_act,ts_y_pred); tsrecl_zero = recl_zero(ts_y_act,ts_y_pred) \n tsprec_one = prec_one(ts_y_act,ts_y_pred); tsrecl_one = recl_one(ts_y_act,ts_y_pred) \n tsprec_ovll = tsprec_zero *frac_tszero + tsprec_one*frac_tsone\n tsrecl_ovll = tsrecl_zero *frac_tszero + tsrecl_one*frac_tsone\n\n print(paste(\"KNN Test accuracy:\",ts_acc)) \n print(paste(\"KNN - Test Classification Report\"))\n\nprint(paste(\"Zero_Precision\",tsprec_zero,\"Zero_Recall\",tsrecl_zero))\nprint(paste(\"One_Precision\",tsprec_one,\"One_Recall\",tsrecl_one))\nprint(paste(\"Overall_Precision\",round(tsprec_ovll,4),\"Overall_Recall\",round(tsrecl_ovll,4)))\n\n k_valchart[i,1] =kv \n k_valchart[i,2] =tr_acc \n k_valchart[i,3] =ts_acc i = i+1 } \n# Plotting the graph \nlibrary(ggplot2) \nlibrary(grid) \nggplot(k_valchart, aes(K_value)) \n+ geom_line(aes(y = Train_acc, colour = \"Train_Acc\")) + \ngeom_line(aes(y = Test_acc, colour = \"Test_Acc\"))+\nlabs(x=\"K_value\",y=\"Accuracy\") + \ngeom_text(aes(label = Train_acc, y = Train_acc), size = 3)+\ngeom_text(aes(label = Test_acc, y = Test_acc), size = 3) \n```", "```py\n>>> import csv \n\n>>> smsdata = open('SMSSpamCollection.txt','r') \n>>> csv_reader = csv.reader(smsdata,delimiter='\\t') \n```", "```py\n>>> import sys \n>>> reload (sys) \n>>> sys.setdefaultendocing('utf-8') \n```", "```py\n>>> smsdata_data = [] \n>>> smsdata_labels = [] \n\n>>> for line in csv_reader: \n...     smsdata_labels.append(line[0]) \n...     smsdata_data.append(line[1]) \n\n>>> smsdata.close() \n```", "```py\n>>> for i in range(5): \n...     print (smsdata_data[i],smsdata_labels[i])\n```", "```py\n>>> from collections import Counter \n>>> c = Counter( smsdata_labels ) \n>>> print(c) \n```", "```py\n>>> import nltk \n>>> from nltk.corpus import stopwords \n>>> from nltk.stem import WordNetLemmatizer \n>>> import string \n>>> import pandas as pd \n>>> from nltk import pos_tag \n>>> from nltk.stem import PorterStemmer  \n```", "```py\n>>> def preprocessing(text): \n```", "```py\n...     text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split()) \n```", "```py\n...     tokens = [word for sent in nltk.sent_tokenize(text2) for word in \n              nltk.word_tokenize(sent)] \n```", "```py\n...     tokens = [word.lower() for word in tokens] \n```", "```py\n...     stopwds = stopwords.words('english') \n...     tokens = [token for token in tokens if token not in stopwds]  \n```", "```py\n...     tokens = [word for word in tokens if len(word)>=3] \n```", "```py\n...     stemmer = PorterStemmer() \n...     tokens = [stemmer.stem(word) for word in tokens]  \n```", "```py\n...     tagged_corpus = pos_tag(tokens)     \n```", "```py\n...    Noun_tags = ['NN','NNP','NNPS','NNS'] \n...    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ'] \n...    lemmatizer = WordNetLemmatizer()\n```", "```py\n...     def prat_lemmatize(token,tag): \n...         if tag in Noun_tags: \n...             return lemmatizer.lemmatize(token,'n') \n...         elif tag in Verb_tags: \n...             return lemmatizer.lemmatize(token,'v') \n...         else: \n...             return lemmatizer.lemmatize(token,'n') \n```", "```py\n...     pre_proc_text =  \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])              \n...     return pre_proc_text \n```", "```py\n>>> smsdata_data_2 = [] \n>>> for i in smsdata_data: \n...     smsdata_data_2.append(preprocessing(i))  \n```", "```py\n>>> import numpy as np \n>>> trainset_size = int(round(len(smsdata_data_2)*0.70)) \n>>> print ('The training set size for this classifier is ' + str(trainset_size) + '\\n') \n>>> x_train = np.array([''.join(rec) for rec in smsdata_data_2[0:trainset_size]]) \n>>> y_train = np.array([rec for rec in smsdata_labels[0:trainset_size]]) \n>>> x_test = np.array([''.join(rec) for rec in smsdata_data_2[trainset_size+1:len( smsdata_data_2)]]) \n>>> y_test = np.array([rec for rec in smsdata_labels[trainset_size+1:len( smsdata_labels)]]) \n```", "```py\n# building TFIDF vectorizer  \n>>> from sklearn.feature_extraction.text import TfidfVectorizer \n>>> vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2),  stop_words='english',  \n    max_features= 4000,strip_accents='unicode',  norm='l2') \n```", "```py\n>>> x_train_2 = vectorizer.fit_transform(x_train).todense() \n>>> x_test_2 = vectorizer.transform(x_test).todense() \n```", "```py\n>>> from sklearn.naive_bayes import MultinomialNB \n>>> clf = MultinomialNB().fit(x_train_2, y_train) \n\n>>> ytrain_nb_predicted = clf.predict(x_train_2) \n>>> ytest_nb_predicted = clf.predict(x_test_2) \n\n>>> from sklearn.metrics import classification_report,accuracy_score \n\n>>> print (\"\\nNaive Bayes - Train Confusion Matrix\\n\\n\",pd.crosstab(y_train, ytrain_nb_predicted,rownames = [\"Actuall\"],colnames = [\"Predicted\"]))       \n>>> print (\"\\nNaive Bayes- Train accuracy\",round(accuracy_score(y_train, ytrain_nb_predicted),3)) \n>>> print (\"\\nNaive Bayes  - Train Classification Report\\n\",classification_report(y_train, ytrain_nb_predicted)) \n\n>>> print (\"\\nNaive Bayes - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test, ytest_nb_predicted,rownames = [\"Actuall\"],colnames = [\"Predicted\"]))       \n>>> print (\"\\nNaive Bayes- Test accuracy\",round(accuracy_score(y_test, ytest_nb_predicted),3)) \n>>> print (\"\\nNaive Bayes  - Test Classification Report\\n\",classification_report( y_test, ytest_nb_predicted)) \n```", "```py\n# printing top features  \n>>> feature_names = vectorizer.get_feature_names() \n>>> coefs = clf.coef_ \n>>> intercept = clf.intercept_ \n>>> coefs_with_fns = sorted(zip(clf.coef_[0], feature_names)) \n\n>>> print (\"\\n\\nTop 10 features - both first & last\\n\") \n>>> n=10 \n>>> top_n_coefs = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1]) \n>>> for (coef_1, fn_1), (coef_2, fn_2) in top_n_coefs: \n...     print('\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s' % (coef_1, fn_1, coef_2, fn_2)) \n```", "```py\n# Naive Bayes \nsmsdata = read.csv(\"SMSSpamCollection.csv\",stringsAsFactors = FALSE) \n# Try the following code for reading in case if you have \n#issues while reading regularly with above code \n#smsdata = read.csv(\"SMSSpamCollection.csv\", \n#stringsAsFactors = FALSE,fileEncoding=\"latin1\") \nstr(smsdata) \nsmsdata$Type = as.factor(smsdata$Type) \ntable(smsdata$Type) \n\nlibrary(tm) \nlibrary(SnowballC) \n# NLP Processing \nsms_corpus <- Corpus(VectorSource(smsdata$SMS_Details)) \ncorpus_clean_v1 <- tm_map(sms_corpus, removePunctuation)\ncorpus_clean_v2 <- tm_map(corpus_clean_v1, tolower) \ncorpus_clean_v3 <- tm_map(corpus_clean_v2, stripWhitespace)\ncorpus_clean_v4 <- tm_map(corpus_clean_v3, removeWords, stopwords())\ncorpus_clean_v5 <- tm_map(corpus_clean_v4, removeNumbers)\ncorpus_clean_v6 <- tm_map(corpus_clean_v5, stemDocument) \n\n# Check the change in corpus \ninspect(sms_corpus[1:3]) \ninspect(corpus_clean_v6[1:3]) \n\nsms_dtm <- DocumentTermMatrix(corpus_clean_v6) \n\nsmsdata_train <- smsdata[1:4169, ] \nsmsdata_test <- smsdata[4170:5572, ] \n\nsms_dtm_train <- sms_dtm[1:4169, ] \nsms_dtm_test <- sms_dtm[4170:5572, ] \n\nsms_corpus_train <- corpus_clean_v6[1:4169] \nsms_corpus_test <- corpus_clean_v6[4170:5572]\n\nprop.table(table(smsdata_train$Type))\nprop.table(table(smsdata_test$Type)) \nfrac_trzero = (table(smsdata_train$Type)[[1]])/nrow(smsdata_train)\nfrac_trone = (table(smsdata_train$Type)[[2]])/nrow(smsdata_train)\nfrac_tszero = (table(smsdata_test$Type)[[1]])/nrow(smsdata_test)\nfrac_tsone = (table(smsdata_test$Type)[[2]])/nrow(smsdata_test)\n\nDictionary <- function(x) { \n if( is.character(x) ) { \n return (x) \n } \n stop('x is not a character vector') \n} \n# Create the dictionary with at least word appears 1 time \nsms_dict <- Dictionary(findFreqTerms(sms_dtm_train, 1)) \nsms_train <- DocumentTermMatrix(sms_corpus_train,list(dictionary = sms_dict)) \nsms_test <- DocumentTermMatrix(sms_corpus_test,list(dictionary = sms_dict)) \nconvert_tofactrs <- function(x) { \n x <- ifelse(x > 0, 1, 0) \n x <- factor(x, levels = c(0, 1), labels = c(\"No\", \"Yes\")) \n return(x) \n} \nsms_train <- apply(sms_train, MARGIN = 2, convert_tofactrs) \nsms_test <- apply(sms_test, MARGIN = 2, convert_tofactrs) \n\n# Application of Naïve Bayes Classifier with laplace Estimator\nlibrary(e1071) \nnb_fit <- naiveBayes(sms_train, smsdata_train$Type,laplace = 1.0)\n\ntr_y_pred = predict(nb_fit, sms_train) \nts_y_pred = predict(nb_fit,sms_test) \ntr_y_act = smsdata_train$Type;ts_y_act = smsdata_test$Type \n\ntr_tble = table(tr_y_act,tr_y_pred) \nprint(paste(\"Train Confusion Matrix\")) \nprint(tr_tble) \n\ntr_acc = accrcy(tr_y_act,tr_y_pred) \ntrprec_zero = prec_zero(tr_y_act,tr_y_pred);  trrecl_zero = recl_zero(tr_y_act,tr_y_pred) \ntrprec_one = prec_one(tr_y_act,tr_y_pred); trrecl_one = recl_one(tr_y_act,tr_y_pred) \ntrprec_ovll = trprec_zero *frac_trzero + trprec_one*frac_trone\ntrrecl_ovll = trrecl_zero *frac_trzero + trrecl_one*frac_trone\n\nprint(paste(\"Naive Bayes Train accuracy:\",tr_acc)) \nprint(paste(\"Naive Bayes - Train Classification Report\"))\nprint(paste(\"Zero_Precision\",trprec_zero,\"Zero_Recall\",trrecl_zero))\nprint(paste(\"One_Precision\",trprec_one,\"One_Recall\",trrecl_one))\nprint(paste(\"Overall_Precision\",round(trprec_ovll,4),\"Overall_Recall\",round(trrecl_ovll,4))) \n\nts_tble = table(ts_y_act,ts_y_pred) \nprint(paste(\"Test Confusion Matrix\")) \nprint(ts_tble) \n\nts_acc = accrcy(ts_y_act,ts_y_pred) \ntsprec_zero = prec_zero(ts_y_act,ts_y_pred); tsrecl_zero = recl_zero(ts_y_act,ts_y_pred) \ntsprec_one = prec_one(ts_y_act,ts_y_pred); tsrecl_one = recl_one(ts_y_act,ts_y_pred) \ntsprec_ovll = tsprec_zero *frac_tszero + tsprec_one*frac_tsone\ntsrecl_ovll = tsrecl_zero *frac_tszero + tsrecl_one*frac_tsone\n\nprint(paste(\"Naive Bayes Test accuracy:\",ts_acc)) \nprint(paste(\"Naive Bayes - Test Classification Report\"))\nprint(paste(\"Zero_Precision\",tsprec_zero,\"Zero_Recall\",tsrecl_zero))\nprint(paste(\"One_Precision\",tsprec_one,\"One_Recall\",tsrecl_one))\nprint(paste(\"Overall_Precision\",round(tsprec_ovll,4),\"Overall_Recall\",round(tsrecl_ovll,4))) \n```"]