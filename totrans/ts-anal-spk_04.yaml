- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: End-to-End View of a Time Series Analysis Project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building on the foundation set in the previous chapters in which we were introduced
    to time series analysis, its multiple use cases, and Apache Spark, a key tool
    for such analysis, this chapter guides us through the entire process of a time
    series analysis project. Starting with use cases, we will move on to the end-to-end
    approach with DataOps, ModelOps, and DevOps. We will cover key stages such as
    data processing, feature engineering, model selection, and evaluation, offering
    practical insights into building a time series analysis pipeline with Spark and
    other tools.
  prefs: []
  type: TYPE_NORMAL
- en: This holistic view of a time series analysis project will equip us with a structured
    approach to handling real-world projects, enhancing our ability to implement end-to-end
    solutions. The information here will guide us as practitioners through a framework
    for using Spark in a cohesive manner and ensuring the successful execution of
    time series analysis projects. We will conclude with two approaches for implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Driven by use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From DataOps to ModelOps to DevOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation examples and tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The hands-on part of this chapter will be to implement end-to-end examples
    for a time series analysis project. The code for this chapter can be found in
    the `ch4` folder of the GitHub repository at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch4](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch4)'
  prefs: []
  type: TYPE_NORMAL
- en: The hands-on section of this chapter (*Implementation examples and tools*) will
    go into further detail. This requires some skills in building an open source environment.
    If you do not intend to build your own Apache Spark environment and your focus
    is instead on time series and using but not deploying Spark and other tools, you
    can skip the hands-on section of this chapter. You can use a managed platform
    such as Databricks, which comes pre-built with Spark, MLflow, and tools for workflows
    and notebooks, as we will do in future chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Driven by use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we get into the *how* of doing an end-to-end time series analysis project,
    as always, it is good to start with the *why*. There can be many reasons, often
    a combination, to justify the inception of a time series analysis project. Some
    of the reasons are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Technology refresh**: The emphasis can be on the technology due to an aging
    platform needing replacement and not being able to meet requirements anymore,
    or when a new technology is available, offering better performance, lower costs,
    or more capabilities, such as advanced machine learning models or scalable cloud-based
    resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Research on methods**: For organizations or departments focused on research,
    the main driver is finding new and better methods such as developing and testing
    new algorithms for analyzing time series.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data exploration**: Similar to research in its nature, but requiring to be
    nearer to the data, this is usually embedded within businesses’ data teams. The
    need here is to understand time series data without necessarily a predefined end
    application. The objective is to uncover patterns, trends, and anomalies in the
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use case**: With this approach, we begin with the end in mind, first identifying
    specific needs and expectations of the end users or stakeholders. We then set
    the project to answer those needs based on the analysis of time series data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While all the preceding reasons have their merit and are certainly valid, over
    the years, I have seen the business-driven use case approach as the one with the
    highest return on investment. We started the discussion on time series-based use
    cases across various industries in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044),
    such as inventory forecasting, predicting energy usage, financial market trend
    analysis, or anomaly detection in sensor data; here, we will focus on this use
    case-driven approach and take it further.
  prefs: []
  type: TYPE_NORMAL
- en: The use case approach first identifies and defines real-world specific business
    applications or challenges. It then chooses the technical solution best fit to
    address these requirements. At first glance, this does not sound very different
    from any project in a business setup. The key difference here is highlighted by
    the word “specific” in that the use case approach is about a specific, measurable
    business outcome. This follows a lean approach in the sense that we want to avoid
    features that do not contribute to the business outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Use cases can be compared to user stories in the agile approach to software
    development. As a matter of fact, the agile methodology is often how the use cases
    are implemented, with a streamlined iterative development process involving users
    all the way.
  prefs: []
  type: TYPE_NORMAL
- en: The following *Figure 4**.1* gives an overview of the use case driven approach,
    based on what has been discussed so far, including their key characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Use case driven approach'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have defined the use case-driven approach, we will look at the
    key characteristics of this approach, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Business outcome**: Project success is measured by the outcome in terms of
    business metrics on higher revenue, cost reduction, efficiency gain, and better
    and quicker decision-making.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User-focused**: Working from the start with the end users and stakeholders
    to identify their specific needs, the project’s objectives include answering those
    needs, in addition to the preceding business outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specific**: We’ve discussed this term a couple of times. The specificity
    of a project provides a focused direction to its scope, making its execution more
    agile. We want to address a specific need, for example, sales forecasting, and
    this can be even more granular, such as forecasting for a specific line of product
    or region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative**: Feedback and refinement loops involving end users and stakeholders
    ensure the project remains on track to meet the expected business outcomes. This
    again highlights the similarity to an agile approach with its short development
    cycles, incremental delivery, continuous feedback, and adaptability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adhering to these characteristics, the use cases are scoped small enough to
    be achievable and bring value within a few months, if not even weeks. These smaller
    use cases usually mean that there are several of them competing in parallel for
    development resources. This requires prioritization to ensure that resources are
    well invested. The following criteria are commonly used to prioritize use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Impact**: This is a measurement of the expected business impact of the use
    case, preferably calculated in monetary value. If the outcome is a reduction in
    time, the equivalent monetary value of the time saving is estimated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost**: We need to account for all costs related to the use case from the
    moment of use case ideation to the time the use case is live in production and
    bringing value to the business. Costs can be related to development, infrastructure,
    migration, training, support, and production operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Return on investment** (**ROI**): This can be simply estimated by dividing
    the impact by the cost. As an example, for a retailer who wants to better forecast
    stocks in stores, if the total cost to get the stock forecasting use case in production
    is $50k, and the improvement in stock forecasting is estimated to bring $200k
    in savings over 3 years, the ROI is 4x over this period.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Technical feasibility**: The technical solution for the use case exists and
    can be achieved within time and budget.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data availability and accessibility**: Data is available and accessible to
    build the use case and then operationalize it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the preceding criteria, in the case of competing need for resources, a
    use case with a high impact and an ROI of 10x that is feasible and has data available
    is done before another use case with a lower impact and an ROI of 3x, or before
    one that does not have data access.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, starting with a clear understanding of the user’s needs, a use case-based
    project ensures applicability and relevance to the business, closely aligning
    with the stakeholders’ objectives, with measurable impact. Having a good use case
    is only the start, though. We will now deep-dive into the next steps, from the
    use case to the successful completion of a time series analysis project that delivers
    business outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: From DataOps to ModelOps to DevOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once a significant use case has been identified, a few phases play a crucial
    role, starting from **data operations** (**DataOps**) to **model operations**
    (**ModelOps**) and finally to **deployment** (**DevOps**) to a live business environment
    delivering value. A solid end-to-end process covering these phases ensures that
    we can consistently deliver from one use case to the next while ensuring that
    the results are reproducible. *Figure 4**.2* gives an overview of these phases,
    which will be detailed in this section.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_04_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: DataOps, ModelOps, and DevOps'
  prefs: []
  type: TYPE_NORMAL
- en: DataOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DataOps for a time series analysis project encompasses best practices and processes
    to ensure the flow, quality, and access to time series data as part of its life
    cycle. The aim is for timely, efficient, and accurate time series analysis and
    modeling to derive actionable business insights.
  prefs: []
  type: TYPE_NORMAL
- en: DataOps practices follow the complete data and metadata life cycle and can be
    broken down broadly into data source integration, data processing, data governance,
    and data sharing.
  prefs: []
  type: TYPE_NORMAL
- en: Source integration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data source integration involves first identifying the data source and gaining
    access, and then ingesting data from the source.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data sources** can be internal or external. Internal sources are primarily
    databases such as transactional records, system logs, or sensor data for telemetry.
    External sources include examples such as market data, weather data, or social
    media, which is now becoming predominant. Sources vary greatly across industries
    in volume, frequency of updates, and data format. Once the sources have been identified
    and accessed, **data ingestion** is the process of bringing the data into the
    platform for processing. This is usually achieved with automated ingestion pipelines,
    running in batches at specific frequencies (hourly, daily, etc.) or streaming
    continuously. Mechanisms for ingestion include database connections, API calls,
    or web-based scraping, among others.'
  prefs: []
  type: TYPE_NORMAL
- en: Processing and storage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data processing includes cleaning up the data, transforming it into the right
    format, and storing it for analysis. A recommended approach is the medallion approach,
    as illustrated in *Figure 4**.3*, which involves multiple stages of processing
    from raw data to curated to report-ready data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_04_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Medallion stages of processing'
  prefs: []
  type: TYPE_NORMAL
- en: Medallion approach
  prefs: []
  type: TYPE_NORMAL
- en: 'The medallion approach to data processing organizes data into three stages:
    Bronze, Silver, and Gold. This is often used in data lakes and Delta Lake architecture.
    Raw data is ingested from various sources without transformation in the Bronze
    stage. The Silver stage results from data cleaning, enrichment, and transformation
    to create a curated dataset. Finally, the Gold stage represents the highest quality
    data, cleansed, aggregated, and read-optimized for advanced analytics, reporting,
    and business intelligence. This multi-tiered structure augments data quality and
    facilitates data management.'
  prefs: []
  type: TYPE_NORMAL
- en: Once the data has been ingested from sources, **data quality checks and cleaning**
    are the first steps to building trustworthiness in the data. These involve the
    handling of missing values, detecting and correcting errors, removing duplicates,
    and filtering outliers. These improve the data quality and give confidence that
    the analysis is built on a solid foundation. The specific requirement at this
    stage for time series data is to verify and maintain temporal integrity due to
    the sequential nature of the data.
  prefs: []
  type: TYPE_NORMAL
- en: The raw data from sources is usually not apt for direct analysis use and requires
    several **transformations** to be appropriate for time series analysis. This involves,
    among other transformations, changing semi-structured data to a structured format
    for quicker access. Data at granular or irregular intervals is aggregated to a
    higher-level interval such as from every minute to hourly, from hourly to daily,
    and so on. Date and time fields may require special processing to be in a sortable
    format and used to set a time index for faster retrieval. Different time zones
    need to be handled accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Frequently disregarded in smaller projects, **metadata** is an essential requirement
    in an enterprise environment to enable data traceability and lineage for governance.
    This data about the data captures, for example, source identifier, ingestion and
    update times, changes made, as well as historical versions. Metadata is captured
    as part of the data ingestion and transformation pipelines, and natively with
    storage protocols such as Delta.
  prefs: []
  type: TYPE_NORMAL
- en: While all the data processing described so far can be done in memory, there
    is a requirement for longer-term **storage** and retrieval for analysis over time.
    This storage needs to be cost-effective, scalable, and secure while providing
    the high performance required for timely analysis. Based on the volume and velocity
    of data, options include dedicated time series databases such as InfluxDB, or
    cloud-based storage in combination with a storage protocol such as Delta.
  prefs: []
  type: TYPE_NORMAL
- en: We will delve deeper into data processing in [*Chapter 5*](B18568_05.xhtml#_idTextAnchor103)
    on data preparation. For now, let’s shift our focus to governance and security,
    which are among the most critical considerations for DataOps from a risk perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring, security, and governance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data monitoring, security, and governance encompass several overlapping data
    practice areas, including data quality, privacy, access control, compliance, and
    policies. To appreciate the importance of these practices, let’s consider the
    following in the news at the time of this writing:'
  prefs: []
  type: TYPE_NORMAL
- en: A cybersecurity breach just impacted major organizations, including Ticketmaster,
    Banco Santander, and Ticketek. A hacking group called ShinyHunters gained access
    to Ticketmaster’s database, and in doing so, compromised the personal information
    of 560 million users. This includes names, addresses, phone numbers, email addresses,
    and payment details. Reports are that this data is being sold on hacking forums
    for substantial amounts. Banco Santander had a similar breach, affecting customers
    and employees.
  prefs: []
  type: TYPE_NORMAL
- en: '[Source: [https://www.wired.com/story/snowflake-breach-ticketmaster-santander-ticketek-hacked/](https://www.wired.com/story/snowflake-breach-ticketmaster-santander-ticketek-hacked/)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These breaches, linked to a third-party cloud data warehouse service, highlight
    the challenges in cybersecurity and the need for strong measures for monitoring,
    security, and governance.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The goal here is to promptly identify issues and be able to take corrective
    measures, ideally before it has a negative consequence. The monitoring is of the
    data itself, of the execution status of transformation pipelines, as well as for
    security and governance breaches. For data monitoring, this means tracking the
    data quality by measuring its accuracy and completeness while catching data gaps
    and anomalies. One way to achieve this is by comparing against a range or specific
    time series pattern, as we have seen in the anomaly detection example in [*Chapter
    2*](B18568_02.xhtml#_idTextAnchor044). As for the data pipeline monitoring, these
    are tracked for performance to ensure data freshness, **service-level agreements**
    (**SLAs**) are honored, and lineage to track provenance and integrity. From a
    security point of view, we want to catch any attempt at a data breach in time
    to act upon it. The monitoring should be an automated process, with alerting in
    place.
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Related to both data at rest and in transit, we need to define roles and related
    permissions to ensure access control. Some time series data is sensitive and only
    authorized personnel should be able to view or manipulate the data.
  prefs: []
  type: TYPE_NORMAL
- en: In regulated industries, when handling personal data, we need to ensure that
    data handling and storage practices comply with relevant regulations (HIPAA, GDPR,
    etc.). This also involves ensuring privacy and managing consent for personal data.
  prefs: []
  type: TYPE_NORMAL
- en: Governance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In addition to the preceding, the data governance practice is responsible for
    assigning roles and responsibilities to manage data. As part of this, the data
    stewards oversee data quality, compliance, and policies.
  prefs: []
  type: TYPE_NORMAL
- en: By establishing the right processes, people, and tools in place, we can ensure
    the prevention of data breaches and effective mitigation if they do occur.
  prefs: []
  type: TYPE_NORMAL
- en: We have now covered the process of ingesting and transforming data into trustworthy
    and useful data in a governed and secure way. The step left now as part of DataOps
    is to share the data for analysis and consumption by users or other systems.
  prefs: []
  type: TYPE_NORMAL
- en: Sharing and consumption
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After ingestion and processing the data, we want the curated data and outcome
    of analytics to be visible and available to users. A centralized **data catalog**,
    complete with descriptions and usage guidelines, allows users to easily discover
    and access available datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, as part of the DataOps phase, we want data scientists, analysts, and
    other users to be able to consume the data for exploration, analysis, and reporting.
    Ideally, we want to tie this to governance to ensure that the right set of users
    are accessing and consuming permitted datasets only. Access and methods for consumption
    include file-based access, database connection, and APIs, among others.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in this section, DataOps is the set of processes to ensure that
    data is available, accessible, and usable. It is iterative, with feedback from
    consumers and continuous improvement to data, pipelines, and practices. By establishing
    a scalable and flexible infrastructure with Apache Spark’s processing power and
    versatility at its core, DataOps ensures that data scientists and analysts have
    the high-quality data they need when they need it, to derive insights and drive
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover the practical considerations for DataOps in [*Chapter 5*](B18568_05.xhtml#_idTextAnchor103),
    *Data Preparation*. For now, let’s focus on ModelOps, which is the next phase
    after DataOps.
  prefs: []
  type: TYPE_NORMAL
- en: ModelOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While DataOps is about the data life cycle, ModelOps is about the model life
    cycle – more specifically, statistical and machine learning models. The objective
    is to manage the models from development to deployment, ensuring that they are
    reliable, accurate, and scalable while delivering actionable insights based on
    the use cases’ requirements.
  prefs: []
  type: TYPE_NORMAL
- en: ModelOps, MLOps, and LLMOps
  prefs: []
  type: TYPE_NORMAL
- en: These terms have overlapping definitions and are sometimes used interchangeably.
    In this book, we will refer to ModelOps as the broader life cycle management practice
    for different types of models, including simulations, statistical, and machine
    learning models. We will use **machine learning operations** (**MLOps**) more
    specifically for machine learning models and **large language model operations**
    (**LLMOps**) for the specific considerations that apply to the life cycle of LLMs.
    As such, ModelOps will refer to the superset of practices.
  prefs: []
  type: TYPE_NORMAL
- en: ModelOps practices can be categorized broadly into model development and testing,
    and model deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Model development and testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model development and testing involves creating and fine-tuning time series
    analysis models based on historical data. This process starts with feature engineering,
    selecting appropriate algorithms, such as Autoregressive Integrated Moving Average
    (ARIMA) or Long Short-Term Memory (LSTM), and splitting the data into training
    and testing sets. Then, models are iteratively trained and evaluated using performance
    metrics. This ensures accuracy. After that, by testing the model on unseen data,
    we can ensure that the model can generalize well to new, real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now detail further each of these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature engineering**: Overlapping with the DataOps phase, feature engineering
    is the initial stage of model development, concerned with the identification of
    existing and creation of new features from the time series data. These include
    creating lags and rolling averages features, where information from previous time
    steps is used to calculate new features, and creating temporal features that capture
    the time-based characteristics such as specific time of day, day of the week,
    month, or holidays. In addition, the feature engineering stage covers transformations
    to make time series stationary, such as differencing or log transformation, or
    resampling to make time series regular, as discussed in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016).
    We will see how Apache Spark can be used for feature engineering in [*Chapter
    8*](B18568_08.xhtml#_idTextAnchor151), on model development.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model selection**: The model to choose is from an ever-growing list of candidate
    models for time series: ARIMA, Prophet, machine learning, deep learning models
    such as LSTM, and many others. The right time series model depends on the data
    available and the use case we are implementing, as we have seen with the use case
    examples in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044). **Exploratory data
    analysis** (**EDA**), detailed in [*Chapter 6*](B18568_06.xhtml#_idTextAnchor116),
    guides us in this process by providing an understanding of the data’s trends,
    seasonality, and underlying patterns. Finding the best model, however, is part
    of an iterative process, refined by model validation, which we will now present
    as the next step.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Dataset splitting**: Once we have candidate models, the first step before
    training the models is to split the historical data into training, validation,
    and test datasets. The specific consideration in doing so for time series data
    is twofold: to preserve the chronological order within datasets, and to ensure
    that there is no data leakage between the splits.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training**: During this phase, the model is fitted to the training dataset
    by adjusting its parameters. This can be supervised with predefined labels or
    actual outcomes, or unsupervised, as explained in [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044).
    In the case of supervised training, the model parameters are adjusted using a
    process such as gradient descent to minimize the difference, using a loss function,
    between model prediction and actual outcome. For unsupervised training, the model
    is adjusted until a stopping criterion is met, such as the number of runs or the
    number of categorization classes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Validation**: As part of training iterations, model validation uses unseen
    validation datasets with techniques such as time-based cross-validation. This
    is to check that there is no overfitting and that the trained model can generalize
    to unseen data with acceptable accuracy. The model is evaluated for accuracy using
    metrics such as **mean absolute percentage error** (**MAPE**) or **mean absolute
    error** (**MAE**). As an iterative process, this stage includes hyperparameter
    tuning, where models with different settings are trained and validated to find
    the best model configuration. Techniques such as grid search or Bayesian optimization
    are used to search for the optimal hyperparameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parameters and hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: Note the difference between parameters and hyperparameters. These terms are
    often confused. Model **parameters** are learned from the data as part of a training
    run, such as a neural network’s weights and biases. **Hyperparameters** define
    the configuration of the model prior to model training, which, in the case of
    a neural network, can be the number of nodes and layers defining its architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '**Testing** – As a final step in model development, the model is evaluated
    against the unseen testing dataset and compared with different algorithms or types
    of models. Testing can also include other criteria beyond model accuracy, such
    as response time, and integration testing with the application code used with
    the model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model training, validation, and testing will be covered in detail in [*Chapter
    7*](B18568_07.xhtml#_idTextAnchor133).
  prefs: []
  type: TYPE_NORMAL
- en: Model deployment and monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model deployment and monitoring involves the transition of time series analysis
    models from development to a live production environment, together with continuous
    oversight of their performance. This ongoing monitoring allows retraining and
    updating the model to adapt to changes in the data patterns or in the behavior
    of the underlying system being analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now detail further each of these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deployment**: Models are deployed for production into a model-serving framework.
    This can be containerized with tools such as Kubernetes and Docker or deployed
    to a cloud-based solution such as Databricks Model Serving, Amazon SageMaker,
    Azure Machine Learning, or Google Vertex AI. Once deployed, the model can be used
    for batch inferencing, scheduled at recurring intervals, or real-time inferencing
    based on continuously streaming data sources or in response to API requests.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Monitoring**: Once the model is deployed in production, monitoring is required
    to ensure that it remains fit for purpose and of value. With **data drift** (the
    change in the characteristics of the data over time) and **concept drift**, where
    the model’s representation of reality worsens over time, model accuracy decreases.
    This can be detected with model monitoring, and alerts sent accordingly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Retraining**: When the monitoring alerts about a drift, and if the drift
    is significant enough, retraining the model is the next step. This can be manually
    launched or automated. If retraining does not yield a sufficiently accurate model,
    then we will have to go back to the model development cycle to find another model
    fit for purpose.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Governance**: This includes several key considerations. We need to keep track
    of model versions and life cycle stages throughout the model’s life cycle and
    associated processes. In addition, for auditability purposes, logs of training,
    deployment, and accuracy metrics are kept, and in some cases, requests to and
    responses from model inference are also saved. Additional considerations include
    access control to the model and ensuring it meets all legal and regulatory compliance
    requirements, including when dealing with personal or sensitive data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In summary, ModelOps for a time series analysis project covers the end-to-end
    process of developing, deploying, and maintaining models, while overlapping with
    DataOps for its data requirements. ModelOps ensures continuous improvement, reproducibility,
    collaboration, and fitness for purpose with respect to business objectives. It
    also maintains the model’s effectiveness and ensures that it keeps delivering
    value over time.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover the practical considerations for ModelOps in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133),
    *Building and Testing Models*. The next phase is DevOps, which we will detail
    now.
  prefs: []
  type: TYPE_NORMAL
- en: DevOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next after ModelOps, DevOps is a set of practices and tools that smoothen the
    handover between **development** (**Dev**) and **operations** (**Ops**). This
    is for both the model and related application code. By automating the building,
    testing, deployment, and monitoring of time series applications, DevOps ensures
    that they are reliable, scalable, and continuously deliver value to the business.
  prefs: []
  type: TYPE_NORMAL
- en: The DevOps practices can be broadly broken down into **continuous integration/continuous
    deployment** (**CI/CD**), infrastructure management, and monitoring and governance.
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD involves automating the integration and deployment of time series analysis
    models for seamless updates to production.
  prefs: []
  type: TYPE_NORMAL
- en: 'This includes the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code and model versioning and repository**: Code and model changes require
    tracking, with the possibility of rolling back to previous versions if needed.
    This means that the code and models need to be version-controlled and stored in
    a repository from where the different versions can be accessed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Testing**: It is crucial that there is no regression whenever changes are
    made to the time series model and associated code. One way to ensure this is through
    automated testing, with unit and integration testing, which can be kicked off
    either when production monitoring detects a degradation or when there are model
    or associated code changes in development.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Deployment**: Once the time series model and code are ready in development,
    the next steps are deployment to staging and production. It is recommended to
    automate this deployment with CI/CD pipelines to minimize the risks of errors
    due to manual steps and make this a seamless, repeatable, and scalable process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In summary, CI/CD pipelines ensure that new features, improvements, and bug
    fixes are consistently integrated, tested, and deployed while minimizing downtime
    and enhancing the efficiency of new code rollout.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Infrastructure as code** (**IaC**) is a recommended approach to provisioning
    as it enables the infrastructure configurations to be version-controlled, self-documented,
    reproducible, and scalable. This is how compute, storage, and networking configurations
    can be set consistently. In a virtual environment such as a cloud environment,
    the infrastructure itself is, in a sense, version-controlled as it is software-defined
    in nature.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the previous core resources, security-specific configurations
    require provisioning for access controls, encryption, and firewalls for network
    security.
  prefs: []
  type: TYPE_NORMAL
- en: As demand for the application changes, the corresponding workload changes with
    a requirement for additional or fewer infrastructure resources. A scalable infrastructure
    management process ensures that the infrastructure is automatically scaled based
    on demand.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring, security, and governance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DevOps has similar requirements for monitoring, security, and governance to
    DataOps and ModelOps. The scope for DevOps encompasses everything that is deployed
    to the production environment, including models, code, and configurations. This
    is typically fulfilled via processes such as application, security, and compliance
    monitoring, logging and alerting, and incident management.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, DevOps ensures that applications, including time series analysis,
    are highly available and scalable by automating their deployment, management,
    and scaling. The key here is to make the transition from *Dev* to *Ops* seamless
    by facilitating collaboration and using automation to ensure that a time series
    analysis project can evolve from a use case concept to its technical implementation
    to a fully operational system that drives significant business impact and value.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the end-to-end phases of a time series analysis project,
    the next section will provide practical examples and tools for implementing what
    we have learned so far in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation examples and tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the end-to-end phases defined, this section will examine two implementation
    examples: a notebook-based approach and an orchestrator-based approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you do not intend to build your own end-to-end environment, you can skip
    the practical part of this section and use a managed platform such as Databricks,
    as we will do in future chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by setting up the environment required to run the examples.
  prefs: []
  type: TYPE_NORMAL
- en: Environment setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using Docker containers, as in [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063),
    for the platform infrastructure. Refer to the *Using a container for deployment*
    section in [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063) for instructions on
    installing Docker.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative to Docker
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use Podman as an open source alternative to Docker. You can find more
    information here: [https://podman.io/](https://podman.io/).'
  prefs: []
  type: TYPE_NORMAL
- en: Before we can deploy the Docker containers, we will validate in the next section
    that there is no conflict with the network ports that will be used by the containers.
  prefs: []
  type: TYPE_NORMAL
- en: Network ports
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following network ports need to be available on your local machine or development
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Spark: `7077`, `8070`, and `8081`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jupyter Notebook: `4040`, `4041`, `4042`, and `8888`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MLflow: `5001`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Airflow: `8080`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can check for the current use of these ports by existing applications with
    the following command, run from the command line or terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you see the required ports in the list of ports already in use, you must
    either stop the application using that port or change the `docker-compose` file
    to use another port.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s assume that the output of the preceding `netstat` command
    reveals that port `8080` is already in use on your local machine or development
    environment, and you are not able to stop the existing application using this
    port.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, you will need to change port `8080` (meant for the Airflow web
    server) in the `docker-compose.yaml` file to another, unused port. Just search
    and replace `8080` on the left of the colon (`:`) to say `8090` if this port is
    free, as per the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Keep note of the new port and use this instead of the existing one whenever
    you need to type the corresponding URL. In this example, port `8080` is changed
    to `8090`, and the matching URL change for the Airflow web server is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http://localhost:8080/`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http://localhost:8090/`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You will need to change the network port in all URLs in the following sections
    that you had to modify as per this section.
  prefs: []
  type: TYPE_NORMAL
- en: Environment startup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once Docker is installed and running, and the network port configuration is
    validated, the following instructions guide you to set up and start the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first download the deployment script from the Git repository for this chapter,
    which is at the following URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch4](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch4)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will be using the `git clone`-friendly URL, which is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To do this, start a terminal or command line and run the following commands:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools),
    missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: xcode-select --install
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now start the container build and startup. A makefile is provided to
    simplify the process of starting and stopping the containers. The following command
    builds the Docker images for the containers and then starts them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Windows environment
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using a Windows environment, you can install a Windows version of
    `make`, as per the following documentation: [https://gnuwin32.sourceforge.net/packages/make.htm](https://gnuwin32.sourceforge.net/packages/make.htm)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `make up` command will give the following or equivalent output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You may see the following error when you run the preceding `make` `up` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You may get an error if you have `bash` instead of`sh` in your environment and
    the script cannot locate the `sh` file. In this case, change the last line in
    makefile from "`sh prep-airflow.sh`" to "`bash prep-airflow.sh`" and then run
    the `make` `up` command again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By the end of the process, as in [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063),
    you will have a running Spark cluster and a separate node for Jupyter Notebook.
    In addition, we have deployed the following components here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**MLflow** – An open source platform, originally developed by Databricks, for
    managing the end-to-end machine learning life cycle. With features for experimentation
    and deployment, MLflow is designed to work with any machine learning library and
    programming language. This makes it flexible for various environments and use
    cases, which explains its broad adoption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find more information here: [https://mlflow.org/](https://mlflow.org/).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Apache Airflow** – Created by Airbnb, Airflow is an open source platform
    for orchestrating data processing pipelines and computational workflows. With
    the ability to programmatically define, schedule, and monitor workflows at scale,
    Airflow is widely adopted, including by data engineers and data scientists, for
    diverse types of workflows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find more information here: [https://airflow.apache.org/](https://airflow.apache.org/).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Postgres** – This is the relational database used in the backend by Airflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now validate the environment that we have just deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the UIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will now access the **user interfaces** (**UIs**) of the different components
    as a quick way to validate the deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the instructions in [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063)
    to validate the deployment of Jupyter Notebook and the Apache Spark cluster. Note
    that due to the Airflow web server using port `8080`, which is the same port we
    used in [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063) for Apache Spark, we have
    changed the Spark master node to the following local URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`http://localhost:8070/`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'MLflow is accessible at the following local URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`http://localhost:5001/`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This will open the web page as per *Figure 4**.4*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18568_04_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: MLflow'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next UI, *Figure 4**.5*, is for Airflow, accessible via the following local
    URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`http://localhost:8080/`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The default username and password are `airflow`, which is highly recommended
    to change.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18568_04_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Airflow'
  prefs: []
  type: TYPE_NORMAL
- en: We now have our environment set up, which we will use next.
  prefs: []
  type: TYPE_NORMAL
- en: Notebook approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have used notebooks from [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016),
    where we started with the Databricks Community edition. In [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063),
    we deployed our own notebook environment with Jupyter, which is an open source
    implementation. As we have seen by now, notebooks give us a feature-rich document-type
    interface where we can combine executable code, visualizations, and text. This
    makes notebooks popular for interactive and collaborative work in data science
    and machine learning. Notebooks can also be built for execution in a non-interactive
    way, which, together with the fact that they have already been used in the earlier
    data science and experimentation phases, makes them readily adaptable to an end-to-end
    notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this first example, we will use an all-in-one notebook based on the Prophet-based
    code introduced in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016). If you followed
    the instructions in the earlier *Environment setup* section, the example notebook
    should be accessible directly within the Jupyter Notebook UI, at the `work / notebooks`
    location on the left folder navigation panel, as shown in *Figure 4**.6*, at the
    following URL: `http://localhost:8888/lab`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_04_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Notebook'
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebook is also downloadable from the following GitHub location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch4/notebooks/ts-spark_ch4_data-ml-ops.ipynb](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch4/notebooks/ts-spark_ch4_data-ml-ops.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: 'With a focus more on structure here than on the code itself, which does not
    change much from [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016), we structured
    the notebook into the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Config
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingest data from source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transform data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ModelOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train and log model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forecast with model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The notable addition to the code from [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016),
    in addition to the structure explained previously, is the MLOps part, which we
    will detail next.
  prefs: []
  type: TYPE_NORMAL
- en: MLOps with MLflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this notebook example, we use MLflow as the tool to implement several MLOps
    requirements. The following code extract focuses on this specific part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The MLflow functionalities used in the preceding code are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`set_tracking_uri` – This sets the URI of the tracking server where MLflow
    will store model-related information. This centralizes model data and facilitates
    collaboration among team members. The tracking server can be a remote server or
    a local file path.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`set_experiment` – This creates a new experiment or uses an existing one. An
    experiment is a logical grouping of runs (separate model training or trials),
    useful to organize and compare different trials.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_run` – This starts a new MLflow run, which can be within a given experiment.
    As a representation of a single training or trial, `run` groups related artifacts
    such as parameters, metrics, and models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prophet.log_model` – This function logs a Prophet model as an artifact in
    the current MLflow run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_params` – This logs key-value pairs of parameters used during the run.
    Parameters are model configurations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`log_metrics` – This logs key-value pairs of metrics evaluated during the run.
    Metrics are numerical values about the model’s performance (e.g., Mean Squared
    Error, accuracy).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The outcome of this can then be accessed via the MLflow UI at the following
    URL: `http://localhost:5001/`.'
  prefs: []
  type: TYPE_NORMAL
- en: This will open the UI to a similar page as per *Figure 4**.4*, from where you
    can navigate on the left panel to the experiment named `ts-spark_ch4_data-ml-ops_time_series_prophet_notebook`.
    This experiment name seen in the UI comes from the code, which is highlighted
    in the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: The **Overview** tab for the experiment, shown in *Figure 4**.7*, has information
    about the experiment such as the creator, creation date, status, source code creating
    the experiment, and model logged from the experiment. It also shows the model
    parameters and metrics as logged in the code.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_04_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: MLflow experiment overview'
  prefs: []
  type: TYPE_NORMAL
- en: The **Model metrics** tab, shown in *Figure 4**.8*, allows one to search and
    view the metrics graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_04_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: MLflow model metrics'
  prefs: []
  type: TYPE_NORMAL
- en: The initial screen of the **Artifacts** tab, shown in *Figure 4**.9*, shows
    the model schema, which we logged in the code as the signature. It also gives
    code examples of how to use the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_04_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: MLflow model schema'
  prefs: []
  type: TYPE_NORMAL
- en: The **MLmodel** section of the **Artifacts** tab, shown in *Figure 4**.10*,
    shows the model artifact with its path.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: MLflow model artifact'
  prefs: []
  type: TYPE_NORMAL
- en: This is as far as we will go with MLflow in this example. We will use MLflow
    in a similar way in the next example with an orchestrator and expand into further
    use of MLflow in [*Chapter 9*](B18568_09.xhtml#_idTextAnchor169), *Going to Production*.
    For now, we will be looking at other considerations with the notebook approach.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple notebooks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The notebook example here is only a starting point that can be adapted and
    extended based on the requirements of your own use case, as well as the techniques
    that will be discussed in the following chapters. For more complex requirements,
    it is recommended to use separate notebooks for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis and data science
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model development, selection, and deployment of the best model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Production data pipeline, potentially including feature engineering as well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Production model inferencing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model retraining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While notebooks are great for their interactivity, collaborative ease, relative
    simplicity, and versatility, they have limitations, which we will cover in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'However good they are, notebooks for end-to-end time series analysis present
    several challenges. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: No scheduling and orchestration capabilities. This makes it hard to go beyond
    simple sequential workflows and develop complex workflows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability issue. The notebook code runs in the notebook kernel, which is limited
    to the resource of the single machine where it is located. Note that this can
    be resolved by submitting the task from the notebook to run on the Apache Spark
    cluster, as we have done in our example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of error handling. If the code in a notebook cell fails, the whole workflow
    execution stops. It is, of course, possible to write error-handling code, but
    this adds additional coding effort.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To answer these challenges, we will be considering another approach next, using
    an orchestrator.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrator approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before diving into the approach, let’s first understand what an orchestrator
    means. Airflow, which we will use here, was mentioned earlier as an example.
  prefs: []
  type: TYPE_NORMAL
- en: An **orchestrator** plays a central role in managing workflows, including data
    engineering and processing. A workflow or pipeline is a set of computing tasks
    executed together in a certain order, in parallel or sequentially, usually with
    dependency on the outcome of the preceding task or tasks. In addition to scheduling
    the workflows, an orchestrator usually has features to author them before and
    monitor their execution post-scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of an orchestrator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using an orchestrator provides the following benefits over the limitations
    of the notebook-only approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling the tasks within the workflows, considering their dependencies and
    parallel or sequential execution requirements. This also includes conditional
    logic for task execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalable and distributed task execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and logging workflow execution, including performance and errors.
    This is crucial for production environments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error handling and alerting with possibilities to retry, skip to the next task,
    or fail the entire pipeline. This is also a key requirement for production environments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with other systems and tools. This is required to build end-to-end
    workflows, covering DataOps, ModelOps, and DevOps, which usually means working
    with different specialized tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have seen the benefits and have the environment set up with Airflow
    as the orchestrator, let’s get into the practice.
  prefs: []
  type: TYPE_NORMAL
- en: Authoring the workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first step is to create the workflow or **direct acyclic graph** (**DAG**)
    as it is also called.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you followed the instructions in the earlier *Environment setup* section,
    the example DAG is already loaded and accessible directly within the Airflow UI,
    as shown in *Figure 4**.5*, at the following URL: `http://localhost:8080/`. At
    this point, you can jump to the next section to run the DAG or continue here for
    details on the DAG code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The DAG definition is in a Python code file in the `dags` folder and is also
    downloadable from the following GitHub location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch4/dags/ts-spark_ch4_airflow-dag.py](https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch4/dags/ts-spark_ch4_airflow-dag.py)'
  prefs: []
  type: TYPE_NORMAL
- en: The core of the code is very similar to what we saw in the previous notebook
    example. This section focuses on integrating with Airflow and defining the DAG’s
    tasks, which are the individual steps of the DAG.
  prefs: []
  type: TYPE_NORMAL
- en: Task definition – Python code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When the orchestrator runs the tasks, it calls the following corresponding
    Python functions as the underlying code that needs to be executed. Note the function
    parameters that are passed in and the return values. These are aligned with the
    task’s definition, which we will see next:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ingest_data` – for task `t1`. Note that `spark.read` will run on the Spark
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`transform_data` – for task `t2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`train_and_log_model` – for task `t3`. Note that the MLflow functions, such
    as `mlflow.set_experiment` and `mlflow.prophet.log_model`, make calls to the MLflow
    server. A partial extract of the code is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`forecast` – for task `t4`. Note that `mlflow.prophet.load_model` loads the
    model from the MLflow server. This is done in this way here only to show how to
    retrieve the model from an MLflow server. It is not strictly required here as
    we could have kept the reference to the model locally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These tasks are referenced by the DAG, which we will define next.
  prefs: []
  type: TYPE_NORMAL
- en: DAG definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Overarching the preceding task definitions, we have the high-level Airflow
    DAG, which is defined as per the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This points to `default_args`, which contains the following DAG parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Further information on these is available in the following Airflow documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/baseoperator/index.html#airflow.models.baseoperator.BaseOperator](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/baseoperator/index.html#airflow.models.baseoperator.BaseOperator)'
  prefs: []
  type: TYPE_NORMAL
- en: We have not set `schedule_interval` as we want to trigger the DAG manually from
    the Airflow UI.
  prefs: []
  type: TYPE_NORMAL
- en: DAG tasks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The DAG tasks are defined as per the following. Note the reference to `dag`
    and to the underlying Python function defined previously. The use of `PythonOperator`
    means that the tasks will be calling Python functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`t1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`t2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notable for task `t2` is how the output from task `t1`, `t1.output`, is passed
    as input, `pdf`, to task `t2`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`t3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output from task `t2`, `t2.output`, is passed as input, `pdf`, to task `t3`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`t4`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output from task `t3`, `t3.output`, is passed as input, `model_uri`, to
    task `t4`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'These tasks are then configured with the following code to be orchestrated
    sequentially by Airflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This concludes the workflow definition as a DAG in Airflow. The example here
    is only a starting point, with a simple sequential workflow that can be adapted
    and extended based on your specific requirements and the additional time series
    analysis tasks that will be discussed in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrating notebooks
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that it is also possible to combine the orchestrator and notebook approach
    by calling notebooks from Airflow tasks using the `PapermillOperator` operator.
    You can find more information on this operator here: [https://airflow.apache.org/docs/apache-airflow-providers-papermill/stable/operators.html](https://airflow.apache.org/docs/apache-airflow-providers-papermill/stable/operators.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Once the DAG is written and placed in Airflow’s `dags` folder, it will be automatically
    picked up by Airflow, checked for syntax errors in the Python definition file,
    and then listed in the list of DAGs available to run, which we will cover next.
  prefs: []
  type: TYPE_NORMAL
- en: Running the workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The workflow can be launched by clicking on the run button (>) on the right
    of the DAG, as seen in *Figure 4**.5* of the *Accessing the UIs* section. By clicking
    on the DAG name on the left panel, the details and graph of the DAG can be viewed
    in the Airflow UI, as shown in *Figure 4**.11*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18568_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: Airflow DAG'
  prefs: []
  type: TYPE_NORMAL
- en: To view information on a specific run and task of the DAG, select the run on
    the left, and then the task from the graph. This will provide the option to view
    the execution log of the task.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting piece of information is the execution time of the different
    tasks, which can be viewed from the **Gantt** tab on the same screen.
  prefs: []
  type: TYPE_NORMAL
- en: We are only exploring the surface of Airflow here, which is a feature-rich tool
    beyond the scope of this book. Refer to the Airflow documentation for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'As was mentioned earlier, some of the code runs on the Apache Spark cluster.
    This can be visualized from the Spark master node, as per *Figure 3**.6* in [*Chapter
    3*](B18568_03.xhtml#_idTextAnchor063). The URL is the following: `http://localhost:8070/`.
    The Spark UI will show a running application if it is still running. This application
    is the Spark code launched from the Airflow tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As for MLflow, you can view the outcome in the MLflow UI at the following URL:
    `http://localhost:5001/`.'
  prefs: []
  type: TYPE_NORMAL
- en: From the MLflow UI page, similar to *Figure 4**.4*, you can navigate on the
    left panel to the experiment named `ts-spark_ch4_data-ml-ops_time_series_prophet`.
    This experiment name seen in the UI comes from the code, which is highlighted
    in the code for `train_and_log_model` previously.
  prefs: []
  type: TYPE_NORMAL
- en: '`This concludes the second approach discussed in this chapter. We will build
    on this orchestrator example using the concepts we learn in the upcoming chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: Environment shutdown
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can now stop the container environment. The makefile provided simplifies
    the process with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following or equivalent output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'If you do not intend to use it further, you can go ahead and delete the Docker
    containers created with the `Delete` action, as explained here: [https://docs.docker.com/desktop/use-desktop/container/#container-actions](https://docs.docker.com/desktop/use-desktop/container/#container-actions).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we detailed the important phases of a time series analysis
    project, starting with the choice of a use case corresponding to a business requirement.
    The use case was then mapped to the technical solution, with DataOps, ModelOps,
    and DevOps components. We finally looked at two approaches for implementation,
    including examples of baseline implementations with an all-in-one notebook and
    with an orchestrator, which will be further extended in the rest of this book.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will do just that, focusing on DataOps with data
    preparation.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/ds](https://packt.link/ds)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ds_(1).jpg)'
  prefs: []
  type: TYPE_IMG
