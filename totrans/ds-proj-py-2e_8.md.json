["```py\nimport pandas as pd\nimport matplotlib.pyplot as plt #import plotting package\n#render plotting automatically\n%matplotlib inline\nimport matplotlib as mpl #additional plotting functionality\nmpl.rcParams['figure.dpi'] = 400 #high resolution figures\nmpl.rcParams['font.size'] = 4 #font size for figures\nfrom scipy import stats\nimport numpy as np\ndf = pd.read_csv('../../Data/Chapter_1_cleaned_data.csv')\n```", "```py\n    bill_feats = ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', \\\n                  'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']\n    pay_amt_feats = ['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', \\\n                     'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n    ```", "```py\n    df[bill_feats].describe()\n    ```", "```py\n    df[bill_feats].hist(bins=20, layout=(2,3))\n    ```", "```py\n    df[pay_amt_feats].describe()\n    ```", "```py\n    df[pay_amt_feats].hist(layout=(2,3), xrot=30)\n    ```", "```py```", "```\n    pay_zero_mask = df[pay_amt_feats] == 0\n    pay_zero_mask.sum()\n    ```", "```\n    df[pay_amt_feats][~pay_zero_mask].apply(np.log10)\\\n                                     .hist(layout=(2,3))\n    ```", "```\n    X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split\\\n                                              (df['LIMIT_BAL']\\\n                                               .values\\\n                                               .reshape(-1,1),\\\n                                               df['default'\\\n                                                  'payment next'\\\n                                                  'month'].values,\\\n                                               test_size=0.2,\\\n                                               random_state=24))\n    ```", "```\n    example_lr.fit(X_train_2, y_train_2)\n    ```", "```\n    y_test_2_pred_proba = example_lr.predict_proba(X_test_2)\n    ```", "```\n    metrics.roc_auc_score(y_test_2, y_test_2_pred_proba[:,1])\n    ```", "```\n    0.6201990844642832\n    ```", "```\n    fpr_2, tpr_2, thresholds_2 = metrics.roc_curve\\\n                                 (y_test_2, \\\n                                  y_test_2_pred_proba[:,1])\n    plt.plot(fpr_2, tpr_2, '*-')\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.legend(['Logistic regression', 'Random chance'])\n    plt.xlabel('FPR')\n    plt.ylabel('TPR')\n    plt.title('ROC curve for logistic regression with '\\\n              'LIMIT_BAL feature')\n    ```", "```\n    precision, recall, thresh_3 = metrics.precision_recall_curve\\\n                                  (y_test_2,\\\n                                   y_test_2_pred_proba[:,1])\n    ```", "```\n    plt.plot(recall, precision, '-x')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision and recall for the logistic'\\\n              'regression 'with LIMIT_BAL')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    ```", "```\n    metrics.auc(recall, precision)\n    ```", "```\n    0.31566964427378624\n    ```", "```\n    y_train_2_pred_proba = example_lr.predict_proba(X_train_2)\n    metrics.roc_auc_score(y_train_2, y_train_2_pred_proba[:,1])\n    ```", "```\n    0.6182918113358344\n    ```", "```\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(\n        df[['PAY_1', 'LIMIT_BAL']].values,\n        df['default payment next month'].values,\n        test_size=0.2, random_state=24)\n    ```", "```\n    from sklearn.linear_model import LogisticRegression\n    lr_model = LogisticRegression(solver='liblinear')\n    ```", "```\n    lr_model.fit(X_train, y_train)\n    y_pred = lr_model.predict(X_test)\n    y_pred_proba = lr_model.predict_proba(X_test)\n    ```", "```\n    ones_and_features = np.hstack\\\n                        ([np.ones((X_test.shape[0],1)), X_test])\n    ```", "```\n    intercept_and_coefs = np.concatenate\\\n                          ([lr_model.intercept_.reshape(1,1), \\\n                            lr_model.coef_], axis=1)\n    ```", "```\n    X_lin_comb = np.dot(intercept_and_coefs,\\\n                        np.transpose(ones_and_features))\n    ```", "```\n    y_pred_proba_manual = sigmoid(X_lin_comb)\n    ```", "```\n    y_pred_manual = y_pred_proba_manual >= 0.5\n    ```", "```\n    np.array_equal(y_pred.reshape(1,-1), y_pred_manual)\n    ```", "```\n    from sklearn.metrics import roc_auc_score\n    ```", "```\n    features = features_response[:-1]\n    X = df[features].values\n    ```", "```\n    X_train, X_test, y_train, y_test = \\\n    train_test_split(X, df['default payment next month'].values,\n                     test_size=0.2, random_state=24)\n    ```", "```\n    from sklearn.preprocessing import MinMaxScaler\n    min_max_sc = MinMaxScaler()\n    ```", "```\n    lr = LogisticRegression(solver='saga', penalty='l1',\n                            max_iter=1000)\n    ```", "```\n    from sklearn.pipeline import Pipeline\n    scale_lr_pipeline = Pipeline(\n        steps=[('scaler', min_max_sc), ('model', lr)])\n    ```", "```\n    scale_lr_pipeline.get_params()\n    scale_lr_pipeline.get_params()['model__C']\n    scale_lr_pipeline.set_params(model__C = 2)\n    ```", "```\n    C_val_exponents = np.linspace(2,-3,6)\n    C_vals = np.float(10)**C_val_exponents\n    ```", "```\n    def cross_val_C_search_pipe(k_folds, C_vals, pipeline, X, Y):\n    ##[…]\n    pipeline.set_params(model__C = C_vals[c_val_counter])\n    ##[…]\n    pipeline.fit(X_cv_train, y_cv_train)\n    ##[…]\n    y_cv_train_predict_proba = pipeline.predict_proba(X_cv_train)\n    ##[…]\n    y_cv_test_predict_proba = pipeline.predict_proba(X_cv_test)\n    ##[…]\n    print('Done with C = {}'.format(pipeline.get_params()\\\n                                    ['model__C']))\n    ```", "```\n    cv_train_roc_auc, cv_test_roc_auc, cv_test_roc = \\\n    cross_val_C_search_pipe(k_folds, C_vals, scale_lr_pipeline,\n                            X_train, y_train)\n    ```", "```\n    Done with C = 100.0\n    Done with C = 10.0\n    Done with C = 1.0\n    Done with C = 0.1\n    Done with C = 0.01\n    Done with C = 0.001\n    ```", "```\n    plt.plot(C_val_exponents, np.mean(cv_train_roc_auc, axis=0),\n             '-o', label='Average training score')\n    plt.plot(C_val_exponents, np.mean(cv_test_roc_auc, axis=0),\n             '-x', label='Average testing score')\n    plt.ylabel('ROC AUC')\n    plt.xlabel('log$_{10}$(C)')\n    plt.legend()\n    plt.title('Cross-validation on Case Study problem')\n    ```", "```\n    from sklearn.preprocessing import PolynomialFeatures\n    make_interactions = PolynomialFeatures(degree=2,\n                                           interaction_only=True,\n                                           include_bias=False)\n    X_interact = make_interactions.fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_interact, df['default payment next month'].values,\n        test_size=0.2, random_state=24)\n    print(X_train.shape)\n    print(X_test.shape)\n    ```", "```\n    (21331, 153)\n    (5333, 153)\n    ```", "```\n    rf_params = {'max_depth':[3, 6, 9, 12],\n                 'n_estimators':[10, 50, 100, 200]}\n    ```", "```\n    rf = RandomForestClassifier(n_estimators=10,\\\n                                criterion='gini',\\\n                                max_depth=3,\\\n                                min_samples_split=2,\\\n                                min_samples_leaf=1,\\\n                                min_weight_fraction_leaf=0.0,\\\n                                max_features='auto',\\\n                                max_leaf_nodes=None,\\\n                                min_impurity_decrease=0.0,\\\n                                min_impurity_split=None,\\\n                                bootstrap=True,\\\n                                oob_score=False,\\\n                                n_jobs=None,\n                                random_state=4,\\\n                                verbose=0,\\\n                                warm_start=False,\\\n                                class_weight=None)\n    cv_rf = GridSearchCV(rf, param_grid=rf_params,\\\n                         scoring='roc_auc',\\\n                         n_jobs=-1,\\\n                         refit=True,\\\n                         cv=4,\\\n                         verbose=2,\\\n                         error_score=np.nan,\\\n                         return_train_score=True)\n    ```", "```\n    cv_rf.fit(X_train, y_train)\n    ```", "```\n    cv_rf_results_df = pd.DataFrame(cv_rf.cv_results_)\n    ```", "```\n    ax_rf = plt.axes()\n    pcolor_graph = ax_rf.pcolormesh\\\n                   (xx_rf, yy_rf,\\\n                    cv_rf_results_df['mean_test_score']\\\n                    .values.reshape((4,4)), cmap=cm_rf)\n    plt.colorbar(pcolor_graph, label='Average testing ROC AUC')\n    ax_rf.set_aspect('equal')\n    ax_rf.set_xticks([0.5, 1.5, 2.5, 3.5])\n    ax_rf.set_yticks([0.5, 1.5, 2.5, 3.5])\n    ax_rf.set_xticklabels\\\n    ([str(tick_label) for tick_label in rf_params['n_estimators']])\n    ax_rf.set_yticklabels\\\n    ([str(tick_label) for tick_label in rf_params['max_depth']])\n    ax_rf.set_xlabel('Number of trees')\n    ax_rf.set_ylabel('Maximum depth')\n    ```", "```\n    with open('../../Data/Activity_6_01_data.pkl', 'rb') as f:\n        features_response, X_train_all, y_train_all, X_test_all,\\\n        y_test_all = pickle.load(f)\n    ```", "```\n    from sklearn.model_selection import train_test_split\n    X_train_2, X_val_2, y_train_2, y_val_2 = \\\n    train_test_split(X_train_all, y_train_all,\\\n                     test_size=0.2, random_state=24)\n    ```", "```\n    xgb_model_4 = xgb.XGBClassifier(\n        n_estimators=1000,\n        max_depth=0,\n        learning_rate=0.1,\n        verbosity=1,\n        objective='binary:logistic',\n        use_label_encoder=False,\n        n_jobs=-1,\n        tree_method='hist',\n        grow_policy='lossguide')\n    ```", "```\n    max_leaves_values = list(range(5,205,5))\n    ```", "```\n    eval_set_2 = [(X_train_2, y_train_2), (X_val_2, y_val_2)]\n    ```", "```\n    %%time\n    val_aucs = []\n    for max_leaves in max_leaves_values:\n        #Set parameter and fit model\n        xgb_model_4.set_params(**{'max_leaves':max_leaves})\n        xgb_model_4.fit(X_train_2, y_train_2,\\\n                        eval_set=eval_set_2,\\\n                        eval_metric='auc',\\\n                        verbose=False,\\\n                        early_stopping_rounds=30)\n        #Get validation score\n        val_set_pred_proba = xgb_model_4.predict_proba(X_val_2)[:,1]\n        val_aucs.append(roc_auc_score(y_val_2, val_set_pred_proba))\n    ```", "```\n    max_leaves_df_2 = \\\n    pd.DataFrame({'Max leaves':max_leaves_values,\\\n                  'Validation AUC':val_aucs})\n    mpl.rcParams['figure.dpi'] = 400\n    max_leaves_df_2.set_index('Max leaves').plot()\n    ```", "```\n    max_auc_2 = max_leaves_df_2['Validation AUC'].max()\n    max_auc_2\n    max_ix_2 = max_leaves_df_2['Validation AUC'] == max_auc_2\n    max_leaves_df_2[max_ix_2]\n    ```", "```\n    xgb_model_4.set_params(**{'max_leaves':40})\n    xgb_model_4.fit(X_train_2, y_train_2, eval_set=eval_set_2,\n                    eval_metric='auc',\n                    verbose=False, early_stopping_rounds=30)\n    ```", "```\n    X_val_2_df = pd.DataFrame(data=X_val_2,\n                              columns=features_response[:-1])\n    ```", "```\n    explainer_2 = shap.explainers.Tree(xgb_model_4, data=X_val_2_df)\n    shap_values_2 = explainer_2(X_val_2_df)\n    mpl.rcParams['figure.dpi'] = 75\n    shap.summary_plot(shap_values_2.values, X_val_2_df)\n    ```", "```\n    shap.plots.scatter(shap_values_2[:,'LIMIT_BAL'],\n                       color=shap_values_2)\n    ```", "```\n    with open('../Data/xgb_model_w_data.pkl', 'wb') as f:\n        pickle.dump([X_train_all, y_train_all,\\\n                     X_test_all, y_test_all,\\\n                     xgb_model_4], f)\n    ```", "```\n    cost_of_defaults = np.sum(y_test_all * X_test_all[:,5])\n    cost_of_defaults \n    ```", "```\n    60587763.0\n    ```", "```\n    net_savings[max_savings_ix]/cost_of_defaults\n    ```", "```\n    0.2214260658542551\n    ```", "```\n    net_savings[max_savings_ix]/len(y_test_all)\n    ```", "```\n    2259.2977433479286\n    ```", "```\n    plt.plot(total_cost/len(y_test_all),\n             net_savings/len(y_test_all))\n    plt.xlabel\\\n    ('Upfront investment: cost of counselings per account (NT$)')\n    plt.ylabel('Net savings per account (NT$)')\n    ```", "```\n    plt.plot(thresholds, n_pos_pred/len(y_test_all))\n    plt.ylabel('Flag rate')\n    plt.xlabel('Threshold')\n    ```", "```\n    plt.plot(n_true_pos/sum(y_test_all),\\\n             np.divide(n_true_pos, n_pos_pred)) \n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    ```", "```\n    plt.plot(thresholds, np.divide(n_true_pos, n_pos_pred),\n             label='Precision')\n    plt.plot(thresholds, n_true_pos/sum(y_test_all),\n             label='Recall')\n    plt.xlabel('Threshold')\n    plt.legend()\n    ```"]