["```py\nconf_run1 = {\n    'runid':          1,\n    'START_DATE':     '1981-01-01',\n    'TRAIN_END_DATE': '1985-12-31',\n    'EVAL_END_DATE':  '1986-12-31',\n}\nconf_run2 = {\n    'runid':          2,\n    'START_DATE':     '1982-01-01',\n    'TRAIN_END_DATE': '1986-12-31',\n    'EVAL_END_DATE':  '1987-12-31',\n}\n…\n```", "```py\n# Define tasks\nt1 = TriggerDagRunOperator(\n    task_id=\"ts-spark_ch9_data-ml-ops_1\",\n    trigger_dag_id=\"ts-spark_ch9_data-ml-ops\",\n    conf=conf_run1,\n    wait_for_completion=True,\n    dag=dag,\n)\nt2 = TriggerDagRunOperator(\n    task_id=\"ts-spark_ch9_data-ml-ops_2\",\n    trigger_dag_id=\"ts-spark_ch9_data-ml-ops\",\n    conf=conf_run2,\n    wait_for_completion=True,\n    dag=dag,\n)\n…\n```", "```py\nt1 >> t2 >> t3 >> t4 >> t5\n```", "```py\nt0 = PythonOperator(\n    task_id='get_config',\n    python_callable=get_config,\n    op_kwargs={'_vars': {\n        'runid': \"{{ dag_run.conf['runid'] }}\",\n        'START_DATE': \"{{ dag_run.conf['START_DATE'] }}\",\n        'TRAIN_END_DATE': \"{{ dag_run.conf['TRAIN_END_DATE'] }}\",\n        'EVAL_END_DATE': \"{{ dag_run.conf['EVAL_END_DATE'] }}\",\n        },\n    },\n    provide_context=True,\n    dag=dag,\n)\n```", "```py\ndef get_config(_vars, **kwargs):\n    print(f\"dag_config: {_vars}\")\n    return _vars variable for use by subsequent tasks.\nWe can see the status of the task in the DAG view in Airflow as per *Figure 9**.1*.\nData ingestion and storage\nAt this step, after completion of the `t0` task, the `t1` task is launched by Airflow. It calls the `ingest_train_data` function to ingest the training data from the input CSV file as specified by the `DATASOURCE` variable. In this example, as it is a relatively small file, we ingest the full file every time. You will likely ingest only new data points incrementally at this stage.\nThe code for this step is as follows:\n\n```", "```py\n\n The data is ingested using Spark, with the `spark.read.csv` function, into a Spark DataFrame. We then filter the data for the range of dates that fall within the training dataset as per the `START_DATE` and `TRAIN_END_DATE` parameters.\nWe want to be able to later report on how much data we ingest every time. To enable this, we count the number of rows in the DataFrame.\nFinally, in this task, we persist the ingested data with the `write` function to disk storage in `delta` format for use by the next steps of the workflow. As we will parallelize the workflow tasks in the future, and to avoid multiple parallel writes to the same disk location, we store the data for this specific run in its own table appended with `runid`. Note as well how we used the term `bronze` in the name. This corresponds to the **medallion** approach, which we discussed in the *Data processing and storage* section of [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087). Persisting the data to storage at this stage can come in handy when we are ingesting a lot of data. This makes it possible in the future to change and rerun the rest of the pipeline without having to re-ingest the data.\nThe status of the task is visible in the DAG view in Airflow as per *Figure 9**.1*.\nWith the data ingested from the source and persisted, we can move on to the data transformation stage.\nData transformations\nThis stage corresponds to Airflow task `t2`, which calls the `transform_train_data` function. As its name suggests, this function transforms the training data into the right format for the upcoming training stage.\nThe code for this step is as follows:\n\n```", "```py\n\n We first read the data from `bronze`, where it was stored by the previous task, `t1`. This stored data can then be used as input to run the current task.\nIn this example, we do the following simple transformations:\n\n*   Column level: Rename the `date` column as `ds`\n*   Column level: Change `daily_min_temperature` to the double data type (the `cast` function) and rename it as `y`\n*   DataFrame level: Remove all rows with missing values using the `dropna` function\n\nAs in the previous stage, we want to collect metrics specific to this stage so that we can later report on the transformations. To do this, we count the number of rows in the DataFrame after the transformations.\nNote\nThis stage is likely to include several data checks and transformations, as discussed in the *Data quality checks, cleaning, and transformations* section of [*Chapter 5*](B18568_05.xhtml#_idTextAnchor103).\nFinally, in this task, we persist the ingested data with the `write` function to disk storage in `delta` format for use by the next steps of the workflow. We call this data stage `silver`, as per the medallion approach explained previously.\nSimilarly to the previous tasks, we can see the task’s status in the DAG view in Airflow, as per *Figure 9**.1*.\nWith the data curated and persisted, we can move on to the model training stage.\nModel training and validation\nThis stage is the longest in our example and corresponds to Airflow task `t3`, which calls the `train_and_log_model` function. This function trains and validates a Prophet forecasting model using the training data from the previous stage. As we saw in [*Chapter 7*](B18568_07.xhtml#_idTextAnchor133), choosing the right model involves a whole process, which we have simplified here to a minimum.\nThe code extract for this step is as follows:\n\n```", "```py\n\n In this code example, we do the following:\n\n1.  We first read the data from `silver`, where it was stored by the previous task, `t2`. Then, we can run the current task using the stored data as input.\n2.  MLflow Tracking Server is used to save all the parameters and metrics for each run. We group them under an experiment called `ts-spark_ch9_data-ml-ops_time_series_prophet_train` and use `log_param` and `log_metric` functions to capture the parameters and metrics gathered so far in the run.\n3.  We then train the Prophet model with the training data using the `fit` function.\n4.  As a model validation step, we use the `cross_validation` function and retrieve the corresponding metrics with the `performance_metrics` function.\n5.  The final step is to log the model to the MLflow Model Registry, using the `log_model` function, and all the related training and validation metrics with MLflow. Note that we log the model signature as a best practice to document the model in the MLflow Model Registry.\n\nWe can see the task’s status in the DAG view in Airflow, as per *Figure 9**.1*. The logged parameters and metrics are visible in MLflow Tracking server, as shown in *Figure 9**.4*.\n![](img/B18568_09_04.jpg)\n\nFigure 9.4: MLflow experiment tracking (training)\nThe model saved in the MLflow Model Registry is shown in *Figure 9**.5*.\n![](img/B18568_09_05.jpg)\n\nFigure 9.5: MLflow Model Registry\nAfter the conclusion of the model training stage, we can progress to the next stage, where we will use the trained model to do forecasting.\nForecasting\nThis stage corresponds to Airflow task `t4`, which calls the `forecast` function. As its name suggests, this function infers future values of the time series. While we have this task in the same workflow as the prior training tasks, it is common for the forecasting task to be in a separate inferencing pipeline. This separation allows for scheduling the training and inferencing at different times.\nThe code for this step is as follows:\n\n```", "```py\n\n We first load the model from the model registry, where it was stored by the previous task, `t3`. This model can then be used for forecasting in the current task.\nIn this example, we want to generate a forecast for 365 days in advance by calling the following:\n\n*   The `make_future_dataframe` function to generate the future period\n*   The `predict` function to forecast for these future times\n\nAnother approach to generating the future period is to get this as input from the user or another application calling the model. As for the 365-day forecasting horizon, this is a relatively long time to forecast. We discussed in the *Forecasting* section of [*Chapter 2*](B18568_02.xhtml#_idTextAnchor044) how a shorter forecasting horizon is likely to yield better forecasting accuracy. We have used a long period in this example for practical reasons to showcase forecasting over a 5-year period, with 5 runs of 365 days each. These runs were explained in the earlier *Simulation and runs* section. Moving beyond the requirement of the example here, keep the forecasting horizon shorter relative to the span of the training dataset and the level of granularity.\nFinally, in this task, we persist the forecasted data with the `write` function to disk storage in `delta` format for use by the next steps of the workflow. We call this data stage `gold` as per the medallion approach explained previously. This delta table, where the forecasting outcome is stored, is also known as the inference table.\nWith the forecasts persisted, we can move on to the model evaluation stage.\nModel evaluation\nThis stage corresponds to Airflow tasks `t5`, `t6`, and `t7`, which call the `ingest_eval_data`, `transform_eval_data`, and `eval_forecast` functions respectively.\nNote\nIn a production environment, we want to monitor the accuracy of our model’s forecast against real data so that we can detect when the model is not accurate enough and needs retraining. In the example here, we have these tasks in the same workflow as the prior forecasting task to keep the example simple enough to fit within this chapter. These tasks will be a separately scheduled workflow, which will be executed a posteriori of the event being forecasted. In the example, we are simulating the post-event evaluation by using the data points following the training data.\nThe `ingest_eval_data` and `transform_eval_data` functions are very similar to the `ingest_train_data` and `transform_train_data` functions, which we have seen in the previous sections. The main difference, as the name suggests, is that they operate on the evaluation and training data respectively.\nWe will focus on the `eval_forecast` function in this section, with the code extract as follows:\n\n```", "```py\n\n In this code example, we do the following:\n\n1.  We first read the evaluation data from `silver`, where it was stored by the previous task, `t6`. We also read the forecasted data from `gold`, where it was stored earlier by the forecasting task, `t4`. We join both datasets with the `join` function so that we can compare the forecasts to the actuals.\n2.  In this example, we use `RegressionEvaluator` from the `pyspark.ml.evaluation` library is used to do the calculation.\n3.  As a final step, MLflow Tracking Server is used to save all the parameters and metrics for each run. We group them under an experiment called `ts-spark_ch9_data-ml-ops_time_series_prophet_eval` and use the `log_param` and `log_metric` functions to capture the parameters and metrics gathered so far in the run.\n\nWe can see the task’s status in the DAG view in Airflow, as per *Figure 9**.1*. The logged parameters and metrics are visible in MLflow Tracking Server, as shown in *Figure 9**.6*.\n![](img/B18568_09_06.jpg)\n\nFigure 9.6: MLflow experiment tracking (evaluation)\nAs with the training experiment tracking shown in *Figure 9**.4*, we can see in the evaluation experiment in *Figure 9**.6* the ingest, transform, and forecast data counts, as well as the evaluation RMSE.\nWith this concluding the model evaluation stage, we have seen the end-to-end workflow example. In the next section, we will cover the monitoring and reporting part of the example.\nMonitoring and reporting\nThe workflows we covered in the previous section are the backend processes in our end-to-end time series analysis example. In this section, we will cover the operational monitoring of the runs and the end user reporting of the forecasting outcome.\nMonitoring\nThe work to collect the metrics has been done as part of the code executed by the workflows we have seen in this chapter. Our focus in this section is on the visualizations to monitor the workflows and the metrics.\nWorkflow\nStarting with the workflow, as we have seen in *Figures 9.1* and *9.2*, the Airflow DAG shows the status of the runs. In case a task fails, as shown in *Figure 9**.7*, we can select the failed task in Airflow and inspect the event log and logs to troubleshoot.\n![](img/B18568_09_07.jpg)\n\nFigure 9.7: Airflow DAG with failed task\nTraining\nWe can visualize the training metrics for a specific run in MLflow Tracking Server, as shown in *Figure 9**.4*. We can also monitor the metrics across multiple runs and compare them in a table, as per *Figure 9**.8*.\n![](img/B18568_09_08.jpg)\n\nFigure 9.8: MLflow experiments (training) – select and compare\nBy selecting the five runs of our `_runall` workflow and clicking on the **Compare** button as in *Figure 9**.8*, we can create a scatter plot as per *Figure 9**.9*. This allows us to see the details for a specific run as well by hovering the mouse pointer over a data point in the graph.\n![](img/B18568_09_09.jpg)\n\nFigure 9.9: Training – plot by runs with details\nAn interesting metric to monitor is the count of training data transformed and ready for training, as per *Figure 9**.10*. We can see here that the first four runs had fewer data points for training than the last run.\n![](img/B18568_09_10.jpg)\n\nFigure 9.10: Training – transform count by runs\nWe can similarly monitor the RMSE for each training run, as per *Figure 9**.11*. We can see here that the model accuracy has improved (lower RMSE) in the last two runs. If the accuracy had dropped instead, then the question from an operational point of view would have been whether this drop is acceptable or there is a need to develop another model. In this situation, this decision is dependent on your specific requirement and what was agreed as an acceptable drop in accuracy.\n![](img/B18568_09_11.jpg)\n\nFigure 9.11: Training – RMSE by runs\nAfter the model has been trained and is used for forecasting, we can evaluate the model’s forecasted values against actuals. We will cover the monitoring of the evaluation metrics next.\nEvaluation\nWe can visualize the evaluation metrics for a specific run in MLflow Tracking Server, as shown in *Figure 9**.6*. We can also monitor the metrics across multiple runs and compare them in a table, as per *Figure 9**.12*.\n![](img/B18568_09_12.jpg)\n\nFigure 9.12: MLflow experiments (evaluation) – select and compare\nBy selecting the five runs of our `_runall` workflow and clicking on the **Compare** button as in *Figure 9**.12*, we can create a scatter plot as per *Figure 9**.13*. This allows us to also see the details for a specific run by hovering the mouse pointer over a data point in the graph.\n![](img/B18568_09_13.jpg)\n\nFigure 9.13: Evaluation – RMSE by runs with details\nAn interesting metric to monitor is the count of forecasted data points, as per *Figure 9**.14*. We can see here that all the runs had the expected number of data points, except the fourth run, having one less. This can be explained by the fact that the evaluation dataset missed one data point during this time period.\n![](img/B18568_09_14.jpg)\n\nFigure 9.14: Evaluation – forecast count by runs\nWe can similarly monitor the RMSE for each evaluation run, as per *Figure 9**.13*. We can see here that the model accuracy dropped gradually (higher RMSE) until the fourth run and then improved in the last run. If the drop had persisted instead, the question from an operational point of view would have been whether this drop is acceptable or there is a need to develop another model. This decision is dependent on your specific requirement and what has been agreed as an acceptable drop in accuracy.\nThis concludes the section on monitoring. While using MLflow was sufficient for the example here, most organizations have dedicated monitoring solutions into which MLflow metrics can be integrated. These solutions also include alerting capabilities, which we have not covered here.\nWe have explored the process to reach an outcome so far, but have not seen the outcome yet. In the next section, we will report on the forecasting outcome.\nReporting\nWe will use a Jupyter notebook in this example to create a set of graphs to represent the forecasting outcome. The `ts-spark_ch9_data-ml-ops_results.ipynb` notebook can be accessed from the local web location, as shown in *Figure 9**.15*. This Jupyter environment was deployed as part of the *Environment* *setup* section.\n![](img/B18568_09_15.jpg)\n\nFigure 9.15: Reporting – notebook to create a graph\nAfter running the notebook, we can see at the end of the notebook the graph, as per *Figure 9**.16*, of the forecasts (gray lines) and actuals (scatter plot) for the different runs. The forecast captures the seasonality well, and most of the actuals fall within the uncertainty intervals, which are set at 80% by default on Prophet.\n![](img/B18568_09_16.jpg)\n\nFigure 9.16: Reporting – actuals (scatter plot) compared to forecasts (gray lines)\nWe can zoom into specific runs as per *Figures 9.17* and *9.18*. These match with the RMSE values we saw in the earlier *Monitoring* section, as we will detail next.\nAs we can see in *Figure 9**.13*, the first run had the lowest RMSE. This is reflected in *Figure 9**.17*, with most actuals falling within the forecasting interval.\n![](img/B18568_09_17.jpg)\n\nFigure 9.17: Reporting – actuals compared to forecasts (run with lowest RMSE)\nIn *Figure 9**.13*, the fourth run had the highest RMSE. This is reflected in *Figure 9**.18*, with many more actuals than in the first run falling outside the forecasting interval.\n![](img/B18568_09_18.jpg)\n\nFigure 9.18: Reporting – actuals compared to forecasts (run with highest RMSE)\nAt this point, the output of the Jupyter notebook can be exported as a report in several formats, such as HTML or PDF. While using Jupyter was sufficient for the example here, most organizations have reporting solutions into which the forecasting outcome can be integrated.\nAdditional considerations\nWe will discuss here some of the additional considerations that apply when going to production, in addition to what we already covered in the example in this chapter.\nScaling\nWe covered scaling extensively in [*Chapter 8*](B18568_08.xhtml#_idTextAnchor151). The environment and workflows in this chapter can be scaled as well. At a high level, this can be achieved in the following ways:\n\n*   Airflow server: scale up by adding more CPU and memory resources\n*   Airflow DAG: run the tasks in parallel\n*   Spark cluster: scale up by adding more CPU and memory resources\n*   Spark cluster: scale out by adding more workers\n*   Model: use Spark-enabled models or parallelize the use of pandas, as discussed in the previous chapter\n\nYou can find more information about Airflow DAGs, including parallel tasks, here: [https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html)\nTo relate this to our Airflow DAG example in this chapter, we defined the tasks as sequential in the following way in the code for `ts-spark_ch9_data-ml-ops_runall`:\n\n```", "```py\n\n The code to run tasks `t3` and `t4` in parallel is as follows:\n\n```", "```py\n\n With regard to the considerations for scaling the Spark cluster, refer to [*Chapter 3*](B18568_03.xhtml#_idTextAnchor063) and, more specifically, the *Driver and worker nodes* section, for a detailed discussion.\nModel retraining\nWe already included retraining in our example workflow at every run using a sliding window of the most recent data. In practice, and to optimize resource utilization, the retraining can be scheduled at a less frequent interval in its own separate workflow. We discussed tracking the model’s accuracy metrics across runs of the workflow in the *Monitoring* section. The trigger of this retraining workflow can be based on the accuracy dropping below a predefined threshold. The appropriate value for the threshold depends on your specific requirements.\nGovernance and security\nIn [*Chapter 4*](B18568_04.xhtml#_idTextAnchor087), in the *From DataOps to ModelOps to DevOps* section, we discussed the considerations for governance and security at various points. Securing your environment and production rollout is beyond the scope of this book. As these are key requirements, and we will not be going into further details here, we highly recommend referring to the following resources to secure the components used in our example:\n\n  **Apache Spark**\n |\n  [https://spark.apache.org/docs/latest/security.html](https://spark.apache.org/docs/latest/security.html)\n |\n\n  **MLflow**\n |\n  [https://mlflow.org/docs/latest/auth/index.html](https://mlflow.org/docs/latest/auth/index.html)\n[https://github.com/mlflow/mlflow/security](https://github.com/mlflow/mlflow/security)\n |\n\n  **Airflow**\n |\n  [https://airflow.apache.org/docs/apache-airflow/stable/security/index.html](https://airflow.apache.org/docs/apache-airflow/stable/security/index.html)\n |\n\n  **Jupyter**\n |\n  [https://jupyter.org/security](https://jupyter.org/security)\n |\n\n  **Docker**\n |\n  [https://www.docker.com/blog/container-security-and-why-it-matters/](https://www.docker.com/blog/container-security-and-why-it-matters/)\n |\n\n  **Unity Catalog**\n |\n  [https://www.unitycatalog.io/](https://www.unitycatalog.io/)\n |\n\nTable 9.1: Resources on security and governance for components in use\nThis concludes the section on the additional considerations before going to production.\nSummary\nIn this chapter, we focused on the crucial phase of moving projects into production, especially given the challenges many projects face in achieving this transition and delivering measurable business results. We saw an example of an end-to-end workflow, covering the stages of data ingestion, storage, data transformations, model training and validation, forecasting, model evaluation, and monitoring. With this example, we brought together what we have learned in this book in view of planning for a production rollout.\nIn the next chapter, we will explore how to go further with Apache Spark for time series analysis by leveraging the advanced capabilities of a managed cloud platform for data and AI.\nJoin our community on Discord\nJoin our community’s Discord space for discussions with the authors and other readers:\n[https://packt.link/ds](https://packt.link/ds)\n![](img/ds_(1).jpg)\n\n```"]