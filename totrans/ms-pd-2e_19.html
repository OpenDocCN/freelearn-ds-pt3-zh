<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">A Brief Tour of Machine Learning</h1>
                </header>
            
            <article>
                
<p>This chapter will take you on a whirlwind tour of machine learning, focusing on using the <kbd>pandas</kbd> library as a tool to preprocess the data used by machine learning programs. It will also introduce you to the <kbd>scikit-learn</kbd> library, which is the most popular machine learning toolkit in Python.</p>
<p>In this chapter, we will illustrate machine learning techniques by applying them to a well-known problem about classifying which passengers survived the Titanic disaster at the turn of the last century. The various topics addressed in this chapter include the following:</p>
<ul>
<li>The role of pandas in machine learning</li>
<li>Installing <kbd>scikit-learn</kbd></li>
<li>Introduction to machine learning concepts</li>
<li>Applying machine learning—Kaggle Titanic competition</li>
<li>Data analysis and preprocessing using pandas</li>
<li>A naïve approach to the Titanic problem</li>
<li>The <kbd>scikit-learn</kbd> ML classifier interface</li>
<li>Supervised learning algorithms</li>
<li>Unsupervised learning algorithms</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The role of pandas in machine learning</h1>
                </header>
            
            <article>
                
<p>The library we will be considering for machine learning is called <kbd>scikit-learn</kbd>. The <kbd>scikit-learn</kbd> Python library is an extensive library of machine learning algorithms that can be used to create adaptive programs that learn from data inputs.</p>
<p class="mce-root"/>
<p>However, before this data can be used by <kbd>scikit-learn</kbd>, it must undergo some preprocessing. This is where pandas comes in. pandas can be used to preprocess and filter data before passing it to the algorithm implemented in <kbd>scikit-learn</kbd>.</p>
<p>In the coming sections, we will see how <kbd>scikit-learn</kbd> can be used for machine learning. So, as the first step, we will learn how to install it on our machines. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installation of scikit-learn</h1>
                </header>
            
            <article>
                
<p>As was mentioned in <span class="ChapterrefPACKT">Chapter 2</span>, <em>Installation of Python and pandas from Third-Party Vendors</em>, the easiest way to install pandas and its accompanying libraries is to use a third-party distribution such as Anaconda and be done with it. Installing <kbd>scikit-learn</kbd> should be no different. I will briefly highlight the steps for installation on various platforms and third-party distributions, starting with Anaconda. The <kbd>scikit-learn</kbd> library requires the following libraries:</p>
<ul>
<li>Python 2.6.x or higher</li>
<li>NumPy 1.6.1 or higher</li>
<li>SciPy 0.9 or higher</li>
</ul>
<p>Assuming that you have already installed pandas as described in <a href="34a6977b-f807-4ee6-9da8-034d9216bb49.xhtml"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Installation of pandas and Supporting Software</em>, these dependencies should already be in place. The various options to install <kbd>scikit-learn</kbd> on different platforms are discussed in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing via Anaconda</h1>
                </header>
            
            <article>
                
<p>You can install <kbd>scikit-learn</kbd> on Anaconda by running the <kbd>conda</kbd> Python package manager:</p>
<pre>    <strong>conda install scikit-learn</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing on Unix (Linux/macOS)</h1>
                </header>
            
            <article>
                
<p>For Unix, it is best to install from the source (C compiler is required). Assuming that pandas and NumPy are already installed and the required dependent libraries are already in place, you can install <kbd>scikit-learn</kbd> via Git by running the following commands:</p>
<pre><strong>git clone https://github.com/scikit-learn/scikit-learn.git cd scikit-learn python setup.py install</strong>  </pre>
<p>The pandas library can also be installed on Unix by using <kbd>pip</kbd> from <kbd>PyPi</kbd>:</p>
<pre><strong>pip install pandas</strong>  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing on Windows</h1>
                </header>
            
            <article>
                
<p>To install on Windows, you can open a console and run the following:</p>
<pre><strong>pip install -U scikit-learn</strong>  </pre>
<div class="packt_infobox">For more in-depth information on installation, you can take a look at the official <kbd>scikit-learn</kbd> documentation at <a href="http://scikit-learn.org/stable/install.html"><span class="URLPACKT">http://scikit-learn.org/stable/install.html</span></a>.<br/>
You can also take a look at the README file for the <kbd>scikit-learn</kbd> Git repository at <a href="https://github.com/scikit-learn/scikit-learn/blob/master/README.rst"><span class="URLPACKT">https://github.com/scikit-learn/scikit-learn/blob/master/README.rst</span></a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to machine learning</h1>
                </header>
            
            <article>
                
<p>Machine learning is the art of creating software programs that learn from data. More formally, it can be defined as the practice of building adaptive programs that use tunable parameters to improve predictive performance. It is a sub-field of artificial intelligence.</p>
<p>We can separate machine learning programs based on the type of problems they are trying to solve. These problems are appropriately called learning problems. The two categories of these problems, broadly speaking, are referred to as supervised and unsupervised learning problems. Furthermore, there are some hybrid problems that have aspects that involve both categories—supervised and unsupervised.</p>
<p class="mce-root"/>
<p>The input to a learning problem consists of a dataset of <em>n</em> rows. Each row represents a sample and may involve one or more fields referred to as attributes or features. A dataset can be canonically described as consisting of <em>n</em> samples, each consisting of <em>n</em> features. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised versus unsupervised learning</h1>
                </header>
            
            <article>
                
<p>For supervised learning problems, the input to a learning problem is a dataset consisting of <em>labeled</em> data. By this, we mean that we have outputs whose values are known. The learning program is fed with input samples and their corresponding outputs and its goal is to decipher the relationship between them. Such input is known as labeled data. Supervised learning problems include the following:</p>
<ul>
<li><strong>Classification</strong>: The learned attribute is categorical (nominal) or discrete</li>
<li><strong>Regression</strong>: The learned attribute is numeric/continuous</li>
</ul>
<p>In unsupervised learning or data mining, the learning program is fed with inputs but does without the corresponding outputs. This input data is referred to as unlabeled data. The goal of machine learning in such cases is to learn or decipher the hidden label. Such problems include the following:</p>
<ul>
<li>Clustering</li>
<li>Dimensionality reduction</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Illustration using document classification</h1>
                </header>
            
            <article>
                
<p>A common usage of machine learning techniques is in the area of document classification. The two main categories of machine learning can be applied to this problem—supervised and unsupervised learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised learning</h1>
                </header>
            
            <article>
                
<p>Each document in the input collection is assigned to a category; that is, a label. The learning program/algorithm uses the input collection of documents to learn how to make predictions for another set of documents with no labels. This method is known as <strong>classification</strong>.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unsupervised learning</h1>
                </header>
            
            <article>
                
<p>The documents in the input collection are not assigned to categories; hence, they are unlabeled. The learning program takes this as input and tries to <em>cluster</em> or discover groups of related or similar documents. This method is known as <strong>clustering</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How machine learning systems learn</h1>
                </header>
            
            <article>
                
<p>Machine learning systems utilize what is known as a classifier to learn from data. A <em>classifier</em> is an interface that takes a matrix of what is known as <em>feature values</em> and produces an output vector, also known as the class. These feature values may be discrete or continuously valued. There are three core components of classifiers:</p>
<ul>
<li><strong>Representation</strong>: What type of classifier is it?</li>
<li><strong>Evaluation</strong>: How good is the classifier?</li>
<li><strong>Optimization</strong>: How can you search among the alternatives?</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Application of machine learning – Kaggle Titanic competition</h1>
                </header>
            
            <article>
                
<p>To illustrate how we can use pandas to assist us at the start of our machine learning journey, we will apply it to a classic problem, which is hosted on the Kaggle website (<a href="http://www.kaggle.com"><span class="URLPACKT">http://www.kaggle.com</span></a>). <strong>Kaggle</strong> is a competition platform for machine learning problems. The idea behind Kaggle is to enable companies that are interested in solving predictive analytics problems with their data to post their data on Kaggle and invite data scientists to come up with proposed solutions to their problems. A competition can be ongoing over a period of time, and the rankings of the competitors are posted on a leaderboard. At the close of the competition, the top-ranked competitors receive cash prizes.</p>
<p>The classic problem that we will study to illustrate the use of pandas for machine learning with <kbd>scikit-learn</kbd> is the <em>Titanic: Machine Learning from Disaster</em> problem hosted on Kaggle as their classic introductory machine learning problem. The dataset involved in the problem is a raw dataset. Hence, pandas is very useful in the preprocessing and cleansing of the data before it is submitted as input to the machine learning algorithm implemented in <kbd>scikit-learn</kbd>.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Titanic: Machine Learning from Disaster problem</h1>
                </header>
            
            <article>
                
<p>The dataset for the Titanic consists of the passenger manifest for the doomed trip, along with various features and an indicator variable telling whether the passenger survived the sinking of the ship or not. The essence of the problem is to be able to predict, given a passenger and his/her associated features, whether this passenger survived the sinking of the Titanic or not. The features are as follows.</p>
<p>The data consists of two datasets: one training dataset and one test dataset. The training dataset consists of 891 passenger cases, and the test dataset consists of 491 passenger cases.</p>
<p>The training dataset also consists of 11 variables, of which 10 are features and 1 dependent/indicator variable, <kbd>Survived</kbd>, which indicated whether the passenger survived the disaster or not.</p>
<p>The feature variables are as follows:</p>
<ul>
<li>PassengerID</li>
<li>Cabin</li>
<li>Sex</li>
<li>Pclass (passenger class)</li>
<li>Fare</li>
<li>Parch (number of parents and children)</li>
<li>Age</li>
<li>Sibsp (number of siblings)</li>
<li>Embarked</li>
</ul>
<p>We can make use of pandas to help us to preprocess data in the following ways:</p>
<ul>
<li>Data cleaning and the categorization of some variables</li>
<li>The exclusion of unnecessary features that obviously have no bearing on the survivability of the passenger; for example, name</li>
<li>Handling missing data</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>There are various algorithms that we can use to tackle this problem. They are as follows:</p>
<ul>
<li>Decision trees</li>
<li>Neural networks</li>
<li>Random forests</li>
<li>Support vector machines</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The problem of overfitting</h1>
                </header>
            
            <article>
                
<p><strong>Overfitting</strong> is a well-known problem in machine learning, whereby the program memorizes the specific data that it is fed as input, leading to perfect results on the training data and abysmal results on the test data.</p>
<p>To prevent overfitting, the tenfold cross-validation technique can be used to introduce variability in the data during the training phase.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data analysis and preprocessing using pandas</h1>
                </header>
            
            <article>
                
<p>In this section, we will utilize pandas to do some analysis and preprocessing of the data before submitting it as input to <kbd>scikit-learn</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Examining the data</h1>
                </header>
            
            <article>
                
<p>To start our preprocessing of the data, let's read in the training dataset and examine what it looks like.</p>
<p>Here, we read the training dataset into a pandas DataFrame and display the first rows:</p>
<pre><strong>In [2]: import pandas as pd</strong>
    <strong>    import numpy as np</strong>
<strong># For .read_csv, always use header=0 when you know row 0 is the header row</strong>
    <strong>   train_df = pd.read_csv('csv/train.csv', header=0)</strong>
<strong>In [3]: train_df.head(3)</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>The output is as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/f0f654ad-9bf5-41ce-a31f-ef012a4e523f.png"/></div>
<p>Hence, we can see the various features: <span class="packt_screen">PassengerId</span>, <span class="packt_screen">Survived</span>, <span class="packt_screen">PClass</span>, <span class="packt_screen">Name</span>, <span class="packt_screen">Sex</span>, <span class="packt_screen">Age</span>, <span class="packt_screen">Sibsp</span>, <span class="packt_screen">Parch</span>, <span class="packt_screen">Ticket</span>, <span class="packt_screen">Fare</span>, <span class="packt_screen">Cabin</span>, and <span class="packt_screen">Embarked</span>. One question that springs to mind immediately is this: which of the features are likely to influence whether a passenger survived or not?</p>
<p>It should seem obvious that <span class="packt_screen">PassengerID</span>, <span class="packt_screen">Ticket Code</span>, and <span class="packt_screen">Name</span> should not be influencers on survivability since they're <em>identifier</em> variables. We will skip these in our analysis.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handling missing values</h1>
                </header>
            
            <article>
                
<p>One issue that we have to deal with in datasets for machine learning is how to handle missing values in the training set.</p>
<p>Let's visually identify where we have missing values in our feature set.</p>
<p>For that, we can make use of an equivalent of the <kbd>missmap</kbd> function in R, written by Tom Augspurger. The next screenshot shows how much data is missing for the various features in an intuitively appealing manner:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/0f9de889-cbba-4253-b9bc-50fad33d7ff9.png" style="width:34.00em;height:28.25em;"/></div>
<p>For more information and the code used to generate this data, see the following: <a href="http://tomaugspurger.github.io/blog/2014/02/22/Visualizing%20Missing%20Data/"><span class="URLPACKT">http://tomaugspurger.github.io/blog/2014/02/22/Visualizing%20Missing%20Data/</span></a>.</p>
<p>We can also calculate how much data is missing for each of the features:</p>
<pre><strong>In [83]: missing_perc=train_df.apply(lambda x: 100*(1-x.count().sum()/(1.0*len(x))))</strong>
<strong>In [85]: sorted_missing_perc=missing_perc.order(ascending=False)</strong>
    <strong>        sorted_missing_perc</strong>
<strong>Out[85]: Cabin          77.104377</strong>
    <strong>     Age            19.865320</strong>
    <strong>     Embarked        0.224467</strong>
    <strong>     Fare            0.000000</strong>
    <strong>     Ticket          0.000000</strong>
    <strong>     Parch           0.000000</strong>
    <strong>     SibSp           0.000000</strong>
    <strong>     Sex             0.000000</strong>
    <strong>     Name            0.000000</strong>
    <strong>     Pclass          0.000000</strong>
    <strong>     Survived        0.000000</strong>
    <strong>     PassengerId     0.000000</strong>
    <strong>     dtype: float64</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>Hence, we can see that most of the <kbd>Cabin</kbd> data is missing (77%), while around 20% of the <kbd>Age</kbd> data is missing. We then decide to drop the <kbd>Cabin</kbd> data from our learning feature set as the data is too sparse to be of much use.</p>
<p>Let's do a further breakdown of the various features that we would like to examine. In the case of categorical/discrete features, we use bar plots; for continuous valued features, we use histograms. The code to generate the charts is as shown:</p>
<pre><strong>In [137]:  import random</strong>
    <strong>               bar_width=0.1</strong>
    <strong>               categories_map={'Pclass':{'First':1,'Second':2, 'Third':3},</strong>
    <strong>               'Sex':{'Female':'female','Male':'male'},</strong>
    <strong>               'Survived':{'Perished':0,'Survived':1},</strong>
    <strong>               'Embarked':{'Cherbourg':'C','Queenstown':'Q','Southampton':'S'},</strong>
    <strong>               'SibSp': { str(x):x for x in [0,1,2,3,4,5,8]},</strong>
    <strong>               'Parch': {str(x):x for x in range(7)}</strong>
    <strong>               }</strong>
    <strong>             colors=['red','green','blue','yellow','magenta','orange']</strong>
    <strong>             subplots=[111,211,311,411,511,611,711,811]</strong>
    <strong>             cIdx=0</strong>
    <strong>             fig,ax=plt.subplots(len(subplots),figsize=(10,12))</strong>
    
    <strong>             keyorder = ['Survived','Sex','Pclass','Embarked','SibSp','Parch']</strong>
    
    <strong>for category_key,category_items in sorted(categories_map.iteritems(),</strong>
    <strong>                                          key=lambda i:keyorder.index(i[0])):</strong>
    <strong>    num_bars=len(category_items)</strong>
    <strong>    index=np.arange(num_bars)</strong>
    <strong>    idx=0</strong>
    <strong>    for cat_name,cat_val in sorted(category_items.iteritems()):</strong>
    <strong>        ax[cIdx].bar(idx,len(train_df[train_df[category_key]==cat_val]), label=cat_name,</strong>
    <strong>                color=np.random.rand(3,1))</strong>
    <strong>        idx+=1</strong>
    <strong>    ax[cIdx].set_title('%s Breakdown' % category_key)</strong>
    <strong>    xlabels=sorted(category_items.keys()) </strong>
    <strong>    ax[cIdx].set_xticks(index+bar_width)</strong>
    <strong>    ax[cIdx].set_xticklabels(xlabels)</strong>
    <strong>    ax[cIdx].set_ylabel('Count')</strong>
    <strong>    cIdx +=1 </strong>
    <strong>fig.subplots_adjust(hspace=0.8)</strong>
    <strong>for hcat in ['Age','Fare']:</strong>
    <strong>    ax[cIdx].hist(train_df[hcat].dropna(),color=np.random.rand(3,1))</strong>
    <strong>    ax[cIdx].set_title('%s Breakdown' % hcat)</strong>
    <strong>    #ax[cIdx].set_xlabel(hcat)</strong>
    <strong>    ax[cIdx].set_ylabel('Frequency')</strong>
    <strong>    cIdx +=1</strong>
    
    <strong>fig.subplots_adjust(hspace=0.8)</strong>
    <strong>plt.show()</strong></pre>
<p>Take a look at the following output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/f90ae22d-a5f1-448a-85d8-0979f1254429.png" style="width:39.83em;height:46.08em;"/></div>
<p>From the data and illustration in the preceding screenshot, we can observe the following:</p>
<ul>
<li>About twice as many passengers perished than survived (62% versus 38%).</li>
<li>There were about twice as many male passengers as female passengers (65% versus 35%).</li>
<li>There were about 20% more passengers in the third class versus the first and second together (55% versus 45%).</li>
<li>Most passengers were solo; that is, had no children, parents, siblings, or spouse on board.</li>
</ul>
<p>These observations might lead us to dig deeper and investigate whether there is some correlation between the chances of survival and gender and fare class, particularly if we take into account the fact that the Titanic had a women-and-children-first policy and the fact that the Titanic was carrying fewer lifeboats (20) than it was designed to (32).</p>
<p>In light of this, let's further examine the relationships between survival and some of these features. We'll start with gender:</p>
<pre><strong>In [85]: from collections import OrderedDict</strong>
    <strong>         num_passengers=len(train_df)</strong>
    <strong>         num_men=len(train_df[train_df['Sex']=='male'])</strong>
    <strong>         men_survived=train_df[(train_df['Survived']==1 ) &amp; (train_df['Sex']=='male')]</strong>
    <strong>         num_men_survived=len(men_survived)</strong>
    <strong>         num_men_perished=num_men-num_men_survived</strong>
    <strong>         num_women=num_passengers-num_men</strong>
    <strong>         women_survived=train_df[(train_df['Survived']==1) &amp; (train_df['Sex']=='female')]</strong>
    <strong>         num_women_survived=len(women_survived)</strong>
    <strong>         num_women_perished=num_women-num_women_survived</strong>
    <strong>         gender_survival_dict=OrderedDict()</strong>
    <strong>         gender_survival_dict['Survived']={'Men':num_men_survived,'Women':num_women_survived}</strong>
    <strong>         gender_survival_dict['Perished']={'Men':num_men_perished,'Women':num_women_perished}</strong>
    <strong>         gender_survival_dict['Survival Rate']= {'Men' : <br/>            round(100.0*num_men_survived/num_men,2),<br/>            'Women':round(100.0*num_women_survived/num_women,2)}</strong>
    <strong>pd.DataFrame(gender_survival_dict)</strong>
    <strong>Out[85]:</strong></pre>
<p class="mce-root"/>
<p>Take a look at the following table:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Gender</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Survived</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Perished</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Survival Rate</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Men</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>109</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>468</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>18.89</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Women</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>233</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>81</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>74.2</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>We'll now illustrate this data in a bar chart:</p>
<pre><strong>In [76]: #code to display survival by gender</strong>
    <strong>        fig = plt.figure()</strong>
    <strong>        ax = fig.add_subplot(111)</strong>
    <strong>        perished_data=[num_men_perished, num_women_perished]</strong>
    <strong>        survived_data=[num_men_survived, num_women_survived]</strong>
    <strong>        N=2</strong>
    <strong>        ind = np.arange(N)     # the x locations for the groups</strong>
    <strong>        width = 0.35</strong>
    
    <strong>        survived_rects = ax.barh(ind, survived_data, width,color='green')</strong>
    <strong>        perished_rects = ax.barh(ind+width, perished_data, width,color='red')</strong>
    
    <strong>        ax.set_xlabel('Count')</strong>
    <strong>        ax.set_title('Count of Survival by Gender')</strong>
    <strong>        yTickMarks = ['Men','Women']</strong>
    <strong>        ax.set_yticks(ind+width)</strong>
    <strong>        ytickNames = ax.set_yticklabels(yTickMarks)</strong>
    <strong>        plt.setp(ytickNames, rotation=45, fontsize=10)</strong>
    
    <strong>        ## add a legend</strong>
    <strong>        ax.legend((survived_rects[0], perished_rects[0]), ('Survived', 'Perished') )</strong>
    <strong>        plt.show()</strong></pre>
<p>The preceding code produces the following bar graph diagram:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/23a1457e-6572-4ee0-9e3f-276d3a8b03ad.png" style="width:33.00em;height:23.83em;"/></div>
<p>From the preceding diagram, we can see that the majority of the women survived (74%), while most of the men perished (only 19% survived).</p>
<p>This leads us to the conclusion that the gender of the passenger may be a contributing factor to whether a passenger survived or not.</p>
<p>Next, let's look at passenger class. First, we generate the survived and perished data for each of the three passenger classes, as well as survival rates:</p>
<pre>    <strong>In [86]: </strong>
    <strong>from collections import OrderedDict</strong>
    <strong>num_passengers=len(train_df)</strong>
    <strong>num_class1=len(train_df[train_df['Pclass']==1])</strong>
    <strong>class1_survived=train_df[(train_df['Survived']==1 ) &amp; (train_df['Pclass']==1)]</strong>
    <strong>num_class1_survived=len(class1_survived)</strong>
    <strong>num_class1_perished=num_class1-num_class1_survived</strong>
    <strong>num_class2=len(train_df[train_df['Pclass']==2])</strong>
    <strong>class2_survived=train_df[(train_df['Survived']==1) &amp; (train_df['Pclass']==2)]</strong>
    <strong>num_class2_survived=len(class2_survived)</strong>
    <strong>num_class2_perished=num_class2-num_class2_survived</strong>
    <strong>num_class3=num_passengers-num_class1-num_class2</strong>
    <strong>class3_survived=train_df[(train_df['Survived']==1 ) &amp; (train_df['Pclass']==3)]</strong>
    <strong>num_class3_survived=len(class3_survived)</strong>
    <strong>num_class3_perished=num_class3-num_class3_survived</strong>
    <strong>pclass_survival_dict=OrderedDict()</strong>
    <strong>pclass_survival_dict['Survived']={'1st Class':num_class1_survived,</strong>
    <strong>                                  '2nd Class':num_class2_survived,</strong>
    <strong>                                  '3rd Class':num_class3_survived}</strong>
    <strong>pclass_survival_dict['Perished']={'1st Class':num_class1_perished,</strong>
    <strong>                                  '2nd Class':num_class2_perished,</strong>
    <strong>                                 '3rd Class':num_class3_perished}</strong>
    <strong>pclass_survival_dict['Survival Rate']= {'1st Class' : round(100.0*num_class1_survived/num_class1,2),</strong>
    <strong>               '2nd Class':round(100.0*num_class2_survived/num_class2,2),</strong>
    <strong>               '3rd Class':round(100.0*num_class3_survived/num_class3,2),}</strong>
    <strong>pd.DataFrame(pclass_survival_dict)</strong>
    
    <strong>Out[86]:</strong></pre>
<p><span>Then, we show them in a table:</span></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Class</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Survived</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Perished</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Survival Rate</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>First Class</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>136</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>80</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>62.96</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Second Class</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>87</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>97</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>47.28</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Third Class</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>119</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>372</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>24.24</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>We can then plot the data by using <kbd>matplotlib</kbd> in a similar manner to that for the survivor count by gender described earlier:</p>
<pre>    <strong>In [186]:</strong>
    <strong>fig = plt.figure()</strong>
    <strong>ax = fig.add_subplot(111)</strong>
    <strong>perished_data=[num_class1_perished, num_class2_perished, num_class3_perished]</strong>
    <strong>survived_data=[num_class1_survived, num_class2_survived, num_class3_survived]</strong>
    <strong>N=3</strong>
    <strong>ind = np.arange(N)                # the x locations for the groups</strong>
    <strong>width = 0.35</strong>
    <strong>survived_rects = ax.barh(ind, survived_data, width,color='blue')</strong>
    <strong>perished_rects = ax.barh(ind+width, perished_data, width,color='red')</strong>
    <strong>ax.set_xlabel('Count')</strong>
    <strong>ax.set_title('Survivor Count by Passenger class')</strong>
    <strong>yTickMarks = ['1st Class','2nd Class', '3rd Class']</strong>
    <strong>ax.set_yticks(ind+width)</strong>
    <strong>ytickNames = ax.set_yticklabels(yTickMarks)</strong>
    <strong>plt.setp(ytickNames, rotation=45, fontsize=10)</strong>
    <strong>## add a legend</strong>
    <strong>ax.legend( (survived_rects[0], perished_rects[0]), ('Survived', 'Perished'),</strong>
    <strong>          loc=10 )</strong>
    <strong>plt.show()</strong></pre>
<p>This produces the following bar plot diagram:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/a2771a28-dae4-414b-bfe0-e48e2e22cb91.png" style="width:37.50em;height:26.75em;"/></div>
<p>It seems clear from the preceding data and diagram that the higher the passenger fare class, the greater the passenger's chances of survival.</p>
<p>Given that both gender and fare class seem to influence the chances of a passenger's survival, let's see what happens when we combine these two features and plot a combination of both. For this, we will use the <kbd>crosstab</kbd> function in pandas:</p>
<pre><strong>In [173]: survival_counts=pd.crosstab([train_df.Pclass,train_df.Sex],train_df.Survived.astype(bool))</strong>
<strong>survival_counts</strong>
<strong>Out[173]:      Survived False  True</strong>
    <strong>           Pclass       Sex             </strong>
    <strong>           1            female    3     91</strong>
    <strong>                        male     77     45</strong>
    <strong>           2            female    6     70</strong>
    <strong>                        male     91     17</strong>
    <strong>           3            female   72     72</strong>
    <strong>                        male    300     47</strong></pre>
<p>Let's now display this data using <kbd>matplotlib</kbd>. First, let's do some re-labeling for display purposes:</p>
<pre><strong>In [183]: survival_counts.index=survival_counts.index.set_levels([['1st', '2nd', '3rd'], ['Women', 'Men']])</strong>
<strong>In [184]: survival_counts.columns=['Perished','Survived']</strong></pre>
<p>Now, we plot the passenger data by using the <kbd>plot</kbd> function of a pandas <kbd>DataFrame</kbd>:</p>
<pre><strong>In [185]: fig = plt.figure()</strong>
    <strong>          ax = fig.add_subplot(111)</strong>
    <strong>          ax.set_xlabel('Count')</strong>
    <strong>          ax.set_title('Survivor Count by Passenger class, Gender')</strong>
    <strong>          survival_counts.plot(kind='barh',ax=ax,width=0.75,</strong>
    <strong>                               color=['red','black'], xlim=(0,400))</strong>
<strong>Out[185]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f714b187e90&gt;</strong></pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/2f1447b0-5947-4214-af44-5a4c924e9e20.png" style="width:40.67em;height:26.67em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A naive approach to the Titanic problem</h1>
                </header>
            
            <article>
                
<p>Our first attempt at classifying the Titanic data is to use a naive, yet very intuitive, approach. This approach involves the following steps:</p>
<ol>
<li>Select a set of features, <em>S</em>, that influence whether a person survived or not.</li>
<li>For each possible combination of features, use the training data to indicate whether the majority of cases survived or not. This can be evaluated in what is known as a survival matrix.</li>
<li>For each test example that we wish to predict survival, look up the combination of features that corresponds to the values of its features and assign its predicted value to the survival value in the survival table. This approach is a <span>naive </span>K-nearest neighbor approach.</li>
</ol>
<p>Based on what we have seen earlier in our analysis, three features seem to have the most influence on the survival rate:</p>
<ul>
<li>Passenger class</li>
<li>Gender</li>
<li>Passenger fare (bucketed)</li>
</ul>
<p>We include passenger fare as it is related to passenger class.</p>
<p>The survival table looks something similar to the following:</p>
<pre>    <strong>     NumberOfPeople  Pclass  PriceBucket     Sex  Survived</strong>
    <strong>0                0       1            0  female         0</strong>
    <strong>1                1       1            0    male         0</strong>
    <strong>2                0       1            1  female         0</strong>
    <strong>3                0       1            1    male         0</strong>
    <strong>4                7       1            2  female         1</strong>
    <strong>5               34       1            2    male         0</strong>
    <strong>6                1       1            3  female         1</strong>
    <strong>7               19       1            3    male         0</strong>
    <strong>8                0       2            0  female         0</strong>
    <strong>9                0       2            0    male         0</strong>
    <strong>10              35       2            1  female         1</strong>
    <strong>11              63       2            1    male         0</strong>
    <strong>12              31       2            2  female         1</strong>
    <strong>13              25       2            2    male         0</strong>
    <strong>14               4       2            3  female         1</strong>
    <strong>15               6       2            3    male         0</strong>
    <strong>16              64       3            0  female         1</strong>
    <strong>17             256       3            0    male         0</strong>
    <strong>18              43       3            1  female         1</strong>
    <strong>19              38       3            1    male         0</strong>
    <strong>20              21       3            2  female         0</strong>
    <strong>21              24       3            2    male         0</strong>
    <strong>22              10       3            3  female         0</strong>
    <strong>23               5       3            3    male         0</strong></pre>
<p><span> To see how we use this table, let's take a look at a snippet of our test data:</span></p>
<pre><strong>In [192]: test_df.head(3)[['PassengerId','Pclass','Sex','Fare']]</strong>
<strong>Out[192]: PassengerId   Pclass  Sex     Fare</strong>
    <strong>      0        892     3       male    7.8292</strong>
    <strong>      1        893     3       female  7.0000</strong>
    <strong>      2        894     2       male    9.6875</strong></pre>
<p>For passenger <kbd>892</kbd>, we see that he is male, his ticket price was 7.8292, and he traveled in the third class. Hence, the key for the survival table lookup for this passenger is <em>{Sex='male', Pclass=3, PriceBucket=0 (since 7.8292 falls in bucket 0)}</em>. If we look up the survival value corresponding to this key in our survival table (row 17), we see that the value is <kbd>0</kbd>, that is, perished; this is the value that we will predict.</p>
<p>Similarly, for passenger <kbd>893</kbd>, we have <em>key={Sex='female', Pclass=3, PriceBucket=0}</em>. This corresponds to row 16, and hence, we will predict <kbd>1</kbd>, that is, survived, and her predicted survival is <kbd>1</kbd>, that is, survived.</p>
<p>Hence, our results look like the following command:</p>
<pre><strong>&gt; head -4 csv/surv_results.csv </strong>
<strong>PassengerId,Survived</strong>
   <strong>892,0</strong>
   <strong>893,1</strong>
   <strong>894,0</strong></pre>
<p>The source of this information is at <a href="http://www.markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/"><span class="URLPACKT">http://www.markhneedham.com/blog/2013/10/30/kaggle-titanic-python-pandas-attempt/</span></a>.</p>
<p>Using the survival table approach outlined earlier, we can achieve an accuracy of 0.77990 on Kaggle (<a href="http://www.kaggle.com"><span class="URLPACKT">http://www.kaggle.com</span></a>). The survival table approach, while intuitive, is a very basic approach that represents only the tip of the iceberg of possibilities in machine learning.</p>
<p>In the following sections, we will take a whirlwind tour of various machine learning algorithms that will help you, the reader, to get a feel for what is available in the machine learning universe.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The scikit-learn ML/classifier interface</h1>
                </header>
            
            <article>
                
<p>We'll be diving into the basic principles of machine learning and demonstrate the use of these principles via the <kbd>scikit-learn</kbd> basic API.</p>
<p>The <kbd>scikit-learn</kbd> library has an estimator interface. We illustrate it by using a linear regression model. For example, consider the following:</p>
<pre>    <strong>In [3]: from sklearn.linear_model import LinearRegression</strong>
  </pre>
<p>The estimator interface is instantiated to create a model, which is a linear regression model in this case:</p>
<pre>    <strong>In [4]: model = LinearRegression(normalize=True)   </strong>
    <strong>In [6]: print model</strong>
    <strong>    LinearRegression(copy_X=True, fit_intercept=True, normalize=True)</strong>
  </pre>
<p>Here, we specify <kbd>normalize=True</kbd>, indicating that the <em>x</em>-values will be normalized before regression. <strong>Hyperparameters</strong> (estimator parameters) are passed on as arguments in the model creation. This is an example of creating a model with tunable parameters.</p>
<p>The estimated parameters are obtained from the data when the data is fitted with an estimator. Let's first create some sample training data that is normally distributed about <kbd>y = x/2</kbd>. We first generate our <kbd>x</kbd> and <kbd>y</kbd> values:</p>
<pre>    <strong>In [51]: sample_size=500</strong>
    <strong>         x = []</strong>
    <strong>         y = []</strong>
    
    <strong>        for i in range(sample_size):</strong>
    <strong>            newVal = random.normalvariate(100,10)</strong>
    <strong>            x.append(newVal)</strong>
    <strong>            y.append(newVal / 2.0 + random.normalvariate(50,5))</strong></pre>
<p><kbd>sklearn</kbd> takes a 2D array of <kbd>num_samples × num_features</kbd> as input, so we convert our <kbd>x</kbd> data into a 2D array:</p>
<pre>    <strong>In [67]: X = np.array(x)[:,np.newaxis]</strong>
    <strong>         X.shape</strong>
    <strong>Out[67]: (500, 1)</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In this case, we have 500 samples and 1 feature, <kbd>x</kbd>. We now train/fit the model and display the slope (coefficient) and the intercept of the regression line, which is the prediction:</p>
<pre>    <strong>In [71]: model.fit(X,y)</strong>
    <strong>         print "coeff=%s, intercept=%s" % (model.coef_,model.intercept_)</strong>
    <strong>         coeff=[ 0.47071289], intercept=52.7456611783</strong></pre>
<p>This can be visualized as follows:</p>
<pre>    <strong>In [65]: plt.title("Plot of linear regression line and training data")</strong>
    <strong>         plt.xlabel('x')</strong>
    <strong>         plt.ylabel('y')</strong>
    <strong>         plt.scatter(X,y,marker='o', color='green', label='training data');</strong>
    <strong>         plt.plot(X,model.predict(X), color='red', label='regression line')</strong>
    <strong>         plt.legend(loc=2)</strong>
    
    <strong>Out[65]: [&lt;matplotlib.lines.Line2D at 0x7f11b0752350]</strong></pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/533274a2-3d1e-4c27-8033-cfaf94cf0390.png" style="width:28.08em;height:20.00em;"/></div>
<p>To summarize the basic use of the estimator interface, follow these steps:</p>
<ol>
<li>Define your model: <kbd>LinearRegression</kbd>, <kbd>SupportVectorMachine</kbd>, <kbd>DecisionTrees</kbd>, and so on. You can specify the required hyperparameters in this step; for example, <kbd>normalize=True</kbd>, as specified earlier.</li>
<li>Once the model has been defined, you can train your model on your data by calling the <kbd>fit(..)</kbd> method on the model defined in the previous step.</li>
</ol>
<ol start="3">
<li>Once we have fit the model, we can call the <kbd>predict(..)</kbd> method on test data to make predictions or estimations.</li>
<li>In the case of a supervised learning problem, the <kbd>predict(X)</kbd> method is given unlabeled observations, <kbd>X</kbd>, and returns predicted labels, <kbd>y</kbd>.</li>
</ol>
<div class="packt_infobox">For extra information, please see the following: <a href="http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html"><span class="URLPACKT">http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html</span></a> </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised learning algorithms</h1>
                </header>
            
            <article>
                
<p>We will take a brief tour of some well-known supervised learning algorithms and see how we can apply them to the Titanic survival prediction problem described earlier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Constructing a model using Patsy for scikit-learn</h1>
                </header>
            
            <article>
                
<p>Before we start our tour of the machine learning algorithms, we need to know a little bit about the <kbd>Patsy</kbd> library. We will make use of <kbd>Patsy</kbd> to design features that will be used in conjunction with <kbd>scikit-learn</kbd>. <kbd>Patsy</kbd> is a package for creating what is known as design matrices. These design matrices are transformations of the features in our input data. The transformations are specified by expressions known as formulas, which correspond to a specification of what features we wish the machine learning program to utilize in learning.</p>
<p>A simple example of this is as follows: suppose that we wish to linearly regress a variable, y, against some other variables—<kbd>x</kbd>, <kbd>a</kbd>, and <kbd>b</kbd>—and the interaction between <kbd>a</kbd> and <kbd>b</kbd>; then, we can specify the model as follows:</p>
<pre>    <strong>import patsy as pts</strong>
    <strong>pts.dmatrices("y ~ x + a + b + a:b", data)</strong></pre>
<p>In the preceding line of code, the formula is specified by the following expression: <kbd>y ~ x + a + b + a:b</kbd>.</p>
<div class="packt_infobox">For further information, look at <span class="URLPACKT"><a href="http://patsy.readthedocs.org/en/latest/overview.html">http://patsy.readthedocs.org/en/latest/overview.html</a>.</span></div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">General boilerplate code explanation</h1>
                </header>
            
            <article>
                
<p>In this section, we will introduce boilerplate code for the implementation of the following algorithms by using <kbd>Patsy</kbd> and <kbd>scikit-learn</kbd>. The reason for doing this is that most of the code for the following algorithms is repeatable.</p>
<p>In the following sections, the workings of the algorithms will be described together with the code specific to each algorithm:</p>
<ol>
<li>First, let's make sure that we're in the correct folder by using the following command line. Assuming that the working directory is located at <kbd>~/devel/Titanic</kbd>, we have the following:</li>
</ol>
<pre>    <strong>In [17]: %cd ~/devel/Titanic</strong>
    <strong>        /home/youruser/devel/sandbox/Learning/Kaggle/Titanic</strong></pre>
<ol start="2">
<li>Here, we import the required packages and read in our training and test datasets:</li>
</ol>
<pre>    <strong>In [18]: import matplotlib.pyplot as plt</strong>
    <strong>             import pandas as pd</strong>
    <strong>             import numpy as np</strong>
    <strong>             import patsy as pt</strong>
    <strong>In [19]: train_df = pd.read_csv('csv/train.csv', header=0)</strong>
    <strong>         test_df = pd.read_csv('csv/test.csv', header=0) </strong>
  </pre>
<ol start="3">
<li>Next, we specify the formulas we would like to submit to <kbd>Patsy</kbd>:</li>
</ol>
<pre>    <strong>In [21]: formula1 = 'C(Pclass) + C(Sex) + Fare'</strong>
    <strong>         formula2 = 'C(Pclass) + C(Sex)'</strong>
    <strong>         formula3 = 'C(Sex)'</strong>
    <strong>         formula4 = 'C(Pclass) + C(Sex) + Age + SibSp + Parch'</strong>
    <strong>         formula5 = 'C(Pclass) + C(Sex) + Age + SibSp + Parch + <br/>             C(Embarked)' </strong>
    <strong>         formula6 = 'C(Pclass) + C(Sex) + Age + SibSp + C(Embarked)'</strong>
    <strong>         formula7 = 'C(Pclass) + C(Sex) + SibSp + Parch + C(Embarked)'</strong>
    <strong>         formula8 = 'C(Pclass) + C(Sex) + SibSp + Parch + C(Embarked)'</strong>
    
    <strong>In [23]: formula_map = {'PClass_Sex_Fare' : formula1,</strong>
    <strong>                        'PClass_Sex' : formula2,</strong>
    <strong>                        'Sex' : formula3,</strong>
    <strong>                        'PClass_Sex_Age_Sibsp_Parch' : formula4,</strong>
    <strong>                        'PClass_Sex_Age_Sibsp_Parch_Embarked' : <br/>                             formula5,</strong>
    <strong>           'PClass_Sex_Embarked' : formula6,</strong>
    <strong>           'PClass_Sex_Age_Parch_Embarked' : formula7,</strong>
    <strong>           'PClass_Sex_SibSp_Parch_Embarked' : formula8</strong>
    <strong>              }</strong>
  </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>We will define a function that helps us to handle missing values. The following function finds the cells within the DataFrame that have null values, obtains the set of similar passengers, and sets the null value to the mean value of that feature for the set of similar passengers. Similar passengers are defined as those having the same gender and passenger class as the passenger with the null feature value:</p>
<pre>    <strong>In [24]: </strong>
    <strong>def fill_null_vals(df,col_name):</strong>
    <strong>    null_passengers=df[df[col_name].isnull()]</strong>
    <strong>    passenger_id_list = null_passengers['PassengerId'].tolist()</strong>
    <strong>    df_filled=df.copy()</strong>
    <strong>    for pass_id in passenger_id_list:</strong>
    <strong>        idx=df[df['PassengerId']==pass_id].index[0]</strong>
    <strong>        similar_passengers = df[(df['Sex']== </strong>
    <strong>        null_passengers['Sex'][idx]) &amp; </strong>
    <strong>        (df['Pclass']==null_passengers['Pclass'][idx])]</strong>
    <strong>        mean_val = np.mean(similar_passengers[col_name].dropna())</strong>
    <strong>        df_filled.loc[idx,col_name]=mean_val</strong>
    <strong>    return df_filled</strong>
  </pre>
<p>Here, we create filled versions of our training and test DataFrames.</p>
<p>Our test DataFrame is what the fitted <kbd>scikit-learn</kbd> model will generate predictions on to produce output that will be submitted to Kaggle for evaluation:</p>
<pre>    <strong>In [28]: train_df_filled=fill_null_vals(train_df,'Fare')</strong>
    <strong>         train_df_filled=fill_null_vals(train_df_filled,'Age')</strong>
    <strong>         assert len(train_df_filled)==len(train_df)</strong>
    
    <strong>         test_df_filled=fill_null_vals(test_df,'Fare')</strong>
    <strong>         test_df_filled=fill_null_vals(test_df_filled,'Age')</strong>
    <strong>         assert len(test_df_filled)==len(test_df)</strong>
  </pre>
<p>Here is the actual implementation of the call to <kbd>scikit-learn</kbd> to learn from the training data by fitting a model and then generate predictions on the test dataset. Note that even though this is boilerplate code, for the purpose of illustration, an actual call is made to a specific algorithm—in this case, <kbd>DecisionTreeClassifier</kbd>.</p>
<p>The output data is written to files with descriptive names, for example, <kbd>csv/dt_PClass_Sex_Age_Sibsp_Parch_1.csv</kbd> and <kbd>csv/dt_PClass_Sex_Fare_1.csv</kbd>:</p>
<pre>    <strong>In [29]: </strong>
    <strong>from sklearn import metrics,svm, tree</strong>
    <strong>for formula_name, formula in formula_map.iteritems():</strong>
    <strong>        print "name=%s formula=%s" % (formula_name,formula)</strong>
    <strong>      y_train,X_train = pt.dmatrices('Survived ~ ' + formula, </strong>
    <strong>                                    train_df_filled,return_type='dataframe')</strong>
    <strong>     y_train = np.ravel(y_train)</strong>
    <strong>     model = tree.DecisionTreeClassifier(criterion='entropy', </strong>
    <strong>             max_depth=3,min_samples_leaf=5)</strong>
    <strong>     print "About to fit..."</strong>
    <strong>     dt_model = model.fit(X_train, y_train)</strong>
    <strong>     print "Training score:%s" % dt_model.score(X_train,y_train)</strong>
    <strong>     X_test=pt.dmatrix(formula,test_df_filled)</strong>
    <strong>     predicted=dt_model.predict(X_test)</strong>
    <strong>     print "predicted:%s" % predicted[:5]</strong>
    <strong>     assert len(predicted)==len(test_df)</strong>
    <strong>     pred_results = pd.Series(predicted,name='Survived')</strong>
    <strong>     dt_results = pd.concat([test_df['PassengerId'],  </strong>
    <strong>                  pred_results],axis=1)</strong>
    <strong>     dt_results.Survived = dt_results.Survived.astype(int)</strong>
    <strong>     results_file = 'csv/dt_%s_1.csv' % (formula_name)</strong>
    <strong>     print "output file: %s\n" % results_file</strong>
    <strong>     dt_results.to_csv(results_file,index=False)</strong>
  </pre>
<p>The preceding code follows a standard recipe, and the summary is as follows:</p>
<ol>
<li>Read in the training and test datasets.</li>
<li>Fill in any missing values for the features we wish to consider in both datasets.</li>
<li>Define formulas for the various feature combinations we wish to generate machine learning models for in <kbd>Patsy</kbd>.</li>
<li>For each formula, perform the following set of steps:</li>
</ol>
<p style="padding-left: 90px">1. Call <kbd>Patsy</kbd> to create design matrices for our training feature set and training label set (designated by <kbd>X_train</kbd> and <kbd>y_train</kbd>).</p>
<p style="padding-left: 90px">2. Instantiate the appropriate <kbd>scikit-learn</kbd> classifier. In this case, we use <kbd>DecisionTreeClassifier</kbd>.</p>
<p style="padding-left: 90px">3. Fit the model by calling the <kbd>fit(..)</kbd> method.</p>
<p style="padding-left: 90px">4. Make a call to <kbd>Patsy</kbd> to create a design matrix (<kbd>X_test</kbd>) for our predicted output via a call to <kbd>patsy.dmatrix(..)</kbd>.</p>
<p style="padding-left: 90px">5. Predict on the <kbd>X_test</kbd> design matrix, and save the results in the variable predicted.</p>
<p style="padding-left: 90px">6. Write our predictions to an output file, which will be submitted to Kaggle.</p>
<p class="mce-root"/>
<p>We will consider the following supervised learning algorithms:</p>
<ul>
<li>Logistic regression</li>
<li>Support vector machine</li>
<li>Random forest</li>
<li>Decision trees</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic regression</h1>
                </header>
            
            <article>
                
<p>In logistic regression, we attempt to predict the outcome of a categorical, that is, discrete-valued dependent variable based on one or more input predictor variables.</p>
<p>Logistic regression can be thought of as the equivalent of applying linear regression but to discrete or categorical variables. However, in the case of binary logistic regression (which applies to the Titanic problem), the function to which we're trying to fit is not a linear one as we're trying to predict an outcome that can take only two values—0 and 1. Using a linear function for our regression doesn't make sense as the output cannot take values between 0 and 1. Ideally, what we need to model for the regression of a binary valued output is some sort of step function for values 0 and 1. However, such a function is not well-defined and not differentiable, so an approximation with nicer properties was defined: the logistic function. The logistic function takes values between 0 and 1 but is skewed toward the extreme values of 0 and 1 and can be used as a good approximation for the regression of categorical variables. The formal definition of the logistic regression function is as follows:</p>
<p><em>          f(x) = 1/((1+e^(-ax))</em></p>
<p>The following diagram is a good illustration as to why the logistic function is suitable for binary logistic regression:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/a6c93ebc-7c4d-4a2c-991a-8dc9f77c1a32.png" style="width:20.67em;height:15.75em;"/></div>
<p>We can see that as we increase the value of our parameter, <em>a</em>, we get closer to taking on the 0 to 1 values and to the step function we wish to model. A simple application of the preceding function would be to set the output value to 0, if <em>f(x) &lt;0.5</em>, and 1 if not.</p>
<p>The code for plotting the function is included in <kbd>plot_logistic.py</kbd>.</p>
<div class="packt_infobox">A more detailed examination of logistic regression may be found at <a href="http://en.wikipedia.org/wiki/Logit"><span class="URLPACKT">http://en.wikipedia.org/wiki/Logit</span></a> and <a href="http://logisticregressionanalysis.com/86-what-is-logistic-regression"><span class="URLPACKT">http://logisticregressionanalysis.com/86-what-is-logistic-regression</span></a>.</div>
<p>In applying logistic regression to the Titanic problem, we wish to predict a binary outcome, that is, whether a passenger survived or not.</p>
<p>We adapted the boilerplate code to use the <kbd>sklearn.linear_model.LogisticRegression</kbd> class of <kbd>scikit-learn</kbd>.</p>
<p>Upon submitting our data to Kaggle, the following results were obtained:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Formula</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Kaggle Score</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>C(Pclass) + C(Sex) + Fare</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.76077</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>C(Pclass) + C(Sex)</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.76555</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>C(Sex)</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.76555</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>C(Pclass) + C(Sex) + Age + SibSp + Parch</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.74641</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(Embarked)</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.75598</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The code implementing logistic regression can be found in the <kbd>run_logistic_regression_titanic.py</kbd> file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support vector machine</h1>
                </header>
            
            <article>
                
<p>A <strong>Support Vector Machine</strong> (<strong>SVM</strong>) is a powerful supervised learning algorithm used for classification and regression. It is a discriminative classifier—it draws a boundary between clusters or classifications of data, so new points can be classified based on the cluster that they fall into.</p>
<p>SVMs do not just find a boundary line; they also try to determine margins for the boundary on either side. The SVM algorithm tries to find the boundary with the largest possible margin around it.</p>
<p class="mce-root"/>
<p>Support vectors are points that define the largest margin around the boundary—remove these points, and possibly, a larger margin can be found. Hence the name, support, as they support the margin around the boundary line. The support vectors matter. This is illustrated in the following diagram:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/303f105c-1f90-43c1-8d09-accdbeec3d20.png" style="width:20.42em;height:19.92em;"/></div>
<div class="packt_infobox">For more information on this, refer to <a href="http://winfwiki.wi-fom.de/images/c/cf/Support_vector_2.png"><span class="URLPACKT">http://winfwiki.wi-fom.de/images/c/cf/Support_vector_2.png</span></a>.</div>
<p>To use the SVM algorithm for classification, we specify one of the following three kernels: linear, poly, and <strong>rbf</strong> (also known as <strong>radial basis functions</strong>).</p>
<p>Then, we import the <strong>Support Vector Classifier</strong> (<strong>SVC</strong>):</p>
<pre>    <strong>from sklearn import svm</strong>
  </pre>
<p>We then instantiate an SVM classifier, fit the model, and predict the following:</p>
<pre>    <strong>model = svm.SVC(kernel=kernel)</strong>
    <strong>svm_model = model.fit(X_train, y_train)</strong>
    <strong>X_test = pt.dmatrix(formula, test_df_filled)</strong>
    <strong>. . .</strong>
  </pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>Upon submitting our data to Kaggle, the following results were obtained:</p>
<table style="border-collapse: collapse;width: 814px;height: 256px" class="NormalTable" border="1">
<tbody>
<tr>
<td style="width: 552.465px" class="CDPAlignCenter CDPAlign"><strong><span class="fontstyle0">Formula</span></strong></td>
<td style="width: 121.354px" class="CDPAlignCenter CDPAlign"><strong><span class="fontstyle0">Kernel Type</span></strong></td>
<td style="width: 129.132px" class="CDPAlignCenter CDPAlign"><strong><span class="fontstyle0">Kaggle Score</span></strong></td>
</tr>
<tr>
<td style="width: 552.465px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">C(Pclass) + C(Sex) + Fare</span></td>
<td style="width: 121.354px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">poly</span></td>
<td style="width: 129.132px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">0.71292</span></td>
</tr>
<tr>
<td style="width: 552.465px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">C(Pclass) + C(Sex)</span></td>
<td style="width: 121.354px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">poly</span></td>
<td style="width: 129.132px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">0.76555</span></td>
</tr>
<tr>
<td style="width: 552.465px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">C(Sex)</span></td>
<td style="width: 121.354px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">poly</span></td>
<td style="width: 129.132px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">0.76555</span></td>
</tr>
<tr>
<td style="width: 552.465px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">C(Pclass) + C(Sex) + Age + SibSp + Parch</span></td>
<td style="width: 121.354px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">poly</span></td>
<td style="width: 129.132px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">0.75598</span></td>
</tr>
<tr>
<td style="width: 552.465px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">C(Pclass) + C(Sex) + Age + Parch + C(Embarked)</span></td>
<td style="width: 121.354px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">poly</span></td>
<td style="width: 129.132px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">0.77512</span></td>
</tr>
<tr>
<td style="width: 552.465px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(embarked)</span></td>
<td style="width: 121.354px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">poly</span></td>
<td style="width: 129.132px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">0.79426</span></td>
</tr>
<tr>
<td style="width: 552.465px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(Embarked)</span></td>
<td style="width: 121.354px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">rbf</span></td>
<td style="width: 129.132px" class="CDPAlignCenter CDPAlign"><span class="fontstyle2">0.7512</span></td>
</tr>
</tbody>
</table>
<p> </p>
<p>The code can be seen in its entirety in the following file: <kbd>run_svm_titanic.py</kbd>.</p>
<p>Here, we see that the SVM with a kernel type of poly (polynomial) and the combination of the <kbd>Pclass</kbd>, <kbd>Sex</kbd>, <kbd>Age</kbd>, <kbd>Sibsp</kbd>, and <kbd>Parch</kbd> features produces the best results when submitted to Kaggle. Surprisingly, it seems as if the embarkation point (<strong>Embarked</strong>) and whether the passenger traveled alone or with family members (<strong>Sibsp + Parch</strong>) do have a material effect on a passenger's chances of survival.</p>
<p>The latter effect was probably due to the women-and-children-first policy on the Titanic.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision trees</h1>
                </header>
            
            <article>
                
<p>The basic idea behind decision trees is to use the training dataset to create a tree of decisions to make a prediction.</p>
<p>It recursively splits the training dataset into subsets based on the value of a single feature. Each split corresponds to a node in the decision tree. The splitting process is continued until every subset is pure; that is, all elements belong to a single class. This always works except in cases where there are duplicate training examples that fall into different classes. In this case, the majority class wins.</p>
<p>The end result is a ruleset for making predictions on the test dataset.</p>
<p>Decision trees encode a sequence of binary choices in a process that mimics how a human might classify things, but decide which question is most useful at each step by using the information criteria.</p>
<p class="mce-root"/>
<p>An example of this would be if you wished to determine whether animal <kbd>x</kbd> is a mammal, fish, or reptile; in this case, we would ask the following questions:</p>
<pre>    <strong>- Does x have fur?</strong>
    <strong>Yes: x is a mammal</strong>
    <strong>No: Does x have feathers?</strong>
    <strong>Yes: x is a bird</strong>
    <strong>No: Does x have scales?</strong>
    <strong>Yes: Does x have gills?</strong>
    <strong>Yes: x is a fish</strong>
    <strong>No: x is a reptile</strong>
    <strong>No: x is an amphibian</strong>
  </pre>
<p>This generates a decision tree that looks similar to the following:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/5e758343-82d7-41e7-89ab-fb37b10cd099.png"/></div>
<div class="packt_infobox">Refer to the following link for more information: <a href="https://labs.opendns.com/wp-content/uploads/2013/09/animals.gif"><span class="URLPACKT">https://labs.opendns.com/wp-content/uploads/2013/09/animals.gif</span></a>.</div>
<p class="mce-root"/>
<p>The binary splitting of questions at each node is the essence of a decision tree algorithm. A major drawback of decision trees is that they can <em>overfit</em> the data. They are so flexible that, given a large depth, they can memorize the inputs, and this results in poor results when they are used to classify unseen data.</p>
<p>The way to fix this is to use multiple decision trees, and this is known as using an ensemble estimator. An example of an ensemble estimator is the random forest algorithm, which we will address next.</p>
<p>To use a decision tree in <kbd>scikit-learn</kbd>, we import the <kbd>tree</kbd> module:</p>
<pre>    <strong>from sklearn import tree</strong></pre>
<p>We then instantiate an SVM classifier, fit the model, and predict the following:</p>
<pre>    <strong>model = tree.DecisionTreeClassifier(criterion='entropy', </strong>
    <strong>             max_depth=3,min_samples_leaf=5)</strong>
    <strong>dt_model = model.fit(X_train, y_train)<br/> X_test = dt.dmatrix(formula, test_df_filled)</strong>
    <strong>#. . .</strong></pre>
<p>Upon submitting our data to Kaggle, the following results are obtained:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Formula</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Kaggle Score</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>C(Pclass) + C(Sex) + Fare</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.77033</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>C(Pclass) + C(Sex)</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.76555</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>C(Sex)</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.76555</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>C(Pclass) + C(Sex) + Age + SibSp + Parch</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.76555</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>C(Pclass) + C(Sex) + Age + Parch +</p>
<p>C(Embarked)</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.78947</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(Embarked)</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.79426</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random forest</h1>
                </header>
            
            <article>
                
<p>Random forest is an example of a non-parametric model, as are decision trees. Random forests are based on decision trees. The decision boundary is learned from the data itself. It doesn't have to be a line or a polynomial or radial basis function. The random forest model builds upon the decision tree concept by producing a large number of, or a forest of, decision trees. It takes a random sample of the data and identifies a set of features to grow each decision tree. The error rate of the model is compared across sets of decision trees to find the set of features that produce the strongest classification model.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To use a random forest in <kbd>scikit-learn</kbd>, we import the <kbd>RandomForestClassifier</kbd> module:</p>
<pre><strong>from sklearn import RandomForestClassifier</strong>  </pre>
<p>We then instantiate a random forest classifier, fit the model, and predict the following:</p>
<pre><strong>model = RandomForestClassifier(n_estimators=num_estimators,    </strong>
    <strong>                              random_state=0)</strong>
<strong>rf_model = model.fit(X_train, y_train)</strong>
<strong>X_test = dt.dmatrix(formula, test_df_filled)</strong>
   <strong>. . .</strong></pre>
<p>Upon submitting our data to Kaggle (using the formula: <em>C(Pclass) + C(Sex) + Age + Sibsp + Parch + C(Embarked)</em>), the following results are obtained:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Formula</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Kaggle Score</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>10</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.74163</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>100</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.76077</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>1000</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.76077</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>10000</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.77990</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>100000</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.77990</p>
</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unsupervised learning algorithms</h1>
                </header>
            
            <article>
                
<p>There are two tasks that we are mostly concerned with in unsupervised learning: dimensionality reduction and clustering.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dimensionality reduction</h1>
                </header>
            
            <article>
                
<p>Dimensionality reduction is used to help to visualize higher-dimensional data systematically. This is useful because the human brain can visualize only three spatial dimensions (and possibly, a temporal one), but most datasets involve much higher dimensions.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The typical technique used in dimensionality reduction is <strong>Principal Component Analysis</strong> (<strong>PCA</strong>). PCA involves using linear algebra techniques to project higher-dimensional data onto a lower-dimensional space. This inevitably involves the loss of information, but often, by projecting along the correct set and number of dimensions, the information loss can be minimized. A common dimensionality reduction technique is to find the combination of variables that explain the most variance (proxy for information) in our data and project along those dimensions.</p>
<p>In the case of unsupervised learning problems, we do not have the set of labels (<kbd>Y</kbd>), and so, we only call <kbd>fit()</kbd> on the input data, <kbd>X</kbd>, itself, and for PCA, we call <kbd>transform()</kbd> instead of <kbd>predict()</kbd> as we're trying to transform the data into a new representation.</p>
<p>One of the datasets that we will be using to demonstrate USL is the iris dataset, possibly the most famous dataset in all of machine learning.</p>
<p>The <kbd>scikit-learn</kbd> library provides a set of pre-packaged datasets, which are available via the <kbd>sklearn.datasets</kbd> modules. The iris dataset is one of them.</p>
<p>The iris dataset consists of 150 samples of data from three different species of iris flowers—versicolor, setosa, and virginica<span>—</span>with 50 samples of each type. The dataset consists of four features/dimensions:</p>
<ul>
<li>Petal length</li>
<li>Petal width</li>
<li>Sepal length</li>
<li>Sepal width</li>
</ul>
<p>The length and width values are in centimeters. It can be loaded as follows:</p>
<pre><strong>from sklearn.datasets import load_iris </strong>
<strong>iris = load_iris()</strong></pre>
<p>In our examination of unsupervised learning, we will be focusing on how to visualize and cluster this data.</p>
<p>Before discussing unsupervised learning, let's examine the iris data a bit. The <kbd>load_iris()</kbd> command returns what is known as a bunch object, which is essentially a dictionary with keys in addition to the key containing the data. Hence, we have the following:</p>
<pre><strong>In [2]: iris_data.keys()</strong>
   <strong>Out[2]: ['target_names', 'data', 'target', 'DESCR', 'feature_names']</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Further, the data itself looks similar to the following:</p>
<pre><strong>In [3]: iris_data.data.shape</strong>
   <strong>Out[3]: (150, 4)</strong></pre>
<p>This corresponds to 150 samples of four features. These four features are shown as follows:</p>
<pre><strong>In [4]: print iris_data.feature_names</strong>
   <strong>['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']</strong></pre>
<p>We can also take a peek at the actual data:</p>
<pre><strong>In [9]: print iris_data.data[:2]</strong>
   <strong>[[ 5.1  3.5  1.4  0.2]</strong>
    <strong>[ 4.9  3.   1.4  0.2]]</strong></pre>
<p>Our target names (what we're trying to predict) look similar to the following:</p>
<pre><strong>In [10]: print iris_data.target_names</strong>
    <strong>        ['setosa' 'versicolor' 'virginica']</strong></pre>
<p>As noted earlier, the iris feature set corresponds to five-dimensional data and we cannot visualize this on a color plot. One thing that we can do is pick two features and plot them against each other while using color to differentiate between the species features. We do this next for all possible combinations of features, selecting two at a time for a set of six different possibilities. These combinations are as follows:</p>
<ul>
<li>Sepal width versus sepal length</li>
<li>Sepal width versus petal width</li>
<li>Sepal width versus petal length</li>
<li>Sepal length versus petal width</li>
<li>Sepal length versus petal length</li>
<li>Petal width versus petal length</li>
</ul>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/df7ca771-5f28-4cb2-91c3-c4558134174a.png"/></div>
<p>The code for this <span><span>can</span></span> be found in the following file: <kbd>display_iris_dimensions.py</kbd>. From the preceding plots, we can observe that the setosa points tend to be clustered by themselves, while there is a bit of overlap between the virginica and the versicolor points. This may lead us to conclude that the latter two species are more closely related to one another than to the setosa species.</p>
<p>These are, however, two-dimensional slices of data. What if we wanted a somewhat more holistic view of the data, with some representation of all four sepal and petal dimensions? What if there were some hitherto undiscovered connection between the dimensions that our two-dimensional plot wasn't showing? Is there a means of visualizing this? Enter dimensionality reduction. We will use dimensionality reduction to extract two combinations of sepal and petal dimensions to help to visualize it.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>We can apply dimensionality reduction to do this as follows:</p>
<pre>    <strong>In [118]: X, y = iris_data.data, iris_data.target</strong>
    <strong>              from sklearn.decomposition import PCA</strong>
    <strong>              pca = PCA(n_components=2)</strong>
    <strong>              pca.fit(X)</strong>
    <strong>              X_red=pca.transform(X)</strong>
    <strong>              print "Shape of reduced dataset:%s" % str(X_red.shape)</strong>
    
    <strong>         Shape of reduced dataset:(150, 2)</strong></pre>
<p>Hence, we see that the reduced dataset is now in two dimensions. Let's display the data visually in two dimensions as follows:</p>
<pre>    <strong>In [136]: figsize(8,6)</strong>
    <strong>          fig=plt.figure()</strong>
    <strong>          fig.suptitle("Dimensionality reduction on iris data")</strong>
    <strong>          ax=fig.add_subplot(1,1,1)</strong>
    <strong>          colors=['red','yellow','magenta']</strong>
    <strong>          cols=[colors[i] for i in iris_data.target]</strong>
    <strong>          ax.scatter(X_red[:,0],X[:,1],c=cols)</strong>
    <strong>Out[136]:</strong>
    <strong>&lt;matplotlib.collections.PathCollection at 0x7fde7fae07d0&gt;</strong></pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/2b93c4f6-41e2-4b88-b741-428741ef005d.png" style="width:31.83em;height:26.33em;"/></div>
<p>We can examine the makeup of the two PCA-reduced dimensions as follows:</p>
<pre>    <strong>In [57]:</strong>
    <strong>print "Dimension Composition:"</strong>
    <strong>idx=1</strong>
    <strong>for comp in pca.components_:</strong>
    <strong>    print "Dim %s" % idx</strong>
    <strong>    print " + ".join("%.2f x %s" % (value, name)</strong>
    <strong>                     for value, name in zip(comp, iris_data.feature_names))</strong>
    <strong>    idx += 1</strong>
    
    <strong>Dimension Composition:</strong>
    <strong>Dim 1</strong>
    <strong>0.36 x sepal length (cm) + -0.08 x sepal width (cm) + 0.86 x petal length (cm) + 0.36 x petal width (cm)</strong>
    <strong>Dim 2</strong>
    <strong>-0.66 x sepal length (cm) + -0.73 x sepal width (cm) + 0.18 x petal length (cm) + 0.07 x petal width (cm)</strong></pre>
<p>Hence, we can see that the two reduced dimensions are a linear combination of all four sepal and petal dimensions.</p>
<p>The source of this information is at <a href="https://github.com/jakevdp/sklearn_pycon2014"><span class="URLPACKT">https://github.com/jakevdp/sklearn_pycon2014</span></a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-means clustering</h1>
                </header>
            
            <article>
                
<p>The idea behind clustering is to group together similar points in a dataset based on a given criterion, hence finding clusters in the data.</p>
<p>The K-means algorithm aims to partition a set of data points into <em>K</em> clusters such that each data point belongs to the cluster with the nearest mean point or centroid.</p>
<p>To illustrate K-means clustering, we can apply it to the set of reduced iris data that we obtained via PCA, but in this case, we do not pass the actual labels to the <kbd>fit(..)</kbd> method as we do for supervised learning:</p>
<pre>    <strong>In [142]: from sklearn.cluster import KMeans</strong>
    <strong>          k_means = KMeans(n_clusters=3, random_state=0)</strong>
    <strong>          k_means.fit(X_red)</strong>
    <strong>          y_pred = k_means.predict(X_red)</strong></pre>
<p>We now display the clustered data as follows:</p>
<pre>    <strong>In [145]: figsize(8,6)</strong>
    <strong>          fig=plt.figure()</strong>
    <strong>          fig.suptitle("K-Means clustering on PCA-reduced iris data, <br/>              K=3")</strong>
    <strong>          ax=fig.add_subplot(1,1,1)</strong>
    <strong>          ax.scatter(X_red[:, 0], X_red[:, 1], c=y_pred);</strong></pre>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/356cef4d-149b-4986-9f9d-adf0907aa1d9.png" style="width:35.00em;height:28.42em;"/></div>
<p>Note that our K-means algorithm clusters do not exactly correspond to the dimensions obtained via PCA. The source code is available at <a href="https://github.com/jakevdp/sklearn_pycon2014"><span class="URLPACKT">https://github.com/jakevdp/sklearn_pycon2014</span></a>.</p>
<div class="packt_infobox">More information on K-means clustering in <kbd>scikit-learn</kbd> and, in general can be found at <a href="http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html"><span class="URLPACKT">http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html</span></a> and <a href="http://en.wikipedia.org/wiki/K-means_clustering"><span class="URLPACKT">http://en.wikipedia.org/wiki/K-means_clustering</span></a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">XGBoost case study</h1>
                </header>
            
            <article>
                
<p><strong>XGBoost</strong> is an ensemble algorithm popular for its outstanding performance. An ensemble algorithm involves multiple models instead of just one. Ensemble algorithms are of two types:</p>
<ul>
<li><strong>Bagging</strong>: Here, a result from the algorithm is the average of results from individual models.</li>
<li><strong>Boosting</strong>: Here, we start from a base learner model. Each successive model gets created with better-trained parameters. The learning of new parameters happens through optimization algorithms such as gradient descent.</li>
</ul>
<p>Next, we will look at the application of XGBoost on a dataset to predict the testing time of a newly manufactured car.</p>
<p>It is a step-by-step guide and you can just follow along:</p>
<ol>
<li>Import the required packages: </li>
</ol>
<pre style="padding-left: 60px"><strong># In[1]:</strong><br/>    <strong>import os</strong>
    <strong>import pandas as pd</strong>
    <strong>import numpy as np</strong>
    <strong>from sklearn import preprocessing</strong>
    <strong>from sklearn.decomposition import PCA</strong>
    <strong>from sklearn.preprocessing import scale</strong>
    <strong>import matplotlib.pyplot as plt</strong>
    <strong>import xgboost</strong></pre>
<ol start="2">
<li>Change the working directory:</li>
</ol>
<pre style="padding-left: 60px"><strong># In[24]:</strong><br/>    <strong>os.chdir('C:/')</strong></pre>
<ol start="3">
<li>Read the train and test data:</li>
</ol>
<pre style="padding-left: 60px"><strong># In[19]:</strong><br/>    <strong>train = pd.read_csv('train.csv')</strong>
    <strong>test = pd.read_csv('test.csv')</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li>Prepare for removing columns with zero variance:</li>
</ol>
<pre style="padding-left: 60px"><strong># In[31]:</strong><br/>    <strong>#train.dtypes[train.dtypes=='float64'or train.dtypes == 'int64']</strong>
    <strong>varcs = train.var(axis=0)</strong>
    <strong>varcs = varcs[varcs == 0]</strong>
    <strong>to_drop = list(varcs.index)</strong>
    <strong>dt = train.drop(to_drop, axis=1)</strong>
    <strong>print("The variables {} have been dropped as they have zero variance".format(to_drop))</strong></pre>
<ol start="5">
<li>Input the function for removing columns with zero variance:</li>
</ol>
<pre><strong># In[20]:</strong><br/><strong># drops the variables with zero variance in a given dataset</strong><br/>    <strong>def drop_zerovarcs(data):</strong>
    <strong>    varcs = data.var(axis=0)</strong>
    <strong>    varcs = varcs[varcs == 0]</strong>
    <strong>    to_drop = list(varcs.index)</strong>
    <strong>    #data_new = data.drop(to_drop, axis=1, inplace=True)</strong>
    <strong>    print("The variables {} have zero variance".format(to_drop))</strong>
    <strong>    return to_drop</strong></pre>
<ol start="6">
<li>Get a list of zero variance columns in the train and test datasets:</li>
</ol>
<pre style="padding-left: 60px"><strong># drops columns from test where variance is zero in test data as well as the columns for which variance is zero in train data<br/># In[21]:<br/></strong>    <strong>test_drops = drop_zerovarcs(test)</strong>
    <strong>train_drops = drop_zerovarcs(train)</strong>
    <strong>test_train_drop = [x for x in train_drops if x not in test_drops]</strong>
  <br/><strong># train and test have different columns which have zero variance</strong><br/><strong># Hence dropping the same columns in train and test data. Dropping the columns with zero variance in train data from test data.  </strong> <br/><strong>test.drop(test_train_drop, axis=1,inplace=True)</strong></pre>
<ol start="7">
<li> Remove zero variance columns in the train data from test data:</li>
</ol>
<pre style="padding-left: 60px"><strong># In[22]:</strong><br/><strong># drop the columns in test for which variance is zero in train data</strong><em><br/></em>    <strong>train.drop(train_drops, axis=1,inplace=True)</strong>
    <strong>#len(list(train.drop(train_drops,axis=1).columns))</strong>
    <strong>test.drop(train_drops,axis=1,inplace=True)</strong>
    <strong>#len(list(test.drop(train_drops,axis=1).columns))</strong></pre>
<p class="mce-root"/>
<ol start="8">
<li><span>Find</span> <kbd>Unique, Total Count and NAs</kbd> <span>and write it to a CSV file:</span></li>
</ol>
<pre style="padding-left: 60px"><strong># In[25]:</strong><br/><strong># Find Unique, Total Count and NAs</strong><br/>    <strong>def uni_ct_na(data):</strong>
    <strong>    unique = data.apply(lambda x: x.nunique(), axis=0)</strong>
    <strong>    count = data.apply(lambda x: x.count(), axis=0)</strong>
    <strong>    null = data.isnull().sum()</strong>
    <strong>    na = data.isna().sum()</strong>
    <strong>    summary_df = pd.DataFrame([unique, count, null, na],index=['Unique', 'Count', 'Null', 'NA'])</strong>
    <strong>    summary_df.T.to_csv('summary_df.csv')</strong>
  <br/><strong># In[26]:</strong><br/>    <strong>uni_ct_na(train)</strong></pre>
<ol start="9">
<li>Find a list of categorical variables:</li>
</ol>
<pre style="padding-left: 60px"><strong># In[27]:<br/></strong><strong>#Finding the list of categorical variables<br/></strong>    <strong>obj = list(train.dtypes[train.dtypes=='object'].index)</strong></pre>
<ol start="10">
<li>Create dummy variables from the categorical variables:</li>
</ol>
<pre style="padding-left: 60px"><strong># In[28]:<br/></strong><strong>#Dummy variables using categorical variables<br/></strong>   <strong>obj_dum_train = pd.get_dummies(train[obj])<br/></strong>    <strong>train = pd.concat([train,obj_dum_train],axis=1).drop(obj,axis=1)</strong>
    <strong>obj_dum_test = pd.get_dummies(test[obj])</strong>
    <strong>test = pd.concat([test,obj_dum_test],axis=1).drop(obj,axis=1)</strong></pre>
<ol start="11">
<li>Delete the categorical variables from <kbd>train</kbd> and <kbd>test</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong># In[29]:<br/></strong><strong># Keeping only numeric variables to apply PCA<br/></strong>    <strong>train_cols = train.columns</strong>
    <strong>train_not_obj = [x for x in train_cols if x not in obj]</strong>
    <strong>train = train[train_not_obj]</strong>
    
    <strong>test_cols = test.columns</strong>
    <strong>test_not_obj = [x for x in test_cols if x not in obj]</strong>
    <strong>test = test[test_not_obj]</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="12">
<li>Plot a scree plot to get the number of components that will explain the 90% variance in the data:</li>
</ol>
<pre style="padding-left: 60px"><strong># In[30]:<br/></strong><strong># Plotting Scree plot to get the number of components which will<br/>explain 90% variance in data<br/></strong>    <strong>X=train.iloc[:,1:].values</strong>
    <strong>X = scale(X)</strong>
    <strong>pca = PCA()</strong>
    <strong>pca.fit(X)</strong>
    <strong>var= pca.explained_variance_ratio_</strong>
    <strong>var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)</strong>
    <strong>plt.plot(var1)</strong></pre>
<ol start="13">
<li><span>Perform PCA on the train and test data:</span></li>
</ol>
<pre style="padding-left: 60px"><strong># In[31]:<br/></strong><strong># Performing PCA on train and test data<br/></strong><span>    </span><strong>X=train.iloc[:,1:].values<br/></strong>    <strong>X = scale(X)</strong>
    <strong>pca = PCA(n_components=300)</strong>
    <strong>pca.fit(X)</strong>
    <strong>train_pca = pca.transform(X)</strong>
    <strong>train_pca.shape</strong>
    
    <strong>X_test=train.iloc[:,1:].values</strong>
    <strong>X_test = scale(X)</strong>
    <strong>test_pca = pca.transform(X_test)</strong></pre>
<ol start="14">
<li>Separate the <kbd>x</kbd> and <kbd>y</kbd> variables to be passed to <kbd>xgboost</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong># In[32]:</strong><br/><strong># Separating x and y variables to be passed to xgboost</strong><br/>    <strong>train_y = train_pca[:,1]</strong>
    <strong>train_x = train_pca[:,2:]</strong>
    <strong>test_y = test_pca[:,1]</strong>
    <strong>test_x = test_pca[:,2:]</strong></pre>
<ol start="15">
<li>Define the <kbd>xgboost</kbd> model:</li>
</ol>
<pre style="padding-left: 60px"><strong># In[33]:<br/></strong><strong># Fitting a xgboost model with default options<br/></strong>    <strong>model = xgboost.XGBRegressor() </strong>
    <strong>model.fit(train_x, train_y)</strong></pre>
<p class="mce-root"/>
<ol start="16">
<li><span>Predict from the <kbd>xgboost</kbd> model:</span></li>
</ol>
<pre style="padding-left: 60px"><strong># In[34]:<br/></strong><strong># Predict from the model on test data<br/></strong>    <strong>pred_y = model.predict(test_x)
  </strong><br/><strong># In[189]:</strong><br/>    <strong>test_y</strong></pre>
<ol start="17">
<li>Calculate the root mean square error:</li>
</ol>
<pre style="padding-left: 60px"><strong># In[35]:<br/></strong><strong># Calculating Root Mean Square Error<br/></strong>    <strong>rmse = np.sqrt(np.sum((pred_y-test_y)**2)/len(pred_y))</strong>
    <strong>rmse</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Entropy</h1>
                </header>
            
            <article>
                
<p>Entropy is a measure of the homogeneity (or heterogeneity) of data. The more homogeneous the data, the more entropy it has. Please keep in mind that, to make a better classification decision, heterogeneous data is better.</p>
<p>For example, consider a dataset where 1,000 people were surveyed about whether they smoke or not. In the first case, let's say that 500 people said yes and 500 said no. In the second case, let's assume that 800 people said yes and 200 said no. In which case <span>would</span><span> </span><span>the entropy be more?</span></p>
<p>Yes, you guessed right. It is the first one because it is more homogeneous or, in other words, the decisions are equally distributed. If a person had to guess whether a survey participant answered yes or no, without knowing the actual answer, then the chances of them getting the right answer are less in the first case. Hence, we say that the data is messier in terms of classification information and hence has more entropy.</p>
<p>The goal of any classification problem, especially decision tree (and hence random forest and XGBoost), is to decrease this entropy and gain information. Next, let's see how we can quantify this seemingly qualitative term.</p>
<p>Equation for calculating entropy for the overall dataset is mathematically defined as follows:</p>
<p style="padding-left: 240px"><img src="assets/cca91546-a2b0-4241-a59a-3c5f7af90e04.png" style="width:12.92em;height:3.83em;"/></p>
<p>Here, <em>p<sub>i</sub></em> is the proportion of the dataset with the <em>i</em><sup>th</sup> class.</p>
<p class="mce-root"/>
<p>For example, in the first case we suggested earlier, <em>p<sub>yes</sub></em> would be 500/1,000 and <em>p<sub>no</sub></em> would be 500/1,000.</p>
<p>The following diagram shows the variation of entropy (the <em>y</em> variable) as the <em>p<sub>i</sub></em> (the <em>x</em> variable) changes from 0 to 1. Please notice that the <em>p<sub>i</sub>s</em> have been multiplied by 100 for plotting purposes:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a39a2df8-a350-4bf7-8c0b-19503b03a95b.png" style="width:29.33em;height:19.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Plot of entropy versus fraction / proportion (0 to 1)</div>
<p>Observe the following about the graph:</p>
<ul>
<li>It's almost symmetric, about <em>p=0.5</em>.</li>
<li>It maximizes at <em>p=0.5</em>, which makes sense as the messiness is maximal when the two classes are equally distributed.</li>
</ul>
<p>The code used to generate this plot is as follows:</p>
<pre><strong> import matplotlib.pyplot as plt %matplotlib inline entropies = [-(p/100)*np.log2(p/100) for p in range(1,101)] plt.plot(entropies)</strong></pre>
<p>Next, let's see how we can write a function using pandas to calculate the entropy of a dataset on its own and of a column from a dataset. For this purpose, we can first create dummy data with two columns: <kbd>purchase</kbd> (the <em>y</em> variable) and <kbd>se_status</kbd> (the predictor variables).</p>
<p>Define the unique values of the categorical variables:</p>
<pre><strong> se_status_u = ['Rich','Poor','Affluent'] </strong><br/><strong>purchase_u = ['yes','no','yes']<br/><br/># Creating the dataframe with 10,000 rows and 2 columns viz. purchase and se_status
<span>import random <br/></span>import pandas as pd <br/><span>se_status = []<br/></span><span>purchase = []<br/></span><span>for i in range(10000):
    </span>se_status.append(random.choice(se_status_u))<span>
    </span>purchase.append(random.choice(purchase_u))<span>
</span>df = pd.DataFrame({'se_status':se_status,'purchase':purchase})</strong></pre>
<p>Next, we write a function to calculate the initial entropy of a dataset given the dataset and the name of the <em>y</em> variable:</p>
<pre># Function for calculating initial entropy of the dataframe<br/>def int_entropy(df, ycol):<span>
    </span>    y_u = list(df[ycol].unique())<span>
    </span>    p = [df[df[ycol] == res].shape[0]/df.shape[0] for res in y_u]<span>
    </span>    entropy = np.sum([-(e*np.log2(e)) for e in p]) <span>
    </span>    return entropy<br/><span>
</span>df_int_entropy = int_entropy(df,'purchase')<span>
</span>df_int_entropy</pre>
<p>Once we have the initial entropy, the next goal is to find the entropy assuming that one of the predictor variables was used for classification. To calculate entropy for such a case, we follow these steps:</p>
<ol>
<li>
<p>Subset the data based on categories in the particular predictor column—one dataset for one category.</p>
</li>
<li>
<p>Calculate the entropy for each of these datasets so that you have one entropy value for each category of the variable.</p>
</li>
<li>
<p>Take a weighted average of these entropies. Weights are given by the proportion of that category in that dataset.</p>
</li>
</ol>
<p>Mathematically, it can be represented by the following:</p>
<p style="padding-left: 150px"><img src="assets/7b052203-f238-4dd5-a011-69f464bbc0b5.png" style="width:20.17em;height:5.58em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Equation for calculating entropy for a column</div>
<p>Here, <em>f<sub>j</sub></em><sub> </sub>represents the proportion of the <em>i<sup><sub>th</sub></sup></em> category in the dataset, and <em>p<sub>ij</sub></em> represents the proportion of the <em>j<sub>th</sub></em> category of the <em>y</em> variable in the dataset for the <em>i</em><sup>th</sup> category of the predictor column. Let's see how a function can be written to calculate this entropy for a given dataset, the <em>y</em> variable, and a predictor variable:</p>
<pre><strong><strong># Function for calculating entropy of a particular column of the dataframe</strong> </strong> <br/><strong>def col_entropy(df,ycol,col):</strong> <strong> <br/>    y_u = df[ycol].unique()</strong> <strong> <br/>    col_u = df[col].unique()</strong> <strong> <br/>    ent_colval = []</strong> <strong> <br/>    final_ent_col = 0 </strong> <strong> <br/>    for colval in col_u:</strong> <strong> <br/>        p = [(df[(df[ycol] == yval) &amp; (df[col] == colval)]).shape[0]/(df[col] == colval).shape[0] for yval in y_u]</strong> <strong> ent_colval = np.sum([-(e*np.log2(e)) for e in p]) </strong> <strong> <br/>final_ent_col += ent_colval* ((df[df[col] == colval]).shape[0]/(df.shape[0]))</strong> <strong> return final_ent_col</strong></pre>
<p>Information gain is defined as the reduction in entropy when we move from making a classification decision based on only the <kbd>y</kbd> variable distribution to making this decision based on a column. This can be calculated as follows:</p>
<pre>    <strong>df_se_entropy = col_entropy(df,'purchase','se_status')</strong>
    <strong>print(df_int_entropy)</strong>
    <strong>information_gain = df_int_entropy - df_se_entropy</strong>
    <strong>print(information_gain)</strong></pre>
<p>For the dataset that I have in this instance, I got an information gain of around 0.08. While making a decision tree, this information gain is calculated for every column. The column with the highest information gain is selected as the next branching node in the tree.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we embarked on a whirlwind tour of machine learning, examining the role of pandas in feature extraction, selection, and engineering as well as learning about key concepts in machine learning such as supervised versus unsupervised learning. We also had a brief introduction to a few key algorithms in both methods of machine learning, and we used the <kbd>scikit-learn</kbd> package to utilize these algorithms to learn and make predictions on data. This chapter was not intended to be a comprehensive treatment of machine learning, but rather to illustrate how pandas can be used to assist users in the machine learning space.</p>


            </article>

            
        </section>
    </body></html>