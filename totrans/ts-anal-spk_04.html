<html><head></head><body>
<div id="_idContainer062">
<h1 class="chapter-number" id="_idParaDest-87"><a id="_idTextAnchor087"/><span class="koboSpan" id="kobo.1.1">4</span></h1>
<h1 id="_idParaDest-88"><a id="_idTextAnchor088"/><span class="koboSpan" id="kobo.2.1">End-to-End View of a Time Series Analysis Project</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Building on the foundation set in the previous chapters in which we were introduced to time series analysis, its multiple use cases, and Apache Spark, a key tool for such analysis, this chapter guides us through the entire process of a time series analysis project. </span><span class="koboSpan" id="kobo.3.2">Starting with use cases, we will move on to the end-to-end approach with DataOps, ModelOps, and DevOps. </span><span class="koboSpan" id="kobo.3.3">We will cover key stages such as data processing, feature engineering, model selection, and evaluation, offering practical insights into building a time series analysis pipeline with Spark and </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">other tools.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">This holistic view of a time series analysis project will equip us with a structured approach to handling real-world projects, enhancing our ability to implement end-to-end solutions. </span><span class="koboSpan" id="kobo.5.2">The information here will guide us as practitioners through a framework for using Spark in a cohesive manner and ensuring the successful execution of time series analysis projects. </span><span class="koboSpan" id="kobo.5.3">We will conclude with two approaches </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">for implementation.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">We will cover the following topics in </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">this chapter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.9.1">Driven by </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">use cases</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">From DataOps to ModelOps </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">to DevOps</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Implementation examples </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">and tools</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.15.1">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">get started!</span></span></p>
<h1 id="_idParaDest-89"><a id="_idTextAnchor089"/><span class="koboSpan" id="kobo.17.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.18.1">The hands-on part of this chapter will be to implement end-to-end examples for a time series analysis project. </span><span class="koboSpan" id="kobo.18.2">The code for this chapter can be found in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.19.1">ch4</span></strong><span class="koboSpan" id="kobo.20.1"> folder of the GitHub repository at the </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">following link:</span></span></p>
<p><a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch4"><span class="No-Break"><span class="koboSpan" id="kobo.22.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch4</span></span></a></p>
<p><span class="koboSpan" id="kobo.23.1">The hands-on section of this chapter (</span><em class="italic"><span class="koboSpan" id="kobo.24.1">Implementation examples and tools</span></em><span class="koboSpan" id="kobo.25.1">) will go into further detail. </span><span class="koboSpan" id="kobo.25.2">This requires some skills in building an open source environment. </span><span class="koboSpan" id="kobo.25.3">If you do not intend to build your own Apache Spark environment and your focus is instead on time series and using but not deploying Spark and other tools, you can skip the hands-on section of this chapter. </span><span class="koboSpan" id="kobo.25.4">You can use a managed platform such as Databricks, which comes pre-built with Spark, MLflow, and tools for workflows and notebooks, as we will do in </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">future chapters.</span></span></p>
<h1 id="_idParaDest-90"><a id="_idTextAnchor090"/><span class="koboSpan" id="kobo.27.1">Driven by use cases</span></h1>
<p><span class="koboSpan" id="kobo.28.1">Before we get into the </span><em class="italic"><span class="koboSpan" id="kobo.29.1">how</span></em><span class="koboSpan" id="kobo.30.1"> of doing an end-to-end time series analysis project, as always, it is good to start with the </span><em class="italic"><span class="koboSpan" id="kobo.31.1">why</span></em><span class="koboSpan" id="kobo.32.1">. </span><span class="koboSpan" id="kobo.32.2">There can be many reasons, often a combination, to justify the inception of a time series analysis project. </span><span class="koboSpan" id="kobo.32.3">Some of the reasons are </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.34.1">Technology refresh</span></strong><span class="koboSpan" id="kobo.35.1">: The emphasis can be on the technology due to an aging platform needing replacement and not being able to meet requirements anymore, or when a new</span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.36.1"> technology is available, offering better performance, lower costs, or more capabilities, such as advanced machine learning models or scalable </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">cloud-based resources.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.38.1">Research on methods</span></strong><span class="koboSpan" id="kobo.39.1">: For organizations or departments focused on research, the main driver is finding new and better methods such as developing and testing new algorithms for analyzing </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">time series.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.41.1">Data exploration</span></strong><span class="koboSpan" id="kobo.42.1">: Similar to research in its nature, but requiring to be nearer to the data, this is usually embedded within businesses’ data teams. </span><span class="koboSpan" id="kobo.42.2">The need here is to understand time series data without necessarily a predefined end application. </span><span class="koboSpan" id="kobo.42.3">The objective is to uncover patterns, trends, and anomalies in </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">the data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.44.1">Use case</span></strong><span class="koboSpan" id="kobo.45.1">: With this approach, we begin with the end in mind, first identifying specific needs and expectations of the end users or stakeholders. </span><span class="koboSpan" id="kobo.45.2">We then set the project to answer those </span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.46.1">needs based on the analysis of time </span><span class="No-Break"><span class="koboSpan" id="kobo.47.1">series data.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.48.1">While all the preceding reasons have their merit and are certainly valid, over the years, I have seen the business-driven use case approach as the one with the highest return on investment. </span><span class="koboSpan" id="kobo.48.2">We started the discussion on time series-based use cases across various industries in </span><a href="B18568_02.xhtml#_idTextAnchor044"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.49.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.50.1">, such as inventory forecasting, predicting energy usage, financial market trend analysis, or anomaly detection in sensor data; here, we will focus on this use case-driven approach and take </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">it further.</span></span></p>
<p><span class="koboSpan" id="kobo.52.1">The use case approach first identifies and defines real-world specific business applications or challenges. </span><span class="koboSpan" id="kobo.52.2">It then chooses the technical solution best fit to address these requirements. </span><span class="koboSpan" id="kobo.52.3">At first glance, this does not sound very different from any project in a business setup. </span><span class="koboSpan" id="kobo.52.4">The key difference here is highlighted by the word “specific” in that the use case approach is about a specific, measurable business outcome. </span><span class="koboSpan" id="kobo.52.5">This follows a lean approach in the sense that we want to avoid features that do not contribute to the </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">business outcome.</span></span></p>
<p><span class="koboSpan" id="kobo.54.1">Use cases can be compared to user stories in the agile approach to software development. </span><span class="koboSpan" id="kobo.54.2">As a matter of fact, the agile methodology is often how the use cases are implemented, with a streamlined iterative development process involving users all </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">the way.</span></span></p>
<p><span class="koboSpan" id="kobo.56.1">The following </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.57.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.58.1">.1</span></em><span class="koboSpan" id="kobo.59.1"> gives an overview of the use case driven approach, based on what has been discussed so far, including their </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">key characteristics.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer050">
<span class="koboSpan" id="kobo.61.1"><img alt="" src="image/B18568_04_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.62.1">Figure 4.1: Use case driven approach</span></p>
<p><span class="koboSpan" id="kobo.63.1">Now that we have defined the use case-driven approach, we will look at the key characteristics of this approach, </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.65.1">Business outcome</span></strong><span class="koboSpan" id="kobo.66.1">: Project success is measured by the outcome in terms of business metrics on</span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.67.1"> higher revenue, cost reduction, efficiency gain, and better and </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">quicker decision-making.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.69.1">User-focused</span></strong><span class="koboSpan" id="kobo.70.1">: Working from the start with the end users and stakeholders to identify their specific needs, the project’s objectives include answering those needs, in addition to the preceding </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">business outcomes.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.72.1">Specific</span></strong><span class="koboSpan" id="kobo.73.1">: We’ve discussed this term a couple of times. </span><span class="koboSpan" id="kobo.73.2">The specificity of a project provides a focused direction to its scope, making its execution more agile. </span><span class="koboSpan" id="kobo.73.3">We want to address a specific need, for example, sales forecasting, and this can be even more granular, such as forecasting for a specific line of product </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">or region.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.75.1">Iterative</span></strong><span class="koboSpan" id="kobo.76.1">: Feedback and refinement loops involving end users and stakeholders ensure the project remains on track to meet the expected business outcomes. </span><span class="koboSpan" id="kobo.76.2">This again highlights the similarity to an agile approach with its short development cycles, incremental delivery, continuous feedback, </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">and adaptability.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.78.1">Adhering to these characteristics, the use cases are scoped small enough to be achievable and bring value within a few months, if not even weeks. </span><span class="koboSpan" id="kobo.78.2">These smaller use cases usually mean that there are several of them competing in parallel for development resources. </span><span class="koboSpan" id="kobo.78.3">This requires prioritization to ensure that resources are well invested. </span><span class="koboSpan" id="kobo.78.4">The following criteria are commonly used to prioritize </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">use cases:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.80.1">Impact</span></strong><span class="koboSpan" id="kobo.81.1">: This is a measurement </span><a id="_idIndexMarker355"/><span class="koboSpan" id="kobo.82.1">of the expected business impact of the use case, preferably calculated in monetary value. </span><span class="koboSpan" id="kobo.82.2">If the outcome is a reduction in time, the equivalent monetary value of the time saving </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">is estimated.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.84.1">Cost</span></strong><span class="koboSpan" id="kobo.85.1">: We need to account for all costs related to the use case from the moment of use case ideation to the time the use case is live in production and bringing value to the business. </span><span class="koboSpan" id="kobo.85.2">Costs can be related to development, infrastructure, migration, training, support, and </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">production operations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.87.1">Return on investment</span></strong><span class="koboSpan" id="kobo.88.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.89.1">ROI</span></strong><span class="koboSpan" id="kobo.90.1">): This can be simply estimated by dividing the impact by the cost. </span><span class="koboSpan" id="kobo.90.2">As an </span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.91.1">example, for a retailer who wants to better forecast stocks in stores, if the total cost to get the stock forecasting use case in production is $50k, and the improvement in stock forecasting is estimated to bring $200k in savings over 3 years, the ROI is 4x over </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">this period.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.93.1">Technical feasibility</span></strong><span class="koboSpan" id="kobo.94.1">: The technical solution for the use case exists and can be achieved within time </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">and budget.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.96.1">Data availability and accessibility</span></strong><span class="koboSpan" id="kobo.97.1">: Data is available and accessible to build the use case and then </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">operationalize it.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.99.1">Using the preceding criteria, in the case of competing need for resources, a use case with a high impact and an ROI of 10x that is feasible and has data available is done before another use case with a lower impact and an ROI of 3x, or before one that does not have </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">data access.</span></span></p>
<p><span class="koboSpan" id="kobo.101.1">In summary, starting with a clear understanding of the user’s needs, a use case-based project ensures applicability and relevance to the business, closely aligning with the stakeholders’ objectives, with measurable impact. </span><span class="koboSpan" id="kobo.101.2">Having a good use case is only the start, though. </span><span class="koboSpan" id="kobo.101.3">We will now deep-dive into the next steps, from the use case to the successful completion of a time series analysis project that delivers </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">business outcomes.</span></span></p>
<h1 id="_idParaDest-91"><a id="_idTextAnchor091"/><span class="koboSpan" id="kobo.103.1">From DataOps to ModelOps to DevOps</span></h1>
<p><span class="koboSpan" id="kobo.104.1">Once a significant use case has been identified, a few phases play a crucial role, starting from </span><strong class="bold"><span class="koboSpan" id="kobo.105.1">data operations</span></strong><span class="koboSpan" id="kobo.106.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.107.1">DataOps</span></strong><span class="koboSpan" id="kobo.108.1">) to </span><strong class="bold"><span class="koboSpan" id="kobo.109.1">model operations</span></strong><span class="koboSpan" id="kobo.110.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.111.1">ModelOps</span></strong><span class="koboSpan" id="kobo.112.1">) and finally to </span><strong class="bold"><span class="koboSpan" id="kobo.113.1">deployment</span></strong><span class="koboSpan" id="kobo.114.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.115.1">DevOps</span></strong><span class="koboSpan" id="kobo.116.1">) to a live</span><a id="_idIndexMarker357"/><span class="koboSpan" id="kobo.117.1"> business environment delivering value. </span><span class="koboSpan" id="kobo.117.2">A solid end-to-end process covering these phases ensures</span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.118.1"> that we can consistently deliver from one use case to the next while ensuring that the results are </span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.119.1">reproducible. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.120.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.121.1">.2</span></em><span class="koboSpan" id="kobo.122.1"> gives an overview of these phases, which will be detailed in </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">this section.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer051">
<span class="koboSpan" id="kobo.124.1"><img alt="" src="image/B18568_04_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.125.1">Figure 4.2: DataOps, ModelOps, and DevOps</span></p>
<h2 id="_idParaDest-92"><a id="_idTextAnchor092"/><span class="koboSpan" id="kobo.126.1">DataOps</span></h2>
<p><span class="koboSpan" id="kobo.127.1">DataOps for a time series analysis </span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.128.1">project encompasses best practices and processes to ensure the flow, quality, and access to time series data as part of its life cycle. </span><span class="koboSpan" id="kobo.128.2">The aim is for timely, efficient, and accurate time series analysis and modeling to derive actionable </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">business insights.</span></span></p>
<p><span class="koboSpan" id="kobo.130.1">DataOps practices follow the complete data and metadata life cycle and can be broken down broadly into data source integration, data processing, data governance, and </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">data sharing.</span></span></p>
<h3><span class="koboSpan" id="kobo.132.1">Source integration</span></h3>
<p><span class="koboSpan" id="kobo.133.1">Data source integration involves first </span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.134.1">identifying the data source and gaining access, and then ingesting data from </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">the source.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.136.1">Data sources</span></strong><span class="koboSpan" id="kobo.137.1"> can be internal or </span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.138.1">external. </span><span class="koboSpan" id="kobo.138.2">Internal sources are primarily databases such as transactional records, system logs, or sensor data for telemetry. </span><span class="koboSpan" id="kobo.138.3">External sources include examples such as market data, weather data, or social media, which is now becoming predominant. </span><span class="koboSpan" id="kobo.138.4">Sources vary greatly across industries in volume, frequency of updates, and data format. </span><span class="koboSpan" id="kobo.138.5">Once the sources have been identified and accessed, </span><strong class="bold"><span class="koboSpan" id="kobo.139.1">data ingestion</span></strong><span class="koboSpan" id="kobo.140.1"> is the process of bringing the data into the platform for</span><a id="_idIndexMarker363"/><span class="koboSpan" id="kobo.141.1"> processing. </span><span class="koboSpan" id="kobo.141.2">This is usually achieved with </span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.142.1">automated ingestion pipelines, running in batches at specific frequencies (hourly, daily, etc.) or streaming continuously. </span><span class="koboSpan" id="kobo.142.2">Mechanisms for ingestion include database connections, API calls, or web-based scraping, </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">among others.</span></span></p>
<h3><span class="koboSpan" id="kobo.144.1">Processing and storage</span></h3>
<p><span class="koboSpan" id="kobo.145.1">Data processing includes cleaning up the data, transforming it into the right format, and storing it for analysis. </span><span class="koboSpan" id="kobo.145.2">A recommended </span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.146.1">approach is the medallion approach, as illustrated in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.147.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.148.1">.3</span></em><span class="koboSpan" id="kobo.149.1">, which involves multiple stages of processing from raw data to curated to </span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">report-ready data.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer052">
<span class="koboSpan" id="kobo.151.1"><img alt="" src="image/B18568_04_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.152.1">Figure 4.3: Medallion stages of processing</span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.153.1">Medallion approach</span></p>
<p class="callout"><span class="koboSpan" id="kobo.154.1">The medallion approach to data processing organizes data into three stages: Bronze, Silver, and Gold. </span><span class="koboSpan" id="kobo.154.2">This is often used in data lakes and Delta Lake architecture. </span><span class="koboSpan" id="kobo.154.3">Raw data is ingested from various sources</span><a id="_idIndexMarker366"/><span class="koboSpan" id="kobo.155.1"> without transformation in the Bronze stage. </span><span class="koboSpan" id="kobo.155.2">The Silver stage results from data cleaning, enrichment, and transformation to create a curated dataset. </span><span class="koboSpan" id="kobo.155.3">Finally, the Gold stage represents the highest quality data, cleansed, aggregated, and read-optimized for advanced analytics, reporting, and business intelligence. </span><span class="koboSpan" id="kobo.155.4">This multi-tiered structure augments data quality and facilitates </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">data management.</span></span></p>
<p><span class="koboSpan" id="kobo.157.1">Once the data has been ingested from sources, </span><strong class="bold"><span class="koboSpan" id="kobo.158.1">data quality checks and cleaning</span></strong><span class="koboSpan" id="kobo.159.1"> are the first steps to building trustworthiness in the data. </span><span class="koboSpan" id="kobo.159.2">These involve the handling of missing values, detecting and correcting errors, removing duplicates, and filtering outliers. </span><span class="koboSpan" id="kobo.159.3">These improve the data quality and give confidence that the analysis is built on a solid foundation. </span><span class="koboSpan" id="kobo.159.4">The specific requirement at this stage for time series data is to verify and maintain temporal integrity due to the sequential nature of </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.161.1">The raw data from sources is usually not apt for direct analysis use and requires several </span><strong class="bold"><span class="koboSpan" id="kobo.162.1">transformations</span></strong><span class="koboSpan" id="kobo.163.1"> to be</span><a id="_idIndexMarker367"/><span class="koboSpan" id="kobo.164.1"> appropriate for time series analysis. </span><span class="koboSpan" id="kobo.164.2">This involves, among other transformations, changing semi-structured data to a structured format for quicker access. </span><span class="koboSpan" id="kobo.164.3">Data at granular or</span><a id="_idIndexMarker368"/><span class="koboSpan" id="kobo.165.1"> irregular intervals is aggregated to a higher-level interval such as from every minute to hourly, from hourly to daily, and so on. </span><span class="koboSpan" id="kobo.165.2">Date and time fields may require special processing to be in a sortable format and used to set a time index for faster retrieval. </span><span class="koboSpan" id="kobo.165.3">Different time zones need to be </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">handled accordingly.</span></span></p>
<p><span class="koboSpan" id="kobo.167.1">Frequently disregarded in smaller projects, </span><strong class="bold"><span class="koboSpan" id="kobo.168.1">metadata</span></strong><span class="koboSpan" id="kobo.169.1"> is an essential requirement in an enterprise environment to </span><a id="_idIndexMarker369"/><span class="koboSpan" id="kobo.170.1">enable data traceability and lineage for governance. </span><span class="koboSpan" id="kobo.170.2">This data about the data captures, for example, source identifier, ingestion and update times, changes made, as well as historical versions. </span><span class="koboSpan" id="kobo.170.3">Metadata is captured as part of the data ingestion and transformation pipelines, and natively with storage protocols such </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">as Delta.</span></span></p>
<p><span class="koboSpan" id="kobo.172.1">While all the data processing described so far can be done in memory, there is a requirement for longer-term </span><strong class="bold"><span class="koboSpan" id="kobo.173.1">storage</span></strong><span class="koboSpan" id="kobo.174.1"> and retrieval for analysis over time. </span><span class="koboSpan" id="kobo.174.2">This storage needs to be cost-effective, scalable, and secure while providing the high performance required for timely analysis. </span><span class="koboSpan" id="kobo.174.3">Based on the volume and velocity of data, options include dedicated time series databases such as InfluxDB, or cloud-based storage in combination with a storage protocol such </span><span class="No-Break"><span class="koboSpan" id="kobo.175.1">as Delta.</span></span></p>
<p><span class="koboSpan" id="kobo.176.1">We will delve deeper into data processing in </span><a href="B18568_05.xhtml#_idTextAnchor103"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.177.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.178.1"> on data preparation. </span><span class="koboSpan" id="kobo.178.2">For now, let’s shift our focus to governance and security, which are among the most critical considerations for DataOps from a </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">risk perspective.</span></span></p>
<h3><span class="koboSpan" id="kobo.180.1">Monitoring, security, and governance</span></h3>
<p><span class="koboSpan" id="kobo.181.1">Data monitoring, security, and governance</span><a id="_idIndexMarker370"/><span class="koboSpan" id="kobo.182.1"> encompass several overlapping data </span><a id="_idIndexMarker371"/><span class="koboSpan" id="kobo.183.1">practice areas, including data quality, privacy, access control, compliance, and policies. </span><span class="koboSpan" id="kobo.183.2">To </span><a id="_idIndexMarker372"/><span class="koboSpan" id="kobo.184.1">appreciate the importance of these practices, let’s consider the following in the news at the time of </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">this writing:</span></span></p>
<p><span class="koboSpan" id="kobo.186.1">A cybersecurity breach just impacted major organizations, including Ticketmaster, Banco Santander, and Ticketek. </span><span class="koboSpan" id="kobo.186.2">A hacking group called ShinyHunters gained access to Ticketmaster’s database, and in doing so, compromised the personal information of 560 million users. </span><span class="koboSpan" id="kobo.186.3">This includes names, addresses, phone numbers, email addresses, and payment details. </span><span class="koboSpan" id="kobo.186.4">Reports are that this data is being sold on hacking forums for substantial amounts. </span><span class="koboSpan" id="kobo.186.5">Banco Santander had a similar breach, affecting customers and employees. </span></p>
<ul>
<li><span class="koboSpan" id="kobo.187.1">[</span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">Source: </span></span><a href="https://www.wired.com/story/snowflake-breach-ticketmaster-santander-ticketek-hacked/"><span class="No-Break"><span class="koboSpan" id="kobo.189.1">https://www.wired.com/story/snowflake-breach-ticketmaster-santander-ticketek-hacked/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.190.1">]</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.191.1">These breaches, linked to a third-party cloud data warehouse service, highlight the challenges in cybersecurity and the need for strong measures for monitoring, security, </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">and governance.</span></span></p>
<h4><span class="koboSpan" id="kobo.193.1">Monitoring</span></h4>
<p><span class="koboSpan" id="kobo.194.1">The goal here is to </span><a id="_idIndexMarker373"/><span class="koboSpan" id="kobo.195.1">promptly identify issues and be able to take corrective measures, ideally before it has a negative consequence. </span><span class="koboSpan" id="kobo.195.2">The monitoring is of the data itself, of the execution status of transformation pipelines, as well as for security and governance breaches. </span><span class="koboSpan" id="kobo.195.3">For data monitoring, this means tracking the data quality by measuring its accuracy and completeness while catching data gaps and anomalies. </span><span class="koboSpan" id="kobo.195.4">One way to achieve this is by comparing against a range or specific time series pattern, as we have seen in the anomaly detection example in </span><a href="B18568_02.xhtml#_idTextAnchor044"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.196.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.197.1">. </span><span class="koboSpan" id="kobo.197.2">As for the data pipeline monitoring, these are tracked for performance to ensure data freshness, </span><strong class="bold"><span class="koboSpan" id="kobo.198.1">service-level agreements</span></strong><span class="koboSpan" id="kobo.199.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.200.1">SLAs</span></strong><span class="koboSpan" id="kobo.201.1">) are honored, and lineage to track provenance and integrity. </span><span class="koboSpan" id="kobo.201.2">From a </span><a id="_idIndexMarker374"/><span class="koboSpan" id="kobo.202.1">security point of view, we want to catch any attempt at a data breach in time to act upon it. </span><span class="koboSpan" id="kobo.202.2">The monitoring should be an automated process, with alerting </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">in place.</span></span></p>
<h4><span class="koboSpan" id="kobo.204.1">Security</span></h4>
<p><span class="koboSpan" id="kobo.205.1">Related to both data at rest </span><a id="_idIndexMarker375"/><span class="koboSpan" id="kobo.206.1">and in transit, we need to define roles and related permissions to ensure access control. </span><span class="koboSpan" id="kobo.206.2">Some time series data is sensitive and only authorized personnel should be able to view or manipulate </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.208.1">In regulated industries, when handling personal data, we need to ensure that data handling and storage practices comply with relevant regulations (HIPAA, GDPR, etc.). </span><span class="koboSpan" id="kobo.208.2">This also involves ensuring </span><a id="_idIndexMarker376"/><span class="koboSpan" id="kobo.209.1">privacy and managing consent for </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">personal data.</span></span></p>
<h4><span class="koboSpan" id="kobo.211.1">Governance</span></h4>
<p><span class="koboSpan" id="kobo.212.1">In addition to the preceding, the data governance practice is responsible for assigning roles and responsibilities to </span><a id="_idIndexMarker377"/><span class="koboSpan" id="kobo.213.1">manage data. </span><span class="koboSpan" id="kobo.213.2">As part of this, the data stewards oversee data quality, compliance, </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">and policies.</span></span></p>
<p><span class="koboSpan" id="kobo.215.1">By establishing the right processes, people, and tools in place, we can ensure the prevention of data breaches and effective mitigation if they </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">do occur.</span></span></p>
<p><span class="koboSpan" id="kobo.217.1">We have now covered the process of ingesting and transforming data into trustworthy and useful data in a governed and secure way. </span><span class="koboSpan" id="kobo.217.2">The step left now as part of DataOps is to share the data for analysis and consumption by users or </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">other systems.</span></span></p>
<h3><span class="koboSpan" id="kobo.219.1">Sharing and consumption</span></h3>
<p><span class="koboSpan" id="kobo.220.1">After ingestion and processing the data, we want the curated data and outcome of analytics to be visible and available </span><a id="_idIndexMarker378"/><span class="koboSpan" id="kobo.221.1">to users. </span><span class="koboSpan" id="kobo.221.2">A centralized </span><strong class="bold"><span class="koboSpan" id="kobo.222.1">data catalog</span></strong><span class="koboSpan" id="kobo.223.1">, complete with descriptions and usage guidelines, allows users to easily discover and access </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">available </span></span><span class="No-Break"><a id="_idIndexMarker379"/></span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.226.1">Finally, as part of the DataOps phase, we want data scientists, analysts, and other users to be able to consume the data for exploration, analysis, and reporting. </span><span class="koboSpan" id="kobo.226.2">Ideally, we want to tie this to governance to ensure that the right set of users are accessing and consuming permitted datasets only. </span><span class="koboSpan" id="kobo.226.3">Access and methods for consumption include file-based access, database connection, and APIs, </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">among others.</span></span></p>
<p><span class="koboSpan" id="kobo.228.1">As discussed in this section, DataOps is the set of processes to ensure that data is available, accessible, and usable. </span><span class="koboSpan" id="kobo.228.2">It is iterative, with feedback from consumers and continuous improvement to data, pipelines, and practices. </span><span class="koboSpan" id="kobo.228.3">By establishing a scalable and flexible infrastructure with Apache Spark’s processing power and versatility at its core, DataOps ensures that data scientists and analysts have the high-quality data they need when they need it, to derive insights and </span><span class="No-Break"><span class="koboSpan" id="kobo.229.1">drive decisions.</span></span></p>
<p><span class="koboSpan" id="kobo.230.1">We will cover the practical considerations for DataOps in </span><a href="B18568_05.xhtml#_idTextAnchor103"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.231.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.232.1">, </span><em class="italic"><span class="koboSpan" id="kobo.233.1">Data Preparation</span></em><span class="koboSpan" id="kobo.234.1">. </span><span class="koboSpan" id="kobo.234.2">For now, let’s focus on ModelOps, which is the next phase </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">after DataOps.</span></span></p>
<h2 id="_idParaDest-93"><a id="_idTextAnchor093"/><span class="koboSpan" id="kobo.236.1">ModelOps</span></h2>
<p><span class="koboSpan" id="kobo.237.1">While DataOps is about the data life cycle, ModelOps is about the model life cycle – more specifically, statistical and machine learning models. </span><span class="koboSpan" id="kobo.237.2">The objective is to manage the models from development to</span><a id="_idIndexMarker380"/><span class="koboSpan" id="kobo.238.1"> deployment, ensuring that they are reliable, accurate, and scalable while delivering actionable insights based on the use </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">cases’ requirements.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.240.1">ModelOps, MLOps, and LLMOps</span></p>
<p class="callout"><span class="koboSpan" id="kobo.241.1">These terms have overlapping definitions and are sometimes used interchangeably. </span><span class="koboSpan" id="kobo.241.2">In this book, we will refer to ModelOps as the broader life cycle management practice for different types of </span><a id="_idIndexMarker381"/><span class="koboSpan" id="kobo.242.1">models, including simulations, statistical, and machine learning models. </span><span class="koboSpan" id="kobo.242.2">We will use </span><strong class="bold"><span class="koboSpan" id="kobo.243.1">machine learning operations</span></strong><span class="koboSpan" id="kobo.244.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.245.1">MLOps</span></strong><span class="koboSpan" id="kobo.246.1">) more specifically for machine learning models and </span><strong class="bold"><span class="koboSpan" id="kobo.247.1">large language model operations</span></strong><span class="koboSpan" id="kobo.248.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.249.1">LLMOps</span></strong><span class="koboSpan" id="kobo.250.1">) for the specific considerations that apply to the life cycle of LLMs. </span><span class="koboSpan" id="kobo.250.2">As such, ModelOps will refer</span><a id="_idIndexMarker382"/><span class="koboSpan" id="kobo.251.1"> to the superset </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">of practices.</span></span></p>
<p><span class="koboSpan" id="kobo.253.1">ModelOps practices can be categorized broadly into model development and testing, and </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">model deployment.</span></span></p>
<h3><span class="koboSpan" id="kobo.255.1">Model development and testing</span></h3>
<p><span class="koboSpan" id="kobo.256.1">Model development and testing involves creating and fine-tuning time series analysis models based on historical data. </span><span class="koboSpan" id="kobo.256.2">This process starts with feature engineering, selecting appropriate algorithms, such as Autoregressive</span><a id="_idIndexMarker383"/><span class="koboSpan" id="kobo.257.1"> Integrated Moving Average (ARIMA) or Long Short-Term Memory (LSTM), and splitting the data into training and testing sets. </span><span class="koboSpan" id="kobo.257.2">Then, models are iteratively trained and evaluated using performance metrics. </span><span class="koboSpan" id="kobo.257.3">This ensures accuracy. </span><span class="koboSpan" id="kobo.257.4">After that, by testing the model on unseen data, we can ensure that the model can generalize well to new, </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">real-world scenarios.</span></span></p>
<p><span class="koboSpan" id="kobo.259.1">We will now detail further each of </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">these steps:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.261.1">Feature engineering</span></strong><span class="koboSpan" id="kobo.262.1">: Overlapping with the DataOps phase, feature engineering is the initial stage of model</span><a id="_idIndexMarker384"/><span class="koboSpan" id="kobo.263.1"> development, concerned with the identification of existing and creation of new features from the time series data. </span><span class="koboSpan" id="kobo.263.2">These include creating lags and rolling averages features, where information from previous time steps is used to calculate new features, and creating temporal features that capture the time-based characteristics such as specific time of day, day of the week, month, or holidays. </span><span class="koboSpan" id="kobo.263.3">In addition, the feature engineering stage covers transformations to make time series stationary, such as differencing or log transformation, or resampling to make time series regular, as discussed in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.264.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.265.1">. </span><span class="koboSpan" id="kobo.265.2">We will see how Apache Spark can be used for feature engineering in </span><a href="B18568_08.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.266.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.267.1">, on </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">model development.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.269.1">Model selection</span></strong><span class="koboSpan" id="kobo.270.1">: The model to choose is from an ever-growing list of candidate models for time series: ARIMA, Prophet, machine learning, deep learning models such as LSTM, and many others. </span><span class="koboSpan" id="kobo.270.2">The right time series model depends on the data</span><a id="_idIndexMarker385"/><span class="koboSpan" id="kobo.271.1"> available and the use case we are implementing, as we have seen with the use case examples in </span><a href="B18568_02.xhtml#_idTextAnchor044"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.272.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.273.1">. </span><strong class="bold"><span class="koboSpan" id="kobo.274.1">Exploratory data analysis</span></strong><span class="koboSpan" id="kobo.275.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.276.1">EDA</span></strong><span class="koboSpan" id="kobo.277.1">), detailed in </span><a href="B18568_06.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.278.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.279.1">, guides us in this </span><a id="_idIndexMarker386"/><span class="koboSpan" id="kobo.280.1">process by providing an understanding of the data’s trends, seasonality, and underlying patterns. </span><span class="koboSpan" id="kobo.280.2">Finding the best model, however, is part of an iterative process, refined by model validation, which we will now present as the </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">next step.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.282.1">Dataset splitting</span></strong><span class="koboSpan" id="kobo.283.1">: Once we have candidate models, the first step before training the models is to split</span><a id="_idIndexMarker387"/><span class="koboSpan" id="kobo.284.1"> the historical data into training, validation, and test datasets. </span><span class="koboSpan" id="kobo.284.2">The specific consideration in doing so for time series data is twofold: to preserve the chronological order within datasets, and to ensure that there is no data leakage between </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">the splits.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.286.1">Training</span></strong><span class="koboSpan" id="kobo.287.1">: During this phase, the model is fitted to the training dataset by adjusting its parameters. </span><span class="koboSpan" id="kobo.287.2">This can be supervised with predefined labels or actual outcomes, or unsupervised, as explained in </span><a href="B18568_02.xhtml#_idTextAnchor044"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.288.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.289.1">. </span><span class="koboSpan" id="kobo.289.2">In the case of supervised training, the model parameters are adjusted using a process such as gradient descent to minimize the difference, using a loss function, between model prediction and actual outcome. </span><span class="koboSpan" id="kobo.289.3">For unsupervised training, the model is adjusted until a stopping criterion is met, such as the number of runs or the number of </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">categorization classes.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.291.1">Validation</span></strong><span class="koboSpan" id="kobo.292.1">: As part of training iterations, model validation uses unseen validation datasets with techniques such as time-based cross-validation. </span><span class="koboSpan" id="kobo.292.2">This is to check that there is no overfitting and that the trained model can generalize to unseen data with acceptable </span><a id="_idIndexMarker388"/><span class="koboSpan" id="kobo.293.1">accuracy. </span><span class="koboSpan" id="kobo.293.2">The model is evaluated for accuracy using metrics such as </span><strong class="bold"><span class="koboSpan" id="kobo.294.1">mean absolute percentage error</span></strong><span class="koboSpan" id="kobo.295.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.296.1">MAPE</span></strong><span class="koboSpan" id="kobo.297.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.298.1">mean absolute error</span></strong><span class="koboSpan" id="kobo.299.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.300.1">MAE</span></strong><span class="koboSpan" id="kobo.301.1">). </span><span class="koboSpan" id="kobo.301.2">As an iterative process, this stage</span><a id="_idIndexMarker389"/><span class="koboSpan" id="kobo.302.1"> includes hyperparameter tuning, where models with different settings are trained and validated to find the best</span><a id="_idIndexMarker390"/><span class="koboSpan" id="kobo.303.1"> model configuration. </span><span class="koboSpan" id="kobo.303.2">Techniques such as grid search or Bayesian optimization are used to search for the </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">optimal hyperparameters.</span></span></li>
</ol>
<p class="callout-heading"><span class="koboSpan" id="kobo.305.1">Parameters and hyperparameters</span></p>
<p class="callout"><span class="koboSpan" id="kobo.306.1">Note the difference between parameters and hyperparameters. </span><span class="koboSpan" id="kobo.306.2">These terms are often confused. </span><span class="koboSpan" id="kobo.306.3">Model </span><strong class="bold"><span class="koboSpan" id="kobo.307.1">parameters</span></strong><span class="koboSpan" id="kobo.308.1"> are</span><a id="_idIndexMarker391"/><span class="koboSpan" id="kobo.309.1"> learned from the data as part of a training run, such as a neural network’s weights and biases. </span><strong class="bold"><span class="koboSpan" id="kobo.310.1">Hyperparameters</span></strong><span class="koboSpan" id="kobo.311.1"> define the </span><a id="_idIndexMarker392"/><span class="koboSpan" id="kobo.312.1">configuration of the model prior to model training, which, in the case of a neural network, can be the number of nodes and layers defining </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">its architecture.</span></span></p>
<ol>
<li value="6"><strong class="bold"><span class="koboSpan" id="kobo.314.1">Testing</span></strong><span class="koboSpan" id="kobo.315.1"> – As a final step in model development, the model is evaluated against the unseen testing dataset and compared with different algorithms or types of models. </span><span class="koboSpan" id="kobo.315.2">Testing can also include other criteria beyond model accuracy, such as response time, and integration testing with the application code used with </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">the model.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.317.1">Model training, validation, and testing will be covered in detail in </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.318.1">Chapter 7</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.319.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.320.1">Model deployment and monitoring</span></h3>
<p><span class="koboSpan" id="kobo.321.1">Model deployment and monitoring involves the transition of time series analysis models from development to </span><a id="_idIndexMarker393"/><span class="koboSpan" id="kobo.322.1">a live production environment, together with continuous oversight of their performance. </span><span class="koboSpan" id="kobo.322.2">This ongoing monitoring allows retraining and updating the model to adapt to changes in the data patterns or in the behavior of the underlying system </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">being analyzed.</span></span></p>
<p><span class="koboSpan" id="kobo.324.1">We will now detail further each of </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">these steps:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.326.1">Deployment</span></strong><span class="koboSpan" id="kobo.327.1">: Models are deployed for production into a model-serving framework. </span><span class="koboSpan" id="kobo.327.2">This can be containerized with tools such as Kubernetes and Docker or deployed to a cloud-based solution such as Databricks Model Serving, Amazon SageMaker, Azure Machine Learning, or Google Vertex AI. </span><span class="koboSpan" id="kobo.327.3">Once deployed, the model can be used for batch inferencing, scheduled at recurring intervals, or real-time inferencing based on continuously streaming data sources or in response to </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">API requests.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.329.1">Monitoring</span></strong><span class="koboSpan" id="kobo.330.1">: Once the model is deployed in production, monitoring is required to ensure that it remains fit for </span><a id="_idIndexMarker394"/><span class="koboSpan" id="kobo.331.1">purpose and of value. </span><span class="koboSpan" id="kobo.331.2">With </span><strong class="bold"><span class="koboSpan" id="kobo.332.1">data drift</span></strong><span class="koboSpan" id="kobo.333.1"> (the change in the characteristics of the data over time) and </span><strong class="bold"><span class="koboSpan" id="kobo.334.1">concept drift</span></strong><span class="koboSpan" id="kobo.335.1">, where the</span><a id="_idIndexMarker395"/><span class="koboSpan" id="kobo.336.1"> model’s representation of reality worsens over time, model accuracy decreases. </span><span class="koboSpan" id="kobo.336.2">This can be detected with model monitoring, and alerts </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">sent accordingly.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.338.1">Retraining</span></strong><span class="koboSpan" id="kobo.339.1">: When the </span><a id="_idIndexMarker396"/><span class="koboSpan" id="kobo.340.1">monitoring alerts about a drift, and if the drift is significant enough, retraining the model is the next step. </span><span class="koboSpan" id="kobo.340.2">This can be manually launched or automated. </span><span class="koboSpan" id="kobo.340.3">If retraining does not yield a sufficiently accurate model, then we will have to go back to the model development cycle to find another model fit </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">for purpose.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.342.1">Governance</span></strong><span class="koboSpan" id="kobo.343.1">: This includes several key considerations. </span><span class="koboSpan" id="kobo.343.2">We need to keep track of model versions and life cycle stages throughout the model’s life cycle and associated processes. </span><span class="koboSpan" id="kobo.343.3">In addition, for auditability purposes, logs of training, deployment, and accuracy metrics are kept, and in some cases, requests to and responses from model inference are also saved. </span><span class="koboSpan" id="kobo.343.4">Additional considerations include access control to the model and ensuring it meets all legal and regulatory compliance requirements, including when dealing with personal or </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">sensitive data.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.345.1">In summary, ModelOps for a time series analysis project covers the end-to-end process of developing, deploying, and maintaining models, while overlapping with DataOps for its data requirements. </span><span class="koboSpan" id="kobo.345.2">ModelOps ensures continuous improvement, reproducibility, collaboration, and fitness for purpose with respect to business objectives. </span><span class="koboSpan" id="kobo.345.3">It also maintains the model’s effectiveness and ensures that it keeps delivering value </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">over time.</span></span></p>
<p><span class="koboSpan" id="kobo.347.1">We will cover the practical considerations for ModelOps in </span><a href="B18568_07.xhtml#_idTextAnchor133"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.348.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.349.1">, </span><em class="italic"><span class="koboSpan" id="kobo.350.1">Building and Testing Models</span></em><span class="koboSpan" id="kobo.351.1">. </span><span class="koboSpan" id="kobo.351.2">The next phase is</span><a id="_idIndexMarker397"/><span class="koboSpan" id="kobo.352.1"> DevOps, which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">detail now.</span></span></p>
<h2 id="_idParaDest-94"><a id="_idTextAnchor094"/><span class="koboSpan" id="kobo.354.1">DevOps</span></h2>
<p><span class="koboSpan" id="kobo.355.1">Next after ModelOps, DevOps is a set of practices and tools that smoothen the handover between </span><strong class="bold"><span class="koboSpan" id="kobo.356.1">development</span></strong><span class="koboSpan" id="kobo.357.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.358.1">Dev</span></strong><span class="koboSpan" id="kobo.359.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.360.1">operations</span></strong><span class="koboSpan" id="kobo.361.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.362.1">Ops</span></strong><span class="koboSpan" id="kobo.363.1">). </span><span class="koboSpan" id="kobo.363.2">This is for both the model and related application code. </span><span class="koboSpan" id="kobo.363.3">By automating the building, testing, deployment, and monitoring of time series</span><a id="_idIndexMarker398"/><span class="koboSpan" id="kobo.364.1"> applications, DevOps ensures that they are reliable, scalable, and </span><a id="_idIndexMarker399"/><span class="koboSpan" id="kobo.365.1">continuously deliver value to </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">the business.</span></span></p>
<p><span class="koboSpan" id="kobo.367.1">The DevOps practices can </span><a id="_idIndexMarker400"/><span class="koboSpan" id="kobo.368.1">be broadly broken down into </span><strong class="bold"><span class="koboSpan" id="kobo.369.1">continuous integration/continuous deployment</span></strong><span class="koboSpan" id="kobo.370.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.371.1">CI/CD</span></strong><span class="koboSpan" id="kobo.372.1">), infrastructure management, and monitoring </span><span class="No-Break"><span class="koboSpan" id="kobo.373.1">and governance.</span></span></p>
<p><span class="No-Break"><span class="koboSpan" id="kobo.374.1">CI/CD</span></span></p>
<p><span class="koboSpan" id="kobo.375.1">CI/CD involves automating the integration and deployment of time series analysis models for seamless updates </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">to </span></span><span class="No-Break"><a id="_idIndexMarker401"/></span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">production.</span></span></p>
<p><span class="koboSpan" id="kobo.378.1">This includes the </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">following steps:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.380.1">Code and model versioning and repository</span></strong><span class="koboSpan" id="kobo.381.1">: Code and model changes require tracking, with the possibility of rolling back to previous versions if needed. </span><span class="koboSpan" id="kobo.381.2">This means that the code and models need to be version-controlled and stored in a repository from where the different versions can </span><span class="No-Break"><span class="koboSpan" id="kobo.382.1">be accessed.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.383.1">Testing</span></strong><span class="koboSpan" id="kobo.384.1">: It is crucial that there is no regression whenever changes are made to the time series model and associated code. </span><span class="koboSpan" id="kobo.384.2">One way to ensure this is through automated testing, with unit and integration testing, which can be kicked off either when production monitoring detects a degradation or when there are model or associated code changes </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">in development.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.386.1">Deployment</span></strong><span class="koboSpan" id="kobo.387.1">: Once the time series model and code are ready in development, the next steps are deployment to staging and production. </span><span class="koboSpan" id="kobo.387.2">It is recommended to automate this deployment with CI/CD pipelines to minimize the risks of errors due to manual steps and make this a seamless, repeatable, and </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">scalable process.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.389.1">In summary, CI/CD </span><a id="_idIndexMarker402"/><span class="koboSpan" id="kobo.390.1">pipelines ensure that new features, improvements, and bug fixes are consistently integrated, tested, and deployed while minimizing downtime and enhancing the efficiency of new </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">code rollout.</span></span></p>
<h3><span class="koboSpan" id="kobo.392.1">Infrastructure management</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.393.1">Infrastructure as code</span></strong><span class="koboSpan" id="kobo.394.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.395.1">IaC</span></strong><span class="koboSpan" id="kobo.396.1">) is a recommended approach to provisioning as it enables the infrastructure configurations to</span><a id="_idIndexMarker403"/><span class="koboSpan" id="kobo.397.1"> be version-controlled, self-documented, reproducible, and scalable. </span><span class="koboSpan" id="kobo.397.2">This is</span><a id="_idIndexMarker404"/><span class="koboSpan" id="kobo.398.1"> how compute, storage, and networking configurations can be set consistently. </span><span class="koboSpan" id="kobo.398.2">In a virtual environment such as a cloud environment, the infrastructure itself is, in a sense, version-controlled as it is software-defined </span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">in nature.</span></span></p>
<p><span class="koboSpan" id="kobo.400.1">In addition to the previous core resources, security-specific configurations require provisioning for access controls, encryption, and firewalls for </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">network security.</span></span></p>
<p><span class="koboSpan" id="kobo.402.1">As demand for the application changes, the corresponding workload changes with a requirement for additional or fewer infrastructure resources. </span><span class="koboSpan" id="kobo.402.2">A scalable infrastructure management process ensures that the infrastructure is automatically scaled based </span><span class="No-Break"><span class="koboSpan" id="kobo.403.1">on demand.</span></span></p>
<h3><span class="koboSpan" id="kobo.404.1">Monitoring, security, and governance</span></h3>
<p><span class="koboSpan" id="kobo.405.1">DevOps has similar requirements for monitoring, security, and governance to DataOps and ModelOps. </span><span class="koboSpan" id="kobo.405.2">The scope for DevOps </span><a id="_idIndexMarker405"/><span class="koboSpan" id="kobo.406.1">encompasses everything that is deployed to the production environment, including models, code, and configurations. </span><span class="koboSpan" id="kobo.406.2">This is typically fulfilled via processes such as application, security, and compliance monitoring, logging and alerting, and </span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">incident management.</span></span></p>
<p><span class="koboSpan" id="kobo.408.1">In summary, DevOps ensures that applications, including time series analysis, are highly available and scalable by automating their deployment, management, and scaling. </span><span class="koboSpan" id="kobo.408.2">The key here is to make the transition from </span><em class="italic"><span class="koboSpan" id="kobo.409.1">Dev</span></em><span class="koboSpan" id="kobo.410.1"> to </span><em class="italic"><span class="koboSpan" id="kobo.411.1">Ops</span></em><span class="koboSpan" id="kobo.412.1"> seamless by facilitating collaboration and using automation to ensure that a time series analysis project can evolve from a use case concept to its technical implementation to a fully operational system that drives significant business impact </span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">and value.</span></span></p>
<p><span class="koboSpan" id="kobo.414.1">Now that we understand the end-to-end phases of a time series analysis project, the next section will provide</span><a id="_idIndexMarker406"/><span class="koboSpan" id="kobo.415.1"> practical examples and tools for implementing what we have learned so far in </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">this chapter.</span></span></p>
<h1 id="_idParaDest-95"><a id="_idTextAnchor095"/><span class="koboSpan" id="kobo.417.1">Implementation examples and tools</span></h1>
<p><span class="koboSpan" id="kobo.418.1">With the end-to-end phases defined, this section will examine two implementation examples: a notebook-based </span><a id="_idIndexMarker407"/><span class="koboSpan" id="kobo.419.1">approach and an </span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">orchestrator-based approach.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.421.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.422.1">If you do not intend to build your own end-to-end environment, you can skip the practical part of this section and use a managed platform such as Databricks, as we will do in </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">future chapters.</span></span></p>
<p><span class="koboSpan" id="kobo.424.1">Let’s start by setting up the environment required to run </span><span class="No-Break"><span class="koboSpan" id="kobo.425.1">the examples.</span></span></p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor096"/><span class="koboSpan" id="kobo.426.1">Environment setup</span></h2>
<p><span class="koboSpan" id="kobo.427.1">We will be using Docker</span><a id="_idIndexMarker408"/><span class="koboSpan" id="kobo.428.1"> containers, as in </span><a href="B18568_03.xhtml#_idTextAnchor063"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.429.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.430.1">, for the platform infrastructure. </span><span class="koboSpan" id="kobo.430.2">Refer to the </span><em class="italic"><span class="koboSpan" id="kobo.431.1">Using a container for deployment</span></em><span class="koboSpan" id="kobo.432.1"> section in </span><a href="B18568_03.xhtml#_idTextAnchor063"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.433.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.434.1"> for instructions on </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">installing Docker.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.436.1">Alternative to Docker</span></p>
<p class="callout"><span class="koboSpan" id="kobo.437.1">You can use Podman as an open source alternative to</span><a id="_idIndexMarker409"/><span class="koboSpan" id="kobo.438.1"> Docker. </span><span class="koboSpan" id="kobo.438.2">You can find more</span><a id="_idIndexMarker410"/><span class="koboSpan" id="kobo.439.1"> information </span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">here: </span></span><a href="https://podman.io/"><span class="No-Break"><span class="koboSpan" id="kobo.441.1">https://podman.io/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.442.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.443.1">Before we can deploy the Docker containers, we will validate in the next section that there is no conflict with the network ports that will be used by </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">the containers.</span></span></p>
<h3><span class="koboSpan" id="kobo.445.1">Network ports</span></h3>
<p><span class="koboSpan" id="kobo.446.1">The following network ports </span><a id="_idIndexMarker411"/><span class="koboSpan" id="kobo.447.1">need to be available on your local machine or </span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">development environment:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.449.1">Apache Spark: </span><strong class="source-inline"><span class="koboSpan" id="kobo.450.1">7077</span></strong><span class="koboSpan" id="kobo.451.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.452.1">8070</span></strong><span class="koboSpan" id="kobo.453.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.455.1">8081</span></strong></span></li>
<li><span class="koboSpan" id="kobo.456.1">Jupyter Notebook: </span><strong class="source-inline"><span class="koboSpan" id="kobo.457.1">4040</span></strong><span class="koboSpan" id="kobo.458.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.459.1">4041</span></strong><span class="koboSpan" id="kobo.460.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.461.1">4042</span></strong><span class="koboSpan" id="kobo.462.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.463.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.464.1">8888</span></strong></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.465.1">MLflow: </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.466.1">5001</span></strong></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.467.1">Airflow: </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.468.1">8080</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.469.1">You can check for the current use of these ports by existing applications with the following command, run from the command line </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">or terminal:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.471.1">
% netstat -an | grep LISTEN</span></pre> <p><span class="koboSpan" id="kobo.472.1">If you see the required ports in the list of ports already in use, you must either stop the application using that port or change the </span><strong class="source-inline"><span class="koboSpan" id="kobo.473.1">docker-compose</span></strong><span class="koboSpan" id="kobo.474.1"> file to use </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">another port.</span></span></p>
<p><span class="koboSpan" id="kobo.476.1">As an example, let’s assume that the output of the preceding </span><strong class="source-inline"><span class="koboSpan" id="kobo.477.1">netstat</span></strong><span class="koboSpan" id="kobo.478.1"> command reveals that port </span><strong class="source-inline"><span class="koboSpan" id="kobo.479.1">8080</span></strong><span class="koboSpan" id="kobo.480.1"> is already in use on your local machine or development environment, and you are not able to stop the existing application using </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">this port.</span></span></p>
<p><span class="koboSpan" id="kobo.482.1">In this case, you will need to change port </span><strong class="source-inline"><span class="koboSpan" id="kobo.483.1">8080</span></strong><span class="koboSpan" id="kobo.484.1"> (meant for the Airflow web server) in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.485.1">docker-compose.yaml</span></strong><span class="koboSpan" id="kobo.486.1"> file to another, unused port. </span><span class="koboSpan" id="kobo.486.2">Just search and replace </span><strong class="source-inline"><span class="koboSpan" id="kobo.487.1">8080</span></strong><span class="koboSpan" id="kobo.488.1"> on the left of the colon (</span><strong class="source-inline"><span class="koboSpan" id="kobo.489.1">:</span></strong><span class="koboSpan" id="kobo.490.1">) to say </span><strong class="source-inline"><span class="koboSpan" id="kobo.491.1">8090</span></strong><span class="koboSpan" id="kobo.492.1"> if this port is free, as per the </span><span class="No-Break"><span class="koboSpan" id="kobo.493.1">following example:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.494.1">From this:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.495.1">
     ports:
      - '7077:7077'
      - '</span><strong class="bold"><span class="koboSpan" id="kobo.496.1">8080</span></strong><span class="koboSpan" id="kobo.497.1">:8080'</span></pre></li> <li><span class="No-Break"><span class="koboSpan" id="kobo.498.1">To this:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.499.1">
     ports:
      - '7077:7077'
      - '</span><strong class="bold"><span class="koboSpan" id="kobo.500.1">8090</span></strong><span class="koboSpan" id="kobo.501.1">:8080'</span></pre></li> </ul>
<p><span class="koboSpan" id="kobo.502.1">Keep note of the new port and use this instead of the existing one whenever you need to type the corresponding URL. </span><span class="koboSpan" id="kobo.502.2">In this example, port </span><strong class="source-inline"><span class="koboSpan" id="kobo.503.1">8080</span></strong><span class="koboSpan" id="kobo.504.1"> is changed to </span><strong class="source-inline"><span class="koboSpan" id="kobo.505.1">8090</span></strong><span class="koboSpan" id="kobo.506.1">, and the matching URL change for the Airflow web server is </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">as follows:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.508.1">From this:</span></span><p class="list-inset"><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.509.1">http://localhost:8080/</span></strong></span></p></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.510.1">To this:</span></span><p class="list-inset"><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.511.1">http://localhost:8090/</span></strong></span></p></li>
</ul>
<p class="callout-heading"><span class="koboSpan" id="kobo.512.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.513.1">You will need to change the </span><a id="_idIndexMarker412"/><span class="koboSpan" id="kobo.514.1">network port in all URLs in the following sections that you had to modify as per </span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">this section.</span></span></p>
<h3><span class="koboSpan" id="kobo.516.1">Environment startup</span></h3>
<p><span class="koboSpan" id="kobo.517.1">Once Docker is installed and</span><a id="_idIndexMarker413"/><span class="koboSpan" id="kobo.518.1"> running, and the network port configuration is validated, the following instructions guide you to set up and start </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">the environment:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.520.1">We first download the deployment script from the Git repository for this chapter, which is at the </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">following URL:</span></span><p class="list-inset"><a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch4"><span class="No-Break"><span class="koboSpan" id="kobo.522.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/tree/main/ch4</span></span></a></p><p class="list-inset"><span class="koboSpan" id="kobo.523.1">We will be using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.524.1">git clone</span></strong><span class="koboSpan" id="kobo.525.1">-friendly URL, which is </span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">as follows:</span></span></p><p class="list-inset"><a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git"><span class="No-Break"><span class="koboSpan" id="kobo.527.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git</span></span></a></p><p class="list-inset"><span class="koboSpan" id="kobo.528.1">To do this, start a terminal or command line and run the </span><span class="No-Break"><span class="koboSpan" id="kobo.529.1">following commands:</span></span></p><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.530.1">git clone https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark.git</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.531.1">cd Time-Series-Analysis-with-Spark/ch4</span></strong></pre><p class="list-inset"><span class="koboSpan" id="kobo.532.1">Note that the preceding is for a macOS or Linux/Unix-based system, and you will need to run the equivalent</span><a id="_idIndexMarker414"/><span class="koboSpan" id="kobo.533.1"> for Windows as per the following </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">GitHub documentation:</span></span></p><p class="list-inset"><a href="https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository?platform=windows&amp;tool=cli"><span class="No-Break"><span class="koboSpan" id="kobo.535.1">https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository?platform=windows&amp;tool=cli</span></span></a></p></li> <li><span class="koboSpan" id="kobo.536.1">On macOS, you may see the following error when you run the preceding </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.537.1">git</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.538.1"> command:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.539.1">xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun</span></strong></pre><p class="list-inset"><span class="koboSpan" id="kobo.540.1">In this case, you will need to reinstall the command-line tools with the </span><span class="No-Break"><span class="koboSpan" id="kobo.541.1">following command:</span></span></p><pre class="source-code"><strong class="bold"><span class="koboSpan" id="kobo.542.1">xcode-select --install</span></strong></pre></li> <li><span class="koboSpan" id="kobo.543.1">We can now start the container build and startup. </span><span class="koboSpan" id="kobo.543.2">A makefile is provided to simplify the process of starting and stopping the containers. </span><span class="koboSpan" id="kobo.543.3">The following command builds the Docker images for the containers and then </span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">starts them:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.545.1">make up</span></strong></pre></li> </ol>
<p class="callout-heading"><span class="koboSpan" id="kobo.546.1">Windows environment</span></p>
<p class="callout"><span class="koboSpan" id="kobo.547.1">If you are using a Windows environment, you can install a Windows version of </span><strong class="source-inline"><span class="koboSpan" id="kobo.548.1">make</span></strong><span class="koboSpan" id="kobo.549.1">, as per the following </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">documentation: </span></span><a href="https://gnuwin32.sourceforge.net/packages/make.htm"><span class="No-Break"><span class="koboSpan" id="kobo.551.1">https://gnuwin32.sourceforge.net/packages/make.htm</span></span></a></p>
<p class="list-inset"><span class="koboSpan" id="kobo.552.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.553.1">make up</span></strong><span class="koboSpan" id="kobo.554.1"> command will give the following or </span><span class="No-Break"><span class="koboSpan" id="kobo.555.1">equivalent output:</span></span></p>
<pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.556.1">make prep &amp;&amp; docker-compose up -d</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.557.1">sh prep-airflow.sh</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.558.1">[+] Running 9/9</span></strong>
<strong class="bold"> </strong><strong class="bold"><span class="koboSpan" id="kobo.559.1">✔</span></strong><strong class="bold"><span class="koboSpan" id="kobo.560.1"> Container ts-spark-env-spark-master-1      Started</span></strong>
<strong class="bold"> </strong><strong class="bold"><span class="koboSpan" id="kobo.561.1">✔</span></strong><strong class="bold"><span class="koboSpan" id="kobo.562.1"> Container ts-spark-env-postgres-1          Healthy </span></strong><strong class="bold"><span class="koboSpan" id="kobo.563.1">✔</span></strong><strong class="bold"><span class="koboSpan" id="kobo.564.1"> Container ts-spark-env-mlflow-server-1     Started </span></strong><strong class="bold"><span class="koboSpan" id="kobo.565.1">✔</span></strong><strong class="bold"><span class="koboSpan" id="kobo.566.1"> Container ts-spark-env-jupyter-1           Started </span></strong><strong class="bold"><span class="koboSpan" id="kobo.567.1">✔</span></strong><strong class="bold"><span class="koboSpan" id="kobo.568.1"> Container ts-spark-env-airflow-init-1      Exited </span></strong><strong class="bold"><span class="koboSpan" id="kobo.569.1">✔</span></strong><strong class="bold"><span class="koboSpan" id="kobo.570.1"> Container ts-spark-env-spark-worker-1-1    Started </span></strong><strong class="bold"><span class="koboSpan" id="kobo.571.1">✔</span></strong><strong class="bold"><span class="koboSpan" id="kobo.572.1"> Container ts-spark-env-airflow-scheduler-1 Running </span></strong><strong class="bold"><span class="koboSpan" id="kobo.573.1">✔</span></strong><strong class="bold"><span class="koboSpan" id="kobo.574.1"> Container ts-spark-env-airflow-triggerer-1 Running </span></strong><strong class="bold"><span class="koboSpan" id="kobo.575.1">✔</span></strong><strong class="bold"><span class="koboSpan" id="kobo.576.1"> Container ts-spark-env-airflow-webserver-1 Running</span></strong></pre> <ol>
<li value="4"><span class="koboSpan" id="kobo.577.1">You may see the following </span><a id="_idIndexMarker415"/><span class="koboSpan" id="kobo.578.1">error when you run the preceding </span><strong class="source-inline"><span class="koboSpan" id="kobo.579.1">make </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.580.1">up</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.581.1"> command:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.582.1">open /Users/&lt;USER_LOGIN&gt;/.docker/buildx/current: permission denied</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.583.1">make: *** [up] Error 1</span></strong></pre><p class="list-inset"><span class="koboSpan" id="kobo.584.1">In this case, you will need to change the permission for the Docker folder with the following command, replacing </span><strong class="source-inline"><span class="koboSpan" id="kobo.585.1">&lt;USER_LOGIN&gt;</span></strong><span class="koboSpan" id="kobo.586.1"> with your own </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">user login:</span></span></p><pre class="source-code"><strong class="bold"><span class="koboSpan" id="kobo.588.1">chmod 755 /Users/&lt;USER_LOGIN&gt;/.docker/buildx/current</span></strong></pre><p class="list-inset"><span class="koboSpan" id="kobo.589.1">Then, rerun the </span><strong class="source-inline"><span class="koboSpan" id="kobo.590.1">make </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.591.1">up</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.592.1"> command.</span></span></p></li> <li><span class="koboSpan" id="kobo.593.1">You may get an error if you have </span><strong class="source-inline"><span class="koboSpan" id="kobo.594.1">bash</span></strong><span class="koboSpan" id="kobo.595.1"> instead of</span><strong class="bold"> </strong><strong class="source-inline"><span class="koboSpan" id="kobo.596.1">sh</span></strong><span class="koboSpan" id="kobo.597.1"> in your environment and the script cannot locate the </span><strong class="source-inline"><span class="koboSpan" id="kobo.598.1">sh</span></strong><span class="koboSpan" id="kobo.599.1"> file. </span><span class="koboSpan" id="kobo.599.2">In this case, change the last line in makefile from "</span><strong class="source-inline"><span class="koboSpan" id="kobo.600.1">sh prep-airflow.sh</span></strong><span class="koboSpan" id="kobo.601.1">" to "</span><strong class="source-inline"><span class="koboSpan" id="kobo.602.1">bash prep-airflow.sh</span></strong><span class="koboSpan" id="kobo.603.1">" and then run the </span><strong class="source-inline"><span class="koboSpan" id="kobo.604.1">make</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.605.1">up</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.606.1">command again.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.607.1">By the end of the process, as in </span><a href="B18568_03.xhtml#_idTextAnchor063"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.608.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.609.1">, you will have a running Spark cluster and a separate node for Jupyter Notebook. </span><span class="koboSpan" id="kobo.609.2">In addition, we have deployed the following </span><span class="No-Break"><span class="koboSpan" id="kobo.610.1">components here:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.611.1">MLflow</span></strong><span class="koboSpan" id="kobo.612.1"> – An open source platform, originally developed by Databricks, for managing the end-to-end </span><a id="_idIndexMarker416"/><span class="koboSpan" id="kobo.613.1">machine learning life cycle. </span><span class="koboSpan" id="kobo.613.2">With features for experimentation and deployment, MLflow is designed to work with any machine learning library and programming language. </span><span class="koboSpan" id="kobo.613.3">This makes it flexible for various environments and use cases, which explains its </span><span class="No-Break"><span class="koboSpan" id="kobo.614.1">broad adoption.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.615.1">You can find more information </span><span class="No-Break"><span class="koboSpan" id="kobo.616.1">here: </span></span><a href="https://mlflow.org/"><span class="No-Break"><span class="koboSpan" id="kobo.617.1">https://mlflow.org/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.618.1">.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.619.1">Apache Airflow</span></strong><span class="koboSpan" id="kobo.620.1"> – Created by Airbnb, Airflow is an open source platform for orchestrating data processing pipelines and </span><a id="_idIndexMarker417"/><span class="koboSpan" id="kobo.621.1">computational workflows. </span><span class="koboSpan" id="kobo.621.2">With </span><a id="_idIndexMarker418"/><span class="koboSpan" id="kobo.622.1">the ability to programmatically define, schedule, and monitor workflows at scale, Airflow is widely adopted, including by data engineers and data scientists, for diverse types </span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">of workflows.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.624.1">You can find more information </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">here: </span></span><a href="https://airflow.apache.org/"><span class="No-Break"><span class="koboSpan" id="kobo.626.1">https://airflow.apache.org/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.627.1">.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.628.1">Postgres</span></strong><span class="koboSpan" id="kobo.629.1"> – This is the relational </span><a id="_idIndexMarker419"/><span class="koboSpan" id="kobo.630.1">database used in the backend </span><span class="No-Break"><span class="koboSpan" id="kobo.631.1">by Airflow.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.632.1">Let’s now validate the environment that we have </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">just deployed.</span></span></p>
<h2 id="_idParaDest-97"><a id="_idTextAnchor097"/><span class="koboSpan" id="kobo.634.1">Accessing the UIs</span></h2>
<p><span class="koboSpan" id="kobo.635.1">We will now access the </span><strong class="bold"><span class="koboSpan" id="kobo.636.1">user interfaces</span></strong><span class="koboSpan" id="kobo.637.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.638.1">UIs</span></strong><span class="koboSpan" id="kobo.639.1">) of the different components as a quick way to validate </span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">the </span></span><span class="No-Break"><a id="_idIndexMarker420"/></span><span class="No-Break"><span class="koboSpan" id="kobo.641.1">deployment:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.642.1">Follow the instructions in </span><a href="B18568_03.xhtml#_idTextAnchor063"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.643.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.644.1"> to validate the deployment of Jupyter Notebook and the Apache Spark cluster. </span><span class="koboSpan" id="kobo.644.2">Note that due to the Airflow web server using port </span><strong class="source-inline"><span class="koboSpan" id="kobo.645.1">8080</span></strong><span class="koboSpan" id="kobo.646.1">, which is the </span><a id="_idIndexMarker421"/><span class="koboSpan" id="kobo.647.1">same port we used in </span><a href="B18568_03.xhtml#_idTextAnchor063"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.648.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.649.1"> for Apache Spark, we have changed the Spark master node to the following </span><span class="No-Break"><span class="koboSpan" id="kobo.650.1">local URL:</span></span><p class="list-inset"><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.651.1">http://localhost:8070/</span></strong></span></p></li>
<li><span class="koboSpan" id="kobo.652.1">MLflow is accessible at the following </span><span class="No-Break"><span class="koboSpan" id="kobo.653.1">local URL:</span></span><p class="list-inset"><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.654.1">http://localhost:5001/</span></strong></span></p><p class="list-inset"><span class="koboSpan" id="kobo.655.1">This will open the web page as per </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.656.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.657.1">.4</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.658.1">.</span></span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer053">
<span class="koboSpan" id="kobo.659.1"><img alt="" src="image/B18568_04_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.660.1">Figure 4.4: MLflow</span></p>
<ol>
<li value="3"><span class="koboSpan" id="kobo.661.1">The next UI, </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.662.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.663.1">.5</span></em><span class="koboSpan" id="kobo.664.1">, is for Airflow, accessible via the following </span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">local URL:</span></span><p class="list-inset"><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.666.1">http://localhost:8080/</span></strong></span></p><p class="list-inset"><span class="koboSpan" id="kobo.667.1">The default username and password are </span><strong class="source-inline"><span class="koboSpan" id="kobo.668.1">airflow</span></strong><span class="koboSpan" id="kobo.669.1">, which is highly recommended </span><span class="No-Break"><span class="koboSpan" id="kobo.670.1">to </span></span><span class="No-Break"><a id="_idIndexMarker422"/></span><span class="No-Break"><span class="koboSpan" id="kobo.671.1">change.</span></span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer054">
<span class="koboSpan" id="kobo.672.1"><img alt="" src="image/B18568_04_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.673.1">Figure 4.5: Airflow</span></p>
<p><span class="koboSpan" id="kobo.674.1">We now have our environment set up, which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.675.1">use next.</span></span></p>
<h2 id="_idParaDest-98"><a id="_idTextAnchor098"/><span class="koboSpan" id="kobo.676.1">Notebook approach</span></h2>
<p><span class="koboSpan" id="kobo.677.1">We have used notebooks from </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.678.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.679.1">, where we started with the Databricks Community edition. </span><span class="koboSpan" id="kobo.679.2">In </span><a href="B18568_03.xhtml#_idTextAnchor063"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.680.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.681.1">, we deployed our own notebook environment with Jupyter, which is an open source implementation. </span><span class="koboSpan" id="kobo.681.2">As we have seen by now, notebooks give us a feature-rich document-type interface where we can combine executable code, visualizations, and text. </span><span class="koboSpan" id="kobo.681.3">This</span><a id="_idIndexMarker423"/><span class="koboSpan" id="kobo.682.1"> makes notebooks popular for interactive and collaborative work in data science and machine learning. </span><span class="koboSpan" id="kobo.682.2">Notebooks can also be built for execution in a non-interactive way, which, together with</span><a id="_idIndexMarker424"/><span class="koboSpan" id="kobo.683.1"> the fact that they have already been used in the earlier data science and experimentation phases, makes them readily adaptable to an </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">end-to-end notebook.</span></span></p>
<p><span class="koboSpan" id="kobo.685.1">In this first example, we will use an all-in-one notebook based on the Prophet-based code introduced in </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.686.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.687.1">. </span><span class="koboSpan" id="kobo.687.2">If you followed the instructions in the earlier </span><em class="italic"><span class="koboSpan" id="kobo.688.1">Environment setup</span></em><span class="koboSpan" id="kobo.689.1"> section, the example notebook should be accessible directly within the Jupyter Notebook UI, at the </span><strong class="source-inline"><span class="koboSpan" id="kobo.690.1">work / notebooks</span></strong><span class="koboSpan" id="kobo.691.1"> location on the left folder navigation panel, as</span><a id="_idIndexMarker425"/><span class="koboSpan" id="kobo.692.1"> shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.693.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.694.1">.6</span></em><span class="koboSpan" id="kobo.695.1">, at the following </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">URL: </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.697.1">http://localhost:8888/lab</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.698.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer055">
<span class="koboSpan" id="kobo.699.1"><img alt="" src="image/B18568_04_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.700.1">Figure 4.6: Notebook</span></p>
<p><span class="koboSpan" id="kobo.701.1">The notebook is also downloadable from the following </span><span class="No-Break"><span class="koboSpan" id="kobo.702.1">GitHub location:</span></span></p>
<p><a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch4/notebooks/ts-spark_ch4_data-ml-ops.ipynb"><span class="No-Break"><span class="koboSpan" id="kobo.703.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch4/notebooks/ts-spark_ch4_data-ml-ops.ipynb</span></span></a></p>
<p><span class="koboSpan" id="kobo.704.1">With a focus more on structure here than on the code itself, which does not change much from </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.705.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.706.1">, we structured the notebook into the </span><span class="No-Break"><span class="koboSpan" id="kobo.707.1">following sections:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.708.1">Config</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.709.1">DataOps</span></span></li>
<li><span class="koboSpan" id="kobo.710.1">Ingest data </span><span class="No-Break"><span class="koboSpan" id="kobo.711.1">from source</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.712.1">Transform data</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.713.1">ModelOps</span></span></li>
<li><span class="koboSpan" id="kobo.714.1">Train and </span><span class="No-Break"><span class="koboSpan" id="kobo.715.1">log model</span></span></li>
<li><span class="koboSpan" id="kobo.716.1">Forecast </span><span class="No-Break"><span class="koboSpan" id="kobo.717.1">with model</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.718.1">The notable addition to the</span><a id="_idIndexMarker426"/><span class="koboSpan" id="kobo.719.1"> code from </span><a href="B18568_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.720.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.721.1">, in addition to the structure explained previously, is the MLOps </span><a id="_idIndexMarker427"/><span class="koboSpan" id="kobo.722.1">part, which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.723.1">detail next.</span></span></p>
<h3><span class="koboSpan" id="kobo.724.1">MLOps with MLflow</span></h3>
<p><span class="koboSpan" id="kobo.725.1">In this notebook example, we use MLflow as the tool to implement several MLOps requirements. </span><span class="koboSpan" id="kobo.725.2">The following code </span><a id="_idIndexMarker428"/><span class="koboSpan" id="kobo.726.1">extract focuses on</span><a id="_idIndexMarker429"/><span class="koboSpan" id="kobo.727.1"> this </span><span class="No-Break"><span class="koboSpan" id="kobo.728.1">specific part:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.729.1">
mlflow.</span><strong class="bold"><span class="koboSpan" id="kobo.730.1">set_tracking_uri</span></strong><span class="koboSpan" id="kobo.731.1">("http://mlflow-server:5000")
mlflow.</span><strong class="bold"><span class="koboSpan" id="kobo.732.1">set_experiment</span></strong><span class="koboSpan" id="kobo.733.1">(
    </span><strong class="bold"><span class="koboSpan" id="kobo.734.1">'ts-spark_ch4_data-ml-ops_time_series_prophet_notebook'</span></strong><span class="koboSpan" id="kobo.735.1">)
with mlflow.</span><strong class="bold"><span class="koboSpan" id="kobo.736.1">start_run</span></strong><span class="koboSpan" id="kobo.737.1">():
    model = Prophet().fit(pdf)
…
    mlflow.prophet.</span><strong class="bold"><span class="koboSpan" id="kobo.738.1">log_model</span></strong><span class="koboSpan" id="kobo.739.1">(
        model, artifact_path=ARTIFACT_DIR,
        signature=signature)
    mlflow.</span><strong class="bold"><span class="koboSpan" id="kobo.740.1">log_params</span></strong><span class="koboSpan" id="kobo.741.1">(param)
    mlflow.</span><strong class="bold"><span class="koboSpan" id="kobo.742.1">log_metrics</span></strong><span class="koboSpan" id="kobo.743.1">(cv_metrics)</span></pre> <p><span class="koboSpan" id="kobo.744.1">The MLflow functionalities used in the preceding code are </span><span class="No-Break"><span class="koboSpan" id="kobo.745.1">the following:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.746.1">set_tracking_uri</span></strong><span class="koboSpan" id="kobo.747.1"> – This sets the URI of the tracking server where MLflow will store model-related information. </span><span class="koboSpan" id="kobo.747.2">This centralizes model data and facilitates collaboration among team </span><a id="_idIndexMarker430"/><span class="koboSpan" id="kobo.748.1">members. </span><span class="koboSpan" id="kobo.748.2">The tracking server can be a remote server or a local </span><span class="No-Break"><span class="koboSpan" id="kobo.749.1">file path.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.750.1">set_experiment</span></strong><span class="koboSpan" id="kobo.751.1"> – This creates a new experiment or uses an existing one. </span><span class="koboSpan" id="kobo.751.2">An experiment is a logical grouping of runs (separate model training or trials), useful to organize and</span><a id="_idIndexMarker431"/><span class="koboSpan" id="kobo.752.1"> compare </span><span class="No-Break"><span class="koboSpan" id="kobo.753.1">different trials.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.754.1">start_run</span></strong><span class="koboSpan" id="kobo.755.1"> – This starts a new MLflow run, which can be within a given experiment. </span><span class="koboSpan" id="kobo.755.2">As a representation of a single training or trial, </span><strong class="source-inline"><span class="koboSpan" id="kobo.756.1">run</span></strong><span class="koboSpan" id="kobo.757.1"> groups related artifacts such as parameters, metrics, </span><span class="No-Break"><span class="koboSpan" id="kobo.758.1">and models.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.759.1">prophet.log_model</span></strong><span class="koboSpan" id="kobo.760.1"> – This function logs a Prophet model as an artifact in the current </span><span class="No-Break"><span class="koboSpan" id="kobo.761.1">MLflow run.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.762.1">log_params</span></strong><span class="koboSpan" id="kobo.763.1"> – This logs key-value pairs of parameters used during the run. </span><span class="koboSpan" id="kobo.763.2">Parameters are </span><span class="No-Break"><span class="koboSpan" id="kobo.764.1">model configurations.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.765.1">log_metrics</span></strong><span class="koboSpan" id="kobo.766.1"> – This logs key-value pairs of metrics evaluated during the run. </span><span class="koboSpan" id="kobo.766.2">Metrics are numerical values about the model’s performance (e.g., Mean Squared </span><span class="No-Break"><span class="koboSpan" id="kobo.767.1">Error, accuracy).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.768.1">The outcome of this can then be accessed via the MLflow UI at the following </span><span class="No-Break"><span class="koboSpan" id="kobo.769.1">URL: </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.770.1">http://localhost:5001/</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.771.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.772.1">This will open the UI to a similar page as per </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.773.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.774.1">.4</span></em><span class="koboSpan" id="kobo.775.1">, from where you can navigate on the left panel to the experiment named </span><strong class="source-inline"><span class="koboSpan" id="kobo.776.1">ts-spark_ch4_data-ml-ops_time_series_prophet_notebook</span></strong><span class="koboSpan" id="kobo.777.1">. </span><span class="koboSpan" id="kobo.777.2">This experiment name seen in the UI comes from the code, which is highlighted in the </span><span class="No-Break"><span class="koboSpan" id="kobo.778.1">preceding code.</span></span></p>
<p><span class="koboSpan" id="kobo.779.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.780.1">Overview</span></strong><span class="koboSpan" id="kobo.781.1"> tab for the experiment, shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.782.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.783.1">.7</span></em><span class="koboSpan" id="kobo.784.1">, has information about the experiment such as the </span><a id="_idIndexMarker432"/><span class="koboSpan" id="kobo.785.1">creator, creation date, status, source</span><a id="_idIndexMarker433"/><span class="koboSpan" id="kobo.786.1"> code creating the experiment, and model logged from the experiment. </span><span class="koboSpan" id="kobo.786.2">It also shows the model parameters and metrics as logged in </span><span class="No-Break"><span class="koboSpan" id="kobo.787.1">the code.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer056">
<span class="koboSpan" id="kobo.788.1"><img alt="" src="image/B18568_04_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.789.1">Figure 4.7: MLflow experiment overview</span></p>
<p><span class="koboSpan" id="kobo.790.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.791.1">Model metrics</span></strong><span class="koboSpan" id="kobo.792.1"> tab, shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.793.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.794.1">.8</span></em><span class="koboSpan" id="kobo.795.1">, allows one to search and view the </span><span class="No-Break"><span class="koboSpan" id="kobo.796.1">metrics graph.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer057">
<span class="koboSpan" id="kobo.797.1"><img alt="" src="image/B18568_04_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.798.1">Figure 4.8: MLflow model metrics</span></p>
<p><span class="koboSpan" id="kobo.799.1">The initial screen of the </span><strong class="bold"><span class="koboSpan" id="kobo.800.1">Artifacts</span></strong><span class="koboSpan" id="kobo.801.1"> tab, shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.802.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.803.1">.9</span></em><span class="koboSpan" id="kobo.804.1">, shows the model schema, which we logged in the</span><a id="_idIndexMarker434"/><span class="koboSpan" id="kobo.805.1"> code as the signature. </span><span class="koboSpan" id="kobo.805.2">It also gives</span><a id="_idIndexMarker435"/><span class="koboSpan" id="kobo.806.1"> code examples of how to use </span><span class="No-Break"><span class="koboSpan" id="kobo.807.1">the model.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer058">
<span class="koboSpan" id="kobo.808.1"><img alt="" src="image/B18568_04_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.809.1">Figure 4.9: MLflow model schema</span></p>
<p><span class="koboSpan" id="kobo.810.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.811.1">MLmodel </span></strong><span class="koboSpan" id="kobo.812.1">section of the </span><strong class="bold"><span class="koboSpan" id="kobo.813.1">Artifacts</span></strong><span class="koboSpan" id="kobo.814.1"> tab, shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.815.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.816.1">.10</span></em><span class="koboSpan" id="kobo.817.1">, shows the model artifact with </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">its path.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer059">
<span class="koboSpan" id="kobo.819.1"><img alt="" src="image/B18568_04_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.820.1">Figure 4.10: MLflow model artifact</span></p>
<p><span class="koboSpan" id="kobo.821.1">This is as far as we will go with MLflow in this example. </span><span class="koboSpan" id="kobo.821.2">We will use MLflow in a similar way in the next example </span><a id="_idIndexMarker436"/><span class="koboSpan" id="kobo.822.1">with an orchestrator and</span><a id="_idIndexMarker437"/><span class="koboSpan" id="kobo.823.1"> expand into further use of MLflow in </span><a href="B18568_09.xhtml#_idTextAnchor169"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.824.1">Chapter 9</span></em></span></a><span class="koboSpan" id="kobo.825.1">, </span><em class="italic"><span class="koboSpan" id="kobo.826.1">Going to Production</span></em><span class="koboSpan" id="kobo.827.1">. </span><span class="koboSpan" id="kobo.827.2">For now, we will be looking at other considerations with the </span><span class="No-Break"><span class="koboSpan" id="kobo.828.1">notebook approach.</span></span></p>
<h3><span class="koboSpan" id="kobo.829.1">Multiple notebooks</span></h3>
<p><span class="koboSpan" id="kobo.830.1">The notebook example here</span><a id="_idIndexMarker438"/><span class="koboSpan" id="kobo.831.1"> is only a starting point that can be adapted and extended based on the requirements of your own use case, as well as the techniques that will be discussed in the following chapters. </span><span class="koboSpan" id="kobo.831.2">For more complex requirements, it is recommended to use separate notebooks for </span><span class="No-Break"><span class="koboSpan" id="kobo.832.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.833.1">Exploratory data analysis and </span><span class="No-Break"><span class="koboSpan" id="kobo.834.1">data science</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.835.1">Feature engineering</span></span></li>
<li><span class="koboSpan" id="kobo.836.1">Model development, selection, and deployment of the </span><span class="No-Break"><span class="koboSpan" id="kobo.837.1">best model</span></span></li>
<li><span class="koboSpan" id="kobo.838.1">Production data pipeline, potentially including feature engineering </span><span class="No-Break"><span class="koboSpan" id="kobo.839.1">as well</span></span></li>
<li><span class="koboSpan" id="kobo.840.1">Production </span><span class="No-Break"><span class="koboSpan" id="kobo.841.1">model inferencing</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.842.1">Monitoring</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.843.1">Model retraining</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.844.1">While notebooks are great for their interactivity, collaborative ease, relative simplicity, and versatility, they have </span><a id="_idIndexMarker439"/><span class="koboSpan" id="kobo.845.1">limitations, which we will cover in the </span><span class="No-Break"><span class="koboSpan" id="kobo.846.1">next section.</span></span></p>
<h3><span class="koboSpan" id="kobo.847.1">Limitations</span></h3>
<p><span class="koboSpan" id="kobo.848.1">However good they are, notebooks</span><a id="_idIndexMarker440"/><span class="koboSpan" id="kobo.849.1"> for end-to-end time series analysis present several challenges. </span><span class="koboSpan" id="kobo.849.2">These are </span><span class="No-Break"><span class="koboSpan" id="kobo.850.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.851.1">No scheduling and orchestration capabilities. </span><span class="koboSpan" id="kobo.851.2">This makes it hard to go beyond simple sequential workflows and develop </span><span class="No-Break"><span class="koboSpan" id="kobo.852.1">complex workflows.</span></span></li>
<li><span class="koboSpan" id="kobo.853.1">Scalability issue. </span><span class="koboSpan" id="kobo.853.2">The notebook code runs in the notebook kernel, which is limited to the resource of the single machine where it is located. </span><span class="koboSpan" id="kobo.853.3">Note that this can be resolved by submitting the task from the notebook to run on the Apache Spark cluster, as we have done in </span><span class="No-Break"><span class="koboSpan" id="kobo.854.1">our example.</span></span></li>
<li><span class="koboSpan" id="kobo.855.1">Lack of error handling. </span><span class="koboSpan" id="kobo.855.2">If the code in a notebook cell fails, the whole workflow execution stops. </span><span class="koboSpan" id="kobo.855.3">It is, of course, possible to write error-handling code, but this adds additional </span><span class="No-Break"><span class="koboSpan" id="kobo.856.1">coding effort.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.857.1">To answer these challenges, we will be considering another approach next, using </span><span class="No-Break"><span class="koboSpan" id="kobo.858.1">an orchestrator.</span></span></p>
<h2 id="_idParaDest-99"><a id="_idTextAnchor099"/><span class="koboSpan" id="kobo.859.1">Orchestrator approach</span></h2>
<p><span class="koboSpan" id="kobo.860.1">Before diving into the approach, let’s </span><a id="_idIndexMarker441"/><span class="koboSpan" id="kobo.861.1">first understand what an orchestrator means. </span><span class="koboSpan" id="kobo.861.2">Airflow, which we will use here, was mentioned earlier as </span><span class="No-Break"><span class="koboSpan" id="kobo.862.1">an example.</span></span></p>
<p><span class="koboSpan" id="kobo.863.1">An </span><strong class="bold"><span class="koboSpan" id="kobo.864.1">orchestrator</span></strong><span class="koboSpan" id="kobo.865.1"> plays a central role in managing workflows, including data engineering and processing. </span><span class="koboSpan" id="kobo.865.2">A workflow or </span><a id="_idIndexMarker442"/><span class="koboSpan" id="kobo.866.1">pipeline is a set of computing tasks executed together in a certain order, in parallel or sequentially, usually with dependency on the outcome of the preceding task or tasks. </span><span class="koboSpan" id="kobo.866.2">In addition to scheduling the workflows, an orchestrator usually has features to author them before and monitor their </span><a id="_idIndexMarker443"/><span class="No-Break"><span class="koboSpan" id="kobo.867.1">execution post-scheduling.</span></span></p>
<h3><span class="koboSpan" id="kobo.868.1">Benefits of an orchestrator</span></h3>
<p><span class="koboSpan" id="kobo.869.1">Using an orchestrator </span><a id="_idIndexMarker444"/><span class="koboSpan" id="kobo.870.1">provides the following benefits over the limitations of the </span><span class="No-Break"><span class="koboSpan" id="kobo.871.1">notebook-only approach:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.872.1">Scheduling the tasks within the workflows, considering their dependencies and parallel or sequential execution requirements. </span><span class="koboSpan" id="kobo.872.2">This also includes conditional logic for </span><span class="No-Break"><span class="koboSpan" id="kobo.873.1">task execution.</span></span></li>
<li><span class="koboSpan" id="kobo.874.1">Scalable and distributed </span><span class="No-Break"><span class="koboSpan" id="kobo.875.1">task execution.</span></span></li>
<li><span class="koboSpan" id="kobo.876.1">Monitoring and logging workflow execution, including performance and errors. </span><span class="koboSpan" id="kobo.876.2">This is crucial for </span><span class="No-Break"><span class="koboSpan" id="kobo.877.1">production environments.</span></span></li>
<li><span class="koboSpan" id="kobo.878.1">Error handling and alerting with possibilities to retry, skip to the next task, or fail the entire pipeline. </span><span class="koboSpan" id="kobo.878.2">This is also a key requirement for </span><span class="No-Break"><span class="koboSpan" id="kobo.879.1">production environments.</span></span></li>
<li><span class="koboSpan" id="kobo.880.1">Integration with other systems and tools. </span><span class="koboSpan" id="kobo.880.2">This is required to build end-to-end workflows, covering </span><a id="_idIndexMarker445"/><span class="koboSpan" id="kobo.881.1">DataOps, ModelOps, and DevOps, which usually means working with different </span><span class="No-Break"><span class="koboSpan" id="kobo.882.1">specialized tools.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.883.1">Now that we have seen the benefits and have the environment set up with Airflow as the orchestrator, let’s get into </span><span class="No-Break"><span class="koboSpan" id="kobo.884.1">the practice.</span></span></p>
<h3><span class="koboSpan" id="kobo.885.1">Authoring the workflow</span></h3>
<p><span class="koboSpan" id="kobo.886.1">The first step is to create</span><a id="_idIndexMarker446"/><span class="koboSpan" id="kobo.887.1"> the workflow or </span><strong class="bold"><span class="koboSpan" id="kobo.888.1">direct acyclic graph</span></strong><span class="koboSpan" id="kobo.889.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.890.1">DAG</span></strong><span class="koboSpan" id="kobo.891.1">) as it is </span><span class="No-Break"><span class="koboSpan" id="kobo.892.1">also called.</span></span></p>
<p><span class="koboSpan" id="kobo.893.1">If you followed the</span><a id="_idIndexMarker447"/><span class="koboSpan" id="kobo.894.1"> instructions in the earlier </span><em class="italic"><span class="koboSpan" id="kobo.895.1">Environment setup</span></em><span class="koboSpan" id="kobo.896.1"> section, the example DAG is already loaded</span><a id="_idIndexMarker448"/><span class="koboSpan" id="kobo.897.1"> and accessible directly within the Airflow UI, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.898.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.899.1">.5</span></em><span class="koboSpan" id="kobo.900.1">, at the following URL: </span><strong class="source-inline"><span class="koboSpan" id="kobo.901.1">http://localhost:8080/</span></strong><span class="koboSpan" id="kobo.902.1">. </span><span class="koboSpan" id="kobo.902.2">At this point, you can jump to the next section to run the DAG or continue here for details on the </span><span class="No-Break"><span class="koboSpan" id="kobo.903.1">DAG code.</span></span></p>
<p><span class="koboSpan" id="kobo.904.1">The DAG definition is in a Python code file in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.905.1">dags</span></strong><span class="koboSpan" id="kobo.906.1"> folder and is also downloadable from the following </span><span class="No-Break"><span class="koboSpan" id="kobo.907.1">GitHub location:</span></span></p>
<p><a href="https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch4/dags/ts-spark_ch4_airflow-dag.py"><span class="No-Break"><span class="koboSpan" id="kobo.908.1">https://github.com/PacktPublishing/Time-Series-Analysis-with-Spark/raw/main/ch4/dags/ts-spark_ch4_airflow-dag.py</span></span></a></p>
<p><span class="koboSpan" id="kobo.909.1">The core of the code</span><a id="_idIndexMarker449"/><span class="koboSpan" id="kobo.910.1"> is very similar to what we saw in the previous notebook example. </span><span class="koboSpan" id="kobo.910.2">This section focuses </span><a id="_idIndexMarker450"/><span class="koboSpan" id="kobo.911.1">on integrating with Airflow and defining the DAG’s tasks, which are the individual steps of </span><span class="No-Break"><span class="koboSpan" id="kobo.912.1">the DAG.</span></span></p>
<h4><span class="koboSpan" id="kobo.913.1">Task definition – Python code</span></h4>
<p><span class="koboSpan" id="kobo.914.1">When the orchestrator runs the tasks, it calls</span><a id="_idIndexMarker451"/><span class="koboSpan" id="kobo.915.1"> the following corresponding Python functions as the underlying code that needs to be executed. </span><span class="koboSpan" id="kobo.915.2">Note the function parameters that are passed in and the return values. </span><span class="koboSpan" id="kobo.915.3">These are aligned with the task’s definition, which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.916.1">see next:</span></span></p>
<ol>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.917.1">ingest_data</span></strong><span class="koboSpan" id="kobo.918.1"> – for task </span><strong class="source-inline"><span class="koboSpan" id="kobo.919.1">t1</span></strong><span class="koboSpan" id="kobo.920.1">. </span><span class="koboSpan" id="kobo.920.2">Note that </span><strong class="source-inline"><span class="koboSpan" id="kobo.921.1">spark.read</span></strong><span class="koboSpan" id="kobo.922.1"> will run on the </span><span class="No-Break"><span class="koboSpan" id="kobo.923.1">Spark cluster:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.924.1">
def </span><strong class="bold"><span class="koboSpan" id="kobo.925.1">ingest_data</span></strong><span class="koboSpan" id="kobo.926.1">():
    sdf = </span><strong class="bold"><span class="koboSpan" id="kobo.927.1">spark.read</span></strong><span class="koboSpan" id="kobo.928.1">.csv(
        DATASOURCE, header=True, inferSchema=True)
    pdf = sdf.select("date", "daily_min_temperature").toPandas()
    return pdf</span></pre></li> <li><strong class="source-inline"><span class="koboSpan" id="kobo.929.1">transform_data</span></strong><span class="koboSpan" id="kobo.930.1"> – for </span><span class="No-Break"><span class="koboSpan" id="kobo.931.1">task </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.932.1">t2</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.933.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.934.1">
def </span><strong class="bold"><span class="koboSpan" id="kobo.935.1">transform_data</span></strong><span class="koboSpan" id="kobo.936.1">(pdf, **kwargs):
    pdf.columns = ["ds", "y"]
    pdf["y"] = pd.to_numeric(pdf["y"], errors="coerce")
    pdf.drop(index=pdf.index[-2:], inplace=True)
    pdf.dropna()
    return pdf</span></pre></li> <li><strong class="source-inline"><span class="koboSpan" id="kobo.937.1">train_and_log_model</span></strong><span class="koboSpan" id="kobo.938.1"> – for task </span><strong class="source-inline"><span class="koboSpan" id="kobo.939.1">t3</span></strong><span class="koboSpan" id="kobo.940.1">. </span><span class="koboSpan" id="kobo.940.2">Note that the MLflow functions, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.941.1">mlflow.set_experiment</span></strong><span class="koboSpan" id="kobo.942.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.943.1">mlflow.prophet.log_model</span></strong><span class="koboSpan" id="kobo.944.1">, make calls to the</span><a id="_idIndexMarker452"/><span class="koboSpan" id="kobo.945.1"> MLflow server. </span><span class="koboSpan" id="kobo.945.2">A partial extract of the code is </span><span class="No-Break"><span class="koboSpan" id="kobo.946.1">shown here:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.947.1">
def </span><strong class="bold"><span class="koboSpan" id="kobo.948.1">train_and_log_model</span></strong><span class="koboSpan" id="kobo.949.1">(pdf, **kwargs):
    </span><strong class="bold"><span class="koboSpan" id="kobo.950.1">mlflow.set_experiment</span></strong><span class="koboSpan" id="kobo.951.1">(
        'ts-spark_ch4_data-ml-ops_time_series_prophet')
    …
        </span><strong class="bold"><span class="koboSpan" id="kobo.952.1">mlflow.prophet.log_model</span></strong><span class="koboSpan" id="kobo.953.1">(
            model, artifact_path=ARTIFACT_DIR,
            signature=signature)
    …
        return model_uri</span></pre></li> <li><strong class="source-inline"><span class="koboSpan" id="kobo.954.1">forecast</span></strong><span class="koboSpan" id="kobo.955.1"> – for task </span><strong class="source-inline"><span class="koboSpan" id="kobo.956.1">t4</span></strong><span class="koboSpan" id="kobo.957.1">. </span><span class="koboSpan" id="kobo.957.2">Note that </span><strong class="source-inline"><span class="koboSpan" id="kobo.958.1">mlflow.prophet.load_model</span></strong><span class="koboSpan" id="kobo.959.1"> loads the model from the MLflow server. </span><span class="koboSpan" id="kobo.959.2">This is done in this way here only to show how to retrieve the model from an MLflow server. </span><span class="koboSpan" id="kobo.959.3">It is not strictly required here as we could have kept the reference to the </span><span class="No-Break"><span class="koboSpan" id="kobo.960.1">model locally:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.961.1">
def </span><strong class="bold"><span class="koboSpan" id="kobo.962.1">forecast</span></strong><span class="koboSpan" id="kobo.963.1">(model_uri, **kwargs):
    _model = </span><strong class="bold"><span class="koboSpan" id="kobo.964.1">mlflow.prophet.load_model</span></strong><span class="koboSpan" id="kobo.965.1">(model_uri)
    forecast = _model.predict(
        _model.make_future_dataframe(30))
    forecast[
        ['ds', 'yhat', 'yhat_lower','yhat_upper']
    ].to_csv('/data/ts-spark_ch4_prophet-forecast.csv')
    return '/data/ts-spark_ch4_prophet-forecast.csv'</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.966.1">These tasks are</span><a id="_idIndexMarker453"/><span class="koboSpan" id="kobo.967.1"> referenced by the DAG, which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.968.1">define next.</span></span></p>
<h4><span class="koboSpan" id="kobo.969.1">DAG definition</span></h4>
<p><span class="koboSpan" id="kobo.970.1">Overarching the preceding task</span><a id="_idIndexMarker454"/><span class="koboSpan" id="kobo.971.1"> definitions, we have the high-level Airflow DAG, which is defined as per </span><span class="No-Break"><span class="koboSpan" id="kobo.972.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.973.1">
dag = </span><strong class="bold"><span class="koboSpan" id="kobo.974.1">DAG</span></strong><span class="koboSpan" id="kobo.975.1">(
    'ts-spark_ch4_data-ml-ops_time_series_prophet',
    default_args=</span><strong class="bold"><span class="koboSpan" id="kobo.976.1">default_args</span></strong><span class="koboSpan" id="kobo.977.1">,
    description='ts-spark_ch4 - Data/MLOps pipeline example - Time series forecasting with Prophet',
    </span><strong class="bold"><span class="koboSpan" id="kobo.978.1">schedule_interval</span></strong><span class="koboSpan" id="kobo.979.1">=None
)</span></pre> <p><span class="koboSpan" id="kobo.980.1">This points to </span><strong class="source-inline"><span class="koboSpan" id="kobo.981.1">default_args</span></strong><span class="koboSpan" id="kobo.982.1">, which contains the following </span><span class="No-Break"><span class="koboSpan" id="kobo.983.1">DAG parameters.</span></span></p>
<pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.984.1">default_args</span></strong><span class="koboSpan" id="kobo.985.1"> = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=1),
}</span></pre> <p><span class="koboSpan" id="kobo.986.1">Further information</span><a id="_idIndexMarker455"/><span class="koboSpan" id="kobo.987.1"> on these is available in the following </span><span class="No-Break"><span class="koboSpan" id="kobo.988.1">Airflow documentation:</span></span></p>
<p><a href="https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/baseoperator/index.html#airflow.models.baseoperator.BaseOperator"><span class="No-Break"><span class="koboSpan" id="kobo.989.1">https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/baseoperator/index.html#airflow.models.baseoperator.BaseOperator</span></span></a></p>
<p><span class="koboSpan" id="kobo.990.1">We have not set </span><strong class="source-inline"><span class="koboSpan" id="kobo.991.1">schedule_interval</span></strong><span class="koboSpan" id="kobo.992.1"> as we want to trigger the DAG manually from the </span><span class="No-Break"><span class="koboSpan" id="kobo.993.1">Airflow UI.</span></span></p>
<h4><span class="koboSpan" id="kobo.994.1">DAG tasks</span></h4>
<p><span class="koboSpan" id="kobo.995.1">The DAG tasks are defined as</span><a id="_idIndexMarker456"/><span class="koboSpan" id="kobo.996.1"> per the following. </span><span class="koboSpan" id="kobo.996.2">Note the reference to </span><strong class="source-inline"><span class="koboSpan" id="kobo.997.1">dag</span></strong><span class="koboSpan" id="kobo.998.1"> and to the underlying Python function defined previously. </span><span class="koboSpan" id="kobo.998.2">The use of </span><strong class="source-inline"><span class="koboSpan" id="kobo.999.1">PythonOperator</span></strong><span class="koboSpan" id="kobo.1000.1"> means that the tasks will be calling </span><span class="No-Break"><span class="koboSpan" id="kobo.1001.1">Python functions:</span></span></p>
<ol>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1002.1">t1</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1003.1">:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.1004.1">t1</span></strong><span class="koboSpan" id="kobo.1005.1"> = </span><strong class="bold"><span class="koboSpan" id="kobo.1006.1">PythonOperator</span></strong><span class="koboSpan" id="kobo.1007.1">(
    task_id='ingest_data',
    python_callable=</span><strong class="bold"><span class="koboSpan" id="kobo.1008.1">ingest_data</span></strong><span class="koboSpan" id="kobo.1009.1">,
    dag=</span><strong class="bold"><span class="koboSpan" id="kobo.1010.1">dag</span></strong><span class="koboSpan" id="kobo.1011.1">,
)</span></pre></li> <li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1012.1">t2</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1013.1">:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.1014.1">t2</span></strong><span class="koboSpan" id="kobo.1015.1"> = PythonOperator(
    task_id='transform_data',
    python_callable=</span><strong class="bold"><span class="koboSpan" id="kobo.1016.1">transform_data</span></strong><span class="koboSpan" id="kobo.1017.1">,
    op_kwargs={'</span><strong class="bold"><span class="koboSpan" id="kobo.1018.1">pdf</span></strong><span class="koboSpan" id="kobo.1019.1">': </span><strong class="bold"><span class="koboSpan" id="kobo.1020.1">t1.output</span></strong><span class="koboSpan" id="kobo.1021.1">},
    provide_context=True,
    dag=dag,
)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1022.1">Notable for task </span><strong class="source-inline"><span class="koboSpan" id="kobo.1023.1">t2</span></strong><span class="koboSpan" id="kobo.1024.1"> is how the </span><a id="_idIndexMarker457"/><span class="koboSpan" id="kobo.1025.1">output from task </span><strong class="source-inline"><span class="koboSpan" id="kobo.1026.1">t1</span></strong><span class="koboSpan" id="kobo.1027.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1028.1">t1.output</span></strong><span class="koboSpan" id="kobo.1029.1">, is passed as input, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1030.1">pdf</span></strong><span class="koboSpan" id="kobo.1031.1">, to </span><span class="No-Break"><span class="koboSpan" id="kobo.1032.1">task </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1033.1">t2</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1034.1">.</span></span></p></li> <li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1035.1">t3</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1036.1">:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.1037.1">t3</span></strong><span class="koboSpan" id="kobo.1038.1"> = PythonOperator(
    task_id='train_and_log_model',
    python_callable=</span><strong class="bold"><span class="koboSpan" id="kobo.1039.1">train_and_log_model</span></strong><span class="koboSpan" id="kobo.1040.1">,
    op_kwargs={'</span><strong class="bold"><span class="koboSpan" id="kobo.1041.1">pdf</span></strong><span class="koboSpan" id="kobo.1042.1">': </span><strong class="bold"><span class="koboSpan" id="kobo.1043.1">t2.output</span></strong><span class="koboSpan" id="kobo.1044.1">},
    provide_context=True,
    dag=dag,
)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1045.1">The output from task </span><strong class="source-inline"><span class="koboSpan" id="kobo.1046.1">t2</span></strong><span class="koboSpan" id="kobo.1047.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1048.1">t2.output</span></strong><span class="koboSpan" id="kobo.1049.1">, is passed as input, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1050.1">pdf</span></strong><span class="koboSpan" id="kobo.1051.1">, to </span><span class="No-Break"><span class="koboSpan" id="kobo.1052.1">task </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1053.1">t3</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1054.1">.</span></span></p></li> <li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1055.1">t4</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1056.1">:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.1057.1">t4</span></strong><span class="koboSpan" id="kobo.1058.1"> = PythonOperator(
    task_id='forecast',
    python_callable=</span><strong class="bold"><span class="koboSpan" id="kobo.1059.1">forecast</span></strong><span class="koboSpan" id="kobo.1060.1">,
    op_kwargs={'</span><strong class="bold"><span class="koboSpan" id="kobo.1061.1">model_uri</span></strong><span class="koboSpan" id="kobo.1062.1">': </span><strong class="bold"><span class="koboSpan" id="kobo.1063.1">t3.output</span></strong><span class="koboSpan" id="kobo.1064.1">},
    provide_context=True,
    dag=dag,
)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1065.1">The output from task </span><strong class="source-inline"><span class="koboSpan" id="kobo.1066.1">t3</span></strong><span class="koboSpan" id="kobo.1067.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1068.1">t3.output</span></strong><span class="koboSpan" id="kobo.1069.1">, is passed as input, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1070.1">model_uri</span></strong><span class="koboSpan" id="kobo.1071.1">, to </span><span class="No-Break"><span class="koboSpan" id="kobo.1072.1">task </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1073.1">t4</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1074.1">.</span></span></p></li> </ol>
<p><span class="koboSpan" id="kobo.1075.1">These tasks are then configured with the following code to be orchestrated sequentially </span><span class="No-Break"><span class="koboSpan" id="kobo.1076.1">by Airflow:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1077.1">
# Task dependencies
t1 &gt;&gt; t2 &gt;&gt; t3 &gt;&gt; t4</span></pre> <p><span class="koboSpan" id="kobo.1078.1">This concludes the</span><a id="_idIndexMarker458"/><span class="koboSpan" id="kobo.1079.1"> workflow definition as a DAG in Airflow. </span><span class="koboSpan" id="kobo.1079.2">The example here is only a starting point, with a simple sequential workflow that can be adapted and extended based on your specific requirements and the additional time series analysis tasks that will be discussed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1080.1">following chapters.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.1081.1">Orchestrating notebooks</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1082.1">Note that it is also possible to combine the orchestrator and notebook approach by calling notebooks from Airflow tasks</span><a id="_idIndexMarker459"/><span class="koboSpan" id="kobo.1083.1"> using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1084.1">PapermillOperator</span></strong><span class="koboSpan" id="kobo.1085.1"> operator. </span><span class="koboSpan" id="kobo.1085.2">You can find more information on this operator </span><span class="No-Break"><span class="koboSpan" id="kobo.1086.1">here: </span></span><a href="https://airflow.apache.org/docs/apache-airflow-providers-papermill/stable/operators.html"><span class="No-Break"><span class="koboSpan" id="kobo.1087.1">https://airflow.apache.org/docs/apache-airflow-providers-papermill/stable/operators.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1088.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1089.1">Once the DAG is written and placed in Airflow’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.1090.1">dags</span></strong><span class="koboSpan" id="kobo.1091.1"> folder, it will be automatically picked up by Airflow, checked for syntax errors in the Python definition file, and then listed in the list of DAGs available to run, which we will </span><span class="No-Break"><span class="koboSpan" id="kobo.1092.1">cover next.</span></span></p>
<h3><span class="koboSpan" id="kobo.1093.1">Running the workflow</span></h3>
<p><span class="koboSpan" id="kobo.1094.1">The workflow can be launched by clicking </span><a id="_idIndexMarker460"/><span class="koboSpan" id="kobo.1095.1">on the run button (&gt;) on the right of the DAG, as seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1096.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1097.1">.5</span></em><span class="koboSpan" id="kobo.1098.1"> of the </span><em class="italic"><span class="koboSpan" id="kobo.1099.1">Accessing the UIs</span></em><span class="koboSpan" id="kobo.1100.1"> section. </span><span class="koboSpan" id="kobo.1100.2">By clicking on the DAG name on the left panel, the details and graph of the DAG can be viewed in the Airflow UI, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1101.1">Figure 4</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1102.1">.11</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1103.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer060">
<span class="koboSpan" id="kobo.1104.1"><img alt="" src="image/B18568_04_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1105.1">Figure 4.11: Airflow DAG</span></p>
<p><span class="koboSpan" id="kobo.1106.1">To view information on a specific run and task of the DAG, select the run on the left, and then the task from the graph. </span><span class="koboSpan" id="kobo.1106.2">This will provide the option to view the execution log of </span><span class="No-Break"><span class="koboSpan" id="kobo.1107.1">the task.</span></span></p>
<p><span class="koboSpan" id="kobo.1108.1">Another interesting piece of information is the execution time of the different tasks, which can be viewed from the </span><strong class="bold"><span class="koboSpan" id="kobo.1109.1">Gantt</span></strong><span class="koboSpan" id="kobo.1110.1"> tab on the </span><span class="No-Break"><span class="koboSpan" id="kobo.1111.1">same screen.</span></span></p>
<p><span class="koboSpan" id="kobo.1112.1">We are only exploring the surface of Airflow here, which is a feature-rich tool beyond the scope of this book. </span><span class="koboSpan" id="kobo.1112.2">Refer to </span><a id="_idIndexMarker461"/><span class="koboSpan" id="kobo.1113.1">the Airflow documentation for </span><span class="No-Break"><span class="koboSpan" id="kobo.1114.1">more information.</span></span></p>
<p><span class="koboSpan" id="kobo.1115.1">As was mentioned earlier, some of the code runs on the Apache Spark cluster. </span><span class="koboSpan" id="kobo.1115.2">This can be visualized from the Spark master node, as per </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1116.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1117.1">.6</span></em><span class="koboSpan" id="kobo.1118.1"> in </span><a href="B18568_03.xhtml#_idTextAnchor063"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1119.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.1120.1">. </span><span class="koboSpan" id="kobo.1120.2">The URL is the following: </span><strong class="source-inline"><span class="koboSpan" id="kobo.1121.1">http://localhost:8070/</span></strong><span class="koboSpan" id="kobo.1122.1">. </span><span class="koboSpan" id="kobo.1122.2">The Spark UI will show a running application if it is still running. </span><span class="koboSpan" id="kobo.1122.3">This application is the Spark code launched from the </span><span class="No-Break"><span class="koboSpan" id="kobo.1123.1">Airflow tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.1124.1">As for MLflow, you can view the outcome in the MLflow UI at the following </span><span class="No-Break"><span class="koboSpan" id="kobo.1125.1">URL: </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1126.1">http://localhost:5001/</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1127.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1128.1">From the MLflow UI page, similar to </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1129.1">Figure 4</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1130.1">.4</span></em><span class="koboSpan" id="kobo.1131.1">, you can navigate on the left panel to the experiment named </span><strong class="source-inline"><span class="koboSpan" id="kobo.1132.1">ts-spark_ch4_data-ml-ops_time_series_prophet</span></strong><span class="koboSpan" id="kobo.1133.1">. </span><span class="koboSpan" id="kobo.1133.2">This experiment name seen in the UI comes from the code, which is highlighted in the code for </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1134.1">train_and_log_model</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1135.1"> previously.</span></span></p>
<p><span class="koboSpan" id="kobo.1136.1">`This concludes the second approach discussed in this chapter. </span><span class="koboSpan" id="kobo.1136.2">We will build on this orchestrator example using the concepts we learn in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1137.1">upcoming chapters.</span></span></p>
<h2 id="_idParaDest-100"><a id="_idTextAnchor100"/><span class="koboSpan" id="kobo.1138.1">Environment shutdown</span></h2>
<p><span class="koboSpan" id="kobo.1139.1">We can now stop the container environment. </span><span class="koboSpan" id="kobo.1139.2">The makefile provided simplifies the process with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1140.1">following </span></span><span class="No-Break"><a id="_idIndexMarker462"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1141.1">command:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.1142.1">
make down</span></pre> <p><span class="koboSpan" id="kobo.1143.1">This will give the following or </span><span class="No-Break"><span class="koboSpan" id="kobo.1144.1">equivalent output:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.1145.1">
docker-compose down
[+] Running 10/10
 ✔ Container ts-spark-env-spark-worker-1-1  Removed
 ✔ Container ts-spark-env-mlflow-server-1   Removed
 ✔ Container ts-spark-env-airflow-scheduler-1Removed ✔Container ts-spark-env-airflow-webserver-1Removed ✔ ontainer ts-spark-env-jupyter-1          Removed ✔ Container ts-spark-env-airflow-triggerer-1Removed ✔ Container ts-spark-env-airflow-init-1     Removed ✔ Container ts-spark-env-postgres-1         Removed ✔ Container ts-spark-env-spark-master-1     Removed ✔ Network ts-spark-env_default              Removed</span></pre> <p><span class="koboSpan" id="kobo.1146.1">If you do not intend to use it </span><a id="_idIndexMarker463"/><span class="koboSpan" id="kobo.1147.1">further, you can go ahead and delete the Docker containers created with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1148.1">Delete</span></strong><span class="koboSpan" id="kobo.1149.1"> action, as explained </span><span class="No-Break"><span class="koboSpan" id="kobo.1150.1">here: </span></span><a href="https://docs.docker.com/desktop/use-desktop/container/#container-actions"><span class="No-Break"><span class="koboSpan" id="kobo.1151.1">https://docs.docker.com/desktop/use-desktop/container/#container-actions</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1152.1">.</span></span></p>
<h1 id="_idParaDest-101"><a id="_idTextAnchor101"/><span class="koboSpan" id="kobo.1153.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1154.1">In this chapter, we detailed the important phases of a time series analysis project, starting with the choice of a use case corresponding to a business requirement. </span><span class="koboSpan" id="kobo.1154.2">The use case was then mapped to the technical solution, with DataOps, ModelOps, and DevOps components. </span><span class="koboSpan" id="kobo.1154.3">We finally looked at two approaches for implementation, including examples of baseline implementations with an all-in-one notebook and with an orchestrator, which will be further extended in the rest of </span><span class="No-Break"><span class="koboSpan" id="kobo.1155.1">this book.</span></span></p>
<p><span class="koboSpan" id="kobo.1156.1">In the following chapter, we will do just that, focusing on DataOps with </span><span class="No-Break"><span class="koboSpan" id="kobo.1157.1">data preparation.</span></span></p>
<h1 id="_idParaDest-102"><a id="_idTextAnchor102"/><span class="koboSpan" id="kobo.1158.1">Join our community on Discord</span></h1>
<p><span class="koboSpan" id="kobo.1159.1">Join our community’s Discord space for discussions with the authors and </span><span class="No-Break"><span class="koboSpan" id="kobo.1160.1">other readers:</span></span></p>
<p><a href="https://packt.link/ds"><span class="No-Break"><span class="koboSpan" id="kobo.1161.1">https://packt.link/ds</span></span></a></p>
<div>
<div class="IMG---Figure" id="_idContainer061">
<span class="koboSpan" id="kobo.1162.1"><img alt="" src="image/ds_(1).jpg"/></span>
</div>
</div>
</div>
<div>
<div epub:type="chapter" id="_idContainer063">
</div>
</div>
</body></html>