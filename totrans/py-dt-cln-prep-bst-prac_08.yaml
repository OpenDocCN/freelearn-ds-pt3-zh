- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detecting and Handling Missing Values and Outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter discusses the techniques of handling missing values and outliers,
    two critical challenges that can significantly impact the integrity and accuracy
    of our data products. We will explore a wide range of techniques to identify and
    manage these data irregularities, ranging from statistical methods to advanced
    machine learning models. Through practical examples and real-world datasets, we
    will present strategies to tackle these issues head-on, ensuring that our analyses
    are robust, reliable, and capable of generating meaningful insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key points for the chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting and handling missing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting univariate and multivariate outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling univariate and multivariate outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find all the code for the chapter in the link that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter08](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter08)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The different code files follow the names of the different parts of the chapters.
    Let''s install the following library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Detecting missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Missing data is a common and inevitable issue in real-world datasets. It occurs
    when one or more values are absent in a particular observation or record. This
    data gap can greatly impact the validity and reliability of any analysis or model
    built with those data. As we say in the data world: *garbage in, garbage out*,
    meaning that if your data is not correct, then the models or analysis created
    with that data will not be correct either.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following parts, we will use a scenario to demonstrate how to detect
    missing data and how the different imputation methods work. The scenario is the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Imagine you are analyzing a dataset containing information about students,
    including their ages and test scores. However, due to various reasons, some ages
    and test scores* *are missing.*'
  prefs: []
  type: TYPE_NORMAL
- en: The code for this section can be found at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/1.detect_missing_data.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/1.detect_missing_data.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this script, we create the data that we will use across the chapter. Let’s
    start with the import statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s generate student data with missing ages and test scores. This dictionary
    data contains two keys, `Age` and `Test_Score`, each with a list of values. Some
    of these values are `None`, indicating missing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The first five rows of the dataset are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, there are NaN values in both columns of the dataset. To understand
    the extent of the missing values in the dataset, let’s count how many we have
    across the whole DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `df.isnull()` method creates a `missing_values` DataFrame of the same shape
    as `df`, where each cell is `True` if the corresponding cell in `df` is `None`
    (missing value) and `False` otherwise, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous DataFrame, any cell that contained a `NaN` value is now replaced
    with `True`. Having the data in that format helps us calculate how many `NaN`
    values we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `missing_values.any(axis=1)` argument checks each row to see whether it
    contains any missing values, returning a Series of `True` or `False` for each
    row. Then the `.sum()` counts the number of `True` values in this Series, giving
    the number of rows with at least one missing value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now we know how much data is missing from our dataset. The next goal of this
    exercise is to find the best imputation method to fill those.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Addressing missing data involves making careful decisions to minimize its impact
    on analyses and models. The most common strategies include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Removing records with missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filling in missing values using various techniques such as mean, median, mode
    imputation, or more advanced methods such as regression-based imputation or k-nearest
    neighbors imputation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing binary indicator variables to flag missing data; this can inform
    models about the presence of missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging subject matter expertise to understand the reasons for missing data
    and make informed decisions about how to handle it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s deep dive into each of these methods and observe in detail the results
    on the dataset presented in the previous part.
  prefs: []
  type: TYPE_NORMAL
- en: Deletion of missing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One approach to handling missing data is to simply remove records (rows) that
    contain missing values. It is a quick and simple strategy, and is generally more
    suitable when the percentage of missing data is *low* and the missing data appears
    in random places.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start deleting data, we need to understand our dataset a bit better.
    Continuing on the data created in the previous example, let’s print the descriptive
    statistics first before we start deleting data points. The code for this part
    can be found at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/2.delete_missing_data.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/2.delete_missing_data.py).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To keep the chapter to a nice number of pages, we have only presented the key
    code snippets. To see all the examples, please go to the repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the descriptive statistics, we can simply call the `.describe()`
    method in pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The descriptive statistics are presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Let’s also create the distribution plots for each column of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Distribution of features before any alteration](img/B19801_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Distribution of features before any alteration
  prefs: []
  type: TYPE_NORMAL
- en: With this analysis done, we can get some key insights into the dataset. For
    `Age`, with a count of 20, the average age is approximately 33.7 years, with a
    standard deviation of 18.9 years, showing moderate variability. Ages range from
    18 to 90 years, with the middle 50% of ages falling between 21.75 and 38.5 years.
    For `Test_Score`, based on 19 values, the mean score is around 65.8, with a higher
    standard deviation of 27.9, indicating more variability in scores. Test scores
    range from 5 to 94, with the **Interquartile Range** (**IQR**) spanning from 54
    to 87.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s have a look at how to delete the missing data. Let’s pay attention
    to how the dataset changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s explore the distribution of the features after the data deletion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Distribution of features after the data deletion](img/B19801_08_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Distribution of features after the data deletion
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also have a look at the summary statistics for the altered dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Having seen the descriptive statistics for both datasets, the observed changes
    are presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Count change**: The count of observations has decreased from 20 to 16 for
    both, age and test scores, after deleting rows with missing values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean change**: The mean age has increased from 33.75 to 36.50, while the
    mean test score has slightly decreased from 65.89 to 65.50\. This change reflects
    the values present in the remaining dataset after the deletion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standard deviation change**: The standard deviation for age has increased
    from 18.90 to 20.11, indicating a greater spread in age, while the standard deviation
    for test scores has decreased from 27.99 to 26.61.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minimum and maximum values**: The minimum age remains the same at 18, but
    the minimum test score remains at 5\. The maximum values for both age and test
    scores have slightly changed, with the maximum test score decreasing from 94 to
    92.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Percentile changes**: The percentile values (25%, 50%, 75%) have shifted
    due to the altered dataset:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The 25th percentile for age has increased from 21.75 to 23.75, and for test
    scores, from 54.00 to 56.00.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The median (50th percentile) for age has increased from 27.50 to 32.00, while
    for test scores, it decreased slightly from 75.00 to 74.50.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The 75th percentile for age has increased from 38.50 to 40.25, while for test
    scores, it decreased from 87.50 to 85.50.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The deletion of rows with missing values has led to a smaller dataset, and the
    remaining data now has *different statistical properties*. This method is suitable
    when the missing values are deemed to be a small proportion of the dataset and
    removing them does not significantly impact the data.
  prefs: []
  type: TYPE_NORMAL
- en: What is a small proportion though?
  prefs: []
  type: TYPE_NORMAL
- en: A common rule of thumb is that if less than 5% of the data is missing, it is
    often considered a small proportion, and deletion might not significantly impact
    the analysis. The significance of the change caused by deleting data points can
    be assessed by comparing the results of analyses with and without the missing
    data. If the results are consistent, the deletion might not be significant.
  prefs: []
  type: TYPE_NORMAL
- en: In these cases of substantial missing data, other imputation methods or advanced
    techniques may be more appropriate as we will explore in the next part.
  prefs: []
  type: TYPE_NORMAL
- en: Imputation of missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imputation is often used when removing missing records would result in significant
    information loss. Imputation involves filling in missing values with estimated
    or calculated values. Common imputation methods include mean, median, and mode
    imputation, or using more advanced techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at the different imputation methods for our scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Mean imputation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mean imputation fills missing values with *the mean of the observed values*
    in the variable. It is a very simple method, and it does not introduce bias when
    the values missing are completely random. However, this method is sensitive to
    outliers, and it may distort the distribution of the feature. You can find the
    code for this part in the repo at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/3.mean_imputation.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/3.mean_imputation.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the code example for mean imputation. For this example, we will use
    the same dataset as explained before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line fills any missing values in the `Age` column with the mean
    of the `Age` column from the original `df` DataFrame. The `df[''Age''].mean()`
    argument calculates the mean of the `Age` column, and rounds this mean to the
    nearest whole number. The `fillna()` method then replaces any `NaN` values in
    the `Age` column with this rounded mean. The `inplace=True` argument ensures that
    the changes are made directly in `df_mean_imputed` without creating a new DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, the preceding line fills any missing values in the `Test_Score` column
    of `df_mean_imputed` with the mean of the `Test_Score` column from the original
    `df` DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at the dataset after the imputation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the rounded mean has replaced all the `NaN` values for the age
    feature, whereas the absolute mean (abs mean) has replaced the `NaN` values for
    the `Test_Score` column. We rounded up the mean for the `Age` column to make sure
    it represents something meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: 'The updated distributions are presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Distribution of features after mean imputation](img/B19801_08_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Distribution of features after mean imputation
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see from the graphs that the distributions have slightly changed for
    both of the variables. Let’s have a look at the descriptive statistics of the
    imputed dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Having seen the descriptive statistics for both datasets, the observed changes
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Age` and `Test_Score` increased from 20 to 24 after the imputation, indicating
    that missing values were successfully imputed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean and median changes**: The mean age remained stable, increasing slightly
    from 33.75 to 33.79\. The mean test score stayed the same at 65.89\. The median
    age increased from 27.50 to 33.00, reflecting the changes in the distribution
    of ages. The median test score slightly decreased from 75.00 to 66.95.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Age` decreased from 18.90 to 17.18, indicating reduced variability in ages
    after imputation. The standard deviation for `Test_Score` also decreased from
    27.99 to 24.76, reflecting less variability in test scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Age` increased from 21.75 to 22.75, and Q1 for `Test_Score` increased from
    54.00 to 58.75\. The `Age` slightly decreased from 38.50 to 35.75, and Q3 for
    `Test_Score` remained relatively stable, decreasing slightly from 87.50 to 85.50.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mean imputation maintained the overall mean values and increased the dataset
    size by filling in missing values. However, it has reduced the variability (as
    indicated by the decreased standard deviation for `Age` and `Test_Score`) and
    altered the distribution of the data (particularly in the quartiles). These changes
    are typical of mean imputation, as it tends ot underestimate variability and smooth
    out differences in the data, which can impact certain analyses that are sensitive
    to data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s move on to the median imputation to see how this affects the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Median imputation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Median imputation fills the missing values with the median, the middle value
    of the dataset when it is ordered. Median imputation is robust in the presence
    of outliers and can be a good choice when the distribution is skewed. It can preserve
    the shape of the distribution unless dealing with complex distribution. The code
    for this part can be found at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/4.median_imputation.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/4.median_imputation.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at the code example for the median imputation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This following line fills any missing values in the `Age` column of the `df_median_imputed`
    DataFrame with the median of the `Age` column from the original `df` DataFrame.
    The `df[''Age''].median()` argument calculates the median (the middle value) of
    the `Age` column). The `fillna()` method then replaces any `NaN` values in the
    `Age` column with this median. The `inplace=True` argument ensures that the changes
    are made directly within `df_median_imputed`, without creating a new DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the following line fills any missing values in `Test_Score`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s have a look at the dataset after median imputation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the median has replaced all the `NaN` values for the `Age` feature
    (27.5) and for the `Test_Score` column (75). The updated distributions are as
    follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Distribution of features after median imputation](img/B19801_08_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Distribution of features after median imputation
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see from the graphs that the distributions have slightly changed for
    both of the variables. Let’s have a look at the descriptive statistics of the
    imputed dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Having seen the descriptive statistics for both datasets, the observed changes
    are presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Age` and `Test_Score` increased from 20 (for age) and 19 (for test score)
    to 24 for both variables after median imputation, indicating that missing values
    were successfully imputed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean changes**: The mean age decreased from 33.75 to 32.71 after imputation.
    The mean test score increased slightly from 65.89 to 67.79\. These changes reflect
    the nature of the data remaining after the imputation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Age` decreased from 18.90 to 17.35, indicating a reduction in variability
    for age. The standard deviation for `Test_Score` also decreased from 27.99 to
    25.05, reflecting less variability in the test scores after imputation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Age` increased slightly from 21.75 to 22.75, and the Q1 for `Test_Score` increased
    from 54.00 to 58.75\. Q3 (75%) for `Age` decreased from 38.50 to 35.75, and Q3
    for `Test_Score` decreased slightly from 87.50 to 85.50.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Age` remained stable at 27.50, while the median for `Test_Score` also remained
    stable at 75.00 highlighting the central tendency of the data was preserved after
    imputation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Median imputation has successfully filled in the missing values while preserving
    the median for both `Age` and `Test_Score`. It resulted in a slight change in
    the mean and reduced variability, which is typical of median imputation. The central
    tendency (median) was maintained, which is a key advantage of median imputation,
    especially in skewed distributions. But it also reduces the spread of the data
    which may be relevant for certain types of analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In the next part, we will use what we learned so far on the imputation. We will
    also add an extra step, which involves marking where the missing values exist
    in the dataset for later reference.
  prefs: []
  type: TYPE_NORMAL
- en: Creating indicator variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Indicator variable imputation, also known as flag or dummy variable imputation,
    involves creating a binary indicator variable that flags whether an observation
    has a missing value in a particular variable. This separate dummy variable takes
    the value of 1 for missing values and 0 for observed values. Indicator variable
    imputation can be useful when there is a pattern to the missing values, and you
    want to explicitly model and capture the missingness. Remember here that *we are
    adding a completely new variable, creating a higher dimensional dataset*. After
    we build the indicator variables, *whose role is to remind us which values were
    imputed and which were not*, we go ahead and impute the dataset with any method
    we want such as median or mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the code example for this imputation method. As always, you can see
    the full code in the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/5.indicator_imputation.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/5.indicator_imputation.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, remember that we are using exactly the same DataFrame across the chapter,
    so we have skipped the DataFrame creation here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The code creates new columns in the `df` DataFrame that indicate whether a value
    is missing (`NaN`) in the `Age` and `Test_Score` columns. `df['Age'].isnull()`
    checks each value in the `Age` column to see whether it is `NaN` (missing). It
    returns a Boolean series where `True` indicates a missing value, and `False` indicates
    a non-missing value. The `.astype(int)` method converts the Boolean series into
    an integer series where `True` becomes `1` (indicating a missing value) and `False`
    becomes `0` (indicating no missing value). The `df['Age_missing']` DataFrame stores
    this integer series in a new column named `Age_missing`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, `df[''Test_Score_missing'']` is created to indicate missing values
    in the `Test_Score` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This code fills in the missing values in the `Age` and `Test_Score` columns
    of the `df_imputed` DataFrame with the mean of the respective columns, as we learned
    in the previous part. Let’s have a look at the dataset after the indicator variable
    imputation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the imputed dataset, we added two indicator variables (`Age_missing`
    and `Test_Score_missing`) that take the value of 1 if the corresponding variable
    is missing and 0 otherwise. So, we mainly flag *which values from the original
    rows* *were imputed*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how the distribution of the indicator variables looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Distribution of indicator variables](img/B19801_08_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Distribution of indicator variables
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s explore the relationship between the indicator variables and other
    features in your dataset by building some box plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The created box plots can be seen in *Figure 8**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Box plots comparing the relationship between indicator variables
    and the rest of the features](img/B19801_08_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Box plots comparing the relationship between indicator variables
    and the rest of the features
  prefs: []
  type: TYPE_NORMAL
- en: Reminder – how to read the box plots
  prefs: []
  type: TYPE_NORMAL
- en: '**Box extent**: The box in a box plot represents the IQR, which contains the
    central 50% of the data. Values within the box are considered typical or within
    the normal range.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Whiskers**: Whiskers extend from the box and show the range of typical values.
    Outliers are often defined as values outside a certain multiple (e.g., 1.5 times)
    of the IQR.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Outliers**: Individual data points beyond the whiskers are considered potential
    outliers. Outliers are plotted as individual points or asterisks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Suspected outliers**: Sometimes, points just beyond the whiskers may be plotted
    as suspected outliers, marked separately to indicate they are potential outliers
    but not extreme.'
  prefs: []
  type: TYPE_NORMAL
- en: Back to our example, the box plot of `Test_Score` by `Age Missing` shows that
    when the age is missing in the data, the mean of `Test_Score` is around 80 and
    the distribution values are between 55 and 85\. When `Age` is not missing, the
    mean is around 65, with most of the values being around 60 and 80, with some outliers
    around 20\. Now, when the score is missing, the mean age of the students is around
    20, whereas for the students with scores, the mean age is around 35.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When building predictive models, include the indicator variables as additional
    features to capture the impact of missing values on the target variable. Evaluate
    the performance of models with and without the indicator variables to assess their
    contribution.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison between imputation methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following table provides a guide for selecting the appropriate imputation
    method based on the data’s characteristics and objectives of the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that there is no one-size-fits-all solution!
  prefs: []
  type: TYPE_NORMAL
- en: '| **Imputation method** | **Use cases** | **Pros** | **Cons** |'
  prefs: []
  type: TYPE_TB
- en: '| **Mean imputation** | Normally distributed dataMissing values are MCAR or
    MAR | Simple and easy to implementPreserves the mean of the distribution | Sensitive
    to outliersMay distort the distribution if missingness is not random |'
  prefs: []
  type: TYPE_TB
- en: '| **Median imputation** | Skewed or non-normally distributed dataPresence of
    outliers | Robust to outliersPreserves the median of the distribution | Ignores
    potential relationships between variablesMay be less precise for non-skewed data
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Indicator** **variable imputation** | Systematic pattern in missing data
    | Captures missingness pattern | Increases dimensionality Assumes meaningful missingness
    pattern, which may not always be the case |'
  prefs: []
  type: TYPE_TB
- en: '| **Deletion** **of rows** | MCAR or MAR missingness mechanismPresence of outliers
    | Preserves the existing data structureCan be effective when missingness is random
    | Reduces the sample sizeMay lead to biased results if missingness is not completely
    random |'
  prefs: []
  type: TYPE_TB
- en: Table 8.1 – Comparison between the various imputation methods
  prefs: []
  type: TYPE_NORMAL
- en: In the examples provided, we consistently applied the same imputation method
    to each column of the dataset. However, as demonstrated, our analysis and considerations
    were tailored to each column individually. This implies that we have the flexibility
    to tailor our imputation strategy to the specific characteristics and requirements
    of *each column*. As a practical exercise, take some time to experiment with different
    imputation methods for various columns in your dataset and observe how these choices
    impact your results.
  prefs: []
  type: TYPE_NORMAL
- en: To build on the foundation we’ve established with our imputation strategies,
    it’s essential to recognize that data cleaning doesn’t stop with handling missing
    values. Another critical aspect of data preprocessing is identifying and managing
    outliers. In the next part, we will dive deeper into detecting and handling outliers,
    ensuring our dataset is as accurate and reliable as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting and handling outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Outliers are data points that significantly deviate from the general pattern
    or trend shown by most of the data points in a dataset. They lie at an unusually
    distant location from the center of the data distribution and can have a significant
    impact on statistical analyses, visualizations, and model performance. Defining
    outliers involves recognizing data points that do not conform to the expected
    behavior of the data and understanding the context in which they occur.
  prefs: []
  type: TYPE_NORMAL
- en: Impact of outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Outliers, while often a small fraction of a dataset, wield a disproportionate
    influence that can disrupt the integrity of a dataset. Their presence has the
    potential to distort statistical summaries, mislead visualizations, and negatively
    impact the performance of models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go deeper into the various ways in which outliers distort the truth:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distorted summary statistics**: Outliers can significantly skew summary statistics,
    giving a misleading impression of the central tendencies of the data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean and median**: The mean, a common measure of central tendency, can be
    greatly affected by outliers. An outlier with a value much higher or lower than
    the rest can pull the mean in its direction. On the other hand, the median is
    determined by the middle value of a sorted dataset. It effectively serves as the
    central point that divides the data into two equal halves, making it less susceptible
    to the influence of extreme values.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variance and standard deviation**: Outliers can inflate the variance and
    standard deviation, making the data appear more spread out than it actually is.
    This can misrepresent the variability of the majority of the data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Misleading visualizations**: Outliers can distort the scale and shape of
    visualizations, leading to misinterpretation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Box plots**: Outliers can cause box plots to extend excessively, making the
    bulk of the data appear compressed. This can make the distribution seem less spread
    out than it actually is.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Histograms**: Outliers might lead to the creation of bins that capture only
    a few extreme values, causing other bins to seem disproportionately small and
    the distribution shape to be distorted.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Influence on model performance**: Outliers can negatively affect the performance
    of predictive models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression**: Outliers can heavily influence the slope and intercept of the
    regression line, leading to models that are overly influenced by extreme values.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: Outliers can affect the centroids and boundaries of clusters,
    potentially leading to the creation of clusters that do not accurately represent
    the data distribution.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Outliers can be categorized based on dimensions as univariate versus multivariate.
    In the next section, we will use the example presented in the first part to see
    how we can handle the univariate outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying univariate outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Univariate outliers occur when an extreme value is observed in a single variable,
    regardless of the values of other variables. They are detected based on the distribution
    of a single variable and are often identified using visualizations or statistical
    methods such as Z-score or IQR.
  prefs: []
  type: TYPE_NORMAL
- en: In the next part, we will build one of the most common visualizations to identify
    outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Classic visualizations for identifying outliers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before going deeper into the statistical methods to identify outliers, there
    are a couple of easy visualizations we could build to spot them. The data example
    we have been using so far will still be used for this part; you can find the full
    code at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/6.outliers_visualisation.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/6.outliers_visualisation.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the first visualization, the box plot, where outliers are
    depicted as dots on the left or right of the whisker. The following code snippet
    creates the box plots for each variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The created box plots are presented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Box plots to spot outliers](img/B19801_08_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Box plots to spot outliers
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we can see that the `Age` feature has some clear outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another classic plot is the violin chart, as shown in *Figure 8**.8*. Violin
    plots are a powerful visualization tool that combines aspects of box plots and
    kernel density plots. To create the violin plots, run the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The created violin plots are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Violin plots to spot outliers](img/B19801_08_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Violin plots to spot outliers
  prefs: []
  type: TYPE_NORMAL
- en: 'Reminder – how to read the violin plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Width of the violin**: The width of the violin represents the density of
    the data at different values. A wider section indicates a higher density of data
    points at that specific value, meaning a higher probability that members of the
    population will have the given value; the skinnier sections represent a lower
    probability.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Box-and-whisker elements**: Inside the violin, you may see a box-and-whisker
    plot, similar to what you would see in a traditional box plot. The box represents
    the IQR, and the median is usually displayed as a horizontal line inside the box.
    Whiskers extend from the box to indicate the range of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kernel Density Estimation** (**KDE**): The entire shape of the violin is
    a mirrored representation of the KDE. The KDE provides a smooth representation
    of the data distribution, allowing you to see peaks and valleys in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Outliers**: Outliers may be visible as points beyond the ends of the whiskers
    or outside the overall shape of the violin.'
  prefs: []
  type: TYPE_NORMAL
- en: Now having seen these charts, we are starting to form some hypotheses about
    the existence of outliers specifically in the `Age` column. The next step is to
    use some statistical methods to validate these hypotheses starting with the Z-score
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Z-score method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Z-score method is a statistical technique used to identify univariate outliers
    in a dataset by measuring how far individual data points deviate from the mean
    in terms of standard deviations. The Z-score for a data point is calculated using
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Z = (X − Mean) / Standard Deviation
  prefs: []
  type: TYPE_NORMAL
- en: Here, *X* is the data point, *Mean* is the average of the dataset, and *Standard
    Deviation* quantifies the dispersion of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, a threshold Z-score is chosen to determine outliers. Commonly used
    thresholds are *Z > 3* or *Z <− 3*, indicating that data points deviating more
    than three standard deviations from the mean are considered outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go back to our code example to calculate the Z-score for the `Age` and
    `Test_Score` columns. We will continue with the example we started before. You
    can find the full code at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/7.identify_univariate_outliers.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/7.identify_univariate_outliers.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s calculate the Z-score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `stats.zscore(df[''Age''].dropna())` function calculates the Z-scores for
    the `Age` column. A Z-score represents how many standard deviations a data point
    is from the mean. The `dropna()` function is used to exclude `NaN` values before
    calculating the Z-scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `np.abs()` function takes the absolute value of the Z-scores. This is done
    because Z-scores can be negative (indicating a value below the mean) or positive
    (indicating a value above the mean). By using the absolute value, we’re only concerned
    with the magnitude of deviation from the mean, regardless of direction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '`np.where(z_scores_age > z_threshold)[0]` identifies the indices of the data
    points in the `Age` column that have Z-scores greater than the threshold of `3`.
    The `[0]` at the end is used to extract the indices as an array. The `outliers_age`
    and `outliers_test_score` variables store the indices of the outlier data points
    in the `Age` and `Test_Score` columns, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: If we plot the Z-scores for each observation and feature of the data, we can
    start spotting some outliers already.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Outlier detection with Z-score](img/B19801_08_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – Outlier detection with Z-score
  prefs: []
  type: TYPE_NORMAL
- en: In these scatter plots of Z-scores, each point represents the Z-score of an
    individual data point. The red dashed line indicates the chosen Z-score threshold
    (in this case, `3`). Outliers are identified as points above this threshold. As
    we can see, in `Age`, there is an outlier clearly captured.
  prefs: []
  type: TYPE_NORMAL
- en: How to choose the right threshold for the z score?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A Z-score tells you how many standard deviations a data point is from the mean.
    In a normal distribution, the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: Approximately 68% of data falls within *one standard deviation* of the mean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approximately 95% of data falls within *two* *standard deviations*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approximately 99.7% of data falls within *three* *standard deviations*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means that a Z-score threshold of `3` is often used because it captures
    values that are *extremely far from the mean*, identifying the most extreme outliers.
    In a perfectly normal distribution, only 0.3% of data points will have a Z-score
    greater than 3 or less than -3\. This makes it a reasonable threshold for detecting
    outliers that are unlikely to be part of the normal data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Now, apart from the Z-score, another common method is the IQR, which we will
    discuss in the following part.
  prefs: []
  type: TYPE_NORMAL
- en: IQR method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The IQR is a measure of statistical dispersion, representing the range between
    Q1 and Q3 in a dataset. The IQR is a robust measure of *spread* because it is
    less sensitive to outliers. At this point, it is clear that the IQR is based on
    quartiles. Quartiles divide the dataset into segments, and since Q1 and Q3 are
    less sensitive to extreme values, the IQR is not heavily influenced by outliers.
    On the other hand, standard deviation is influenced by each data point’s deviation
    from the mean. Outliers with large deviations can disproportionately impact the
    standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Reminder – how to calculate the IQR
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculate Q1 (25th percentile)**: Identify the value below which 25% of the
    data falls.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculate Q3 (75th percentile)**: Identify the value below which 75% of the
    data falls.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Calculate IQR**: IQR = Q3 - Q1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To identify potential outliers using IQR, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the lower and upper bounds as follows: Lower Bound = Q1 - 1.5 * IQR,
    Upper Bound = Q3 + 1.5 * IQR.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Any data point outside the lower and upper bounds is considered a potential
    outlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s important to note that the choice of the multiplier (in this case, `1.5`)
    is somewhat arbitrary but has been widely adopted in practice. Adjusting this
    multiplier can make the method more or less sensitive to potential outliers. For
    example, using a larger multiplier would result in broader boundaries, potentially
    identifying more data points as potential outliers, while a smaller multiplier
    would make the method less sensitive.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll be using the same script as before, which can be found at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/7.identify_univariate_outliers.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/7.identify_univariate_outliers.py).
    Let’s have a look at how to calculate the IQR and identify the outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This code defines a function to identify outliers in any column of a DataFrame
    using the IQR method. It calculates the IQR, sets upper and lower bounds for normal
    data, and then filters out the rows where the values in the column fall outside
    these bounds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, let’s identify and print outliers for `Age`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Identify and print outliers for `Test_Score`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this code, we can see in the print statement the identified outliers/rows
    based on the `Age` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: As discussed, the simplicity of the IQR, as well as its robustness to outliers,
    contributes to its popularity in various analytical scenarios. However, it comes
    with certain drawbacks. One limitation is the loss of information, as IQR only
    considers the central 50% of the dataset, disregarding the entire range. Additionally,
    IQR’s sensitivity to sample size, especially in smaller datasets, can affect its
    accuracy in reflecting the true spread of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will quickly discuss leveraging domain knowledge to identify outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Domain knowledge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To better understand the use of domain knowledge in outlier detection, let’s
    use the test scores example. Suppose the dataset represents student test scores,
    and based on educational standards, test scores are expected to fall within a
    range of 0 to 100\. Any score outside this range could be considered an outlier.
    By leveraging domain knowledge in education, we can set these boundaries to identify
    potential outliers. For instance, if a test score is recorded as 120, it would
    likely be flagged as an outlier because it exceeds the maximum possible score
    of 100\. Similarly, negative scores or scores below 0 would be considered outliers.
    Integrating domain knowledge in this manner allows us to establish meaningful
    thresholds for outlier detection, ensuring that the analysis aligns with the expected
    norms within the educational context.
  prefs: []
  type: TYPE_NORMAL
- en: Handling univariate outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Handling univariate outliers refers to the process of identifying, assessing,
    and managing data points in individual variables that deviate significantly from
    the typical patterns or distribution of the dataset. The goal is to mitigate the
    impact of these extreme values on data products.
  prefs: []
  type: TYPE_NORMAL
- en: There are several approaches to handling univariate outliers. We will start
    with deletions, always working on the example presented at the beginning of the
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Deletion of outliers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deleting outliers refers to the process of removing data points in a dataset
    that are considered unusually extreme or deviant from the overall pattern of the
    data. The deletion of outliers comes with trade-offs. On the one hand, it is the
    simplest way to deal with extreme values. On the other hand, it leads to a reduction
    in the sample size and potential loss of valuable information. Additionally, if
    outliers are not genuine errors but rather reflect legitimate variability in the
    data, their removal can introduce bias.
  prefs: []
  type: TYPE_NORMAL
- en: Back to our example, after having imputed the missing data with the mean and
    calculated the IQRs, we dropped the outliers that passed the outlier threshold.
    Let’s see the code that performs these steps; you can also find it in the repository
    at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/8.handle_univariate_outliers_deletions.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/8.handle_univariate_outliers_deletions.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s calculate the IQR and use it to set lower and upper bounds for what is
    considered a normal range of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s define the lower and upper outlier bounds. Any value outside this range
    is flagged as an outlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The last line filters the DataFrame (`df`) to include only rows where the `Test_Score`
    values fall within the calculated lower and upper bounds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In the following charts, we can see the updated distribution charts, after the
    removal of the outliers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Distribution charts after deletion of outliers](img/B19801_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Distribution charts after deletion of outliers
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at the descriptive statistics after the outlier deletion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The changes observed after the deletion of outliers are described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean age change**: The mean age, after deleting outliers, decreased slightly
    from 33.75 to approximately 29.27\. This reduction suggests that the removed outliers
    were older individuals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standard deviation change for age**: The standard deviation for age decreased
    from 17.18 to 8.16, indicating that the spread of ages became slightly narrower
    after outlier removal, which were likely contributing to greater variability in
    the original dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minimum and maximum age values**: The minimum age remained the same, at 18,
    while the maximum age decreased from 90 to 45, indicating that older individuals
    (potential outliers) were removed during outlier handling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean test score change**: The mean test score increased slightly from 65.89
    to 71.20 after removing outliers, suggesting that the deleted outliers were lower
    test scores that were pulling down the original mean.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standard deviation change for test scores**: The standard deviation decreased
    from 24.76 to 17.79, indicating a narrower spread of test scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minimum and maximum test scores**: The minimum test score increased from
    5.00 to 20.00, while the maximum test scores remained the same at 94.00\. This
    indicates that extremely low scores were removed as part of the outlier handling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The removal of outliers led to a decrease in both – the mean age and standard
    deviation, as well as a slight increase in the mean test score. While removing
    outliers can improve data quality, especially when the outliers are due to data
    entry errors or measurement inaccuracies, it also reduces the dataset’s variability.
    If the outliers represent true variability in the population, removing them could
    distort the overall picture of the data. Therefore, careful consideration should
    be given to whether the outliers represent genuine data points or errors.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Some statistical models assume normality and can be sensitive to outliers. Removing
    outliers may help meet the assumptions of certain models. So before deleting,
    you need to better understand the problem you are solving and the techniques to
    be used.
  prefs: []
  type: TYPE_NORMAL
- en: There are other ways to deal with outliers in case you don’t want to drop them
    completely from the data. In the next part, we will discuss the trimming and winsorizing
    of outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Trimming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Trimming involves removing a certain percentage of data from both ends of a
    distribution and then calculating the mean. For the trimming, we need to define
    a trimming fraction, which represents the proportion of data to be trimmed from
    *both tails of the distribution* when calculating the trimmed mean. It is used
    to exclude a certain percentage of extreme values (outliers) from the calculation
    of the mean. The trimming fraction is a value between 0 and 0.5, where the following
    is true:'
  prefs: []
  type: TYPE_NORMAL
- en: 0 means no trimming (include all data points)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0.1 means trim 10% of data from each tail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0.2 means trim 20% of data from each tail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0.5 means trim 50% of data from each tail (exclude the most extreme values)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our given scenario, our analysis indicates that the `Age` column exhibits
    the most significant outliers. In response, we have decided to trim our dataset
    by excluding the top and bottom percentiles specific to the `Age` column. The
    following example code demonstrates this trimming process. We are still working
    on the same dataset so we will skip the creation of the DataFrame here. However,
    you can see the whole code at the following link: [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/9.trimming.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/9.trimming.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at the following code snippet that creates a new DataFrame
    (`df_trimmed`), which includes only the rows where the `Age` value is between
    the 10th and 90th percentiles. This effectively drops the lowest 10% and highest
    10% of values in the `Age` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now calculate the trimmed mean for each column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: After trimming the data, the last line calculates the mean for each column in
    the `df_trimmed` DataFrame. The mean calculated after trimming the data is known
    as the *trimmed mean*. It represents the average value of the central 80% of the
    data, excluding the most extreme 20% (10% from each side).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that the trimming fraction is a way to balance the robustness of
    the trimmed mean against the amount of data excluded. You may need to experiment
    with different fractions to find a suitable balance for your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s have a look at the updated distribution after the trimming:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Distribution charts after trimming of outliers at 10% threshold](img/B19801_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – Distribution charts after trimming of outliers at 10% threshold
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also have a look at the updated statistics of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: In the original dataset, the `Age` column displayed a mean of 33.75 with a standard
    deviation of 17.18, while the trimmed data exhibited a higher mean of 30.22 and
    a much lower standard deviation of 6.76\. The minimum age value increased from
    18 to 20 in the trimmed data, indicating the removal of lower outliers. The maximum
    age value decreased from 90 to 41, suggesting the exclusion of higher outliers.
  prefs: []
  type: TYPE_NORMAL
- en: For the `Test_Score` column, the mean in the original dataset was 65.89, and
    the standard deviation was 24.76\. In the trimmed data, the mean increased to
    69.31, and the standard deviation decreased to 18.80 indicating a narrower spread
    of test scores. The minimum test score increased from 5 to 20, indicating the
    removal of lower outliers, while the maximum test score stayed at 94.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the deletion of outliers led to changes in the central tendency (mean)
    and the spread (standard deviation) of the data for both `Age` and `Test_Score`.
    This indicates that the trimmed dataset has become more concentrated around the
    middle values, with extreme values removed.
  prefs: []
  type: TYPE_NORMAL
- en: Remember!
  prefs: []
  type: TYPE_NORMAL
- en: While trimming can help in reducing the influence of extreme values, it also
    involves discarding a portion of the data. This may result in information loss,
    and the trimmed variable may not fully represent the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will present a slightly different way to deal with the
    outlier called **winsorizing**.
  prefs: []
  type: TYPE_NORMAL
- en: Winsorizing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of removing extreme values outright as with trimming, winsorizing involves
    *replacing them with less extreme values*. The extreme values are replaced with
    values closer to the center of the distribution, often at a specified percentile.
    Winsorizing can be useful when you want to *retain the size of your dataset* and
    helps preserve the overall shape of the data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go back to our example use case and have a look at the code. You can
    find the full code at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/10.winsorizing.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/10.winsorizing.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '`winsorizing_fraction` is set to `0.1`, representing the proportion of data
    to be adjusted at each end of the distribution. It is specified as a percentage,
    and its value typically ranges between 0 and 50%. The process of coming up with
    the winsorizing fraction involves considering the desired amount of influence
    you want to reduce from extreme values. A common choice is to winsorize a certain
    percentage from both tails, such as 5% or 10%.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another thing to know here is that the winsorizing process is performed at
    each column *separately and independently of the others*. Remember: we are dealing
    with outliers in a *univariate* way here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The `limits=[winsorizing_fraction, winsorizing_fraction]` argument specifies
    the proportion of data to be winsorized from each end of the distribution. Here,
    10% from the lower end and 10% from the upper end are adjusted. Extreme values
    (the lowest 10% and highest 10%) are replaced with the nearest values within the
    specified limits, thereby reducing their influence on statistical measures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the updated distributions after the winsorizing are presented:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – Distribution charts after winsorizing of outliers](img/B19801_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – Distribution charts after winsorizing of outliers
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also have a look at the updated statistics of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The mean for the `Age` column decreased from 33.75 to 30.67 after winsorizing,
    indicating a shift toward lower values as extreme high values were adjusted. The
    standard deviation also decreased significantly from 17.18 to 8.86, suggesting
    reduced variability in the dataset. The minimum value increased slightly from
    18 to 19, and the maximum value decreased from 90 to 45, reflecting the capping
    of extreme values.
  prefs: []
  type: TYPE_NORMAL
- en: As for `Test_Score`, the mean remained the same at 65.89 after winsorizing.
    The standard deviation stayed constant at 24.76, indicating that variability in
    test scores was not affected by the winsorizing process. The maximum value stayed
    the same at 94, showing no changes to the upper extreme values.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, winsorizing the `Age` column resulted in a more concentrated distribution
    of values, as evidenced by the decreased standard deviation. Winsorizing successfully
    reduced the impact of extreme values in the `Age` column, making the data more
    focused around the middle range. For `Test_Score`, winsorizing did not affect
    the distribution, likely because the extreme values were already within the accepted
    range.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explore how we can apply mathematical transformations to the data
    to minimize the effect of the outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Data transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Applying mathematical transformations such as logarithm or square root is a
    common technique to handle skewed data or stabilize variance.
  prefs: []
  type: TYPE_NORMAL
- en: Reminder
  prefs: []
  type: TYPE_NORMAL
- en: Skewness is a measure of the asymmetry in a distribution. A positive skewness
    indicates a distribution with a tail on the right side, while a negative skewness
    indicates a tail on the left side.
  prefs: []
  type: TYPE_NORMAL
- en: When data is right-skewed (positive skewness), meaning that most of the data
    points are concentrated on the left side with a few larger values on the right
    side, applying a logarithmic transformation compresses larger values, making the
    distribution more symmetric and closer to a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to logarithmic transformation, square root transformation is used to
    mitigate the impact of larger values and make the distribution more symmetric.
    It is particularly effective when the right tail of the distribution contains
    extreme values.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to note is that when the variance of the data increases with the
    mean (heteroscedasticity), logarithmic and square root transformation can compress
    the larger values, reducing the impact of extreme values and stabilizing the variance.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go back to our example and perform a log transformation on both columns
    of our dataset. As always, you can find the full code at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/11.data_transformation.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/11.data_transformation.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s apply a logarithmic transformation to `Age` and `Test_Score`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '`np.log1p` is a NumPy function that computes the natural logarithm of *1 +
    x* for each value in the `Age` and `Test_Score` columns. The `log1p` function
    is used instead of the simple logarithm (`np.log`) to handle zero and negative
    values in a dataset without errors. It’s particularly useful when dealing with
    data that includes zero values or very small numbers. The transformation reduces
    skewness and can make the distribution more normal, which is useful for various
    statistical techniques that assume normally distributed data.'
  prefs: []
  type: TYPE_NORMAL
- en: More transformations implemented
  prefs: []
  type: TYPE_NORMAL
- en: In the code, you’ll find both logarithmic and root transformations applied to
    the data. Take some time to explore and understand the differences between these
    two methods. Evaluate which transformation better suits your data by considering
    how each affects the distribution and variance of your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The updated distributions are presented in the following plot after log transforming
    the `Age` column and applying a square root transformation to the `Test_Score`
    column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – Distribution charts after log and square route transformation](img/B19801_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – Distribution charts after log and square route transformation
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also have a look at the updated statistics of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The descriptive statistics illustrate the impact of applying logarithmic transformation
    to the `Age` variable and square root transformation to the `Test_Score` variable.
    Before the transformations, the original dataset displayed a right-skewed distribution
    for `Age` with a mean of 33.75 and a wide standard deviation of 17.18\. `Test_Score`
    had a mean of 65.89, ranging from 5 to 94, with a high standard deviation of 24.76,
    indicating a large spread in the test scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'After applying the transformations, the distributions of both variables were
    visibly altered:'
  prefs: []
  type: TYPE_NORMAL
- en: The logarithmic transformation on `Age` reduced the spread of values, bringing
    the standard deviation down to 0.40 as compared to the original 17.18\. The transformed
    values now range from 2.94 to 4.51, showing a compression of extreme values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For `Test_Score`, the logarithmic transformation resulted in a much more evenly
    distributed set of values, with the standard deviation decreasing from 24.76 to
    0.69\. The values became more compact and symmetric, ranging from 1.79 to 4.55.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The transformations had a clear leveling effect on both variables, reducing
    skewness and variability. This is evident in the reduction of standard deviations
    and narrower ranges, making the data more symmetric and closer to a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s important to note that transformations, especially logarithmic
    ones, compress the scale of values and may affect interpretability. While they
    can help meet the assumptions of statistical methods by reducing skewness and
    heteroscedasticity, the transformed data may be less intuitive to understand compared
    to the original scale. Despite this, such transformations are often useful when
    preparing data for regression models or other analyses that assume normally distributed
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that log transformation is not suitable for data that contains
    zero or negative values, as the logarithm is undefined for such values.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude this section of the chapter, we have compiled a summary table with
    the various methods discussed for handling outliers. This table highlights the
    optimal scenarios to employ each technique and provides an overview of their respective
    pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Technique** | **When to** **use it** | **Pros** | **Cons** |'
  prefs: []
  type: TYPE_TB
- en: '| **Trimming** | Mild outliers, preserving overall data structure | Retains
    majority of the dataset, maintains data integrity | Reduces sample size, may impact
    representativeness, arbitrary choice of trimming percentage |'
  prefs: []
  type: TYPE_TB
- en: '| **Winsorizing** | Moderate outliers, preserving overall data | Preserves
    data distribution, mitigates the impact of extreme values | Alters data values;
    may distort distribution; requires specifying trimming limits |'
  prefs: []
  type: TYPE_TB
- en: '| **Deleting Data** | Severe outliers | Removes the influence of extreme values,
    simplifies analysis | Reduces sample size, potential loss of information; may
    bias results toward a central tendency |'
  prefs: []
  type: TYPE_TB
- en: '| **Transformation** | Skewed or non-normal distributions | Stabilizes variance,
    makes the data more symmetric and amenable to traditional statistical techniques
    | Interpretation challenges, results may be less intuitive, choice of transformation
    method is subjective |'
  prefs: []
  type: TYPE_TB
- en: Table 8.2 – Summary of the univariate methods to deal with outliers
  prefs: []
  type: TYPE_NORMAL
- en: After exploring various techniques for addressing univariate outliers, ranging
    from simpler to more complex methods, the upcoming section will deep dive into
    the different statistical measures that are generally preferable when working
    with data containing outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Robust statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using robust statistical measures such as median and **Median Absolute Deviation**
    (**MAD**) instead of mean and standard deviation can reduce the influence of outliers.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with datasets that contain outliers or skewed distributions, choosing
    robust statistical measures becomes crucial for obtaining accurate and representative
    summaries of the data. The use of robust measures, such as the median and MAD,
    proves advantageous in scenarios where the presence of extreme values could impact
    traditional measures such as the mean and standard deviation. The median, being
    the middle value when data is ordered, is less sensitive to outliers, providing
    a more reliable measure of central tendency. Additionally, MAD, which assesses
    the spread of data while being robust to outliers, further ensures a more accurate
    representation of the dataset’s variability.
  prefs: []
  type: TYPE_NORMAL
- en: MAD
  prefs: []
  type: TYPE_NORMAL
- en: MAD is a measure of statistical dispersion that quantifies the dispersion or
    spread of a dataset. It is calculated as the median of the absolute differences
    between each data point and the median of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'This table summarizes the key considerations, pros, and cons of using median
    and MAD versus mean and standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Criteria** | **Median** **and MAD** | **Mean and** **standard deviation**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **When** **to use** | Presence of outliers | Normal or symmetric distributions
    |'
  prefs: []
  type: TYPE_TB
- en: '| Skewed distributions | Precision in measurement |'
  prefs: []
  type: TYPE_TB
- en: '| **Pros** | Robustness against outliers | Efficiency for normal distributions
    |'
  prefs: []
  type: TYPE_TB
- en: '| Applicability to skewed data | Ease of interpretation |'
  prefs: []
  type: TYPE_TB
- en: '| **Cons** | Loss of sensitivity without outliers | Sensitivity to outliers
    |'
  prefs: []
  type: TYPE_TB
- en: '| Not robust in the presence of outliers |'
  prefs: []
  type: TYPE_TB
- en: '| **Considerations** | Useful when the central tendency needs stability | Suitable
    for datasets with minimal or no outliers |'
  prefs: []
  type: TYPE_TB
- en: '| Provides precise measures in a normal distribution |'
  prefs: []
  type: TYPE_TB
- en: Table 8.3 – Which statistical methods work better with outliers
  prefs: []
  type: TYPE_NORMAL
- en: In the next section of this chapter, we will discuss how to identify multivariate
    outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying multivariate outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multivariate outliers occur when an observation is extreme in the context of
    multiple variables simultaneously. These outliers cannot be detected by analyzing
    individual variables alone; rather, they require consideration of interactions
    between variables. Detecting multivariate outliers involves assessing data points
    in higher dimensional space. In the following parts, we will outline different
    methods to identify multivariate outliers, along with code examples for each.
  prefs: []
  type: TYPE_NORMAL
- en: Mahalanobis distance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mahalanobis distance is a statistical measure used to identify outliers in multivariate
    data. It accounts for the correlation between variables and calculates the distance
    of each data point from the mean of the dataset in a scaled space. This distance
    is then compared to a threshold to identify observations that deviate significantly
    from the multivariate mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we have created a new dataset with some multivariate student
    data so that we can showcase the technique in the best way possible. The code
    can be fully seen in the repository at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/12.mahalanobis_distance.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/12.mahalanobis_distance.py).
    The key steps of the process are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required libraries first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s generate multivariate student data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We generate a dataset of 100 samples from a multivariate normal distribution
    with a specified mean vector of `[0, 0]` and a covariance matrix of `[[1, 0.5],
    [``0.5, 1]]`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s introduce outliers and create the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following function calculates the Mahalanobis distance for each data point
    based on the mean and the inverse of the covariance matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The mean, covariance matrix, and inverse covariance matrix are calculated for
    the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Mahalanobis distance is calculated for each data point and added as a new column
    in the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set a significance level for outlier detection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The significance level (`alpha`) represents the probability of rejecting the
    null hypothesis when it is true, which in this context is the probability of incorrectly
    identifying a data point as an outlier. A common choice for `alpha` is `0.01`,
    meaning there is a 1% chance of mistakenly classifying a normal data point as
    an outlier. A lower `alpha` value makes the outlier detection more conservative,
    reducing false positives (normal points labeled as outliers). Conversely, a higher
    `alpha` value makes it more permissive, potentially identifying more points as
    outliers but increasing the chance of false positives.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we set the chi-squared threshold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The chi-squared threshold is a critical value from the chi-squared distribution
    used to define the cutoff for outlier detection. The `chi2.ppf` function computes
    the percentile point function (inverse of the cumulative distribution function)
    for the chi-squared distribution. The degrees of freedom is equal to the number
    of features or variables used in the Mahalanobis distance calculation. In this
    case, it’s `2` (for X1 and X2). The chi-squared threshold is used to determine
    the cutoff value beyond which Mahalanobis distances are considered excessively
    high, indicating that the corresponding data points are outliers. For example,
    with `alpha = 0.01`, you are finding the threshold above which only 1% of the
    data points are expected to fall, assuming the data is normally distributed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This step involves comparing each data point’s Mahalanobis distance against
    the chi-squared threshold to determine whether it is an outlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Data points with distances greater than the threshold are flagged as outliers
    and separated from the rest of the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s now visualize the outliers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following chart, we can see all the data points projected in a 3D space
    and we can see the outliers marked with *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – Data plotted with Mahalanobis distance](img/B19801_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – Data plotted with Mahalanobis distance
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Run the visualization on your laptop to be able to see this space and move around
    it in a 3D view; it’s cool!
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the 3D plot, it is very clear to spot the outliers in our
    data. Mahalanobis distance is most effective when dealing with datasets that involve
    multiple variables as it takes both the means and covariances among variables
    into account and allows identifying outliers that may not be apparent when looking
    at individual variables. In situations where variables have different units or
    scales, Mahalanobis distance can normalize the distances across variables, providing
    a more meaningful measure of outliers. Unlike univariate methods, Mahalanobis
    distance is sensitive to relationships among variables. It captures how far each
    data point is from the center of the data distribution, considering correlations
    between variables.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section of the multivariate part, we will discuss how clustering
    methods can help us detect outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Clustering methods such as k-means or hierarchical clustering can be used to
    group similar data points. Points that do not belong to any cluster or form small
    clusters might be considered multivariate outliers.
  prefs: []
  type: TYPE_NORMAL
- en: One popular method for outlier detection using clustering is the **Density-Based
    Spatial Clustering of Applications with Noise** (**DBSACN**) algorithm. DBSCAN
    can identify clusters of dense data points and classify outliers as noise. DBSCAN
    is advantageous because it *doesn’t require specifying the number of clusters
    beforehand* and can effectively identify outliers based on density. It’s a relatively
    simple yet powerful method for outlier detection, especially in cases where clusters
    may not be well-separated or when outliers form isolated points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s deep dive into the code for the DBSCAN. As always, you can find the full
    code in the repository at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/13.clustering.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/13.clustering.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s generate the example data for this method. The dataset consists of 100
    samples from a multivariate normal distribution with a mean vector of `[0, 0]`
    and a covariance matrix of `[[1, 0.5], [0.5, 1]]`. This creates a cluster of points
    that are normally distributed around the origin with some correlation between
    the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s turn the data into a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Standardize the data by removing the mean and scaling to unit variance. `StandardScaler`
    from `sklearn.preprocessing` is used to fit and transform the data. Standardizing
    ensures that all features contribute equally to distance calculations by scaling
    them to have a mean of 0 and a standard deviation of 1\. This is especially important
    for distance-based algorithms such as DBSCAN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply DBSCAN for outlier detection. `eps=0.4` sets the maximum distance between
    points to be considered in the same neighborhood, and `min_samples=5` specifies
    the minimum number of points required to form a dense region. DBSCAN is a clustering
    algorithm that can identify outliers as points that do not belong to any cluster.
    Points labeled `-1` by DBSCAN are considered outliers. The choice of `eps` and
    `min_samples` parameters can significantly impact the detection of outliers, and
    these values might need tuning based on the specific dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following chart, we have plotted all data points in a 2D space and can
    see the outliers on the right side of the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15 – DBSCAN clustering for outlier detection](img/B19801_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – DBSCAN clustering for outlier detection
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a key parameter that needs to be adjusted in DBSCAN: `eps`. The `eps`
    (epsilon) parameter essentially defines the radius around a data point, and all
    other data points within this radius are considered its neighbors.'
  prefs: []
  type: TYPE_NORMAL
- en: When performing DBSCAN clustering, the algorithm starts by selecting a data
    point and identifying all the data points that lie within a distance of `eps`
    from it. If the number of data points within this distance exceeds a specified
    threshold (`min_samples`), the selected data point is considered a core point,
    and all the points within its epsilon-neighborhood become part of the same cluster.
    The algorithm then recursively expands the cluster by finding the neighbors of
    the neighbors until no more points can be added.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of `eps` depends on the specific characteristics of the dataset and
    the desired granularity of clusters. It may require some experimentation and domain
    knowledge to find the appropriate value for `eps`.
  prefs: []
  type: TYPE_NORMAL
- en: Employing k-means instead of DBSCAN offers a different approach. K-means is
    a centroid-based clustering algorithm that requires *pre-specifying the number
    of clusters*, making it essential to have prior knowledge or conduct exploratory
    analysis to determine an appropriate value for *k*. While it is sensitive to outliers,
    the simplicity and computational efficiency of k-means make it an attractive choice
    for certain scenarios. K-means may be well-suited when clusters are well-separated
    and have a relatively uniform structure. However, it is essential to be aware
    that k-means may struggle with irregularly shaped or overlapping clusters and
    can be influenced by outliers in its attempt to minimize the sum of squared distances.
  prefs: []
  type: TYPE_NORMAL
- en: After having spotted the multivariate outliers, we need to decide how we are
    going to deal with those. This is the focus of the next part.
  prefs: []
  type: TYPE_NORMAL
- en: Handling multivariate outliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Handling multivariate outliers involves addressing data points that deviate
    significantly in the context of multiple variables. In this part of the chapter,
    we will provide explanations and code examples for different methods to handle
    multivariate outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate trimming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This method involves limiting extreme values based on a combined assessment
    of the values across multiple variables. For example, the limits for trimming
    can be determined by considering the Mahalanobis distance, which accounts for
    correlations between variables. This technique is particularly useful when dealing
    with datasets containing outliers present across different variables. The idea
    is to preserve the overall structure of the data while mitigating the influence
    of extreme values across variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we are going to continue working on the data from the Mahalanobis
    distance example, and after we have calculated the Mahalanobis distance, we are
    going to drop the outliers passing the threshold. You can find the full code in
    the repository at [https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/14.multivariate_trimming.py](https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter08/14.multivariate_trimming.py):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let’s generate multivariate student data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the function to calculate the Mahalanobis distance, which measures how
    far a data point is from the mean of the distribution, taking into account the
    correlation between features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the data for outlier detection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the Mahalanobis distance for each data point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the threshold for outlier detection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Filter the DataFrame to separate outliers from the rest of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let’s present the distribution plots before the outlier handling.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16 – Distribution charts with multivariate outliers](img/B19801_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – Distribution charts with multivariate outliers
  prefs: []
  type: TYPE_NORMAL
- en: 'The descriptive statistics of the original data are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'After having dropped the data that are considered multivariate outliers, we
    can observe the changes in the following distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17 – Distribution charts after removing multivariate outliers](img/B19801_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 – Distribution charts after removing multivariate outliers
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s have a look at the updated descriptive statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Having trimmed the outliers, let’s discuss the changes observed in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: The count of observations reduced from 102 to 100 after removing outliers, so
    we dropped two records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the `X1` column, the mean decreased from 0.248 to 0.083, and the standard
    deviation reduced from 1.479 to 0.907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the `X2` column, the mean decreased from 0.281 to 0.117, and the standard
    deviation reduced from 1.459 to 0.881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum values for `X1` and `X2` were capped at 1.857815 and 2.679717, respectively,
    indicating the removal of extreme outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, removing outliers has resulted in a dataset with reduced variability,
    particularly in terms of mean and standard deviation. Extreme values that could
    potentially skew the analysis have been mitigated.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s summarize the key takeaways from this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we deep-dived into handling missing values and outliers. We
    understood that missing values can distort our analyses and learned a range of
    imputation techniques, from simple mean imputation to advanced machine learning-based
    strategies. Similarly, we recognized that outliers could skew our results and
    deep-dived into methods to detect and manage them, both in univariate and multivariate
    contexts. By combining theory and practical examples, we gained a deeper understanding
    of the considerations, challenges, and strategies that go into ensuring the quality
    and reliability of our data.
  prefs: []
  type: TYPE_NORMAL
- en: Armed with these insights, we can now move on to the next chapter, where we
    will discuss scaling, normalization, and standardization of features.
  prefs: []
  type: TYPE_NORMAL
