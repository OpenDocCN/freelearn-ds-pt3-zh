<html><head></head><body>
  <div id="_idContainer124" class="Basic-Text-Frame">
    <h1 class="chapterNumber">9</h1>
    <h1 id="_idParaDest-241" class="chapterTitle">Temporal Data Types and Algorithms</h1>
    <p class="normal">Properly working with temporal data (i.e., dates and times) may appear straightforward, but, the further you dive into it, the further you realize how surprisingly complex it is. Here are just a few issues that come to mind:</p>
    <ul>
      <li class="bulletList">Some users measure time in the span of years; others measure in nanoseconds</li>
      <li class="bulletList">Some users ignore timezones; others need to coordinate events around the world</li>
      <li class="bulletList">Not every country has multiple timezones, even if they are wide enough to have them (e.g., China)</li>
      <li class="bulletList">Not every country observes daylight saving time; those that do cannot agree on when</li>
      <li class="bulletList">In countries that observe daylight saving time, not every region participates (e.g., Arizona in the United States (US))</li>
      <li class="bulletList">Different operating systems and versions time differently (see also the Year 2038 problem at <a href="https://en.wikipedia.org/wiki/Year_2038_problem"><span class="url">https://en.wikipedia.org/wiki/Year_2038_problem</span></a>)</li>
    </ul>
    <p class="normal">These problems are really just the tip of the iceberg, and, in spite of all of the potential data quality problems, temporal data is invaluable for purposes of monitoring, trend detection, and forecasting. Fortunately, pandas makes it so that you don’t need to be an expert in dates and times to draw insights from your data. By using the features and abstractions pandas offers, you can very easily cleanse and interpolate your temporal data so that you can focus less on the “problems” of dates and times, and more on the insights that your data has to offer.</p>
    <p class="normal">While we introduced some of the temporal types pandas has to offer back in <em class="chapterRef">Chapter 3</em>,<em class="italic"> Data Types</em>, in the section <em class="italic">Temporal types – datetime</em>, this chapter will start by focusing on things that pandas offers to augment the utility of those types. Beyond that, we will talk about the different ways you can cleanse and interpolate your temporal data, before finishing the chapter with a focus on practical applications.</p>
    <p class="normal">We are going to cover the following recipes in this chapter:</p>
    <ul>
      <li class="bulletList">Timezone handling</li>
      <li class="bulletList">DateOffsets</li>
      <li class="bulletList">Datetime selection</li>
      <li class="bulletList">Resampling</li>
      <li class="bulletList">Aggregating weekly crime and traffic accidents</li>
      <li class="bulletList">Calculating year over year changes in crime by category</li>
      <li class="bulletList">Accurately measuring sensor-collected events with missing values</li>
    </ul>
    <h1 id="_idParaDest-242" class="heading-1">Timezone handling</h1>
    <p class="normal">By far, the most common mistakes with temporal data that I come across stem from a misunderstanding of timezones. On the East Coast of the US where I live, I’ve witnessed many users try to read what they think is a date of 2024-01-01 out of a database, yet ironically end up with a date of 2023-12-31 in their analysis. While that is only offset by a day, the effects of that misalignment can greatly skew summaries that group dates into weekly, monthly, quarterly, or yearly buckets.</p>
    <p class="normal">For those that have been bitten by an issue like that before, you may have already come to realize that the source system you were communicating with probably did give you a timestamp of 2024-01-01 00:00:00, presumed to be at midnight UTC. Somewhere along the line, an analyst on the East Coast of the US where I live may have had that translated into their <em class="italic">local</em> time, which is either four hours offset from UTC during daylight saving time, or five hours offset during standard time. As a result, the timestamp ended up being viewed as 2023-12-31 20:00:00 or 2023-12-31 19:00:00 in EDT/EST, respectively, and the user may have inadvertently tried to convert that to a date.</p>
    <p class="normal">To avoid these types of issues when working with temporal data, it is critical to understand when you are working with <em class="italic">timezone-aware</em> datetimes (i.e., those tied to a timezone like UTC or <code class="inlineCode">America/New_York</code>), and <em class="italic">timezone-naive</em> objects, which have no timezone information attached to them. In this recipe, we will show you how to create and recognize both types of<a id="_idIndexMarker476"/> datetimes, while also diving deeper into the utilities pandas offers that let you convert between different timezones, and from timezone-aware to timezone-naive.</p>
    <h2 id="_idParaDest-243" class="heading-2">How to do it</h2>
    <p class="normal">Back in <em class="chapterRef">Chapter 3</em>, <em class="italic">Data Types</em>, we learned how to create a <code class="inlineCode">pd.Series</code> with datetime data. Let’s take a closer look at that same example:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser = pd.Series([
    "<span class="hljs-number">2024</span>-01-01 <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span>",
    "<span class="hljs-number">2024</span>-01-02 <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:01",
    "<span class="hljs-number">2024</span>-01-03 <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:02"
], dtype="datetime64[ns]")
ser
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0   2024-01-01 00:00:00
1   2024-01-02 00:00:01
2   2024-01-03 00:00:02
dtype: datetime64[ns]
</code></code></pre>
    <p class="normal">These timestamps represent events that occurred at or close to midnight on days ranging from January 1 through January 3, 2024. However, what these datetimes cannot tell us is <em class="italic">where</em> these events occurred; midnight in New York City happens at a different point in time than in Dubai, so it is tough to pinpoint an exact point in time that these events happened. Without that extra metadata, these datetimes are <em class="italic">timezone-naive</em>.</p>
    <p class="normal">For programmatic confirmation that your datetimes are timezone-naive, you can use <code class="inlineCode">pd.Series.dt.tz</code>, which will return <code class="inlineCode">None</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.dt.tz <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">True
</code></code></pre>
    <p class="normal">With the <code class="inlineCode">pd.Series.dt.tz_localize</code> method, we could assign an <strong class="keyWord">Internet Assigned Numbers Authority (IANA</strong>) timezone identifier to these datetimes to make them <em class="italic">timezone-aware</em>. For example, to specify that these events happened on the East Coast of the US, we could write:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ny_ser = ser.dt.tz_localize("America/New_York")
ny_ser
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0   2024-01-01 00:00:00-05:00
1   2024-01-02 00:00:01-05:00
2   2024-01-03 00:00:02-05:00
dtype: datetime64[ns, America/New_York]
</code></code></pre>
    <p class="normal">If you try to use <code class="inlineCode">pd.Series.dt.tz</code> on this <code class="inlineCode">pd.Series</code>, it will report back that you are working with a timezone of <code class="inlineCode">America/New_York</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ny_ser.dt.tz
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">&lt;DstTzInfo 'America/New_York' LMT-1 day, 19:04:00 STD&gt;
</code></code></pre>
    <p class="normal">Now that our <code class="inlineCode">pd.Series</code> is timezone-aware, the datetimes contained therein can be mapped to a point in time anywhere around the world. By using <code class="inlineCode">pd.Series.dt.tz_convert</code>, you can easily translate these events into another timezone:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">la_ser = ny_ser.dt.tz_convert("America/Los_Angeles")
la_ser
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0   2023-12-31 21:00:00-08:00
1   2024-01-01 21:00:01-08:00
2   2024-01-02 21:00:02-08:00
dtype: datetime64[ns, America/Los_Angeles]
</code></code></pre>
    <p class="normal">As a matter of practice, it is<a id="_idIndexMarker477"/> usually best to keep your datetimes attached to a timezone, which will mitigate the risk of being misinterpreted on a different date or at a different point in time. However, not all systems and databases that you may interact with will be able to retain this information, forcing you to drop it for interoperability. In case such a need arises, you could do this by passing <code class="inlineCode">None</code> as an argument to <code class="inlineCode">pd.Series.dt.tz_localize</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">la_ser.dt.tz_localize(<span class="hljs-literal">None</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0   2023-12-31 21:00:00
1   2024-01-01 21:00:01
2   2024-01-02 21:00:02
dtype: datetime64[ns]
</code></code></pre>
    <p class="normal">If you are forced to drop the timezone from your datetime, I would strongly recommend storing the timezone as a string in another column in your <code class="inlineCode">pd.DataFrame</code> and database:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df = la_ser.to_frame().assign(
   datetime=la_ser.dt.tz_localize(None),
   timezone=str(la_ser.dt.tz),
).drop(columns=[<span class="hljs-number">0</span>])
df
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">    datetime              timezone
0   2023-12-31 21:00:00   America/Los_Angeles
1   2024-01-01 21:00:01   America/Los_Angeles
2   2024-01-02 21:00:02   America/Los_Angeles
</code></code></pre>
    <p class="normal">When roundtripping data like this, you can recreate the original <code class="inlineCode">pd.Series</code> by applying the value from the <code class="inlineCode">timezone</code> column to the data in the <code class="inlineCode">datetime</code> column. For added safety, the following code sample uses the combination of <code class="inlineCode">pd.Series.drop_duplicates</code> with <code class="inlineCode">pd.Series.squeeze</code> to extract the single value of <code class="inlineCode">America/Los_Angeles</code> from the <code class="inlineCode">timezone</code> column<a id="_idIndexMarker478"/> before passing it to <code class="inlineCode">pd.Series.dt.tz_localize</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">tz = df["timezone"].drop_duplicates().squeeze()
df["datetime"].dt.tz_localize(tz)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0   2023-12-31 21:00:00-08:00
1   2024-01-01 21:00:01-08:00
2   2024-01-02 21:00:02-08:00
Name: datetime, dtype: datetime64[ns, America/Los_Angeles]
</code></code></pre>
    <h1 id="_idParaDest-244" class="heading-1">DateOffsets</h1>
    <p class="normal">In the <em class="italic">Temporal types – Timedelta</em> recipe <a id="_idIndexMarker479"/>back in <em class="chapterRef">Chapter 3</em>, <em class="italic">Data Types</em>, we introduced the <code class="inlineCode">pd.Timedelta</code> type and mentioned how it could be used to shift datetimes by a finite duration, like 10 seconds or 5 days. However, a <code class="inlineCode">pd.Timedelta</code> cannot be used to offset a date or datetime by say <em class="italic">one month</em> because a month does not always represent the same duration of time. In the Gregorian calendar, months can range in duration from 28–31 days. The month of February is usually 28 days but extends to 29 days for every year that is divisible by 4, unless the year is divisible by 100 but not by 400.</p>
    <p class="normal">Thinking about these issues all of the time would be rather tedious. Fortunately, pandas takes care of all of the mundane details and just lets you shift dates according to a calendar through the use of the <code class="inlineCode">pd.DateOffset</code> object, which we will explore in this recipe.</p>
    <h2 id="_idParaDest-245" class="heading-2">How to do it</h2>
    <p class="normal">To build a foundational knowledge of how this works, let’s start with a very simple <code class="inlineCode">pd.Series</code> containing the first few days of 2024:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser = pd.Series([
    "<span class="hljs-number">2024</span>-01-01",
    "<span class="hljs-number">2024</span>-01-02",
    "<span class="hljs-number">2024</span>-01-03",
], dtype="datetime64[ns]")
ser
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0   2024-01-01
1   2024-01-02
2   2024-01-03
dtype: datetime64[ns]
</code></code></pre>
    <p class="normal">Shifting these dates by one month would typically mean keeping the same day of the month, but just placing<a id="_idIndexMarker480"/> the dates in February instead of January. With <code class="inlineCode">pd.DateOffset</code>, you can pass in an argument to <code class="inlineCode">months=</code> that dictates the number of months you want to move the dates by; so, let’s see how it looks with an argument of <code class="inlineCode">1</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser + pd.DateOffset(months=<span class="hljs-number">1</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0   2024-02-01
1   2024-02-02
2   2024-02-03
dtype: datetime64[ns]
</code></code></pre>
    <p class="normal">Shifting by two months would mean moving these dates from January to March. We shouldn’t really care that there were 31 days in January but 29 in February 2024; the <code class="inlineCode">pd.DateOffset</code> takes care of this for us:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser + pd.DateOffset(months=<span class="hljs-number">2</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0   2024-03-01
1   2024-03-02
2   2024-03-03
dtype: datetime64[ns]
</code></code></pre>
    <p class="normal">For dates that wouldn’t exist (e.g., trying to shift January 30 to February 30), <code class="inlineCode">pd.DateOffset</code> will try and match to the closest date that does exist within the target month:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">pd.Series([
    "<span class="hljs-number">2024</span>-01-<span class="hljs-number">29</span>",
    "<span class="hljs-number">2024</span>-01-<span class="hljs-number">30</span>",
    "<span class="hljs-number">2024</span>-01-<span class="hljs-number">31</span>",
], dtype="datetime64[ns]") + pd.DateOffset(months=<span class="hljs-number">1</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0   2024-02-29
1   2024-02-29
2   2024-02-29
dtype: datetime64[ns]
</code></code></pre>
    <p class="normal">You can also step <a id="_idIndexMarker481"/>backward through the calendar with a negative argument to <code class="inlineCode">months=</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser + pd.DateOffset(months=-<span class="hljs-number">1</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0   2023-12-01
1   2023-12-02
2   2023-12-03
dtype: datetime64[ns]
</code></code></pre>
    <p class="normal">The <code class="inlineCode">pd.DateOffset</code> is flexible enough to accept more than just one keyword argument at a time. For instance, if you wanted to offset your dates by one month, two days, three hours, four minutes, and five seconds, you could do that all in one expression:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser + pd.DateOffset(months=<span class="hljs-number">1</span>, days=<span class="hljs-number">2</span>, hours=<span class="hljs-number">3</span>, minutes=<span class="hljs-number">4</span>, seconds=<span class="hljs-number">5</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0   2024-02-03 03:04:05
1   2024-02-04 03:04:05
2   2024-02-05 03:04:05
dtype: datetime64[ns]
</code></code></pre>
    <p class="normal">Alongside the <code class="inlineCode">pd.DateOffset</code> class, pandas offers you the ability to shift dates to the beginning or the end of a period with various classes exposed in the <code class="inlineCode">pd.offsets</code> module. For instance, if you want to shift your dates to the end of the month, you can use <code class="inlineCode">pd.offsets.MonthEnd</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser + pd.offsets.MonthEnd()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0   2024-01-31
1   2024-01-31
2   2024-01-31
dtype: datetime64[ns]
</code></code></pre>
    <p class="normal"><code class="inlineCode">pd.offsets.MonthBegin</code> will move the dates to the beginning of the next month:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser + pd.offsets.MonthBegin()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0   2024-02-01
1   2024-02-01
2   2024-02-01
dtype: datetime64[ns]
</code></code></pre>
    <p class="normal"><code class="inlineCode">pd.offsets.SemiMonthBegin</code>, <code class="inlineCode">pd.offsets.SemiMonthEnd</code>, <code class="inlineCode">pd.offsets.QuarterBegin</code>, <code class="inlineCode">pd.offsets.QuarterEnd</code>, <code class="inlineCode">pd.offsets.YearBegin</code>, and <code class="inlineCode">pd.offsets.YearEnd</code> all offer similar behavior to shift <a id="_idIndexMarker482"/>your dates to the beginning or end of different periods.</p>
    <h2 id="_idParaDest-246" class="heading-2">There’s more…</h2>
    <p class="normal">The <code class="inlineCode">pd.DateOffset</code>, by default, works against the Gregorian calendar, but different subclasses of this can provide more customized functionality.</p>
    <p class="normal">One of the most used subclasses is the <code class="inlineCode">pd.offsets.BusinessDay</code>, which, by default, only counts the standard “business days” of Monday through Friday when offsetting dates. To see how this works, let’s consider the day of the week each of our dates in <code class="inlineCode">ser</code> fall on:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.dt.day_name()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0       Monday
1      Tuesday
2    Wednesday
dtype: object
</code></code></pre>
    <p class="normal">Now, let’s see what happens when we add three business days to our dates:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">bd_ser = ser + pd.offsets.BusinessDay(n=<span class="hljs-number">3</span>)
bd_ser
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0   2024-01-04
1   2024-01-05
2   2024-01-08
dtype: datetime64[ns]
</code></code></pre>
    <p class="normal">We can use the same <code class="inlineCode">pd.Series.dt.day_name</code> method to check the new days of the week that these dates fall on:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">bd_ser.dt.day_name()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0    Thursday
1      Friday
2      Monday
dtype: object
</code></code></pre>
    <p class="normal">After having added three business days, our dates that started on Monday and Tuesday ended up falling on the Thursday and Friday of the same week, respectively. The Wednesday date we started with was pushed to the Monday of the following week, as neither Saturday nor Sunday qualifies as a business day.</p>
    <p class="normal">If you work with a<a id="_idIndexMarker483"/> business that has different business days from Monday to Friday, you could use the <code class="inlineCode">pd.offsets.CustomBusinessDay</code> to set up your own rules for how offsetting should work. The argument to <code class="inlineCode">weekmask=</code> will dictate the days of the week that are considered business days:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser + pd.offsets.CustomBusinessDay(
    n=<span class="hljs-number">3</span>,
    weekmask="Mon Tue Wed Thu",
)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0   2024-01-04
1   2024-01-08
2   2024-01-09
dtype: datetime64[ns]
</code></code></pre>
    <p class="normal">You can even add a <code class="inlineCode">holidays=</code> argument to account for days when your business may be closed:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser + pd.offsets.CustomBusinessDay(
    n=<span class="hljs-number">3</span>,
    weekmask="Mon Tue Wed Thu",
    holidays=["<span class="hljs-number">2024</span>-01-04"],
)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0   2024-01-08
1   2024-01-09
2   2024-01-10
dtype: datetime64[ns]
</code></code></pre>
    <p class="normal">For the Gregorian calendar, we have already seen <code class="inlineCode">pd.offsets.MonthEnd</code> and <code class="inlineCode">pd.offsets.MonthBegin</code> classes that help you move dates to the beginning or end of a month, respectively. Similar classes exist for you to use when attempting to shift dates toward the beginning<a id="_idIndexMarker484"/> or end of business months:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser + pd.offsets.BusinessMonthEnd()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0   2024-01-31
1   2024-01-31
2   2024-01-31
dtype: datetime64[ns]
</code></code></pre>
    <h1 id="_idParaDest-247" class="heading-1">Datetime selection</h1>
    <p class="normal">Back in <em class="chapterRef">Chapter 2</em>, <em class="italic">Selection and Assignment</em>, we discussed the many robust ways that pandas allows you to <a id="_idIndexMarker485"/>select data from a <code class="inlineCode">pd.Series</code> or <code class="inlineCode">pd.DataFrame</code> by interacting with their associated row <code class="inlineCode">pd.Index</code>. If you happen to create a <code class="inlineCode">pd.Index</code> using datetime data, it ends up being represented as a special subclass called a <code class="inlineCode">pd.DatetimeIndex</code>. This subclass overrides some functionality of the <code class="inlineCode">pd.Index.loc</code> method to give you more flexible selection options tailored to temporal data.</p>
    <h2 id="_idParaDest-248" class="heading-2">How to do it</h2>
    <p class="normal"><code class="inlineCode">pd.date_range</code> is a convenient function that helps you quickly generate a <code class="inlineCode">pd.DatetimeIndex</code>. One of the ways to use this function is to specify a starting date with the <code class="inlineCode">start=</code> parameter, specify a step frequency with the <code class="inlineCode">freq=</code> parameter, and specify the desired length of your <code class="inlineCode">pd.DatetimeIndex</code> with the <code class="inlineCode">periods=</code> argument.</p>
    <p class="normal">For instance, to generate a <code class="inlineCode">pd.DatetimeIndex</code> that starts on December 27, 2023, and provides 5 days in total with 10 days between each record, you would write:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">pd.date_range(start="<span class="hljs-number">2023</span>-<span class="hljs-number">12</span>-<span class="hljs-number">27</span>", freq="10D", periods=<span class="hljs-number">5</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">DatetimeIndex(['2023-12-27', '2024-01-06', '2024-01-16', '2024-01-26',
              '2024-02-05'],
             dtype='datetime64[ns]', freq='10D')
</code></code></pre>
    <p class="normal">A frequency string of <code class="inlineCode">"2W"</code> will generate dates spaced two weeks apart. If the <code class="inlineCode">start=</code> parameter is a Sunday, the dates will begin from that date exactly; otherwise, the next Sunday begins the sequence:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">pd.date_range(start="<span class="hljs-number">2023</span>-<span class="hljs-number">12</span>-<span class="hljs-number">27</span>", freq="2W", periods=<span class="hljs-number">5</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">DatetimeIndex(['2023-12-31', '2024-01-14', '2024-01-28', '2024-02-11',
              '2024-02-25'],
             dtype='datetime64[ns]', freq='2W-SUN')
</code></code></pre>
    <p class="normal">You could even control the<a id="_idIndexMarker486"/> day of the week being used to anchor the dates by appending a suffix like <code class="inlineCode">"-WED"</code>, which will generate dates on Wednesday instead of Sunday:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">pd.date_range(start="<span class="hljs-number">2023</span>-<span class="hljs-number">12</span>-<span class="hljs-number">27</span>", freq="2W-WED", periods=<span class="hljs-number">5</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">DatetimeIndex(['2023-12-27', '2024-01-10', '2024-01-24', '2024-02-07',
              '2024-02-21'],
             dtype='datetime64[ns]', freq='2W-WED')
</code></code></pre>
    <p class="normal">A <code class="inlineCode">freq=</code> argument of <code class="inlineCode">"WOM-3THU"</code> will give you the third Thursday of every month:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">pd.date_range(start="<span class="hljs-number">2023</span>-<span class="hljs-number">12</span>-<span class="hljs-number">27</span>", freq="WOM-3THU", periods=<span class="hljs-number">5</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">DatetimeIndex(['2024-01-18', '2024-02-15', '2024-03-21', '2024-04-18',
              '2024-05-16'],
             dtype='datetime64[ns]', freq='WOM-3THU')
</code></code></pre>
    <p class="normal">The first and fifteenth day of each month can be generated with an argument of <code class="inlineCode">"SMS"</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">pd.date_range(start="<span class="hljs-number">2023</span>-<span class="hljs-number">12</span>-<span class="hljs-number">27</span>", freq="SMS", periods=<span class="hljs-number">5</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">DatetimeIndex(['2024-01-01', '2024-01-15', '2024-02-01', '2024-02-15',
              '2024-03-01'],
             dtype='datetime64[ns]', freq='SMS-15')
</code></code></pre>
    <p class="normal">As you can see, there are countless frequency strings that can be used to describe what pandas refers to as <strong class="keyWord">date offsets</strong>. For a more complete listing, be sure to reference the pandas documentation at <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects"><span class="url">https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects</span></a>.</p>
    <p class="normal">Each element of the <code class="inlineCode">pd.DatetimeIndex</code> is actually a <code class="inlineCode">pd.Timestamp</code>. When using this for selection from a <code class="inlineCode">pd.Series</code> or <code class="inlineCode">pd.DataFrame</code>, users may at first be tempted to write something like the <a id="_idIndexMarker487"/>following to select all records up to and including a date like 2024-01-18:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">index = pd.date_range(start="<span class="hljs-number">2023</span>-<span class="hljs-number">12</span>-<span class="hljs-number">27</span>", freq="10D", periods=<span class="hljs-number">20</span>)
ser = pd.Series(<span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>), index=index)
ser.loc[:pd.Timestamp("<span class="hljs-number">2024</span>-01-<span class="hljs-number">18</span>")]
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2023-12-27    0
2024-01-06    1
2024-01-16    2
Freq: 10D, dtype: int64
</code></code></pre>
    <p class="normal">Similarly, users may be tempted to write the following to select a range of dates:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.loc[pd.Timestamp("<span class="hljs-number">2024</span>-01-06"):pd.Timestamp("<span class="hljs-number">2024</span>-01-<span class="hljs-number">18</span>")]
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-06    1
2024-01-16    2
Freq: 10D, dtype: int64
</code></code></pre>
    <p class="normal">However, these methods of selecting from a <code class="inlineCode">pd.DatetimeIndex</code> are rather verbose. For convenience, pandas lets you pass in strings to represent the desired dates, instead of <code class="inlineCode">pd.Timestamp</code> instances:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.loc["<span class="hljs-number">2024</span>-01-06":"<span class="hljs-number">2024</span>-01-<span class="hljs-number">18</span>"]
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-06    1
2024-01-16    2
Freq: 10D, dtype: int64
</code></code></pre>
    <p class="normal">You also are not required to specify the entire date in YYYY-MM-DD format. For instance, if you wanted to select all of the dates that fall in February 2024, you could just pass the string <code class="inlineCode">2024-02</code> to your <code class="inlineCode">pd.Series.loc</code> call:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.loc["<span class="hljs-number">2024</span>-02"]
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-02-05    4
2024-02-15    5
2024-02-25    6
Freq: 10D, dtype: int64
</code></code></pre>
    <p class="normal">Slicing will be intelligent enough to recognize this pattern, making it easy to select all of the records in both February and March:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.loc["<span class="hljs-number">2024</span>-02":"<span class="hljs-number">2024</span>-03"]
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-02-05    4
2024-02-15    5
2024-02-25    6
2024-03-06    7
2024-03-16    8
2024-03-26    9
Freq: 10D, dtype: int64
</code></code></pre>
    <p class="normal">You can take this<a id="_idIndexMarker488"/> abstraction a step further and select an entire year:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.loc["<span class="hljs-number">2024</span>"].head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-06    1
2024-01-16    2
2024-01-26    3
2024-02-05    4
2024-02-15    5
Freq: 10D, dtype: int64
</code></code></pre>
    <h2 id="_idParaDest-249" class="heading-2">There’s more…</h2>
    <p class="normal">A <code class="inlineCode">pd.DatetimeIndex</code> can <a id="_idIndexMarker489"/>also <a id="_idIndexMarker490"/>be associated with a timezone by providing a <code class="inlineCode">tz=</code> argument:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">index = pd.date_range(start="<span class="hljs-number">2023</span>-<span class="hljs-number">12</span>-<span class="hljs-number">27</span>", freq="12h", periods=<span class="hljs-number">6</span>, tz="US/Eastern")
ser = pd.Series(<span class="hljs-built_in">range</span>(<span class="hljs-number">6</span>), index=index)
ser
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2023-12-27 00:00:00-05:00    0
2023-12-27 12:00:00-05:00    1
2023-12-28 00:00:00-05:00    2
2023-12-28 12:00:00-05:00    3
2023-12-29 00:00:00-05:00    4
2023-12-29 12:00:00-05:00    5
Freq: 12h, dtype: int64
</code></code></pre>
    <p class="normal">When using strings to select from a timezone-aware <code class="inlineCode">pd.DatetimeIndex</code>, be aware that pandas will implicitly convert your string argument into the timezone of the <code class="inlineCode">pd.DatetimeIndex</code>. For instance, the following code will only select one element from our data:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.loc[:"<span class="hljs-number">2023</span>-<span class="hljs-number">12</span>-<span class="hljs-number">27</span> <span class="hljs-number">11</span>:<span class="hljs-number">59</span>:<span class="hljs-number">59</span>"]
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2023-12-27 00:00:00-05:00    0
Freq: 12h, dtype: int64
</code></code></pre>
    <p class="normal">Whereas the following code will correctly select both elements:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.loc[:"<span class="hljs-number">2023</span>-<span class="hljs-number">12</span>-<span class="hljs-number">27</span> <span class="hljs-number">12</span>:<span class="hljs-number">00</span>:<span class="hljs-number">00</span>"]
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2023-12-27 00:00:00-05:00    0
2023-12-27 12:00:00-05:00    1
Freq: 12h, dtype: int64
</code></code></pre>
    <p class="normal">These both work<a id="_idIndexMarker491"/> in spite of the fact that our dates are five hours offset from UTC, and our string makes<a id="_idIndexMarker492"/> no indication of the expected timezone. In this way, pandas makes it very easy to express selection from a <code class="inlineCode">pd.DatetimeIndex</code>, whether it is timezone-aware or timezone-naive.</p>
    <h1 id="_idParaDest-250" class="heading-1">Resampling</h1>
    <p class="normal">Back in <em class="chapterRef">Chapter 8</em>, <em class="italic">Group By</em>, we <a id="_idIndexMarker493"/>went in-depth into the group by functionality that pandas has to offer. A Group By allows you to <em class="italic">split</em> your data based on unique value combinations in your dataset, <em class="italic">apply</em> an algorithm to those splits, and combine the results back together.</p>
    <p class="normal">A <em class="italic">resample</em> is very similar to a Group By, with the only difference happening during the <em class="italic">split</em> phase. Instead of generating groups from unique value combinations, a resample lets you take datetimes and group them into increments like <em class="italic">every 5 seconds</em> or <em class="italic">every 10 minutes</em>.</p>
    <h2 id="_idParaDest-251" class="heading-2">How to do it</h2>
    <p class="normal">Let’s once again reach for the <code class="inlineCode">pd.date_range</code> function we were introduced to back in the <em class="italic">Datetime selection</em> recipe, but this time, we are going to generate a <code class="inlineCode">pd.DatetimeIndex</code> with a frequency of seconds instead of days:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">index = pd.date_range(start="<span class="hljs-number">2024</span>-01-01", periods=<span class="hljs-number">10</span>, freq="s")
ser = pd.Series(<span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>), index=index, dtype=pd.Int64Dtype())
ser
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-01 00:00:00    0
2024-01-01 00:00:01    1
2024-01-01 00:00:02    2
2024-01-01 00:00:03    3
2024-01-01 00:00:04    4
2024-01-01 00:00:05    5
2024-01-01 00:00:06    6
2024-01-01 00:00:07    7
2024-01-01 00:00:08    8
2024-01-01 00:00:09    9
Freq: s, dtype: Int64
</code></code></pre>
    <p class="normal">If viewing this data every<a id="_idIndexMarker494"/> second was deemed too granular, <code class="inlineCode">pd.Series.resample</code> can be used to <em class="italic">downsample</em> the data into a different increment, like <em class="italic">every 3 seconds</em>. Resampling also requires the use of an aggregation function to dictate what happens to all records that fall within each increment; for simplicity, we can start with summation:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.resample("3s").<span class="hljs-built_in">sum</span>()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-01 00:00:00     3
2024-01-01 00:00:03    12
2024-01-01 00:00:06    21
2024-01-01 00:00:09     9
Freq: 3s, dtype: Int64
</code></code></pre>
    <p class="normal">In this particular case, <code class="inlineCode">resample</code> creates buckets using the ranges of <code class="inlineCode">[00:00:00-00:00:03)</code>, <code class="inlineCode">[00:00:03-00:00:06)</code>, <code class="inlineCode">[00:00:06-00:00:09)</code>, and <code class="inlineCode">[00:00:09-00:00:12)</code>. For each of those intervals, the left square bracket indicates that the interval is closed on the left side (i.e., it includes those values). By contrast, the right parentheses indicate an open interval that does not include the value.</p>
    <p class="normal">Technically speaking, all of these intervals created by the resample with a frequency of <code class="inlineCode">"3s"</code> are “left-closed” by default, but the <code class="inlineCode">closed=</code> argument can be used to change that behavior, effectively producing intervals with the values of <code class="inlineCode">(23:59:57-00:00:00]</code>, <code class="inlineCode">(00:00:00-00:00:03]</code>, <code class="inlineCode">(00:00:03-00:00:06]</code>, and <code class="inlineCode">(00:00:06-00:00:09]</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.resample("3s", closed="right").<span class="hljs-built_in">sum</span>()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2023-12-31 23:59:57     0
2024-01-01 00:00:00     6
2024-01-01 00:00:03    15
2024-01-01 00:00:06    24
Freq: 3s, dtype: Int64
</code></code></pre>
    <p class="normal">With the frequency of <code class="inlineCode">"3s"</code>, the left value of the interval is used as the value in the resulting row index. That <a id="_idIndexMarker495"/>behavior can also be changed through the use of the <code class="inlineCode">label=</code> argument:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.resample("3s", closed="right", label="right").<span class="hljs-built_in">sum</span>()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-01 00:00:00     0
2024-01-01 00:00:03     6
2024-01-01 00:00:06    15
2024-01-01 00:00:09    24
Freq: 3s, dtype: Int64
</code></code></pre>
    <p class="normal">One last caveat you may want to be aware of is that the default values for the <code class="inlineCode">closed=</code> and <code class="inlineCode">label=</code> arguments depend upon the frequency that you have chosen. Our frequency of <code class="inlineCode">"3s"</code> creates left-closed intervals, and uses the left interval value in the row index. However, if we had chosen a frequency that is oriented toward the end of a period, like <code class="inlineCode">ME</code> or <code class="inlineCode">YE</code> (month-end and year-end, respectively), pandas will instead produce right-closed intervals and use the right label:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.resample("ME").<span class="hljs-built_in">sum</span>()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-31    45
Freq: ME, dtype: Int64
</code></code></pre>
    <p class="normal">While we are on the topic of downsampling, let’s take a look at a different frequency, like days (<code class="inlineCode">"D"</code>). At this level, <code class="inlineCode">pd.Series.resample</code> can be a convenient way to aggregate daily events into weekly <a id="_idIndexMarker496"/>buckets. To see how this works, let’s just look at the first 10 days of 2024:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">index = pd.date_range(start="<span class="hljs-number">2024</span>-01-01", freq="D", periods=<span class="hljs-number">10</span>)
ser = pd.Series(<span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>), index=index, dtype=pd.Int64Dtype())
ser
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-01    0
2024-01-02    1
2024-01-03    2
2024-01-04    3
2024-01-05    4
2024-01-06    5
2024-01-07    6
2024-01-08    7
2024-01-09    8
2024-01-10    9
Freq: D, dtype: Int64
</code></code></pre>
    <p class="normal">Without looking up which day of the week each of these falls on, we can use <code class="inlineCode">pd.DatetimeIndex.dt.day_name()</code> to ground ourselves:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.index.day_name()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">Index(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday',
      'Sunday', 'Monday', 'Tuesday', 'Wednesday'],
     dtype='object')
</code></code></pre>
    <p class="normal">By default, resampling into weekly buckets will create periods that <em class="italic">end</em> on a Sunday:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.resample("W").<span class="hljs-built_in">sum</span>()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-07    21
2024-01-14    24
Freq: W-SUN, dtype: Int64
</code></code></pre>
    <p class="normal">You are free, however, to pick any day of the week for your period to end on. In the US, considering Saturday to be the end of the week is arguably more common than Sunday:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.resample("W-SAT").<span class="hljs-built_in">sum</span>()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-06    15
2024-01-13    30
Freq: W-SAT, dtype: Int64
</code></code></pre>
    <p class="normal">Though, you can pick any day of the week:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.resample("W-WED").<span class="hljs-built_in">sum</span>()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-03     3
2024-01-10    42
Freq: W-WED, dtype: Int64
</code></code></pre>
    <p class="normal">Now that we have covered<a id="_idIndexMarker497"/> the topic of <em class="italic">downsampling</em> (i.e., going from a more granular to a less granular frequency), let’s take a look at going in the opposite direction with the process of <em class="italic">upsampling</em>. Our data shows events that happen every day, but what if we wanted to create a time series that measured events every 12 hours?</p>
    <p class="normal">Fortunately, the API to achieve this is not all that different. You can still use <code class="inlineCode">pd.Series.resample</code> to start, but will subsequently want to chain in a call to <code class="inlineCode">pandas.core.resample.Resampler.asfreq</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.resample("12h").asfreq().iloc[:<span class="hljs-number">5</span>]
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-01 00:00:00       0
2024-01-01 12:00:00    &lt;NA&gt;
2024-01-02 00:00:00       1
2024-01-02 12:00:00    &lt;NA&gt;
2024-01-03 00:00:00       2
Freq: 12h, dtype: Int64
</code></code></pre>
    <p class="normal">Intervals generated during the <em class="italic">upsample</em>, which have no associated activity, are assigned a missing value. Left alone, there is likely not a ton of value to upsampling like this. However, pandas offers a few ways to fill in this missing data.</p>
    <p class="normal">The first approach to handle missing data may be to forward fill or backward fill values, so that missing values are just replaced with whatever record came <code class="inlineCode">preceding or following</code>, respectively.</p>
    <p class="normal">A forward fill will generate values of <code class="inlineCode">[0, 0, 1, 1, 2, 2, ...]</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.resample("12h").asfreq().ffill().iloc[:<span class="hljs-number">6</span>]
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-01 00:00:00    0
2024-01-01 12:00:00    0
2024-01-02 00:00:00    1
2024-01-02 12:00:00    1
2024-01-03 00:00:00    2
2024-01-03 12:00:00    2
Freq: 12h, dtype: Int64
</code></code></pre>
    <p class="normal">Whereas a backward fill yields <code class="inlineCode">[0, 1, 1, 2, 2, 3, ...]</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.resample("12h").asfreq().bfill().iloc[:<span class="hljs-number">6</span>]
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-01 00:00:00    0
2024-01-01 12:00:00    1
2024-01-02 00:00:00    1
2024-01-02 12:00:00    2
2024-01-03 00:00:00    2
2024-01-03 12:00:00    3
Freq: 12h, dtype: Int64
</code></code></pre>
    <p class="normal">An arguably more robust<a id="_idIndexMarker498"/> solution can be had in the form of <em class="italic">interpolation</em>, where the values preceding and following a missing value can be used to mathematically guess the missing value. The default interpolation will be <em class="italic">linear</em>, essentially taking the average of the value before and after each missing value:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.resample("12h").asfreq().interpolate().iloc[:<span class="hljs-number">6</span>]
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-01 00:00:00    0.0
2024-01-01 12:00:00    0.5
2024-01-02 00:00:00    1.0
2024-01-02 12:00:00    1.5
2024-01-03 00:00:00    2.0
2024-01-03 12:00:00    2.5
Freq: 12h, dtype: Float64
</code></code></pre>
    <h2 id="_idParaDest-252" class="heading-2">There’s more…</h2>
    <p class="normal">In the introduction to <a id="_idIndexMarker499"/>this recipe, we mentioned that a resample was similar to a Group By. In fact, you could rewrite a resample using <code class="inlineCode">pd.DataFrame.groupby</code> with a <code class="inlineCode">pd.Grouper</code> argument.</p>
    <p class="normal">Let’s once again look at a <code class="inlineCode">pd.Series</code> with 10 records occurring every second:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">index = pd.date_range(start="<span class="hljs-number">2024</span>-01-01", periods=<span class="hljs-number">10</span>, freq="s")
ser = pd.Series(<span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>), index=index, dtype=pd.Int64Dtype())
ser
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-01 00:00:00    0
2024-01-01 00:00:01    1
2024-01-01 00:00:02    2
2024-01-01 00:00:03    3
2024-01-01 00:00:04    4
2024-01-01 00:00:05    5
2024-01-01 00:00:06    6
2024-01-01 00:00:07    7
2024-01-01 00:00:08    8
2024-01-01 00:00:09    9
Freq: s, dtype: Int64
</code></code></pre>
    <p class="normal">A resample into three-second increments looks as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.resample("3s").<span class="hljs-built_in">sum</span>()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-01 00:00:00     3
2024-01-01 00:00:03    12
2024-01-01 00:00:06    21
2024-01-01 00:00:09     9
Freq: 3s, dtype: Int64
</code></code></pre>
    <p class="normal">This would be rewritten to get the same result by passing in <code class="inlineCode">"3s"</code> to the <code class="inlineCode">freq=</code> argument of a <code class="inlineCode">pd.Grouper</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">ser.groupby(pd.Grouper(freq="3s")).<span class="hljs-built_in">sum</span>()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">2024-01-01 00:00:00     3
2024-01-01 00:00:03    12
2024-01-01 00:00:06    21
2024-01-01 00:00:09     9
Freq: 3s, dtype: Int64
</code></code></pre>
    <p class="normal">There is no requirement that you use <code class="inlineCode">pd.DataFrame.resample</code>, and, in fact, you will find that the <code class="inlineCode">pd.Grouper</code> approach works better when you must also group by non-datetime values. We will <a id="_idIndexMarker500"/>see this in action in the <em class="italic">Calculating year-over-year changes in crime by category</em> recipe later in this chapter.</p>
    <h1 id="_idParaDest-253" class="heading-1">Aggregating weekly crime and traffic accidents</h1>
    <p class="normal">So far in this chapter, we <a id="_idIndexMarker501"/>have taken a basic tour of pandas’ offerings for dealing with temporal data. Starting with small sample datasets has made it easy to visually inspect the output of our operations, but we are now at the point where we can start focusing on applications to “real world” datasets.</p>
    <p class="normal">The Denver crime dataset is huge, with over 460,000 rows each marked with a datetime of when the crime was reported. As you will see in this recipe, we can use pandas to easily resample these events and ask questions like <em class="italic">How many crimes were reported in a given week</em>?.</p>
    <h2 id="_idParaDest-254" class="heading-2">How to do it</h2>
    <p class="normal">To start, let’s read in the crime dataset, setting our index as the <code class="inlineCode">REPORTED_DATE</code>. This dataset was saved using pandas extension types, so there is no need to specify the <code class="inlineCode">dtype_backend=</code> argument:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df = pd.read_parquet(
    "data/crime.parquet",
).set_index("REPORTED_DATE")
df.head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">REPORTED_DATE           OFFENSE_TYPE_ID               OFFENSE_CATEGORY_ID      2014-06-29 02:01:00     traffic-accident-dui-duid     traffic-accident        
2014-06-29 01:54:00     vehicular-eluding-no-chase    all-other-crimes        
2014-06-29 02:00:00     disturbing-the-peace          public-disorder         
2014-06-29 02:18:00     curfew                        public-disorder         
2014-06-29 04:17:00     aggravated-assault            aggravated-assault      
GEO_LON           NEIGHBORHOOD_ID             IS_CRIME         IS_TRAFFIC   -105.000149       cbd                         0                1
-105.020719       ath-mar-park                1                0
-105.001552       sunny-side                  1                0
-105.018557       college-view-south-platte   1                0
5 rows × 7 columns
</code></code></pre>
    <p class="normal">To count the number of crimes per week, we need to form a group for each week, which we know we can do with <code class="inlineCode">pd.DataFrame.resample</code>. Chaining a call to the <code class="inlineCode">.size</code> method will count the number of crimes within each week for us:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.resample("W").size()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">REPORTED_DATE
2012-01-08     877
2012-01-15    1071
2012-01-22     991
2012-01-29     988
2012-02-05     888
             ...
2017-09-03    1956
2017-09-10    1733
2017-09-17    1976
2017-09-24    1839
2017-10-01    1059
Freq: W-SUN, Length: 300, dtype: int64
</code></code></pre>
    <p class="normal">We now have the<a id="_idIndexMarker502"/> weekly crime count as a <code class="inlineCode">pd.Series</code> with the new index incrementing one week at a time. There are a few things that happen by default that are very important to understand. Sunday is chosen as the last day of the week and is also the date used to label each element in the resulting <code class="inlineCode">pd.Series</code>. For instance, the first index value, January 8, 2012, is a Sunday. There were 877 crimes committed during that week ending on the 8th. The week of Monday, January 9, to Sunday, January 15, recorded 1,071 crimes. Let’s do some sanity checks and ensure that our resampling is doing this:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-built_in">len</span>(df.sort_index().loc[:'<span class="hljs-number">2012</span>-01-08'])
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">877
</code></code></pre>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-built_in">len</span>(df.sort_index().loc['<span class="hljs-number">2012</span>-01-09':'<span class="hljs-number">2012</span>-01-<span class="hljs-number">15</span>'])
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">1071
</code></code></pre>
    <p class="normal">To get an overall <a id="_idIndexMarker503"/>understanding of the trend, it would be helpful to create a plot from our resampled data:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
plt.ion()
df.resample("W").size().plot(title="All Denver Crimes")
</code></code></pre>
    <figure class="mediaobject"><img src="../Images/B31091_09_01.png" alt=""/></figure>
    <p class="normal">The Denver crime dataset has all crime and traffic accidents together in one table and separates them through the binary columns <code class="inlineCode">IS_CRIME</code> and <code class="inlineCode">IS_TRAFFIC</code>. Using <code class="inlineCode">pd.DataFrame.resample</code>, we can select just these two columns and summarize them over a given period. For <a id="_idIndexMarker504"/>a quarterly summary, you would write:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.resample("QS")[["IS_CRIME", "IS_TRAFFIC"]].<span class="hljs-built_in">sum</span>().head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">              IS_CRIME  IS_TRAFFIC
REPORTED_DATE
2012-01-01    7882      4726
2012-04-01    9641      5255
2012-07-01    10566     5003
2012-10-01    9197      4802
2013-01-01    8730      4442
</code></code></pre>
    <p class="normal">Once again, a line plot to understand the trend may be more helpful:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.resample("QS")[["IS_CRIME", "IS_TRAFFIC"]].<span class="hljs-built_in">sum</span>().plot(
  color=["black", "lightgrey"],
  title="Denver Crime <span class="hljs-keyword">and</span> Traffic Accidents"
)
</code></code></pre>
    <figure class="mediaobject"><img src="../Images/B31091_09_02.png" alt=""/></figure>
    <h1 id="_idParaDest-255" class="heading-1">Calculating year-over-year changes in crime by category</h1>
    <p class="normal">Often, users want to <a id="_idIndexMarker505"/>know <em class="italic">How much did this change year over year?</em> or <em class="italic">…quarter over quarter?</em>. In spite of the frequency with which these questions are asked, writing algorithms to try and answer them can be rather complex and time-intensive. Fortunately, pandas gives you much of this functionality out of the box, trivializing much of the effort.</p>
    <p class="normal">To try and make things more complicated, in this recipe, we are going to ask the question of <em class="italic">how much did it change by category</em>? Adding <em class="italic">by category</em> into the equation will prevent us from directly using <code class="inlineCode">pd.DataFrame.resample</code>, but as you will see, pandas can still very easily help you answer these detailed types of questions.</p>
    <h2 id="_idParaDest-256" class="heading-2">How to do it</h2>
    <p class="normal">Let’s read in the crime dataset, but this time, we are not going to set the <code class="inlineCode">REPORTED_DATE</code> as our index:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df = pd.read_parquet(
    "data/crime.parquet",
)
df.head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">   OFFENSE_TYPE_ID  OFFENSE_CATEGORY_ID  REPORTED_DATE  …  NEIGHBORHOOD_ID  IS_CRIME   IS_TRAFFIC
0   traffic-accident-dui-duid   traffic-accident   2014-06-29 02:01:00   …   cbd   0   1
1   vehicular-eluding-no-chase   all-other-crimes   2014-06-29 01:54:00   …   east-colfax   1   0
2   disturbing-the-peace   public-disorder   2014-06-29 02:00:00   …   athmar-park   1   0
3   curfew   public-disorder   2014-06-29 02:18:00   …   sunny-side   1   0
4   aggravated-assault   aggravated-assault   2014-06-29 04:17:00   …   college-view-south-platte   1   0
5 rows × 8 columns
</code></code></pre>
    <p class="normal">By now, you should be<a id="_idIndexMarker506"/> comfortable enough with reshaping to answer questions like, <em class="italic">How many crimes happened in a given year?</em>. But what if we wanted to drill into that analysis and decide how it changed within each <code class="inlineCode">OFFENSE_CATEGORY_ID</code>?</p>
    <p class="normal">Since <code class="inlineCode">pd.DataFrame.resample</code> just works with a <code class="inlineCode">pd.DatetimeIndex</code>, it cannot be used to help us group by <code class="inlineCode">OFFENSE_CATEGORY_ID</code> and <code class="inlineCode">REPORTED_DATE</code>. However, the combination of <code class="inlineCode">pd.DataFrame.groupby</code> with a <code class="inlineCode">pd.Grouper</code> argument can help us express this:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby([
    "OFFENSE_CATEGORY_ID",
    pd.Grouper(key="REPORTED_DATE", freq="YS"),
], observed=<span class="hljs-literal">True</span>).agg(
    total_crime=pd.NamedAgg(column="IS_CRIME", aggfunc="<span class="hljs-built_in">sum</span>"),
)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">                                      total_crime
OFFENSE_CATEGORY_ID    REPORTED_DATE
aggravated-assault     2012-01-01           1707
                       2013-01-01           1631
                       2014-01-01           1788
                       2015-01-01           2007
                       2016-01-01           2139
…                               …              …
white-collar-crime     2013-01-01            771
                       2014-01-01           1043
                       2015-01-01           1319
                       2016-01-01           1232
                       2017-01-01           1058
90 rows × 1 columns
</code></code></pre>
    <p class="normal">As a technical aside, the <code class="inlineCode">observed=True</code> argument suppresses a warning about using categorical data types in a Group By in the pandas 2.x release; future readers may not need to specify this argument, as it will become the default.</p>
    <p class="normal">To add in the “year over year” component, we can try out the <code class="inlineCode">pd.Series.pct_change</code> method, which expresses<a id="_idIndexMarker507"/> each record as a percentage of the one directly preceding it:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.groupby([
    "OFFENSE_CATEGORY_ID",
    pd.Grouper(key="REPORTED_DATE", freq="YS"),
], observed=<span class="hljs-literal">True</span>).agg(
    total_crime=pd.NamedAgg(column="IS_CRIME", aggfunc="<span class="hljs-built_in">sum</span>"),
).assign(
    yoy_change=<span class="hljs-keyword">lambda</span> x: x["total_crime"].pct_change().astype(pd.Float64Dtype())
).head(<span class="hljs-number">10</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">                                          total_crime     yoy_change
OFFENSE_CATEGORY_ID      REPORTED_DATE
aggravated-assault          2012-01-01           1707           &lt;NA&gt;
                            2013-01-01           1631      -0.044523
                            2014-01-01           1788        0.09626
                            2015-01-01           2007       0.122483
                            2016-01-01           2139        0.06577
                            2017-01-01           1689      -0.210379
all-other-crimes            2012-01-01           1999       0.183541
                            2013-01-01           9377       3.690845
                            2014-01-01          15507       0.653727
                            2015-01-01          15729       0.014316
</code></code></pre>
    <p class="normal">Unfortunately, this is not giving us exactly what we want. If you look closely at the first <code class="inlineCode">yoy_change</code> value for all-other-crimes, it shows 0.183541. However, this value is taken by dividing 1999 by 1689, with 1689 coming from the aggravated-assault category. By default, <code class="inlineCode">pd.Series.pct_change</code> is not doing anything intelligent – it just divides the current row by the former.</p>
    <p class="normal">Fortunately, there is a way to fix that, by once again using a Group By. Because our <code class="inlineCode">OFFENSE_CATEGORY_ID</code> is the first index level, we can use a second Group By with <code class="inlineCode">level=0</code> and call the <code class="inlineCode">.pct_change</code> method on that. This will prevent us from accidentally comparing <code class="inlineCode">all-other-crimes</code> to<a id="_idIndexMarker508"/> <code class="inlineCode">aggravated-assault</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">yoy_crime = df.groupby([
    "OFFENSE_CATEGORY_ID",
    pd.Grouper(key="REPORTED_DATE", freq="YS"),
], observed=<span class="hljs-literal">True</span>).agg(
    total_crime=pd.NamedAgg(column="IS_CRIME", aggfunc="<span class="hljs-built_in">sum</span>"),
).assign(
    yoy_change=<span class="hljs-keyword">lambda</span> x: x.groupby(
        level=<span class="hljs-number">0</span>, observed=<span class="hljs-literal">True</span>
    ).pct_change().astype(pd.Float64Dtype())
)
yoy_crime.head(<span class="hljs-number">10</span>)
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">                                         total_crime     yoy_change
OFFENSE_CATEGORY_ID     REPORTED_DATE
aggravated-assault         2012-01-01           1707           &lt;NA&gt;
                           2013-01-01           1631      -0.044523
                           2014-01-01           1788        0.09626
                           2015-01-01           2007       0.122483
                           2016-01-01           2139        0.06577
                           2017-01-01           1689      -0.210379
all-other-crimes           2012-01-01           1999           &lt;NA&gt;
                           2013-01-01           9377       3.690845
                           2014-01-01          15507       0.653727
                           2015-01-01          15729       0.014316
</code></code></pre>
    <p class="normal">For a more visual representation, we may want to plot out the total crime and year-over-year change side by side for all of our different groups, building off of what we learned about visualizations back in <em class="chapterRef">Chapter 6</em>, <em class="italic">Visualization</em>.</p>
    <p class="normal">For brevity and to save<a id="_idIndexMarker509"/> some visual space, we are just going to plot a few crime types:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">crimes = <span class="hljs-built_in">tuple</span>(("aggravated-assault", "arson", "auto-theft"))
fig, axes = plt.subplots(nrows=<span class="hljs-built_in">len</span>(crimes), ncols=<span class="hljs-number">2</span>, sharex=<span class="hljs-literal">True</span>)
<span class="hljs-keyword">for</span> idx, crime <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(crimes):
    crime_df = yoy_crime.loc[crime]
    ax0 = axes[idx][<span class="hljs-number">0</span>]
    ax1 = axes[idx][<span class="hljs-number">1</span>]
    crime_df.plot(kind="bar", y="total_crime", ax=ax0, legend=<span class="hljs-literal">False</span>)
    crime_df.plot(kind="bar", y="yoy_change", ax=ax1, legend=<span class="hljs-literal">False</span>)
    xlabels = [x.year <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> crime_df.index]
    ax0.set_xticklabels(xlabels)
    ax0.set_title(f"{crime} total")
    ax1.set_xticklabels(xlabels)
    ax1.set_title(f"{crime} YoY")
    ax0.set_xlabel("")
    ax1.set_xlabel("")
plt.tight_layout()
</code></code></pre>
    <figure class="mediaobject"><img src="../Images/B31091_09_03.png" alt=""/></figure>
    <h1 id="_idParaDest-257" class="heading-1">Accurately measuring sensor-collected events with missing values</h1>
    <p class="normal">Missing data can have <a id="_idIndexMarker510"/>an immense impact on your data analysis, but it may not always be clear when and to what extent. With detailed and high-volume transactions, it won’t always be immediately obvious that a dataset is incomplete. Extra attention must be paid to measure and appropriately impute missing transactions; otherwise, any aggregations performed on such datasets may show an incomplete or even entirely wrong picture of what happened.</p>
    <p class="normal">For this recipe, we are going to use the <em class="italic">Smart Green Infrastructure Monitoring Sensors - Historical</em> dataset provided by the Chicago Data Portal. This dataset contains a collection of sensors that measured different environmental factors in the city of Chicago, like water runoff and temperature. In theory, the sensors should have constantly run and reported back values, but in practice, they were prone to intermittent outages that resulted in a loss of data.</p>
    <h2 id="_idParaDest-258" class="heading-2">How to do it</h2>
    <p class="normal">While the Chicago Data Portal provides the source data as a CSV file spanning the years 2017 and 2018, for this book, we are going to work with a curated Parquet file that only covers the months<a id="_idIndexMarker511"/> of June 2017 through October 2017. This alone provides almost 5 million rows of data, which we can load with a simple <code class="inlineCode">pd.read_parquet</code> call:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df = pd.read_parquet(
    "data/sgi_monitoring.parquet",
    dtype_backend="numpy_nullable",
)
df.head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">    Measurement Title   Measurement Description   Measurement Type   …   Latitude   Longitude   Location
0   UI Labs Bioswale NWS Proba-bility of Precipi-tation &lt;NA&gt;   TimeWin-dowBounda-ry   …   41.90715   -87.653996   POINT (-87.653996 41.90715)
1   UI Labs Bioswale NWS Proba-bility of Precipi-tation &lt;NA&gt;   TimeWin-dowBounda-ry   …   41.90715   -87.653996   POINT (-87.653996 41.90715)
2   UI Labs Bioswale NWS Proba-bility of Precipi-tation &lt;NA&gt;   TimeWin-dowBounda-ry   …   41.90715   -87.653996   POINT (-87.653996 41.90715)
3   UI Labs Bioswale NWS Proba-bility of Precipi-tation &lt;NA&gt;   TimeWin-dowBounda-ry   …   41.90715   -87.653996   POINT (-87.653996 41.90715)
4   UI Labs Bioswale NWS Proba-bility of Precipi-tation &lt;NA&gt;   TimeWin-dowBounda-ry   …   41.90715   -87.653996   POINT (-87.653996 41.90715)
5 rows × 16 columns
</code></code></pre>
    <p class="normal">The <code class="inlineCode">Measurement Time</code> column should contain the datetime data for when each event occurred, but upon closer inspection, you will see that pandas did not recognize this as a datetime type:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df["Measurement Time"].head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0    07/26/2017 07:00:00 AM
1    06/23/2017 07:00:00 AM
2    06/04/2017 07:00:00 AM
3    09/19/2017 07:00:00 AM
4    06/07/2017 07:00:00 AM
Name: Measurement Time, dtype: string
</code></code></pre>
    <p class="normal">As such, the first step in exploring our data will be to convert this to the real datetime type using <code class="inlineCode">pd.to_datetime</code>. While it isn’t clear from the data itself, the Chicago Data Portal documentation <a id="_idIndexMarker512"/>notes that these values are local to the Chicago timezone, which we can use <code class="inlineCode">pd.Series.dt.tz_localize</code> to set:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df["Measurement Time"] = pd.to_datetime(
    df["Measurement Time"]
).dt.tz_localize("US/Central")
df["Measurement Time"]
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">0         2017-07-26 07:00:00-05:00
1         2017-06-23 07:00:00-05:00
2         2017-06-04 07:00:00-05:00
3         2017-09-19 07:00:00-05:00
4         2017-06-07 07:00:00-05:00
                    ...           
4889976   2017-08-26 20:11:55-05:00
4889977   2017-08-26 20:10:54-05:00
4889978   2017-08-26 20:09:53-05:00
4889979   2017-08-26 20:08:52-05:00
4889980   2017-08-26 20:07:50-05:00
Name: Measurement Time, Length: 4889981, dtype: datetime64[ns, US/Central]
</code></code></pre>
    <p class="normal">As mentioned, this dataset collects feedback from sensors that measure different environmental factors, like water runoff and temperature. Inspecting the <code class="inlineCode">Measurement Type</code> and <code class="inlineCode">Units</code> column should give us a better idea of what we are looking at:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df[["Measurement <span class="hljs-type">Type</span>", "Units"]].value_counts()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">Measurement Type         Units                           
Temperature              degrees Celsius                     721697
DifferentialPressure     pascals                             721671
WindSpeed                meters per second                   721665
Temperature              millivolts                          612313
SoilMoisture             millivolts                          612312
RelativeHumidity         percent                             389424
CumulativePrecipitation  count                               389415
WindDirection            degrees from north                  389413
SoilMoisture             Percent Volumetric Water Content    208391
CumulativePrecipitation  inches                              122762
TimeWindowBoundary       universal coordinated time             918
Name: count, dtype: int64
</code></code></pre>
    <p class="normal">Because the different sensors produce different measurements for different types of data, we must be careful not to compare more than one sensor at a time. For this analysis, we are going to <a id="_idIndexMarker513"/>just focus on the <code class="inlineCode">TM1 Temp Sensor</code>, which only measures the temperature using a unit of millivolts. Additionally, we are going to single in on one <code class="inlineCode">Data Stream ID</code>, which the Chicago Data Portal documents as:</p>
    <blockquote class="packt_quote">
      <p class="quote"> An identifier for the measurement type and location. All records with the same value should be comparable.</p>
    </blockquote>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df[df["Measurement Description"] == "TM1 Temp Sensor"]["Data Stream ID"].value_counts()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">Data Stream ID
33305    211584
39197    207193
39176    193536
Name: count, dtype: Int64
</code></code></pre>
    <p class="normal">For this analysis, we are going to only look at <code class="inlineCode">Data Stream ID</code> <code class="inlineCode">39176</code>. After filtering, we are also going to set <code class="inlineCode">Measurement Time</code> as our row index and sort it:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">mask = (
    (df["Measurement Description"] == "TM1 Temp Sensor")
    &amp; (df["Data Stream ID"] == <span class="hljs-number">39176</span>)
)
df = df[mask].set_index("Measurement Time").sort_index()
df[["Measurement <span class="hljs-type">Type</span>", "Units"]].value_counts()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">Measurement Type  Units     
Temperature       millivolts    193536
Name: count, dtype: int64
</code></code></pre>
    <p class="normal">The <code class="inlineCode">Measurement Value</code> column<a id="_idIndexMarker514"/> contains the actual millivolts reading from the sensors. Let’s start by resampling to the daily level and using mean aggregation on that column to try and understand our data at a higher level:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.resample("D")["Measurement Value"].mean().plot()
</code></code></pre>
    <figure class="mediaobject"><img src="../Images/B31091_09_04.png" alt=""/></figure>
    <p class="normal">Almost immediately, we <a id="_idIndexMarker515"/>can see some issues with our data. Most notably, there are two gaps where the lines break toward the end of July and middle of October, which are almost assuredly records that were not collected due to the sensors being down.</p>
    <p class="normal">Let’s try narrowing our date range so that we can more clearly see what days are missing from our dataset:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.loc["<span class="hljs-number">2017</span>-07-<span class="hljs-number">24</span>":"<span class="hljs-number">2017</span>-08-01"].resample("D")["Measurement Value"].mean()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">Measurement Time
2017-07-24 00:00:00-05:00    3295.908956
2017-07-25 00:00:00-05:00    3296.152968
2017-07-26 00:00:00-05:00    3296.460156
2017-07-27 00:00:00-05:00    3296.697269
2017-07-28 00:00:00-05:00    3296.328725
2017-07-29 00:00:00-05:00    3295.882705
2017-07-30 00:00:00-05:00    3295.800989
2017-07-31 00:00:00-05:00           &lt;NA&gt;
2017-08-01 00:00:00-05:00    3296.126888
Freq: D, Name: Measurement Value, dtype: Float64
</code></code></pre>
    <p class="normal">As you can see, we have no data collected at all for July 31, 2017. To fix this, we can simply chain in a <a id="_idIndexMarker516"/>call to <code class="inlineCode">pd.Series.interpolate</code>, which will fill in the missing days with the average of the values directly preceding and following:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.resample("D")["Measurement Value"].mean().interpolate().plot()
</code></code></pre>
    <figure class="mediaobject"><img src="../Images/B31091_09_05.png" alt=""/></figure>
    <p class="normal">Voila! Now, we no<a id="_idIndexMarker517"/> longer have any gaps in our data collection, yielding a visually appealing, fully drawn-in visual.</p>
    <h2 id="_idParaDest-259" class="heading-2">There’s more…</h2>
    <p class="normal">How you handle missing data also depends on the aggregation function that you are using. In this recipe, the mean is a relatively forgiving function; missing transactions can be masked by the fact that they do not materially change the average being produced.</p>
    <p class="normal">However, if we were looking to measure the daily summation of our readings, we would still have some more work to do. For starters, let’s see what a daily-resampled summation of these readings looks like:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.resample("D")["Measurement Value"].<span class="hljs-built_in">sum</span>().plot()
</code></code></pre>
    <figure class="mediaobject"><img src="../Images/B31091_09_06.png" alt=""/></figure>
    <p class="normal">Things look more dire <a id="_idIndexMarker518"/>than in the case of wanting the average. We still see huge dips in late July and October, which we know go back to a lack of data. However, when we dive into the data at the end of July that we saw before, the summation will reveal a few more interesting things about our data:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.loc["<span class="hljs-number">2017</span>-07-<span class="hljs-number">30</span> <span class="hljs-number">15</span>:<span class="hljs-number">45</span>:<span class="hljs-number">00</span>":"<span class="hljs-number">2017</span>-08-01"].head()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">    Measurement Title   Measurement Description   Measurement Type    …   Latitude   Longitude   Location   Measurement Time
2017-07-30 15:48:44-05:00   Argyle - Thun-der 1: TM1 Temp Sensor   TM1 Temp Sensor   Temperature   …   41.973086   -87.659725   POINT (-87.659725 41.973086)
2017-07-30 15:49:45-05:00   Argyle - Thun-der 1: TM1 Temp Sensor   TM1 Temp Sensor   Temperature   …   41.973086   -87.659725   POINT (-87.659725 41.973086)
2017-07-30 15:50:46-05:00   Argyle - Thun-der 1: TM1 Temp Sensor   TM1 Temp Sensor   Temperature   …   41.973086   -87.659725   POINT (-87.659725 41.973086)
2017-08-01 15:21:33-05:00   Argyle - Thun-der 1: TM1 Temp Sensor   TM1 Temp Sensor   Temperature   …   41.973086   -87.659725   POINT (-87.659725 41.973086)
2017-08-01 15:22:34-05:00   Argyle - Thun-der 1: TM1 Temp Sensor   TM1 Temp Sensor   Temperature   …   41.973086   -87.659725   POINT (-87.659725 41.973086)
5 rows × 15 columns
</code></code></pre>
    <p class="normal">It wasn’t just the day of July 31 when we had an outage. The mean aggregation we did before masked the fact<a id="_idIndexMarker519"/> that the sensors went down sometime after 15:50:46 on July 30 and did not come back online until 15:21:33 on August 1 – an outage of almost 2 full days.</p>
    <p class="normal">Another interesting thing to try and measure is the expected frequency with which our data should be populated. From an initial glance at our data, it appears as if each minute should supply a data point, but if you try to measure how many events were collected each hour, you will see a different story:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.resample("h").size().plot()
</code></code></pre>
    <figure class="mediaobject"><img src="../Images/B31091_09_07.png" alt=""/></figure>
    <p class="normal">Many of the hourly intervals<a id="_idIndexMarker520"/> appear to have close to 60 events collected, although surprisingly, only 1 hour actually collected a full 60:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.resample("h").size().loc[<span class="hljs-keyword">lambda</span> x: x &gt;= <span class="hljs-number">60</span>]
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">Measurement Time
2017-07-05 15:00:00-05:00    60
Freq: h, dtype: int64
</code></code></pre>
    <p class="normal">To fix this, let’s try once again to resample our data by the minute and interpolate where results are missing:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">df.resample("<span class="hljs-built_in">min</span>")["Measurement Value"].<span class="hljs-built_in">sum</span>().interpolate()
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">Measurement Time
2017-06-01 00:00:00-05:00    3295.0
2017-06-01 00:01:00-05:00    3295.0
2017-06-01 00:02:00-05:00    3295.0
2017-06-01 00:03:00-05:00    3295.0
2017-06-01 00:04:00-05:00    3295.0
                             ...  
2017-10-30 23:55:00-05:00    3293.0
2017-10-30 23:56:00-05:00    3293.0
2017-10-30 23:57:00-05:00       0.0
2017-10-30 23:58:00-05:00    3293.0
2017-10-30 23:59:00-05:00    3293.0
Freq: min, Name: Measurement Value, Length: 218880, dtype: Float64
</code></code></pre>
    <p class="normal">There is a slight caveat users should be aware of with the summation of missing values. By default, pandas will sum all missing values to <code class="inlineCode">0</code> instead of a missing value. In the case of our resample to minutes, the data point at 2017-10-30 23:57:00 had no values to sum, so pandas<a id="_idIndexMarker521"/> returned the value of <code class="inlineCode">0</code> instead of a missing value indicator.</p>
    <p class="normal">We need a missing value indicator for the resample to work. Luckily, we can still get this by providing the <code class="inlineCode">sum</code> method with a <code class="inlineCode">min_count=</code> argument that is <code class="inlineCode">1</code> (or greater), essentially establishing how many non-missing values must be seen to yield a non-missing result:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">interpolated = df.resample("<span class="hljs-built_in">min</span>")["Measurement Value"].<span class="hljs-built_in">sum</span>(min_count=<span class="hljs-number">1</span>).interpolate()
interpolated
</code></code></pre>
    <pre class="programlisting con"><code class="hljs-con"><code class="hljs-con">Measurement Time
2017-06-01 00:00:00-05:00    3295.0
2017-06-01 00:01:00-05:00    3295.0
2017-06-01 00:02:00-05:00    3295.0
2017-06-01 00:03:00-05:00    3295.0
2017-06-01 00:04:00-05:00    3295.0
                             ...  
2017-10-30 23:55:00-05:00    3293.0
2017-10-30 23:56:00-05:00    3293.0
2017-10-30 23:57:00-05:00    3293.0
2017-10-30 23:58:00-05:00    3293.0
2017-10-30 23:59:00-05:00    3293.0
Freq: min, Name: Measurement Value, Length: 218880, dtype: Float64
</code></code></pre>
    <p class="normal">As you can see, the value for 2017-10-30 23:57:00 now shows as <code class="inlineCode">3293</code>, which was interpolated by taking both the preceding and following values.</p>
    <p class="normal">With that out of<a id="_idIndexMarker522"/> the way, let’s now confirm that we always see 60 events per hour:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">interpolated.resample("h").size().plot()
</code></code></pre>
    <figure class="mediaobject"><img src="../Images/B31091_09_08.png" alt=""/></figure>
    <p class="normal">That check looks good, so now, we can try to downsample again back to the daily level and see what the <a id="_idIndexMarker523"/>overall summation trend looks like:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="hljs-code">interpolated.resample("D").<span class="hljs-built_in">sum</span>().plot()
</code></code></pre>
    <figure class="mediaobject"><img src="../Images/B31091_09_09.png" alt=""/></figure>
    <p class="normal">This is a drastically different graph from what we started with. We not only removed the extreme outliers from influencing the <em class="italic">y</em>-axis of our graph but we can also see a general lift in the lower bounds of values. In our original graph, the lower bound of the total millivolts measured was commonly in the range of 3.5–4 million per day, but now, our lower bound appears somewhere around 4.74 million.</p>
    <p class="normal">In effect, by paying attention to and handling missing values in our time series data, we were able to yield <a id="_idIndexMarker524"/>many different insights from our dataset. In relatively few lines of code, pandas has helped us clearly and concisely get our data to a much better place than where we started.</p>
    <h1 id="_idParaDest-260" class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/pandas"><span class="url">https://packt.link/pandas</span></a></p>
    <p class="normal"><img src="../Images/QR_Code5040900042138312.png" alt=""/></p>
    <h1 id="_idParaDest-261" class="heading-1">Leave a Review! </h1>
    <p class="normal">Thank you for purchasing this book from Packt Publishing—we hope you enjoy it! Your feedback is invaluable and helps us improve and grow. Once you’ve completed reading it, please take a moment to leave an Amazon review; it will only take a minute, but it makes a big difference for readers like you.</p>
    <p class="normal">Scan the QR code below to receive a free ebook of your choice.</p>
    <p class="normal"><a href="Chapter_9.xhtml"><span class="url">https://packt.link/NzOWQ</span></a></p>
    <p class="normal"><img src="../Images/QR_Code1474021820358918656.png" alt=""/></p>
  </div>
</body></html>