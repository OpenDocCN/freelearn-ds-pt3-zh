<html><head></head><body>
		<div id="_idContainer008">
			<h1 id="_idParaDest-15" class="chapter-number"><a id="_idTextAnchor014"/><span class="koboSpan" id="kobo.1.1">1</span></h1>
			<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/><span class="koboSpan" id="kobo.2.1">Data Ingestion Techniques</span></h1>
			<p><strong class="bold"><span class="koboSpan" id="kobo.3.1">Data ingestion</span></strong><span class="koboSpan" id="kobo.4.1"> is a</span><a id="_idIndexMarker000"/><span class="koboSpan" id="kobo.5.1"> critical component of the data life cycle and sets the foundation for subsequent data transformation and cleaning. </span><span class="koboSpan" id="kobo.5.2">It involves the process of collecting and importing data from various sources into a storage system where it can be accessed and analyzed. </span><span class="koboSpan" id="kobo.5.3">Effective data ingestion is crucial for ensuring data quality, integrity, and availability, which directly impacts the efficiency and accuracy of data transformation and cleaning processes. </span><span class="koboSpan" id="kobo.5.4">In this chapter, we will dive deep into the different types of data sources, explore various data ingestion methods, and discuss their respective advantages, disadvantages, and </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">real-world applications.</span></span></p>
			<p><span class="koboSpan" id="kobo.7.1">In this chapter, we’ll cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">following topics:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.9.1">Ingesting data in </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">batch mode</span></span></li>
				<li><span class="koboSpan" id="kobo.11.1">Ingesting data in </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">streaming mode</span></span></li>
				<li><span class="koboSpan" id="kobo.13.1">Real-time versus </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">semi-real-time ingestion</span></span></li>
				<li><span class="koboSpan" id="kobo.15.1">Data </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">sources technologies</span></span></li>
			</ul>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/><span class="koboSpan" id="kobo.17.1">Technical requirements</span></h1>
			<p><span class="koboSpan" id="kobo.18.1">You can find all the code for the chapter in the following </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">GitHub repository:</span></span></p>
			<p><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter01"><span class="No-Break"><span class="koboSpan" id="kobo.20.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/tree/main/chapter01</span></span></a></p>
			<p><span class="koboSpan" id="kobo.21.1">You can use your favorite IDE (VS Code, PyCharm, Google Colab, etc.) to write and execute </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">your code.</span></span></p>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/><span class="koboSpan" id="kobo.23.1">Ingesting data in batch mode</span></h1>
			<p><strong class="bold"><span class="koboSpan" id="kobo.24.1">Batch ingestion</span></strong><span class="koboSpan" id="kobo.25.1"> is a </span><a id="_idIndexMarker001"/><span class="koboSpan" id="kobo.26.1">data processing technique whereby large</span><a id="_idIndexMarker002"/><span class="koboSpan" id="kobo.27.1"> volumes of data are collected, processed, and loaded into a system at scheduled intervals, rather than in real-time. </span><span class="koboSpan" id="kobo.27.2">This approach allows organizations to handle substantial amounts of data efficiently by grouping data into batches, which are then processed collectively. </span><span class="koboSpan" id="kobo.27.3">For example, a company might collect</span><a id="_idIndexMarker003"/><span class="koboSpan" id="kobo.28.1"> customer transaction data throughout the day and then process it in a single batch during off-peak hours. </span><span class="koboSpan" id="kobo.28.2">This method is particularly useful for organizations that need to process high volumes of data but do not require </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">immediate analysis.</span></span></p>
			<p><span class="koboSpan" id="kobo.30.1">Batch ingestion is beneficial because it optimizes system resources by spreading the processing load across scheduled times, often when the system is underutilized. </span><span class="koboSpan" id="kobo.30.2">This reduces the strain on computational resources and can lower costs, especially in cloud-based environments where computing power is metered. </span><span class="koboSpan" id="kobo.30.3">Additionally, batch processing simplifies data management, as it allows for the easy application of consistent transformations and validations across large datasets. </span><span class="koboSpan" id="kobo.30.4">For organizations with regular, predictable data flows, batch ingestion provides a reliable, scalable, and cost-effective solution for data processing </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">and analytics.</span></span></p>
			<p><span class="koboSpan" id="kobo.32.1">Let’s explore batch ingestion in more detail, starting with its advantages </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">and disadvantages.</span></span></p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor018"/><span class="koboSpan" id="kobo.34.1">Advantages and disadvantages</span></h2>
			<p><span class="koboSpan" id="kobo.35.1">Batch ingestion </span><a id="_idIndexMarker004"/><span class="koboSpan" id="kobo.36.1">offers several notable advantages that make it an attractive choice for many data </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">processing needs:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.38.1">Efficiency</span></strong><span class="koboSpan" id="kobo.39.1"> is a key benefit, as batch processing allows for the handling of large volumes of data in a single operation, optimizing resource usage and </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">minimizing overhead</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.41.1">Cost-effectiveness</span></strong><span class="koboSpan" id="kobo.42.1"> is another benefit, reducing the need for continuous processing resources and lowering </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">operational expenses.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.44.1">Simplicity</span></strong><span class="koboSpan" id="kobo.45.1"> makes it easier to manage and implement periodic data processing tasks compared to real-time ingestion, which often requires more complex infrastructure </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">and management</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.47.1">Robustness</span></strong><span class="koboSpan" id="kobo.48.1">, as batch processing is well-suited for performing complex data transformations and comprehensive data validation, ensuring high-quality, </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">reliable data</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.50.1">However, batch ingestion also </span><a id="_idIndexMarker005"/><span class="koboSpan" id="kobo.51.1">comes with </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">certain drawbacks:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.53.1">There is an inherent delay between the generation of data and its availability for analysis, which can be a critical issue for applications requiring </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">real-time insights.</span></span></li>
				<li><span class="koboSpan" id="kobo.55.1">Resource spikes can occur during batch processing windows, leading to high resource usage and potential </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">performance bottlenecks</span></span></li>
				<li><span class="koboSpan" id="kobo.57.1">Scalability can also be a concern, as handling very large datasets may require significant infrastructure investment </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">and management</span></span></li>
				<li><span class="koboSpan" id="kobo.59.1">Lastly, maintenance is a crucial aspect of batch ingestion; it demands careful scheduling and ongoing maintenance to ensure the timely and reliable execution of </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">batch jobs</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.61.1">Let’s look at some common use cases for ingesting data in </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">batch mode.</span></span></p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/><span class="koboSpan" id="kobo.63.1">Common use cases for batch ingestion</span></h2>
			<p><span class="koboSpan" id="kobo.64.1">Any</span><a id="_idIndexMarker006"/><span class="koboSpan" id="kobo.65.1"> data analytics platform such as data warehouses or data lakes requires regularly updated data for </span><strong class="bold"><span class="koboSpan" id="kobo.66.1">Business Intelligence</span></strong><span class="koboSpan" id="kobo.67.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.68.1">BI</span></strong><span class="koboSpan" id="kobo.69.1">) and reporting. </span><span class="koboSpan" id="kobo.69.2">Batch ingestion</span><a id="_idIndexMarker007"/><span class="koboSpan" id="kobo.70.1"> is integral as it ensures that data is continually updated with the latest information, enabling businesses to perform comprehensive and up-to-date analyses. </span><span class="koboSpan" id="kobo.70.2">By processing data in batches, organizations can efficiently handle vast amounts of transactional and operational data, transforming it into a structured format suitable for querying and reporting. </span><span class="koboSpan" id="kobo.70.3">This supports BI initiatives, allowing analysts and decision-makers to generate insightful reports, track </span><strong class="bold"><span class="koboSpan" id="kobo.71.1">Key Performance Indicators</span></strong><span class="koboSpan" id="kobo.72.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.73.1">KPIs</span></strong><span class="koboSpan" id="kobo.74.1">), and</span><a id="_idIndexMarker008"/><span class="koboSpan" id="kobo.75.1"> make </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">data-driven decisions.</span></span></p>
			<p><strong class="bold"><span class="koboSpan" id="kobo.77.1">Extract, Transform, and Load</span></strong><span class="koboSpan" id="kobo.78.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.79.1">ETL</span></strong><span class="koboSpan" id="kobo.80.1">) processes</span><a id="_idIndexMarker009"/><span class="koboSpan" id="kobo.81.1"> are a cornerstone of data integration projects, and batch ingestion plays a crucial role in these workflows. </span><span class="koboSpan" id="kobo.81.2">In ETL processes, data is extracted from various sources, transformed to fit the operational needs of the target system, and loaded into a database or data warehouse. </span><span class="koboSpan" id="kobo.81.3">Batch processing allows for efficient handling of these steps, particularly when dealing with large datasets that require significant transformation and cleansing. </span><span class="koboSpan" id="kobo.81.4">This method is ideal for periodic data consolidation, where data from disparate systems is integrated to provide a unified view, supporting activities such as data migration, system integration, and master </span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">data management.</span></span></p>
			<p><span class="koboSpan" id="kobo.83.1">Batch ingestion</span><a id="_idIndexMarker010"/><span class="koboSpan" id="kobo.84.1"> is also widely used for backups and archiving, which are critical processes for data preservation and disaster recovery. </span><span class="koboSpan" id="kobo.84.2">Periodic batch processing allows for the scheduled backup of databases, ensuring that all data is captured and securely stored at regular intervals. </span><span class="koboSpan" id="kobo.84.3">This approach minimizes the risk of data loss and provides a reliable restore point in case of system failures or data corruption. </span><span class="koboSpan" id="kobo.84.4">Additionally, batch processing is used for data archiving, where historical data is periodically moved from active systems to long-term storage solutions. </span><span class="koboSpan" id="kobo.84.5">This not only helps in managing storage costs but also ensures that important data is retained and can be retrieved for compliance, auditing, or historical </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">analysis purposes.</span></span></p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/><span class="koboSpan" id="kobo.86.1">Batch ingestion use cases</span></h2>
			<p><span class="koboSpan" id="kobo.87.1">Batch ingestion is a</span><a id="_idIndexMarker011"/><span class="koboSpan" id="kobo.88.1"> methodical process involving several key steps: data extraction, data transformation, data loading, scheduling, and automation. </span><span class="koboSpan" id="kobo.88.2">To illustrate these steps, let’s explore a use case involving an investment bank that needs to process and analyze trading data for regulatory compliance and </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">performance reporting.</span></span></p>
			<h3><span class="koboSpan" id="kobo.90.1">Batch ingestion in an investment bank</span></h3>
			<p><span class="koboSpan" id="kobo.91.1">An investment bank </span><a id="_idIndexMarker012"/><span class="koboSpan" id="kobo.92.1">needs to collect, transform, and load trading data from various financial markets into a central data warehouse. </span><span class="koboSpan" id="kobo.92.2">This data will be used for generating daily compliance reports, evaluating trading strategies, and making informed </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">investment decisions.</span></span></p>
			<h3><span class="koboSpan" id="kobo.94.1">Data extraction</span></h3>
			<p><span class="koboSpan" id="kobo.95.1">The first </span><a id="_idIndexMarker013"/><span class="koboSpan" id="kobo.96.1">step is identifying the sources from which data will be extracted. </span><span class="koboSpan" id="kobo.96.2">For the investment bank, this includes trading systems, market data providers, and internal risk management systems. </span><span class="koboSpan" id="kobo.96.3">These sources contain critical data such as trade execution details, market prices, and risk assessments. </span><span class="koboSpan" id="kobo.96.4">Once the sources are identified, data is collected using connectors or scripts. </span><span class="koboSpan" id="kobo.96.5">This involves setting up data pipelines that extract data from trading systems, import real-time market data feeds, and pull risk metrics from internal systems. </span><span class="koboSpan" id="kobo.96.6">The extracted data is then temporarily stored in staging areas </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">before processing.</span></span></p>
			<h3><span class="koboSpan" id="kobo.98.1">Data transformation</span></h3>
			<p><span class="koboSpan" id="kobo.99.1">The</span><a id="_idIndexMarker014"/><span class="koboSpan" id="kobo.100.1"> extracted data often contains inconsistencies, duplicates, and missing values. </span><span class="koboSpan" id="kobo.100.2">Data cleaning is performed to remove duplicates, fill in missing information, and correct errors. </span><span class="koboSpan" id="kobo.100.3">For the investment bank, this ensures that trade records are accurate and complete, providing a reliable foundation for compliance reporting and performance analysis. </span><span class="koboSpan" id="kobo.100.4">After cleaning, the data undergoes transformations such as aggregations, joins, and calculations. </span><span class="koboSpan" id="kobo.100.5">For example, the investment bank might aggregate trade data to calculate daily trading volumes, join trade records with market data to analyze price movements, and calculate key metrics such as </span><strong class="bold"><span class="koboSpan" id="kobo.101.1">Profit and Loss</span></strong><span class="koboSpan" id="kobo.102.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.103.1">P&amp;L</span></strong><span class="koboSpan" id="kobo.104.1">) and</span><a id="_idIndexMarker015"/><span class="koboSpan" id="kobo.105.1"> risk exposure. </span><span class="koboSpan" id="kobo.105.2">The transformed data must be mapped to the schema of the target system. </span><span class="koboSpan" id="kobo.105.3">This involves aligning the data fields with the structure of the data warehouse. </span><span class="koboSpan" id="kobo.105.4">For instance, trade data might be mapped to tables representing transactions, market data, and risk metrics, ensuring seamless integration with the existing </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">data model.</span></span></p>
			<h3><span class="koboSpan" id="kobo.107.1">Data loading</span></h3>
			<p><span class="koboSpan" id="kobo.108.1">The </span><a id="_idIndexMarker016"/><span class="koboSpan" id="kobo.109.1">transformed data is processed in batches, which allows the investment bank to handle large volumes of data efficiently, performing complex transformations and aggregations in a single run. </span><span class="koboSpan" id="kobo.109.2">Once processed, the data is loaded into the target storage system, such as a data warehouse or data lake. </span><span class="koboSpan" id="kobo.109.3">For the investment bank, this means loading the cleaned and transformed trading data into their data warehouse, where it can be accessed for compliance reporting and </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">performance analysis.</span></span></p>
			<h3><span class="koboSpan" id="kobo.111.1">Scheduling and automation</span></h3>
			<p><span class="koboSpan" id="kobo.112.1">To </span><a id="_idIndexMarker017"/><span class="koboSpan" id="kobo.113.1">ensure that the batch ingestion process runs smoothly and consistently, scheduling tools such as Apache Airflow or Cron jobs are used. </span><span class="koboSpan" id="kobo.113.2">These tools automate the data ingestion workflows, scheduling them to run at regular intervals, such as every night or every day. </span><span class="koboSpan" id="kobo.113.3">This allows the investment bank to have up-to-date data available for analysis without manual intervention. </span><span class="koboSpan" id="kobo.113.4">Implementing monitoring is crucial to track the success and performance of batch jobs. </span><span class="koboSpan" id="kobo.113.5">Monitoring tools provide insights into job execution, identifying any failures or performance bottlenecks. </span><span class="koboSpan" id="kobo.113.6">For the investment bank, this ensures that any issues in the data ingestion process are promptly detected and resolved, maintaining the integrity and reliability of the </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">data pipeline.</span></span></p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/><span class="koboSpan" id="kobo.115.1">Batch ingestion with an example</span></h2>
			<p><span class="koboSpan" id="kobo.116.1">Let’s have</span><a id="_idIndexMarker018"/><span class="koboSpan" id="kobo.117.1"> a look at a simple example of a batch processing ingestion system written in Python. </span><span class="koboSpan" id="kobo.117.2">This example will simulate the ETL process. </span><span class="koboSpan" id="kobo.117.3">We’ll generate some mock data, process it in batches, and load it into a </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">simulated database.</span></span></p>
			<p><span class="koboSpan" id="kobo.119.1">You can find the code for this part in the GitHub repository at </span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/1.batch.py"><span class="koboSpan" id="kobo.120.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/1.batch.py</span></a><span class="koboSpan" id="kobo.121.1">. </span><span class="koboSpan" id="kobo.121.2">To run this example, we don’t need any bespoke library installation. </span><span class="koboSpan" id="kobo.121.3">We just need to ensure that we are running it in a standard Python environment (</span><span class="No-Break"><span class="koboSpan" id="kobo.122.1">Python 3.x):</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.123.1">We create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.124.1">generate_mock_data</span></strong><span class="koboSpan" id="kobo.125.1"> function that generates a list of mock </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">data records:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.127.1">
def generate_mock_data(num_records):
    data = []
    for _ in range(num_records):
        record = {
            'id': random.randint(1, 1000),
            'value': random.random() * 100
        }
        data.append(record)
return data</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.128.1">Each record is a dictionary with </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">two fields:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.130.1">id</span></strong><span class="koboSpan" id="kobo.131.1">: A random integer between 1 </span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">and 1000</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.133.1">value</span></strong><span class="koboSpan" id="kobo.134.1">: A random float between 0 </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">and 100</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.136.1">Let’s have a look at what the data </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">looks like:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.138.1">print("Original data:", data)
{'id': 449, 'value': 99.79699336555473}
{'id': 991, 'value': 79.65999078145887}</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.139.1"> A list of dictionaries is returned, each representing a </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">data record.</span></span></p></li>				<li><span class="koboSpan" id="kobo.141.1">Next, we</span><a id="_idIndexMarker019"/><span class="koboSpan" id="kobo.142.1"> create a batch </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">processing function:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.144.1">
def process_in_batches(data, batch_size):
    for i in range(0, len(data), batch_size):
        yield data[i:i + batch_size]</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.145.1">This function takes the data, which is a list of data records to process, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.146.1">batch_size</span></strong><span class="koboSpan" id="kobo.147.1">, which represents the number of records per batch, as parameters. </span><span class="koboSpan" id="kobo.147.2">The function uses a </span><strong class="source-inline"><span class="koboSpan" id="kobo.148.1">for</span></strong><span class="koboSpan" id="kobo.149.1"> loop to iterate over the data in steps of </span><strong class="source-inline"><span class="koboSpan" id="kobo.150.1">batch_size</span></strong><span class="koboSpan" id="kobo.151.1">. </span><span class="koboSpan" id="kobo.151.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.152.1">yield</span></strong><span class="koboSpan" id="kobo.153.1"> keyword is used to generate batches of data, each of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.154.1">batch_size</span></strong><span class="koboSpan" id="kobo.155.1"> size. </span><span class="koboSpan" id="kobo.155.2">A generator that yields batches of data </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">is returned.</span></span></p></li>				<li><span class="koboSpan" id="kobo.157.1">We create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.158.1">transform_data</span></strong><span class="koboSpan" id="kobo.159.1"> function that transforms each record in </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">the batch:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.161.1">
def transform_data(batch):
    transformed_batch = []
    for record in batch:
        transformed_record = {
            'id': record['id'],
            'value': record['value'],
            'transformed_value': record['value'] * 1.1
        }
        transformed_batch.append(transformed_record)
return transformed_batch</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.162.1">This function takes as an argument the batch, which is a list of data records to be transformed. </span><span class="koboSpan" id="kobo.162.2">The transformation logic is simple: a new </span><strong class="source-inline"><span class="koboSpan" id="kobo.163.1">transformed_value</span></strong><span class="koboSpan" id="kobo.164.1"> field is added to each record, which is the original value multiplied by 1.1. </span><span class="koboSpan" id="kobo.164.2">At the end, we have a list of transformed records. </span><span class="koboSpan" id="kobo.164.3">Let’s have a look at some of our </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">transformed records:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.166.1">{'id': 558, 'value': 12.15160339587219, 'transformed_value': 13.36676373545941}
{'id': 449, 'value': 99.79699336555473, 'transformed_value': 109.77669270211021}
{'id': 991, 'value': 79.65999078145887, 'transformed_value': 87.62598985960477}</span></pre></li>				<li><span class="koboSpan" id="kobo.167.1">Next, we </span><a id="_idIndexMarker020"/><span class="koboSpan" id="kobo.168.1">create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.169.1">load_data</span></strong><span class="koboSpan" id="kobo.170.1"> function to load the data. </span><span class="koboSpan" id="kobo.170.2">This function simulates loading each transformed record into </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">a database:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.172.1">
def load_data(batch):
    for record in batch:
        # Simulate loading data into a database
        print(f"Loading record into database: {record}")</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.173.1">This function takes the batch as a parameter, which is a list of transformed data records that is ready to be loaded. </span><span class="koboSpan" id="kobo.173.2">Each record is printed to the console to simulate loading it into </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">a database.</span></span></p></li>				<li><span class="koboSpan" id="kobo.175.1">Finally, we create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.176.1">main</span></strong><span class="koboSpan" id="kobo.177.1"> function. </span><span class="koboSpan" id="kobo.177.2">This function calls all the </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">aforementioned functions:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.179.1">
def main():
    # Parameters
    num_records = 100 # Total number of records to generate
    batch_size = 10 # Number of records per batch
    # Generate data
    data = generate_mock_data(num_records)
    # Process and load data in batches
    for batch in process_in_batches(data, batch_size):
        transformed_batch = transform_data(batch)
        print("Batch before loading:")
        for record in transformed_batch:
            print(record)
        load_data(transformed_batch)
        time.sleep(1) # Simulate time delay between batches</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.180.1">This</span><a id="_idIndexMarker021"/><span class="koboSpan" id="kobo.181.1"> function calls </span><strong class="source-inline"><span class="koboSpan" id="kobo.182.1">generate_mock_data</span></strong><span class="koboSpan" id="kobo.183.1"> to create the mock data and uses </span><strong class="source-inline"><span class="koboSpan" id="kobo.184.1">process_in_batches</span></strong><span class="koboSpan" id="kobo.185.1"> to divide the data into batches. </span><span class="koboSpan" id="kobo.185.2">For each batch, the function does </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">the following:</span></span></p><ul><li><span class="koboSpan" id="kobo.187.1">Transforms the batch </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.189.1">transform_data</span></strong></span></li><li><span class="koboSpan" id="kobo.190.1">Prints the batch to show its contents </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">before loading</span></span></li><li><span class="koboSpan" id="kobo.192.1">Simulates loading the batch </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.194.1">load_data</span></strong></span></li></ul></li>			</ol>
			<p><span class="koboSpan" id="kobo.195.1">Now, let’s transition from batch processing to a streaming paradigm. </span><span class="koboSpan" id="kobo.195.2">In streaming, data is processed as it arrives, rather than in </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">predefined batches.</span></span></p>
			<h1 id="_idParaDest-23"><a id="_idTextAnchor022"/><span class="koboSpan" id="kobo.197.1">Ingesting data in streaming mode</span></h1>
			<p><strong class="bold"><span class="koboSpan" id="kobo.198.1">Streaming ingestion</span></strong><span class="koboSpan" id="kobo.199.1"> is a</span><a id="_idIndexMarker022"/><span class="koboSpan" id="kobo.200.1"> data processing technique whereby data is collected, processed, and loaded into a system in real-time, as it is generated. </span><span class="koboSpan" id="kobo.200.2">Unlike batch ingestion, which accumulates data for processing at scheduled </span><a id="_idIndexMarker023"/><span class="koboSpan" id="kobo.201.1">intervals, streaming ingestion handles data continuously, allowing organizations to analyze and act on information immediately. </span><span class="koboSpan" id="kobo.201.2">For instance, a company might process customer transaction data the moment it occurs, enabling real-time insights and decision-making. </span><span class="koboSpan" id="kobo.201.3">This method is particularly useful for organizations that require up-to-the-minute data analysis, such as in financial trading, fraud detection, or sensor </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">data monitoring.</span></span></p>
			<p><span class="koboSpan" id="kobo.203.1">Streaming ingestion is advantageous because it enables immediate processing of data, reducing latency and allowing organizations to react quickly to changing conditions. </span><span class="koboSpan" id="kobo.203.2">This is particularly beneficial in scenarios where timely responses are critical, such as detecting anomalies, personalizing user experiences, or responding to real-time events. </span><span class="koboSpan" id="kobo.203.3">Additionally, streaming can lead to more efficient resource utilization by distributing the processing load evenly over time, rather than concentrating it into specific batch windows. </span><span class="koboSpan" id="kobo.203.4">In cloud-based environments, this can also translate into cost savings, as resources can be scaled dynamically to match the real-time data flow. </span><span class="koboSpan" id="kobo.203.5">For organizations with irregular or unpredictable data flows, streaming ingestion offers a flexible, responsive, and scalable approach to data processing and analytics. </span><span class="koboSpan" id="kobo.203.6">Let’s look at some of its advantages </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">and disadvantages.</span></span></p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/><span class="koboSpan" id="kobo.205.1">Advantages and disadvantages</span></h2>
			<p><span class="koboSpan" id="kobo.206.1">Streaming</span><a id="_idIndexMarker024"/><span class="koboSpan" id="kobo.207.1"> ingestion offers several distinct advantages, making it an essential choice for specific data </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">processing needs:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.209.1">One of the primary benefits is the ability to obtain real-time insights from data. </span><span class="koboSpan" id="kobo.209.2">This immediacy is crucial for applications such as fraud detection, real-time analytics, and dynamic pricing, where timely data </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">is vital.</span></span></li>
				<li><span class="koboSpan" id="kobo.211.1">Streaming ingestion supports continuous data processing, allowing systems to handle data as it arrives, thereby reducing latency and </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">improving responsiveness.</span></span></li>
				<li><span class="koboSpan" id="kobo.213.1">This method is highly scalable, as well as capable of managing high-velocity data streams from multiple sources without </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">significant delays.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.215.1">However, streaming</span><a id="_idIndexMarker025"/><span class="koboSpan" id="kobo.216.1"> ingestion also presents </span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">some challenges:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.218.1">Implementing a streaming ingestion system can be </span><em class="italic"><span class="koboSpan" id="kobo.219.1">complex</span></em><span class="koboSpan" id="kobo.220.1">, requiring sophisticated infrastructure and specialized tools to manage data </span><span class="No-Break"><span class="koboSpan" id="kobo.221.1">streams effectively.</span></span></li>
				<li><span class="koboSpan" id="kobo.222.1">Continuous processing demands constant computational resources, which can be costly </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">and resource-intensive.</span></span></li>
				<li><span class="koboSpan" id="kobo.224.1">Ensuring data consistency and accuracy in a streaming environment can be difficult due to the constant influx of data and the potential for out-of-order or </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">duplicate records</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.226.1">Let’s look at common use cases for ingesting data in </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">batch mode.</span></span></p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/><span class="koboSpan" id="kobo.228.1">Common use cases for streaming ingestion</span></h2>
			<p><span class="koboSpan" id="kobo.229.1">While</span><a id="_idIndexMarker026"/><span class="koboSpan" id="kobo.230.1"> batch processing is well-suited for periodic, large-scale data updates and transformations, streaming data ingestion is crucial for real-time data analytics and applications that require immediate insights. </span><span class="koboSpan" id="kobo.230.2">Here are some common use cases for streaming </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">data ingestion.</span></span></p>
			<h3><span class="koboSpan" id="kobo.232.1">Real-time fraud detection and security monitoring</span></h3>
			<p><span class="koboSpan" id="kobo.233.1">Financial institutions</span><a id="_idIndexMarker027"/><span class="koboSpan" id="kobo.234.1"> use streaming data to detect fraudulent activities by analyzing transaction data in real-time. </span><span class="koboSpan" id="kobo.234.2">Immediate anomaly detection helps prevent fraud before it can cause significant damage. </span><span class="koboSpan" id="kobo.234.3">Streaming data is used in cybersecurity to detect and respond to threats immediately. </span><span class="koboSpan" id="kobo.234.4">Continuous monitoring of network traffic, user behavior, and system logs helps identify and mitigate security breaches as </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">they occur.</span></span></p>
			<h3><span class="koboSpan" id="kobo.236.1">IoT and sensor data</span></h3>
			<p><span class="koboSpan" id="kobo.237.1">In manufacturing, streaming data </span><a id="_idIndexMarker028"/><span class="koboSpan" id="kobo.238.1">from sensors on machinery allows for predictive maintenance. </span><span class="koboSpan" id="kobo.238.2">By continuously monitoring equipment health, companies can prevent breakdowns and optimize </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">maintenance schedules.</span></span></p>
			<p><span class="koboSpan" id="kobo.240.1">Another interesting</span><a id="_idIndexMarker029"/><span class="koboSpan" id="kobo.241.1"> application in the IoT and sensors space is </span><strong class="bold"><span class="koboSpan" id="kobo.242.1">smart cities</span></strong><span class="koboSpan" id="kobo.243.1">. </span><span class="koboSpan" id="kobo.243.2">Streaming</span><a id="_idIndexMarker030"/><span class="koboSpan" id="kobo.244.1"> data from various sensors across a city (traffic, weather, pollution, etc.) helps in managing city operations in real-time, improving services such as traffic management and </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">emergency response.</span></span></p>
			<h3><span class="koboSpan" id="kobo.246.1">Online recommendations and personalization</span></h3>
			<p><span class="koboSpan" id="kobo.247.1">Streaming data </span><a id="_idIndexMarker031"/><span class="koboSpan" id="kobo.248.1">enables e-commerce platforms to provide real-time recommendations to users based on their current browsing and purchasing behavior. </span><span class="koboSpan" id="kobo.248.2">This enhances user experience and increases sales. </span><span class="koboSpan" id="kobo.248.3">Platforms such as Netflix and Spotify use streaming data to update recommendations as users interact with the service, providing personalized content suggestions </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">in real-time.</span></span></p>
			<h3><span class="koboSpan" id="kobo.250.1">Financial market data</span></h3>
			<p><span class="koboSpan" id="kobo.251.1">Stock traders </span><a id="_idIndexMarker032"/><span class="koboSpan" id="kobo.252.1">rely on streaming data for up-to-the-second information on stock prices and market conditions to make informed trading decisions. </span><span class="koboSpan" id="kobo.252.2">Automated trading systems use streaming data to execute trades based on predefined criteria, requiring real-time data processing for </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">optimal performance.</span></span></p>
			<h3><span class="koboSpan" id="kobo.254.1">Telecommunications</span></h3>
			<p><span class="koboSpan" id="kobo.255.1">Telecommunication </span><a id="_idIndexMarker033"/><span class="koboSpan" id="kobo.256.1">companies use streaming data to monitor network performance and usage in real-time, ensuring optimal service quality and quick resolution of issues. </span><span class="koboSpan" id="kobo.256.2">Streaming data also helps in tracking customer interactions and service usage in real-time, enabling personalized customer support and improving the </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">overall experience.</span></span></p>
			<h3><span class="koboSpan" id="kobo.258.1">Real-time logistics and supply chain management</span></h3>
			<p><span class="koboSpan" id="kobo.259.1">Streaming </span><a id="_idIndexMarker034"/><span class="koboSpan" id="kobo.260.1">data from GPS devices allows logistics companies to track vehicle locations and optimize routes in real-time, improving delivery efficiency. </span><span class="koboSpan" id="kobo.260.2">Real-time inventory tracking helps businesses maintain optimal stock levels, reducing overstock and stockouts while ensuring </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">timely replenishment.</span></span></p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/><span class="koboSpan" id="kobo.262.1">Streaming ingestion in an e-commerce platform</span></h2>
			<p><span class="koboSpan" id="kobo.263.1">Streaming ingestion</span><a id="_idIndexMarker035"/><span class="koboSpan" id="kobo.264.1"> is a methodical process involving several key steps: data extraction, data transformation, data loading, and monitoring and alerting. </span><span class="koboSpan" id="kobo.264.2">To illustrate these steps, let’s explore a use case involving an e-commerce platform that needs to process and analyze user activity data in real-time for personalized recommendations and dynamic </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">inventory management.</span></span></p>
			<p><span class="koboSpan" id="kobo.266.1">An e-commerce platform needs to collect, transform, and load user activity data from various sources such as website clicks, search queries, and purchase transactions into a central system. </span><span class="koboSpan" id="kobo.266.2">This data will be used for generating real-time personalized recommendations, monitoring user behavior, and managing </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">inventory dynamically.</span></span></p>
			<h3><span class="koboSpan" id="kobo.268.1">Data extraction</span></h3>
			<p><span class="koboSpan" id="kobo.269.1">This </span><a id="_idIndexMarker036"/><span class="koboSpan" id="kobo.270.1">is the first step is identifying the sources from which data will be extracted. </span><span class="koboSpan" id="kobo.270.2">For the e-commerce platform, this includes web servers, mobile apps, and third-party analytics services. </span><span class="koboSpan" id="kobo.270.3">These sources contain critical data such as user clicks, search queries, and transaction details. </span><span class="koboSpan" id="kobo.270.4">Once the sources are identified, data is collected using streaming connectors or APIs. </span><span class="koboSpan" id="kobo.270.5">This involves setting up data pipelines that extract data from web servers, mobile apps, and analytics services in real-time. </span><span class="koboSpan" id="kobo.270.6">The extracted data is then streamed to processing systems such as Apache Kafka or </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">AWS Kinesis.</span></span></p>
			<h3><span class="koboSpan" id="kobo.272.1">Data transformation</span></h3>
			<p><span class="koboSpan" id="kobo.273.1">The </span><a id="_idIndexMarker037"/><span class="koboSpan" id="kobo.274.1">extracted data often contains inconsistencies and noise. </span><span class="koboSpan" id="kobo.274.2">Real-time data cleaning is performed to filter out irrelevant information, handle missing values, and correct errors. </span><span class="koboSpan" id="kobo.274.3">For the e-commerce platform, this ensures that user activity records are accurate and relevant for analysis. </span><span class="koboSpan" id="kobo.274.4">After cleaning, the data undergoes transformations such as parsing, enrichment, and aggregation. </span><span class="koboSpan" id="kobo.274.5">For example, the e-commerce platform might parse user clickstream data to identify browsing patterns, enrich transaction data with product details, and aggregate search queries to identify trending products. </span><span class="koboSpan" id="kobo.274.6">The transformed data must be mapped to the schema of the target system. </span><span class="koboSpan" id="kobo.274.7">This involves aligning the data fields with the structure of the real-time analytics system. </span><span class="koboSpan" id="kobo.274.8">For instance, user activity data might be mapped to tables representing sessions, products, and user profiles, ensuring seamless integration with the existing </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">data model.</span></span></p>
			<h3><span class="koboSpan" id="kobo.276.1">Data loading</span></h3>
			<p><span class="koboSpan" id="kobo.277.1">The </span><a id="_idIndexMarker038"/><span class="koboSpan" id="kobo.278.1">transformed data is processed continuously using tools such as Apache Flink or Apache Spark Streaming. </span><span class="koboSpan" id="kobo.278.2">Continuous processing allows the e-commerce platform to handle high-velocity data streams efficiently, performing transformations and aggregations in real-time. </span><span class="koboSpan" id="kobo.278.3">Once processed, the data is loaded into the target storage system, such as a real-time database or analytics engine, where it can be accessed for personalized recommendations and dynamic </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">inventory management.</span></span></p>
			<h3><span class="koboSpan" id="kobo.280.1">Monitoring and alerting</span></h3>
			<p><span class="koboSpan" id="kobo.281.1">To ensure </span><a id="_idIndexMarker039"/><span class="koboSpan" id="kobo.282.1">that the streaming ingestion process runs smoothly and consistently, monitoring tools such as Prometheus or Grafana are used. </span><span class="koboSpan" id="kobo.282.2">These tools provide real-time insights into the performance and health of the data ingestion pipelines, identifying any failures or performance bottlenecks. </span><span class="koboSpan" id="kobo.282.3">Implementing alerting mechanisms is crucial to promptly detect and resolve any issues in the streaming ingestion process. </span><span class="koboSpan" id="kobo.282.4">For the e-commerce platform, this ensures that any disruptions in data flow are quickly addressed, maintaining the integrity and reliability of the </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">data pipeline.</span></span></p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/><span class="koboSpan" id="kobo.284.1">Streaming ingestion with an example</span></h2>
			<p><span class="koboSpan" id="kobo.285.1">As we said, in streaming, data</span><a id="_idIndexMarker040"/><span class="koboSpan" id="kobo.286.1"> is processed as it arrives rather than in predefined batches. </span><span class="koboSpan" id="kobo.286.2">Let’s modify the batch example to transition to a streaming paradigm. </span><span class="koboSpan" id="kobo.286.3">For simplicity, we will generate data continuously, process it immediately upon arrival, transform it, and then </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">load it:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.288.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.289.1">generate_mock_data</span></strong><span class="koboSpan" id="kobo.290.1"> function generates records </span><em class="italic"><span class="koboSpan" id="kobo.291.1">continuously</span></em><span class="koboSpan" id="kobo.292.1"> using a generator and simulates a delay between </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">each record:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.294.1">
def generate_mock_data():
    while True:
        record = {
            'id': random.randint(1, 1000),
            'value': random.random() * 100
        }
        yield record
        time.sleep(0.5)  # Simulate data arriving every 0.5 seconds</span></pre></li>				<li><span class="koboSpan" id="kobo.295.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.296.1">process_stream</span></strong><span class="koboSpan" id="kobo.297.1"> function processes each record as it arrives from the data </span><a id="_idIndexMarker041"/><span class="koboSpan" id="kobo.298.1">generator, without waiting for a batch to </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">be filled:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.300.1">
def process_stream(run_time_seconds=10):
    start_time = time.time()
    for record in generate_mock_data():
        transformed_record = transform_data(record)
        load_data(transformed_record)
        # Check if the run time has exceeded the limit
        if time.time() – start_time &gt; run_time_seconds:
            print("Time limit reached. </span><span class="koboSpan" id="kobo.300.2">Terminating the stream processing.")
            break</span></pre></li>				<li><span class="koboSpan" id="kobo.301.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.302.1">transform_data</span></strong><span class="koboSpan" id="kobo.303.1"> function transforms each record individually as </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">it arrives:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.305.1">
def transform_data(record):
    transformed_record = {
        'id': record['id'],
        'value': record['value'],
        'transformed_value': record['value'] * 1.1  # Example transformation
    }
    return transformed_record</span></pre></li>				<li><span class="koboSpan" id="kobo.306.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.307.1">load_data</span></strong><span class="koboSpan" id="kobo.308.1"> function</span><a id="_idIndexMarker042"/><span class="koboSpan" id="kobo.309.1"> simulates loading data by processing each record as it arrives, instead of processing each record within a batch </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">as before:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.311.1">
def load_data(record):
    print(f"Loading record into database: {record}")</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.312.1">Let’s move from real-time to semi-real-time processing, which you can think it as batch processing over short intervals. </span><span class="koboSpan" id="kobo.312.2">It is usually called </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">micro-batch processing.</span></span></p>
			<h1 id="_idParaDest-28"><a id="_idTextAnchor027"/><span class="koboSpan" id="kobo.314.1">Real-time versus semi-real-time ingestion</span></h1>
			<p><span class="koboSpan" id="kobo.315.1">Real-time ingestion </span><a id="_idIndexMarker043"/><span class="koboSpan" id="kobo.316.1">refers to the process of collecting, processing, and loading data almost instantaneously as it is generated, as we have discussed. </span><span class="koboSpan" id="kobo.316.2">This </span><a id="_idIndexMarker044"/><span class="koboSpan" id="kobo.317.1">approach is critical for applications that require immediate insights and actions, such as fraud detection, stock trading, and live monitoring systems. </span><span class="koboSpan" id="kobo.317.2">Real-time ingestion provides </span><em class="italic"><span class="koboSpan" id="kobo.318.1">the lowest latency</span></em><span class="koboSpan" id="kobo.319.1">, enabling businesses to react to events as they occur. </span><em class="italic"><span class="koboSpan" id="kobo.320.1">However, it demands robust infrastructure and continuous resource allocation, making it complex and potentially expensive </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.321.1">to maintain</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">.</span></span></p>
			<p><span class="koboSpan" id="kobo.323.1">Semi-real-time ingestion, on</span><a id="_idIndexMarker045"/><span class="koboSpan" id="kobo.324.1"> the other hand, also </span><a id="_idIndexMarker046"/><span class="koboSpan" id="kobo.325.1">known as near real-time ingestion, involves processing</span><a id="_idIndexMarker047"/><span class="koboSpan" id="kobo.326.1"> data with </span><em class="italic"><span class="koboSpan" id="kobo.327.1">minimal delay</span></em><span class="koboSpan" id="kobo.328.1">, typically in seconds or minutes, rather than instantly. </span><span class="koboSpan" id="kobo.328.2">This approach </span><em class="italic"><span class="koboSpan" id="kobo.329.1">strikes a balance between real-time and batch processing</span></em><span class="koboSpan" id="kobo.330.1">, providing timely insights while reducing the resource intensity and complexity associated with true real-time systems. </span><span class="koboSpan" id="kobo.330.2">Semi-real-time ingestion is suitable for applications such as social media monitoring, customer feedback analysis, and operational dashboards, where near-immediate data processing is beneficial but not </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">critically time-sensitive.</span></span></p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/><span class="koboSpan" id="kobo.332.1">Common use cases for near-real-time ingestion</span></h2>
			<p><span class="koboSpan" id="kobo.333.1">Let’s look at </span><a id="_idIndexMarker048"/><span class="koboSpan" id="kobo.334.1">some of the common use cases wherein we can use </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">near-real-time ingestion.</span></span></p>
			<h3><span class="koboSpan" id="kobo.336.1">Real-time analytics</span></h3>
			<p><span class="koboSpan" id="kobo.337.1">Streaming</span><a id="_idIndexMarker049"/><span class="koboSpan" id="kobo.338.1"> enables organizations to continuously monitor data as it flows in, allowing for real-time dashboards and visualizations. </span><span class="koboSpan" id="kobo.338.2">This is critical in industries such as finance, where stock prices, market trends, and trading activities need to be tracked live. </span><span class="koboSpan" id="kobo.338.3">It also allows for instant report generation, facilitating timely decision-making and reducing the latency between data generation </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">and analysis.</span></span></p>
			<h3><span class="koboSpan" id="kobo.340.1">Social media and sentiment analysis</span></h3>
			<p><span class="koboSpan" id="kobo.341.1">Companies track </span><a id="_idIndexMarker050"/><span class="koboSpan" id="kobo.342.1">mentions and sentiments on social media in real-time to manage brand reputation and respond to customer feedback promptly. </span><span class="koboSpan" id="kobo.342.2">Streaming data allows for the continuous analysis of public sentiment towards brands, products, or events, providing immediate insights that can influence marketing and </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">PR strategies.</span></span></p>
			<h3><span class="koboSpan" id="kobo.344.1">Customer experience enhancement</span></h3>
			<p><span class="koboSpan" id="kobo.345.1">Near-real-time processing</span><a id="_idIndexMarker051"/><span class="koboSpan" id="kobo.346.1"> allows support teams to access up-to-date information on customer issues and behavior, enabling quicker and more accurate responses to customer inquiries. </span><span class="koboSpan" id="kobo.346.2">Businesses can also use near-real-time data to update customer profiles and trigger personalized marketing messages, such as emails or notifications, shortly after a customer interacts with their website </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">or app.</span></span></p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/><span class="koboSpan" id="kobo.348.1">Semi-real-time mode with an example</span></h2>
			<p><span class="koboSpan" id="kobo.349.1">Transitioning</span><a id="_idIndexMarker052"/><span class="koboSpan" id="kobo.350.1"> from real-time to semi-real-time data processing involves adjusting the example to introduce a more structured approach to handling data updates, rather than processing each record immediately upon arrival. </span><span class="koboSpan" id="kobo.350.2">This can be achieved by batching data updates over short intervals, which allows for more efficient processing while still maintaining a responsive data processing pipeline. </span><span class="koboSpan" id="kobo.350.3">Let’s have a look at the example and as always, you can find the code in the GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">repository </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/3.semi_real_time.py"><span class="No-Break"><span class="koboSpan" id="kobo.352.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/3.semi_real_time.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.353.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.354.1">For generating </span><a id="_idIndexMarker053"/><span class="koboSpan" id="kobo.355.1">mock data continuously, there are no changes from the previous example. </span><span class="koboSpan" id="kobo.355.2">This continuously generates mock data records with a slight </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">delay (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.357.1">time.sleep(0.1)</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">).</span></span></li>
				<li><span class="koboSpan" id="kobo.359.1">For processing in semi-real-time, we can use a deque to buffer incoming records. </span><span class="koboSpan" id="kobo.359.2">This function processes records when either the specified time interval has elapsed, or the buffer reaches a specified size (</span><strong class="source-inline"><span class="koboSpan" id="kobo.360.1">batch_size</span></strong><span class="koboSpan" id="kobo.361.1">). </span><span class="koboSpan" id="kobo.361.2">Then, it converts the deque to a list (</span><strong class="source-inline"><span class="koboSpan" id="kobo.362.1">list(buffer)</span></strong><span class="koboSpan" id="kobo.363.1">) before passing it to </span><strong class="source-inline"><span class="koboSpan" id="kobo.364.1">transform_data</span></strong><span class="koboSpan" id="kobo.365.1">, ensuring the data is processed in </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">a batch:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.367.1">
def process_semi_real_time(batch_size, interval):
    buffer = deque()
    start_time = time.time()
    for record in generate_mock_data():
        buffer.append(record)</span></pre></li>				<li><span class="koboSpan" id="kobo.368.1">Check whether the interval has elapsed, or the buffer size has </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">been reached:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.370.1">
        if (time.time() - start_time) &gt;= interval or len(buffer) &gt;= batch_size:</span></pre></li>				<li><span class="koboSpan" id="kobo.371.1">Process and clear </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">the buffer:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.373.1">
            transformed_batch = transform_data(list(buffer))  # Convert deque to list
            print(f"Batch of {len(transformed_batch)} records before loading:")
            for rec in transformed_batch:
                print(rec)
            load_data(transformed_batch)
            buffer.clear()
            start_time = time.time()  # Reset start time</span></pre></li>				<li><span class="koboSpan" id="kobo.374.1">Then, we transform each record in the batch. </span><span class="koboSpan" id="kobo.374.2">There are no changes from the previous example and we load </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">the data.</span></span></li>
			</ol>
			<p><span class="koboSpan" id="kobo.376.1">When you</span><a id="_idIndexMarker054"/><span class="koboSpan" id="kobo.377.1"> run this code, it continuously generates mock data records. </span><span class="koboSpan" id="kobo.377.2">Records are buffered until either the specified time interval (</span><strong class="source-inline"><span class="koboSpan" id="kobo.378.1">interval</span></strong><span class="koboSpan" id="kobo.379.1">) has elapsed, or the buffer reaches the specified size (</span><strong class="source-inline"><span class="koboSpan" id="kobo.380.1">batch_size</span></strong><span class="koboSpan" id="kobo.381.1">). </span><span class="koboSpan" id="kobo.381.2">Once the conditions are met, the buffered records are processed as a batch, transformed, and then “loaded” (printed) into the </span><span class="No-Break"><span class="koboSpan" id="kobo.382.1">simulated database.</span></span></p>
			<p><span class="koboSpan" id="kobo.383.1">When discussing the different types of data sources that are suitable for batch, streaming, or semi-real-time streaming processing, it’s essential to consider the diversity and characteristics of these sources. </span><span class="koboSpan" id="kobo.383.2">Data can originate from various sources, such as databases, logs, IoT devices, social media, or sensors, as we will see in the </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">next section.</span></span></p>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/><span class="koboSpan" id="kobo.385.1">Data source solutions</span></h1>
			<p><span class="koboSpan" id="kobo.386.1">In the world </span><a id="_idIndexMarker055"/><span class="koboSpan" id="kobo.387.1">of modern data analytics and processing, the diversity of data sources available for ingestion spans a wide spectrum. </span><span class="koboSpan" id="kobo.387.2">From traditional file formats such as CSV, JSON, and XML to robust database systems encompassing both SQL and NoSQL variants, the landscape expands further to include dynamic APIs such as REST, facilitating real-time data retrieval. </span><span class="koboSpan" id="kobo.387.3">Message queues such as Kafka offer scalable solutions for handling event-driven data while streaming services such as Kinesis and pub/sub enable continuous data flows crucial for applications demanding immediate insights. </span><span class="koboSpan" id="kobo.387.4">Understanding and effectively harnessing these diverse data ingestion sources is fundamental to building robust data pipelines that support a broad array of analytical and </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">operational needs.</span></span></p>
			<p><span class="koboSpan" id="kobo.389.1">Let’s start with </span><span class="No-Break"><span class="koboSpan" id="kobo.390.1">event processing.</span></span></p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/><span class="koboSpan" id="kobo.391.1">Event data processing solution</span></h2>
			<p><span class="koboSpan" id="kobo.392.1">In a </span><a id="_idIndexMarker056"/><span class="koboSpan" id="kobo.393.1">real-time processing system, data is ingested, processed, and responded to almost instantaneously, as we’ve discussed. </span><span class="koboSpan" id="kobo.393.2">Real-time processing systems often use message queues to handle incoming data streams and ensure that data is processed in the order it is received, </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">without delays.</span></span></p>
			<p><span class="koboSpan" id="kobo.395.1">The following</span><a id="_idIndexMarker057"/><span class="koboSpan" id="kobo.396.1"> Python code demonstrates a basic example of using a message queue for processing messages, which is a foundational concept in both real-time and semi-real-time data processing systems. </span><span class="koboSpan" id="kobo.396.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.397.1">Queue</span></strong><span class="koboSpan" id="kobo.398.1"> class from Python’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.399.1">queue</span></strong><span class="koboSpan" id="kobo.400.1"> module is used to create a queue—a data structure that follows the </span><strong class="bold"><span class="koboSpan" id="kobo.401.1">First-in-First-out</span></strong><span class="koboSpan" id="kobo.402.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.403.1">FIFO</span></strong><span class="koboSpan" id="kobo.404.1">) principle. </span><span class="koboSpan" id="kobo.404.2">In this context, a queue is</span><a id="_idIndexMarker058"/><span class="koboSpan" id="kobo.405.1"> used to manage messages or tasks that need to be processed. </span><span class="koboSpan" id="kobo.405.2">The code simulates an event-based system where messages (in this case, strings such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.406.1">message 0</span></strong><span class="koboSpan" id="kobo.407.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.408.1">message 1</span></strong><span class="koboSpan" id="kobo.409.1">, etc.) are added to a queue. </span><span class="koboSpan" id="kobo.409.2">This mimics a scenario wherein events or tasks are generated and need to be processed in the order they arrive. </span><span class="koboSpan" id="kobo.409.3">Let’s have a look at each part of the code. </span><span class="koboSpan" id="kobo.409.4">You can find the code file </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/4.work_with_queue.py"><span class="No-Break"><span class="koboSpan" id="kobo.411.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/4.work_with_queue.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.412.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.413.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.414.1">read_message_queue()</span></strong><span class="koboSpan" id="kobo.415.1"> function initializes a queue object </span><strong class="source-inline"><span class="koboSpan" id="kobo.416.1">q</span></strong><span class="koboSpan" id="kobo.417.1"> using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.418.1">Queue</span></strong><span class="koboSpan" id="kobo.419.1"> class from the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.420.1">queue</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.421.1"> module:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.422.1">
def read_message_queue():
    q = Queue()</span></pre></li>				<li><span class="koboSpan" id="kobo.423.1">This loop adds 10 messages to the queue. </span><span class="koboSpan" id="kobo.423.2">Each message is a string in the format message </span><strong class="source-inline"><span class="koboSpan" id="kobo.424.1">i</span></strong><span class="koboSpan" id="kobo.425.1">, where </span><strong class="source-inline"><span class="koboSpan" id="kobo.426.1">i</span></strong><span class="koboSpan" id="kobo.427.1"> ranges from 0 </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">to 9:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.429.1">
for i in range(10): # Mocking messages
    q.put(f"message {i}")</span></pre></li>				<li><span class="koboSpan" id="kobo.430.1">This loop continuously retrieves and processes messages from the queue until it is empty. </span><strong class="source-inline"><span class="koboSpan" id="kobo.431.1">q.get()</span></strong><span class="koboSpan" id="kobo.432.1"> retrieves a message from the queue, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.433.1">q.task_done()</span></strong><span class="koboSpan" id="kobo.434.1"> signals that the retrieved message has </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">been processed:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.436.1">
while not q.empty():
    message = q.get()
    process_message(message)
    q.task_done() # Signal that the task is done</span></pre></li>				<li><span class="koboSpan" id="kobo.437.1">The </span><a id="_idIndexMarker059"/><span class="koboSpan" id="kobo.438.1">following function takes a message as input and prints it to the console, simulating the processing of </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">the message:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.440.1">
def process_message(message):
    print(f"Processing message: {message}")</span></pre></li>				<li><span class="koboSpan" id="kobo.441.1">Call the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.442.1">read_message_queue</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.443.1"> function:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.444.1">
read_message_queue()</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.445.1">Here, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.446.1">read_message_queue</span></strong><span class="koboSpan" id="kobo.447.1"> function reads messages from the queue and processes</span><a id="_idIndexMarker060"/><span class="koboSpan" id="kobo.448.1"> them one by one using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.449.1">process_message</span></strong><span class="koboSpan" id="kobo.450.1"> function. </span><span class="koboSpan" id="kobo.450.2">This demonstrates how event-based systems handle tasks—by placing them in a queue and processing each task as it </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">becomes available.</span></span></p>
			<p><span class="koboSpan" id="kobo.452.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.453.1">while not q.empty()</span></strong><span class="koboSpan" id="kobo.454.1"> loop ensures that each message is processed in the exact order it was added to the queue. </span><span class="koboSpan" id="kobo.454.2">This is crucial in many real-world applications where the order of processing matters, such as in handling user requests or </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">processing logs.</span></span></p>
			<p><span class="koboSpan" id="kobo.456.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.457.1">q.task_done()</span></strong><span class="koboSpan" id="kobo.458.1"> method signals that a message has been processed. </span><span class="koboSpan" id="kobo.458.2">This is important in real-world systems where tracking the completion of tasks is necessary for ensuring reliability and correctness, especially in systems with multiple workers </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">or threads.</span></span></p>
			<p><span class="koboSpan" id="kobo.460.1">In real-world applications, message queues are often integrated into more sophisticated data streaming platforms to ensure scalability, fault tolerance, and high availability. </span><span class="koboSpan" id="kobo.460.2">For instance, in real-time data processing, platforms such as Kafka and AWS Kinesis come </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">into play.</span></span></p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/><span class="koboSpan" id="kobo.462.1">Ingesting event data with Apache Kafka</span></h2>
			<p><span class="koboSpan" id="kobo.463.1">There are different </span><a id="_idIndexMarker061"/><span class="koboSpan" id="kobo.464.1">technologies </span><a id="_idIndexMarker062"/><span class="koboSpan" id="kobo.465.1">to ingest and handle event data. </span><span class="koboSpan" id="kobo.465.2">One of the technologies we will discuss is Apache Kafka. </span><span class="koboSpan" id="kobo.465.3">Kafka is an open source distributed </span><a id="_idIndexMarker063"/><span class="koboSpan" id="kobo.466.1">event streaming platform first developed by LinkedIn and later donated to the Apache Software Foundation. </span><span class="koboSpan" id="kobo.466.2">It is designed to handle large amounts of data in real-time and provides </span><a id="_idIndexMarker064"/><span class="koboSpan" id="kobo.467.1">a scalable and fault-tolerant system for processing and </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">storing streams.</span></span></p>
			<div>
				<div id="_idContainer007" class="IMG---Figure">
					<span class="koboSpan" id="kobo.469.1"><img src="image/B19801_01_1.jpg" alt="Figure 1.1 – Components of Apache Kafka"/></span>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.470.1">Figure 1.1 – Components of Apache Kafka</span></p>
			<p><span class="koboSpan" id="kobo.471.1">Let’s see the </span><a id="_idIndexMarker065"/><span class="koboSpan" id="kobo.472.1">different components</span><a id="_idIndexMarker066"/><span class="koboSpan" id="kobo.473.1"> of </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">Apache Kafka:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.475.1">Ingestion</span></strong><em class="italic"><span class="koboSpan" id="kobo.476.1">:</span></em><span class="koboSpan" id="kobo.477.1"> Data streams can be ingested into Kafka using Kafka producers. </span><span class="koboSpan" id="kobo.477.2">Producers are applications that write data to Kafka topics, which are logical channels that can hold and organize </span><span class="No-Break"><span class="koboSpan" id="kobo.478.1">data streams.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.479.1">Processing</span></strong><span class="koboSpan" id="kobo.480.1">: Kafka can process streams of data using Kafka Streams, a client library for building real-time stream processing applications. </span><span class="koboSpan" id="kobo.480.2">Kafka Streams allows developers to build custom stream-processing applications that can perform transformations, aggregations, and other operations on </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">data streams.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.482.1">Storage</span></strong><em class="italic"><span class="koboSpan" id="kobo.483.1">:</span></em><span class="koboSpan" id="kobo.484.1"> Kafka </span><a id="_idIndexMarker067"/><span class="koboSpan" id="kobo.485.1">stores data streams in distributed, fault-tolerant clusters called Kafka brokers. </span><span class="koboSpan" id="kobo.485.2">Brokers store the data streams in partitions, which are replicated across numerous brokers for </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">fault tolerance.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.487.1">Consumption</span></strong><em class="italic"><span class="koboSpan" id="kobo.488.1">:</span></em><span class="koboSpan" id="kobo.489.1"> Data streams can be consumed from Kafka using Kafka consumers. </span><span class="koboSpan" id="kobo.489.2">Consumers are applications that read data from Kafka topics and process it </span><span class="No-Break"><span class="koboSpan" id="kobo.490.1">as needed.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.491.1">Several libraries can </span><a id="_idIndexMarker068"/><span class="koboSpan" id="kobo.492.1">be used to</span><a id="_idIndexMarker069"/><span class="koboSpan" id="kobo.493.1"> interact with Apache Kafka in Python; we will explore the most popular ones in the </span><span class="No-Break"><span class="koboSpan" id="kobo.494.1">next section.</span></span></p>
			<h3><span class="koboSpan" id="kobo.495.1">Which library should you use for your use case?</span></h3>
			<p><strong class="source-inline"><span class="koboSpan" id="kobo.496.1">Kafka-Python</span></strong><span class="koboSpan" id="kobo.497.1"> is a pure</span><a id="_idIndexMarker070"/><span class="koboSpan" id="kobo.498.1"> Python implementation of Kafka’s protocol, offering a more Pythonic interface for interacting with Kafka. </span><span class="koboSpan" id="kobo.498.2">It is designed to be simple and easy to use, making it particularly appealing for beginners. </span><span class="koboSpan" id="kobo.498.3">One of its primary advantages is its simplicity, making it easier to install and use compared to other Kafka libraries. </span><span class="koboSpan" id="kobo.498.4">Kafka-Python is flexible and well-suited for small to medium-sized applications, providing the essential features needed for basic Kafka operations without the complexity of additional dependencies. </span><span class="koboSpan" id="kobo.498.5">Its pure Python nature means that it does not rely on any external libraries beyond Python itself, streamlining the installation and </span><span class="No-Break"><span class="koboSpan" id="kobo.499.1">setup process.</span></span></p>
			<p><strong class="source-inline"><span class="koboSpan" id="kobo.500.1">Confluent-kafka-python</span></strong><span class="koboSpan" id="kobo.501.1"> is a library </span><a id="_idIndexMarker071"/><span class="koboSpan" id="kobo.502.1">developed and maintained by Confluent, the original creator of Kafka. </span><span class="koboSpan" id="kobo.502.2">It stands out for its high-performance and low-latency capabilities, leveraging the </span><strong class="source-inline"><span class="koboSpan" id="kobo.503.1">librdkafka</span></strong><span class="koboSpan" id="kobo.504.1"> C library for efficient operations. </span><span class="koboSpan" id="kobo.504.2">The library offers extensive configuration options akin to the Java Kafka client and closely aligns with Kafka’s feature set, often pioneering support for new Kafka features. </span><span class="koboSpan" id="kobo.504.3">It is particularly well-suited for production environments where both performance and stability are crucial, making it an ideal choice for handling high-throughput data streams and ensuring reliable message processing in </span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">critical applications.</span></span></p>
			<p><span class="koboSpan" id="kobo.506.1">Transitioning from event data processing to databases involves shifting focus from real-time data streams to persistent data storage and retrieval. </span><span class="koboSpan" id="kobo.506.2">While event data processing emphasizes handling continuous streams of data in near real-time for immediate insights or actions, databases are structured r</span><a id="_idTextAnchor033"/><span class="koboSpan" id="kobo.507.1">epositories designed for storing and managing data over the </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">long term.</span></span></p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor034"/><span class="koboSpan" id="kobo.509.1">Ingesting data from databases</span></h2>
			<p><span class="koboSpan" id="kobo.510.1">Databases, whether</span><a id="_idIndexMarker072"/><span class="koboSpan" id="kobo.511.1"> relational or non-relational, serve as foundational components in data management systems. </span><span class="koboSpan" id="kobo.511.2">Classic</span><a id="_idIndexMarker073"/><span class="koboSpan" id="kobo.512.1"> databases and NoSQL databases are two different types of database management systems that differ in architecture and characteristics. </span><span class="koboSpan" id="kobo.512.2">A classic database, also known as a relational database, stores data in tables with a fixed schema. </span><span class="koboSpan" id="kobo.512.3">Classic databases are ideal for applications that require complex querying and transactional consistency, such as financial systems or </span><span class="No-Break"><span class="koboSpan" id="kobo.513.1">enterprise applications.</span></span></p>
			<p><span class="koboSpan" id="kobo.514.1">On the other hand, NoSQL databases do not store data in tables with a fixed schema. </span><span class="koboSpan" id="kobo.514.2">They use a document-based approach to store data in a flexible schema format. </span><span class="koboSpan" id="kobo.514.3">They are designed to be scalable and handle large amounts of data, with a focus on high-performance data retrieval. </span><span class="koboSpan" id="kobo.514.4">NoSQL databases are well-suited for applications that require high performance and scalability, such as real-time analytics, content management s</span><a id="_idTextAnchor035"/><span class="koboSpan" id="kobo.515.1">ystems, and </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">e-commerce platforms.</span></span></p>
			<p><span class="koboSpan" id="kobo.517.1">Let’s start with </span><span class="No-Break"><span class="koboSpan" id="kobo.518.1">relational databases.</span></span></p>
			<h3><span class="koboSpan" id="kobo.519.1">Performing data ingestion from a relational database</span></h3>
			<p><span class="koboSpan" id="kobo.520.1">Relational databases </span><a id="_idIndexMarker074"/><span class="koboSpan" id="kobo.521.1">are useful for batch ETL processes where structured data from various sources needs consolidation, transformation, and loading into a data warehouse or analytical system. </span><span class="koboSpan" id="kobo.521.2">SQL-based operations are efficient for joining and aggregating data before processing. </span><span class="koboSpan" id="kobo.521.3">Let’s try to understand how SQL databases represent data in tables with rows and columns using a code example. </span><span class="koboSpan" id="kobo.521.4">We’ll simulate a basic SQL database interaction using Python dictionaries to represent tables and rows. </span><span class="koboSpan" id="kobo.521.5">You can see the full code example </span><span class="No-Break"><span class="koboSpan" id="kobo.522.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/5.sql_databases.py"><span class="No-Break"><span class="koboSpan" id="kobo.523.1">https://github.com/PacktPublishing/Python-Data-Cleaning-and-Preparation-Best-Practices/blob/main/chapter01/5.sql_databases.py</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.524.1">:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.525.1">We create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.526.1">read_sql</span></strong><span class="koboSpan" id="kobo.527.1"> function that simulates reading rows from a SQL table, represented here as a list of dictionaries where each dictionary corresponds to a row in </span><span class="No-Break"><span class="koboSpan" id="kobo.528.1">the table:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.529.1">
def read_sql():
# Simulating a SQL table with a dictionary
    sql_table = [
        {"id": 1, "name": "Alice", "age": 30},
        {"id": 2, "name": "Bob", "age": 24},
    ]
    for row in sql_table:
        process_row(row)</span></pre></li>				<li><span class="koboSpan" id="kobo.530.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.531.1">process_row</span></strong><span class="koboSpan" id="kobo.532.1"> function</span><a id="_idIndexMarker075"/><span class="koboSpan" id="kobo.533.1"> takes a row (dictionary) as input and prints its contents, simulating the processing of a row from a </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">SQL table:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.535.1">
def process_row(row):
    print(f"Processing row: id={row['id']}, name={row['name']}, age={row['age']}")
read_sql()</span></pre></li>				<li><span class="koboSpan" id="kobo.536.1">Let’s print our SQL table in the </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">proper format:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.538.1">
print(f"{'id':&lt;5} {'name':&lt;10} {'age':&lt;3}")
print("-" * 20)
# Print each row
for row in sql_table:
    print(f"{row['id']:&lt;5} {row['name']:&lt;10} {row['age']:&lt;3}")</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.539.1">This will print the </span><span class="No-Break"><span class="koboSpan" id="kobo.540.1">following output:</span></span></p><pre class="source-code"><strong class="bold"><span class="koboSpan" id="kobo.541.1">id   name     age</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.542.1">------------------</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.543.1">1    Alice    30</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.544.1">2    Bob      24</span></strong></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.545.1">The key to learning</span><a id="_idIndexMarker076"/><span class="koboSpan" id="kobo.546.1"> from the previous example is understanding how SQL databases structure and manage data through tables composed of rows and columns, and how to efficiently retrieve and process these rows programmatically. </span><span class="koboSpan" id="kobo.546.2">This knowledge is crucial because it lays the foundation for effective database management and data manipulation in </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">any application.</span></span></p>
			<p><span class="koboSpan" id="kobo.548.1">In real-world applications, this interaction is often facilitated by libraries and drivers such as </span><strong class="bold"><span class="koboSpan" id="kobo.549.1">Java Database Connectivity</span></strong><span class="koboSpan" id="kobo.550.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.551.1">JDBC</span></strong><span class="koboSpan" id="kobo.552.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.553.1">Open Database Connectivity</span></strong><span class="koboSpan" id="kobo.554.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.555.1">ODBC</span></strong><span class="koboSpan" id="kobo.556.1">), which</span><a id="_idIndexMarker077"/><span class="koboSpan" id="kobo.557.1"> provide standardized methods for </span><a id="_idIndexMarker078"/><span class="koboSpan" id="kobo.558.1">connecting to and querying databases. </span><span class="koboSpan" id="kobo.558.2">These libraries are typically wrapped by higher-level frameworks or libraries in Python, making it easier for developers to ingest data from various SQL databases without worrying about the underlying connectivity details. </span><span class="koboSpan" id="kobo.558.3">Several libraries can be used to interact with SQL databases using Python; we will explore the most popular ones in the </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">following section.</span></span></p>
			<h3><span class="koboSpan" id="kobo.560.1">Which library should you use for your use case?</span></h3>
			<p><span class="koboSpan" id="kobo.561.1">Let’s explore the different libraries available for interacting with SQL databases in Python, and understand when to use </span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">each one:</span></span></p>
			<ul>
				<li><strong class="bold"><span class="koboSpan" id="kobo.563.1">SQLite</span></strong><span class="koboSpan" id="kobo.564.1"> (sqlite3) is ideal </span><a id="_idIndexMarker079"/><span class="koboSpan" id="kobo.565.1">for small to medium-sized applications, local storage, and prototyping. </span><span class="koboSpan" id="kobo.565.2">Its zero-configuration, serverless architecture makes it perfect for lightweight, embedded database needs and quick development cycles. </span><span class="koboSpan" id="kobo.565.3">It is especially useful in scenarios where the overhead of a full-fledged database server is unnecessary. </span><span class="koboSpan" id="kobo.565.4">Avoid using sqlite3 for applications requiring high concurrency or extensive write operations, or where multiple users need to access the database simultaneously. </span><span class="koboSpan" id="kobo.565.5">It is not suitable for large-scale applications or those needing robust security features and advanced </span><span class="No-Break"><span class="koboSpan" id="kobo.566.1">database functionalities.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.567.1">SQLAlchemy</span></strong><span class="koboSpan" id="kobo.568.1"> is </span><a id="_idIndexMarker080"/><span class="koboSpan" id="kobo.569.1">suitable for applications requiring a high level of abstraction over raw SQL, support for multiple database engines, and complex queries and data models. </span><span class="koboSpan" id="kobo.569.2">It is ideal for large-scale production environments that need flexibility, scalability, and the ability to switch between different databases with minimal code changes. </span><span class="koboSpan" id="kobo.569.3">Avoid using SQLAlchemy for small, lightweight applications where the overhead of its comprehensive ORM capabilities is unnecessary. </span><span class="koboSpan" id="kobo.569.4">If you need direct, low-level access to a specific database’s features and are comfortable writing raw SQL queries, a simpler database adapter such as sqlite3, Psycopg2, or MySQL Connector/Python might be </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">more appropriate.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.571.1">Psycopg2</span></strong><span class="koboSpan" id="kobo.572.1"> is the</span><a id="_idIndexMarker081"/><span class="koboSpan" id="kobo.573.1"> go-to choice for interacting with PostgreSQL databases, making it suitable for applications that leverage PostgreSQL’s advanced features, such as ACID compliance, complex queries, and extensive data types. </span><span class="koboSpan" id="kobo.573.2">It is ideal for production environments requiring reliability and efficiency in handling PostgreSQL databases. </span><span class="koboSpan" id="kobo.573.3">Avoid using Psycopg2 if your application does not interact with PostgreSQL. </span><span class="koboSpan" id="kobo.573.4">If you need compatibility with multiple database systems or a higher-level abstraction, consider using SQLAlchemy instead. </span><span class="koboSpan" id="kobo.573.5">Also, it might not be the best choice for lightweight applications where the overhead of a full PostgreSQL setup </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">is unnecessary.</span></span></li>
				<li><strong class="bold"><span class="koboSpan" id="kobo.575.1">MySQL Connector/Python</span></strong><span class="koboSpan" id="kobo.576.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.577.1">mysql-connector-python</span></strong><span class="koboSpan" id="kobo.578.1">) is great for applications </span><a id="_idIndexMarker082"/><span class="koboSpan" id="kobo.579.1">that need to interact directly with MySQL databases. </span><span class="koboSpan" id="kobo.579.2">It is suitable for environments where compatibility and official support from Oracle are critical, as well as for applications leveraging MySQL’s features such as transaction management and connection pooling. </span><span class="koboSpan" id="kobo.579.3">Do not use MySQL Connector/Python if your application requires compatibility with multiple database systems or a higher-level abstraction. </span><span class="koboSpan" id="kobo.579.4">For simpler applications where the overhead of a full MySQL setup is unnecessary, or where MySQL’s features are not specifically needed, consider other </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">lightweight alternatives.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.581.1">After understanding the various libraries and their use cases for interacting with SQL databases, it’s equally important to explore alternatives for scenarios where the traditional relational model of SQL databases may not be the best fit. </span><span class="koboSpan" id="kobo.581.2">This brings us to NoSQL databases, which offer flexibility, scalability, and performance for handling unstructured or semi-structured data. </span><span class="koboSpan" id="kobo.581.3">Let’s delve into the key Python libraries for working with popular NoSQL databases and examine when a</span><a id="_idTextAnchor036"/><span class="koboSpan" id="kobo.582.1">nd how to use </span><span class="No-Break"><span class="koboSpan" id="kobo.583.1">them effectively.</span></span></p>
			<h3><span class="koboSpan" id="kobo.584.1">Performing data ingestion from the NoSQL database</span></h3>
			<p><span class="koboSpan" id="kobo.585.1">Non-relational databases</span><a id="_idIndexMarker083"/><span class="koboSpan" id="kobo.586.1"> can be used for storing and processing large volumes of semi-structured or unstructured data in batch operations. </span><span class="koboSpan" id="kobo.586.2">They are particularly effective when the schema can evolve or when handling diverse data types in a consolidated manner. </span><span class="koboSpan" id="kobo.586.3">NoSQL databases excel in streaming and semi-real-time workloads due to their ability to handle high throughput and low-latency data ingestion. </span><span class="koboSpan" id="kobo.586.4">They are commonly used for capturing and processing real-time data from IoT devices, logs, social media feeds, and other sources that generate continuous streams </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">of data.</span></span></p>
			<p><span class="koboSpan" id="kobo.588.1">The provided Python code mocks a NoSQL database with a dictionary and processes each key-value pair. </span><span class="koboSpan" id="kobo.588.2">Let’s have a look at each part of </span><span class="No-Break"><span class="koboSpan" id="kobo.589.1">the code:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.590.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.591.1">process_entry</span></strong><span class="koboSpan" id="kobo.592.1"> function takes a key and its associated value from the data store and prints a formatted message showing the processing of that key-value pair. </span><span class="koboSpan" id="kobo.592.2">It provides a simple way to view or handle individual entries, highlighting how data is accessed and manipulated based on </span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">its key:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.594.1">
def process_entry(key, value):
    print(f"Processing key: {key} with value: {value}")</span></pre></li>				<li><span class="koboSpan" id="kobo.595.1">The following function prints the entire </span><strong class="source-inline"><span class="koboSpan" id="kobo.596.1">data_store</span></strong><span class="koboSpan" id="kobo.597.1"> dictionary in a </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">tabular format:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.599.1">
def print_data_store(data_store):
    print(f"{'Key':&lt;5} {'Name':&lt;10} {'Age':&lt;3}")
    print("-" * 20)
    for key, value in data_store.items():
        print(f"{key:&lt;5} {value['name']:&lt;10} {value['age']:&lt;3}")</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.600.1">It starts by printing column headers for </span><strong class="source-inline"><span class="koboSpan" id="kobo.601.1">Key</span></strong><span class="koboSpan" id="kobo.602.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.603.1">Name</span></strong><span class="koboSpan" id="kobo.604.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.605.1">Age</span></strong><span class="koboSpan" id="kobo.606.1">, followed by a separator line for clarity. </span><span class="koboSpan" id="kobo.606.2">It then iterates over all key-value pairs in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.607.1">data_store</span></strong><span class="koboSpan" id="kobo.608.1"> dictionary, printing each entry’s key, name, and age. </span><span class="koboSpan" id="kobo.608.2">This function helps visualize the current state of the data store. </span><span class="koboSpan" id="kobo.608.3">The initial state of the data is </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">as follows:</span></span></p><pre class="source-code"><strong class="bold"><span class="koboSpan" id="kobo.610.1">Initial Data Store:</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.611.1">Key   Name      Age</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.612.1">-----------------------</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.613.1">1     Alice     30</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.614.1">2     Bob       24</span></strong></pre></li>				<li><span class="koboSpan" id="kobo.615.1">This</span><a id="_idIndexMarker084"/><span class="koboSpan" id="kobo.616.1"> function adds a new entry to the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.617.1">data_store</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.618.1"> dictionary:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.619.1">
def create_entry(data_store, key, value):
    data_store[key] = value
    return data_store</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.620.1">It takes a key and a value, then inserts the value into </span><strong class="source-inline"><span class="koboSpan" id="kobo.621.1">data_store</span></strong><span class="koboSpan" id="kobo.622.1"> under the specified key. </span><span class="koboSpan" id="kobo.622.2">The updated </span><strong class="source-inline"><span class="koboSpan" id="kobo.623.1">data_store</span></strong><span class="koboSpan" id="kobo.624.1"> dictionary is then returned. </span><span class="koboSpan" id="kobo.624.2">This demonstrates the ability to add new data to the store, showcasing the creation aspect</span><a id="_idIndexMarker085"/><span class="koboSpan" id="kobo.625.1"> of </span><strong class="bold"><span class="koboSpan" id="kobo.626.1">Create, Read, Update, and Delete</span></strong><span class="koboSpan" id="kobo.627.1"> (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.628.1">CRUD</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.629.1">) operations.</span></span></p></li>				<li><span class="koboSpan" id="kobo.630.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.631.1">update_entry</span></strong><span class="koboSpan" id="kobo.632.1"> function updates an existing entry in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.633.1">data_store</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.634.1"> dictionary:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.635.1">
def update_entry(data_store, key, new_value):
    if key in data_store:
        data_store[key] = new_value
    return data_store</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.636.1">It takes a key and </span><strong class="source-inline"><span class="koboSpan" id="kobo.637.1">new_value</span></strong><span class="koboSpan" id="kobo.638.1">, and if the key exists in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.639.1">data_store</span></strong><span class="koboSpan" id="kobo.640.1"> dictionary, it updates the corresponding value with </span><strong class="source-inline"><span class="koboSpan" id="kobo.641.1">new_value</span></strong><span class="koboSpan" id="kobo.642.1">. </span><span class="koboSpan" id="kobo.642.2">The updated </span><strong class="source-inline"><span class="koboSpan" id="kobo.643.1">data_store</span></strong><span class="koboSpan" id="kobo.644.1"> dictionary is then returned. </span><span class="koboSpan" id="kobo.644.2">This illustrates how existing data can be modified, demonstrating the update aspect of </span><span class="No-Break"><span class="koboSpan" id="kobo.645.1">CRUD operations.</span></span></p></li>				<li><span class="koboSpan" id="kobo.646.1">The </span><a id="_idIndexMarker086"/><span class="koboSpan" id="kobo.647.1">following function removes an entry from the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.648.1">data_store</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.649.1"> dictionary:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.650.1">
def delete_entry(data_store, key):
    if key in data_store:
        del data_store[key]
    return data_store</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.651.1">It takes a key, and if the key is found in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.652.1">data_store</span></strong><span class="koboSpan" id="kobo.653.1"> dictionary, it deletes the corresponding entry. </span><span class="koboSpan" id="kobo.653.2">The updated </span><strong class="source-inline"><span class="koboSpan" id="kobo.654.1">data_store</span></strong><span class="koboSpan" id="kobo.655.1"> dictionary is </span><span class="No-Break"><span class="koboSpan" id="kobo.656.1">then returned.</span></span></p></li>				<li><span class="koboSpan" id="kobo.657.1">The following function wraps all the </span><span class="No-Break"><span class="koboSpan" id="kobo.658.1">process together:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.659.1">
def read_nosql():
    data_store = {
        "1": {"name": "Alice", "age": 30},
        "2": {"name": "Bob", "age": 24},
    }
    print("Initial Data Store:")
    print_data_store(data_store)
    # Create: Adding a new entry
    new_key = "3"
    new_value = {"name": "Charlie", "age": 28}
    data_store = create_entry(data_store, new_key, new_value)
    # Read: Retrieving and processing an entry
    print("\nAfter Adding a New Entry:")
    process_entry(new_key, data_store[new_key])
    # Update: Modifying an existing entry
    update_key = "1"
    updated_value = {"name": "Alice", "age": 31}
    data_store = update_entry(data_store, update_key, updated_value)
    # Delete: Removing an entry
    delete_key = "2"
    data_store = delete_entry(data_store, delete_key)
    # Print the final state of the data store
    print("\nFinal Data Store:")
    print_data_store(data_store)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.660.1">This</span><a id="_idIndexMarker087"/><span class="koboSpan" id="kobo.661.1"> code illustrates the core principles of NoSQL databases, including schema flexibility, key-value pair storage, and basic CRUD operations. </span><span class="koboSpan" id="kobo.661.2">It begins with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.662.1">read_nosql()</span></strong><span class="koboSpan" id="kobo.663.1"> function, which simulates a NoSQL database using a dictionary, </span><strong class="source-inline"><span class="koboSpan" id="kobo.664.1">data_store</span></strong><span class="koboSpan" id="kobo.665.1">, where each key-value pair represents a unique identifier and associated user information. </span><span class="koboSpan" id="kobo.665.2">Initially, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.666.1">print_data_store()</span></strong><span class="koboSpan" id="kobo.667.1"> function displays the data in a tabular format, highlighting the schema flexibility inherent in NoSQL systems. </span><span class="koboSpan" id="kobo.667.2">The code then demonstrates CRUD operations. </span><span class="koboSpan" id="kobo.667.3">It starts by adding a new entry with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.668.1">create_entry()</span></strong><span class="koboSpan" id="kobo.669.1"> function, showcasing how new data is inserted into the store. </span><span class="koboSpan" id="kobo.669.2">Following this, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.670.1">process_entry()</span></strong><span class="koboSpan" id="kobo.671.1"> function retrieves and prints the details of the newly added entry, illustrating the read operation. </span><span class="koboSpan" id="kobo.671.2">Next, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.672.1">update_entry()</span></strong><span class="koboSpan" id="kobo.673.1"> function modifies an existing entry, demonstrating the update capability of NoSQL databases. </span><span class="koboSpan" id="kobo.673.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.674.1">delete_entry()</span></strong><span class="koboSpan" id="kobo.675.1"> function is used to remove an entry, showing how data can be deleted from the store. </span><span class="koboSpan" id="kobo.675.2">Finally, the updated state of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.676.1">data_store</span></strong><span class="koboSpan" id="kobo.677.1"> dictionary is </span><a id="_idIndexMarker088"/><span class="koboSpan" id="kobo.678.1">printed again, providing a clear view of how the data evolves through </span><span class="No-Break"><span class="koboSpan" id="kobo.679.1">these operations.</span></span></p></li>				<li><span class="koboSpan" id="kobo.680.1">Let’s execute the </span><span class="No-Break"><span class="koboSpan" id="kobo.681.1">whole process:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.682.1">
read_nosql()</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.683.1">This returns the </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">final datastore:</span></span></p><pre class="source-code"><strong class="bold"><span class="koboSpan" id="kobo.685.1">Final Data Store:</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.686.1">Key   Name</span></strong><strong class="bold"><span class="koboSpan" id="kobo.687.1">      Age</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.688.1">-----------------------</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.689.1">1     Alice     31</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.690.1">2     Charlie   28</span></strong></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.691.1">In the preceding example, we demonstrated an interaction with a </span><em class="italic"><span class="koboSpan" id="kobo.692.1">mocked</span></em><span class="koboSpan" id="kobo.693.1"> NoSQL system using Python so that we can showcase the core principles of NoSQL databases such as schema flexibility, key-value pair storage, and basic CRUD operations. </span><span class="koboSpan" id="kobo.693.2">We can now better grasp how NoSQL databases differ from traditional SQL databases in terms of data modeling and handling unstructured or semi-structured </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">data efficiently.</span></span></p>
			<p><span class="koboSpan" id="kobo.695.1">There are several libraries that can be used to interact with NoSQL databases. </span><span class="koboSpan" id="kobo.695.2">In the next section, we will explore the most </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">popular ones.</span></span></p>
			<h3><span class="koboSpan" id="kobo.697.1">Which library should you use for your use case?</span></h3>
			<p><span class="koboSpan" id="kobo.698.1">Let’s explore the different libraries available for interacting with NoSQL databases in Python, and understand when to use </span><span class="No-Break"><span class="koboSpan" id="kobo.699.1">each one:</span></span></p>
			<ul>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.700.1">pymongo</span></strong><span class="koboSpan" id="kobo.701.1"> is the</span><a id="_idIndexMarker089"/><span class="koboSpan" id="kobo.702.1"> official Python driver for MongoDB, a popular NoSQL database known for its flexibility and scalability. </span><strong class="source-inline"><span class="koboSpan" id="kobo.703.1">pymongo</span></strong><span class="koboSpan" id="kobo.704.1"> allows Python applications to interact seamlessly with MongoDB, offering a straightforward API to perform CRUD operations, manage indexes, and execute complex queries. </span><strong class="source-inline"><span class="koboSpan" id="kobo.705.1">pymongo</span></strong><span class="koboSpan" id="kobo.706.1"> is particularly favored for its ease of use and compatibility with Python’s data structures, making it suitable for a wide range of applications from simple prototypes to large-scale </span><span class="No-Break"><span class="koboSpan" id="kobo.707.1">production systems.</span></span></li>
				<li><strong class="source-inline"><span class="koboSpan" id="kobo.708.1">cassandra-driver</span></strong><span class="koboSpan" id="kobo.709.1"> (Cassandra): The </span><strong class="source-inline"><span class="koboSpan" id="kobo.710.1">cassandra-driver</span></strong><span class="koboSpan" id="kobo.711.1"> library provides Python applications with direct</span><a id="_idIndexMarker090"/><span class="koboSpan" id="kobo.712.1"> access to Apache Cassandra, a highly scalable NoSQL database designed for handling large amounts of data across distributed commodity servers. </span><span class="koboSpan" id="kobo.712.2">Cassandra’s architecture is optimized for write-heavy workloads and offers tunable consistency levels, making it suitable for real-time analytics, IoT data, and other applications requiring high availability and </span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">fault tolerance.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.714.1">Transitioning from databases to file systems involves shifting the focus from structu</span><a id="_idTextAnchor037"/><span class="koboSpan" id="kobo.715.1">red data storage and retrieval mechanisms to more flexible and versatile </span><span class="No-Break"><span class="koboSpan" id="kobo.716.1">storage solutions.</span></span></p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor038"/><span class="koboSpan" id="kobo.717.1">Performing data ingestion from cloud-based file systems</span></h2>
			<p><span class="koboSpan" id="kobo.718.1">Cloud storage</span><a id="_idIndexMarker091"/><span class="koboSpan" id="kobo.719.1"> is a</span><a id="_idIndexMarker092"/><span class="koboSpan" id="kobo.720.1"> service</span><a id="_idIndexMarker093"/><span class="koboSpan" id="kobo.721.1"> model that allows data to be remotely maintained, managed, and backed up over the internet. </span><span class="koboSpan" id="kobo.721.2">It</span><a id="_idIndexMarker094"/><span class="koboSpan" id="kobo.722.1"> involves storing data on remote servers accessed from anywhere via the internet, </span><em class="italic"><span class="koboSpan" id="kobo.723.1">rather than on local devices</span></em><span class="koboSpan" id="kobo.724.1">. </span><span class="koboSpan" id="kobo.724.2">Cloud storage has revolutionized the way we store and access data. </span><span class="koboSpan" id="kobo.724.3">It provides a flexible and scalable solution for individuals and organizations, enabling them to store large amounts of data </span><em class="italic"><span class="koboSpan" id="kobo.725.1">without investing in physical hardware</span></em><span class="koboSpan" id="kobo.726.1">. </span><span class="koboSpan" id="kobo.726.2">This is particularly useful for ensuring that data is always accessible and can be </span><span class="No-Break"><span class="koboSpan" id="kobo.727.1">shared easily.</span></span></p>
			<p><span class="koboSpan" id="kobo.728.1">Amazon S3, Microsoft Azure Blob Storage, and Google Cloud Storage are all cloud-based object storage services that allow you to store and retrieve files in the cloud. </span><span class="koboSpan" id="kobo.728.2">Cloud-based file systems are becoming increasingly popular for </span><span class="No-Break"><span class="koboSpan" id="kobo.729.1">several reasons.</span></span></p>
			<p><span class="koboSpan" id="kobo.730.1">Firstly, they </span><a id="_idIndexMarker095"/><span class="koboSpan" id="kobo.731.1">provide a flexible and scalable storage solution that can easily adapt to the changing needs of an organization. </span><span class="koboSpan" id="kobo.731.2">This means that as the amount of data grows, additional storage capacity can be added without the need for significant capital investment or physical infrastructure changes. </span><span class="koboSpan" id="kobo.731.3">Thus, it can help reduce capital expenditures and operational costs associated with maintaining and upgrading on-premises </span><span class="No-Break"><span class="koboSpan" id="kobo.732.1">storage infrastructure.</span></span></p>
			<p><span class="koboSpan" id="kobo.733.1">Secondly, cloud-based </span><a id="_idIndexMarker096"/><span class="koboSpan" id="kobo.734.1">file systems</span><a id="_idIndexMarker097"/><span class="koboSpan" id="kobo.735.1"> offer high levels of accessibility and availability. </span><span class="koboSpan" id="kobo.735.2">With data stored in the cloud, users can access it from anywhere with an internet connection, making it easier to collaborate and share information across different teams, departments, or locations. </span><span class="koboSpan" id="kobo.735.3">Additionally, cloud-based file systems are designed with redundancy and failover mechanisms, ensuring that data is always available even in the event of a hardware failure or outage. </span><span class="koboSpan" id="kobo.735.4">Finally, they provide enhanced security features to protect data from unauthorized access, breaches, or data loss. </span><span class="koboSpan" id="kobo.735.5">Cloud service providers typically have advanced security protocols, encryption, and monitoring tools to safeguard data and ensure compliance with data </span><span class="No-Break"><span class="koboSpan" id="kobo.736.1">privacy regulations.</span></span></p>
			<p><span class="koboSpan" id="kobo.737.1">Files in cloud-based storage systems are essentially the same as those on local devices, but they are stored on remote servers and accessed over the internet. </span><span class="koboSpan" id="kobo.737.2">However, how are these files organized in these cloud storage systems? </span><span class="koboSpan" id="kobo.737.3">Let’s discuss </span><span class="No-Break"><span class="koboSpan" id="kobo.738.1">that next.</span></span></p>
			<h3><span class="koboSpan" id="kobo.739.1">Organizing files in cloud storage systems</span></h3>
			<p><span class="koboSpan" id="kobo.740.1">One</span><a id="_idIndexMarker098"/><span class="koboSpan" id="kobo.741.1"> of the primary methods of organizing files in cloud storage is by using folder structures, similar to local file systems. </span><span class="koboSpan" id="kobo.741.2">Users can create folders and subfolders to categorize and store files systematically. </span><span class="koboSpan" id="kobo.741.3">Let’s have a look at some </span><span class="No-Break"><span class="koboSpan" id="kobo.742.1">best practices:</span></span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.743.1">Creating a logical and intuitive hierarchy that reflects how you work or how your projects are structured is essential. </span><span class="koboSpan" id="kobo.743.2">This involves designing a folder structure that mimics your workflow, making it easier to locate and manage files. </span><span class="koboSpan" id="kobo.743.3">For instance, you might create main folders for different departments, projects, or clients, with subfolders for specific tasks or document types. </span><span class="koboSpan" id="kobo.743.4">This hierarchical organization not only saves time by reducing the effort needed to find files but also enhances collaboration by providing a clear and consistent framework that team members can </span><span class="No-Break"><span class="koboSpan" id="kobo.744.1">easily navigate.</span></span></li>
				<li><span class="koboSpan" id="kobo.745.1">Using </span><em class="italic"><span class="koboSpan" id="kobo.746.1">consistent naming conventions</span></em><span class="koboSpan" id="kobo.747.1"> for folders and files is crucial for ensuring easy retrieval and maintaining order within your cloud storage. </span><span class="koboSpan" id="kobo.747.2">A standardized </span><a id="_idIndexMarker099"/><span class="koboSpan" id="kobo.748.1">naming scheme helps avoid confusion, reduces errors, and speeds up the process of locating specific documents. </span><span class="koboSpan" id="kobo.748.2">For example, adopting a format such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.749.1">YYYY-MM-DD_ProjectName_DocumentType</span></strong><span class="koboSpan" id="kobo.750.1"> can provide immediate context and make sorting and searching more straightforward. </span><span class="koboSpan" id="kobo.750.2">Consistent naming also facilitates automation and integration with other tools, as predictable file names can be more easily processed by scripts </span><span class="No-Break"><span class="koboSpan" id="kobo.751.1">and applications.</span></span></li>
				<li><span class="koboSpan" id="kobo.752.1">Grouping files by project or client is an effective way to keep related documents together and streamline project management. </span><span class="koboSpan" id="kobo.752.2">This method involves creating dedicated folders for each project or client, where all relevant files, such as contracts, communications, and deliverables, </span><span class="No-Break"><span class="koboSpan" id="kobo.753.1">are stored.</span></span></li>
				<li><span class="koboSpan" id="kobo.754.1">Many cloud storage systems allow tagging files with keywords or metadata, which significantly enhances file categorization and searchability. </span><span class="koboSpan" id="kobo.754.2">Tags are essentially labels that you can attach to files, making it easier to group and find documents based on specific criteria. </span><span class="koboSpan" id="kobo.754.3">Metadata includes detailed information, such as the author, date, project name, and file type, which provides additional context and aids in more precise searches. </span><span class="koboSpan" id="kobo.754.4">By using relevant tags and comprehensive metadata, you can quickly filter and locate files, regardless of their location within the folder hierarchy. </span><span class="koboSpan" id="kobo.754.5">This practice is particularly useful in large storage systems where traditional folder structures might </span><span class="No-Break"><span class="koboSpan" id="kobo.755.1">become cumbersome.</span></span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.756.1">From discussing cloud storage systems</span><a id="_idTextAnchor039"/><span class="koboSpan" id="kobo.757.1">, the focus now shifts to exploring the capabilities and integration opportunities offered </span><span class="No-Break"><span class="koboSpan" id="kobo.758.1">by APIs.</span></span></p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor040"/><span class="koboSpan" id="kobo.759.1">APIs</span></h2>
			<p><span class="koboSpan" id="kobo.760.1">APIs have</span><a id="_idIndexMarker100"/><span class="koboSpan" id="kobo.761.1"> become increasingly popular in recent years due to their ability to enable seamless communication and integration between different systems and services. </span><span class="koboSpan" id="kobo.761.2">APIs provide developers with a standardized and flexible way to access data and functionality from other systems, allowing them to easily build new applications and services that leverage existing resources. </span><span class="koboSpan" id="kobo.761.3">APIs have become a fundamental building block for modern software development and are widely used across a wide range of industries </span><span class="No-Break"><span class="koboSpan" id="kobo.762.1">and applications.</span></span></p>
			<p><span class="koboSpan" id="kobo.763.1">Now that we understand what APIs represent, let’s move on to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.764.1">requests</span></strong><span class="koboSpan" id="kobo.765.1"> Python</span><a id="_idTextAnchor041"/><span class="koboSpan" id="kobo.766.1"> library with which developers can programmatically access and manipulate data from </span><span class="No-Break"><span class="koboSpan" id="kobo.767.1">remote servers.</span></span></p>
			<h3><span class="koboSpan" id="kobo.768.1">The requests library</span></h3>
			<p><span class="koboSpan" id="kobo.769.1">When it comes </span><a id="_idIndexMarker101"/><span class="koboSpan" id="kobo.770.1">to working with APIs in Python, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.771.1">requests</span></strong><span class="koboSpan" id="kobo.772.1"> library is</span><a id="_idIndexMarker102"/><span class="koboSpan" id="kobo.773.1"> the go-to Python library for making HTTP requests to APIs and other web services. </span><span class="koboSpan" id="kobo.773.2">It makes it easy to send HTTP/1.1 requests using Python, and it provides many convenient features for working with </span><span class="No-Break"><span class="koboSpan" id="kobo.774.1">HTTP responses.</span></span></p>
			<p><span class="koboSpan" id="kobo.775.1">Run the following</span><a id="_idIndexMarker103"/><span class="koboSpan" id="kobo.776.1"> command to install the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.777.1">requests</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.778.1"> library:</span></span></p>
			<pre class="console"><span class="koboSpan" id="kobo.779.1">
pip install requests==2.32.3</span></pre>			<p><span class="koboSpan" id="kobo.780.1">Let’s have a </span><a id="_idIndexMarker104"/><span class="koboSpan" id="kobo.781.1">quick look at how we can use </span><span class="No-Break"><span class="koboSpan" id="kobo.782.1">this library:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.783.1">Import the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.784.1">requests</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.785.1"> library:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.786.1">
import requests</span></pre></li>				<li><span class="koboSpan" id="kobo.787.1">Specify the API </span><span class="No-Break"><span class="koboSpan" id="kobo.788.1">endpoint URL:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.789.1">
url = "https://jsonplaceholder.typicode.com/posts"</span></pre></li>				<li><span class="koboSpan" id="kobo.790.1">Make a </span><strong class="source-inline"><span class="koboSpan" id="kobo.791.1">GET</span></strong><span class="koboSpan" id="kobo.792.1"> request to the </span><span class="No-Break"><span class="koboSpan" id="kobo.793.1">API endpoint:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.794.1">
response = requests.get(url)</span></pre></li>				<li><span class="koboSpan" id="kobo.795.1">Get the </span><span class="No-Break"><span class="koboSpan" id="kobo.796.1">response content:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.797.1">
print(response.content)</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.798.1">Here, we’re making a </span><strong class="source-inline"><span class="koboSpan" id="kobo.799.1">GET</span></strong><span class="koboSpan" id="kobo.800.1"> request to the API endpoint at </span><a href="https://jsonplaceholder.typicode.com/posts"><span class="koboSpan" id="kobo.801.1">https://jsonplaceholder.typicode.com/posts</span></a><span class="koboSpan" id="kobo.802.1"> and storing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.803.1">response</span></strong><span class="koboSpan" id="kobo.804.1"> object in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.805.1">response</span></strong><span class="koboSpan" id="kobo.806.1"> variable. </span><span class="koboSpan" id="kobo.806.2">We can then print the response content using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.807.1">content</span></strong><span class="koboSpan" id="kobo.808.1"> attribute of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.809.1">response</span></strong><span class="koboSpan" id="kobo.810.1"> object. </span><span class="koboSpan" id="kobo.810.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.811.1">requests</span></strong><span class="koboSpan" id="kobo.812.1"> library provides many other methods and features for making HTTP requests, including support for </span><strong class="source-inline"><span class="koboSpan" id="kobo.813.1">POST</span></strong><span class="koboSpan" id="kobo.814.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.815.1">PUT</span></strong><span class="koboSpan" id="kobo.816.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.817.1">DELETE</span></strong><span class="koboSpan" id="kobo.818.1">, and other </span><a id="_idIndexMarker105"/><span class="koboSpan" id="kobo.819.1">HTTP methods, handling headers and cookies, and handling redirects </span><span class="No-Break"><span class="koboSpan" id="kobo.820.1">and authentication.</span></span></p>
			<p><span class="koboSpan" id="kobo.821.1">Now that we’ve explained the </span><strong class="source-inline"><span class="koboSpan" id="kobo.822.1">requests</span></strong><span class="koboSpan" id="kobo.823.1"> library, let’s move on to a specific example of retrieving margarita cocktail data from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.824.1">Cocktail DB</span></strong><span class="koboSpan" id="kobo.825.1"> API, which can illustrate </span><a id="_idTextAnchor042"/><span class="koboSpan" id="kobo.826.1">how practical web requests can be in accessing and integrating real-time data sources </span><span class="No-Break"><span class="koboSpan" id="kobo.827.1">into applications.</span></span></p>
			<h3><span class="koboSpan" id="kobo.828.1">Learn how to make a margarita!</span></h3>
			<p><span class="koboSpan" id="kobo.829.1">The use case</span><a id="_idIndexMarker106"/><span class="koboSpan" id="kobo.830.1"> demonstrates</span><a id="_idIndexMarker107"/><span class="koboSpan" id="kobo.831.1"> retrieving cocktail data from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.832.1">Cocktail DB</span></strong><span class="koboSpan" id="kobo.833.1"> API using Python. </span><span class="koboSpan" id="kobo.833.2">If you want to improve your bartending skills and impress your friends, you can use an open API to get real-time information on the ingredients required for any cocktail. </span><span class="koboSpan" id="kobo.833.3">For this, we will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.834.1">Cocktail DB</span></strong><span class="koboSpan" id="kobo.835.1"> API and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.836.1">request</span></strong><span class="koboSpan" id="kobo.837.1"> library to see which ingredients we need for </span><span class="No-Break"><span class="koboSpan" id="kobo.838.1">a margarita:</span></span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.839.1">Define the API endpoint URL. </span><span class="koboSpan" id="kobo.839.2">We are making a request to the Cocktail DB API endpoint to search for cocktails with the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.840.1">margarita</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.841.1"> name:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.842.1">
url = "https://www.thecocktaildb.com/api/json/v1/1/search.php?s=margarita"</span></pre></li>				<li><span class="koboSpan" id="kobo.843.1">Make the API request. </span><span class="koboSpan" id="kobo.843.2">We define the API endpoint URL as a string and pass it to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.844.1">requests.get()</span></strong><span class="koboSpan" id="kobo.845.1"> function to make the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.846.1">GET</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.847.1"> request:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.848.1">
response = requests.get(url)</span></pre></li>				<li><span class="koboSpan" id="kobo.849.1">Check whether the request was successful (status code </span><strong class="source-inline"><span class="koboSpan" id="kobo.850.1">200</span></strong><span class="koboSpan" id="kobo.851.1">) and get the data. </span><span class="koboSpan" id="kobo.851.2">The API response is returned as a JSON string, which we can extract by calling the </span><strong class="source-inline"><span class="koboSpan" id="kobo.852.1">response.json()</span></strong><span class="koboSpan" id="kobo.853.1"> method. </span><span class="koboSpan" id="kobo.853.2">We then assign this JSON data to a variable </span><span class="No-Break"><span class="koboSpan" id="kobo.854.1">called </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.855.1">data</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.856.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.857.1">
if response.status_code == 200:
    # Extract the response JSON data
    data = response.json()
    # Check if the API response contains cocktails data
    if 'drinks' in data:
        # Create DataFrame from drinks data
        df = pd.DataFrame(data['drinks'])
        # Print the resulting DataFrame
        print(df.head())
    else:
        print("No drinks found.")</span></pre></li>				<li><span class="koboSpan" id="kobo.858.1">If the </span><a id="_idIndexMarker108"/><span class="koboSpan" id="kobo.859.1">request was not successful, print this </span><span class="No-Break"><span class="koboSpan" id="kobo.860.1">error message:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.861.1">
else:
    print(f"Failed to retrieve data from API. </span><span class="koboSpan" id="kobo.861.2">Status code: {response.status_code}")</span></pre></li>			</ol>
			<p><span class="koboSpan" id="kobo.862.1">You </span><a id="_idIndexMarker109"/><span class="koboSpan" id="kobo.863.1">can replace the </span><strong class="source-inline"><span class="koboSpan" id="kobo.864.1">margarita</span></strong><span class="koboSpan" id="kobo.865.1"> search parameter with any other cocktail name or ingredient to get data for </span><span class="No-Break"><span class="koboSpan" id="kobo.866.1">different drinks.</span></span></p>
			<p><span class="koboSpan" id="kobo.867.1">With this, we come to the end of our first chapter. </span><span class="koboSpan" id="kobo.867.2">Let’s summarize what we have learned </span><span class="No-Break"><span class="koboSpan" id="kobo.868.1">so far.</span></span></p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor043"/><span class="koboSpan" id="kobo.869.1">Summary</span></h1>
			<p><span class="koboSpan" id="kobo.870.1">Throughout this chapter, we covered essential technologies in modern computing and data management. </span><span class="koboSpan" id="kobo.870.2">We began by discussing batch ingestion, a method whereby large volumes of data are collected and processed at scheduled intervals, offering efficiency and cost-effectiveness for organizations with predictable data flows. </span><span class="koboSpan" id="kobo.870.3">In contrast, we explored streaming ingestion, which allows data to be processed in real-time, enabling immediate analysis and rapid response to changing conditions. </span><span class="koboSpan" id="kobo.870.4">We followed with streaming services such as Kafka for real-time data processing. </span><span class="koboSpan" id="kobo.870.5">We moved to SQL and NoSQL databases—such as PostgreSQL, MySQL, MongoDB, and Cassandra—highlighting their strengths in structured and flexible data storage, respectively. </span><span class="koboSpan" id="kobo.870.6">We explored APIs such as REST for seamless system integration. </span><span class="koboSpan" id="kobo.870.7">Also, we delved into file systems, file types, and attributes, alongside cloud storage solutions such as Amazon S3 and Google Cloud Storage, emphasizing scalability and data management strategies. </span><span class="koboSpan" id="kobo.870.8">These technologies collectively enable robust, scalable, and efficient applications in today’s </span><span class="No-Break"><span class="koboSpan" id="kobo.871.1">digital ecosystem.</span></span></p>
			<p><span class="koboSpan" id="kobo.872.1">In the upcoming chapter, we will dive deep into the critical aspects of data quality and its significance in building reliable data products. </span><span class="koboSpan" id="kobo.872.2">We’ll explore why ensuring high data quality is paramount for making informed business decisions, enhancing customer experiences, and maintaining </span><span class="No-Break"><span class="koboSpan" id="kobo.873.1">operational efficiency.</span></span></p>
		</div>
	</body></html>