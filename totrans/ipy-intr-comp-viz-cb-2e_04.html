<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 4. Profiling and Optimization"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Profiling and Optimization</h1></div></div></div><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Evaluating the time taken by a statement in IPython</li><li class="listitem" style="list-style-type: disc">Profiling your code easily with cProfile and IPython</li><li class="listitem" style="list-style-type: disc">Profiling your code line-by-line with line_profiler</li><li class="listitem" style="list-style-type: disc">Profiling the memory usage of your code with memory_profiler</li><li class="listitem" style="list-style-type: disc">Understanding the internals of NumPy to avoid unnecessary array copying</li><li class="listitem" style="list-style-type: disc">Using stride tricks with NumPy</li><li class="listitem" style="list-style-type: disc">Implementing an efficient rolling average algorithm with stride tricks</li><li class="listitem" style="list-style-type: disc">Making efficient array selections in NumPy</li><li class="listitem" style="list-style-type: disc">Processing huge NumPy arrays with memory mapping</li><li class="listitem" style="list-style-type: disc">Manipulating large arrays with HDF5 and PyTables</li><li class="listitem" style="list-style-type: disc">Manipulating large heterogeneous tables with HDF5 and PyTables</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec33"/>Introduction</h1></div></div></div><p>Although Python is generally known (a bit unfairly) as a <span class="emphasis"><em>slow</em></span> language, it is possible to achieve very good performance with the right methods. This is the objective of this chapter and the next. This chapter describes how to evaluate (<span class="strong"><strong>profile</strong></span>) what makes a program slow, and how this information can be used to <span class="strong"><strong>optimize</strong></span> the code and make it more efficient. The next chapter will deal with more advanced high-performance computing methods that should only be tackled when the methods described here are not sufficient.</p><p>The recipes of this chapter are organized into three parts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Time and memory profiling</strong></span>: Evaluating<a id="id420" class="indexterm"/> the performance of code</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>NumPy optimization</strong></span>: Using<a id="id421" class="indexterm"/> NumPy more efficiently, particularly with large arrays</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Memory mapping with arrays</strong></span>: Implementing <a id="id422" class="indexterm"/>memory mapping techniques for out-of-core computations on huge arrays, notably with the HDF5 file format</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Evaluating the time taken by a statement in IPython"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec34"/>Evaluating the time taken by a statement in IPython</h1></div></div></div><p>The <code class="literal">%timeit</code> magic and the <code class="literal">%%timeit</code> cell magic (that applies to an entire code cell) allow you to quickly evaluate the time taken by one or several Python statements. For more extensive profiling, you may need to use more advanced methods presented in the next recipes.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec104"/>How to do it...</h2></div></div></div><p>We are going to estimate the time taken to calculate the sum of the inverse squares of all positive integer numbers up to a given <code class="literal">n</code>:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's define <code class="literal">n</code>:<div class="informalexample"><pre class="programlisting">In [1]: n = 100000</pre></div></li><li class="listitem">Let's time this computation in pure Python:<div class="informalexample"><pre class="programlisting">In [2]: %timeit sum([1. / i**2 for i in range(1, n)])
10 loops, best of 3: 131 ms per loop</pre></div></li><li class="listitem">Now, let's use the <code class="literal">%%timeit</code> cell magic to time the same computation written on two lines:<div class="informalexample"><pre class="programlisting">In [3]: %%timeit s = 0.
        for i in range(1, n):
            s += 1. / i**2
10 loops, best of 3: 137 ms per loop</pre></div></li><li class="listitem">Finally, let's time the NumPy version of this computation:<div class="informalexample"><pre class="programlisting">In [4]: import numpy as np
In [5]: %timeit np.sum(1. / np.arange(1., n) ** 2)
1000 loops, best of 3: 1.71 ms per loop</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec105"/>How it works...</h2></div></div></div><p>The <code class="literal">%timeit</code> command<a id="id423" class="indexterm"/> accepts several optional parameters. One such parameter is the number of statement evaluations. By default, this number is chosen automatically so that the <code class="literal">%timeit</code> command returns within a few seconds. However, this number can be specified directly with the <code class="literal">-r</code> and <code class="literal">-n</code> parameters. Type <code class="literal">%timeit?</code> in IPython to get more information.</p><p>The <code class="literal">%%timeit</code> cell magic <a id="id424" class="indexterm"/>also accepts an optional setup<a id="id425" class="indexterm"/> statement <a id="id426" class="indexterm"/>in the first line (on the same line as <code class="literal">%%timeit</code>), which is executed but not timed. All variables created in this statement are available inside the cell.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec106"/>There's more...</h2></div></div></div><p>If you are not in an IPython interactive session, you<a id="id427" class="indexterm"/> can use <code class="literal">timeit.timeit()</code>. This function, defined in Python's <code class="literal">timeit</code> module, benchmarks a Python statement stored in a string. IPython's <code class="literal">%timeit</code> magic command is a convenient wrapper around <code class="literal">timeit()</code>, useful in an interactive session. For more information on the <code class="literal">timeit</code> module, refer to <a class="ulink" href="https://docs.python.org/3/library/timeit.html">https://docs.python.org/3/library/timeit.html</a>.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec107"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Profiling your code easily with cProfile and IPython</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Profiling your code line-by-line with line_profiler</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Profiling your code easily with cProfile and IPython"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec35"/>Profiling your code easily with cProfile and IPython</h1></div></div></div><p>The <code class="literal">%timeit</code> magic <a id="id428" class="indexterm"/>command is often helpful, yet a bit limited when you need detailed <a id="id429" class="indexterm"/>information about what takes most <a id="id430" class="indexterm"/>of the execution time in your code. This magic command is meant for <a id="id431" class="indexterm"/><span class="strong"><strong>benchmarking</strong></span> (comparing the execution times of <a id="id432" class="indexterm"/>different versions of a function) rather than <span class="strong"><strong>profiling</strong></span><a id="id433" class="indexterm"/> (getting a detailed report of the execution time, function by function).</p><p>Python includes a profiler named <code class="literal">cProfile</code> that breaks down the execution time into the contributions of all called functions. IPython provides convenient ways to leverage this tool in an interactive session.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec108"/>How to do it...</h2></div></div></div><p>IPython offers the <code class="literal">%prun</code> line magic<a id="id434" class="indexterm"/> and the <code class="literal">%%prun</code> cell magic<a id="id435" class="indexterm"/> to easily profile one or multiple lines of code. The <code class="literal">%run</code> magic command<a id="id436" class="indexterm"/> also accepts a <code class="literal">-p</code> flag to run a Python script under the control of the profiler. These commands accept a lot of options, and you may want to take a look at their documentation with <code class="literal">%prun?</code> and <code class="literal">%run?</code>.</p><p>In this example, we will <a id="id437" class="indexterm"/>profile a numerical simulation of random <a id="id438" class="indexterm"/>walks starting at the origin. We will cover these kinds <a id="id439" class="indexterm"/>of simulations in more <a id="id440" class="indexterm"/>detail in <a class="link" href="ch13.html" title="Chapter 13. Stochastic Dynamical Systems">Chapter 13</a>, <span class="emphasis"><em>Stochastic Dynamical Systems</em></span>.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's import NumPy and matplotlib:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import matplotlib.pyplot as plt
In [2]: %matplotlib inline</pre></div></li><li class="listitem">Let's create a function generating random +1 and -1 values in an array:<div class="informalexample"><pre class="programlisting">In [3]: def step(*shape):
            # Create a random n-vector with +1 or -1
            # values.
            return 2 * (np.random.random_sample(shape) 
                        &lt; .5) - 1</pre></div></li><li class="listitem">Now, let's write the simulation code in a cell starting with <code class="literal">%%prun</code> in order to profile the entire simulation. The various options allow us to save the report in a file and to sort the first 10 results by cumulative time. We will explain these options in more detail in the <span class="emphasis"><em>How it works…</em></span> section.<div class="informalexample"><pre class="programlisting">In [4]: %%prun -s cumulative -q -l 10 -T prun0
        n = 10000
        iterations = 50
        x = np.cumsum(step(iterations, n), axis=0)
        bins = np.arange(-30, 30, 1)
        y = np.vstack([np.histogram(x[i,:], bins)[0]
                       for i in range(iterations)])</pre></div></li><li class="listitem">The profiling report has been saved in a text file named <code class="literal">prun0</code>. Let's display it (the following output is a stripped down version that fits on this page):<div class="informalexample"><pre class="programlisting">In [5]: print(open('prun0', 'r').read())
             2960 function calls in 0.075 seconds
Ordered by: cumulative time
ncalls  cumtime  percall function
    50    0.037    0.001 histogram
     1    0.031    0.031 step
    50    0.024    0.000 sort
     1    0.019    0.019 rand
     1    0.005    0.005 cumsum</pre></div><p>Here, we observe the time taken by the different functions involved, directly or indirectly, in our code.</p></li><li class="listitem">If we run the exact same simulation with 500 iterations instead of 50, we obtain the following results:<div class="informalexample"><pre class="programlisting">   29510 function calls in 1.359 seconds
   ncalls  cumtime  percall function
      500    0.566    0.001 histogram
        1    0.388    0.388 cumsum
        1    0.383    0.383 step
      500    0.339    0.001 sort
        1    0.241    0.241 rand</pre></div><p>We can observe that the number of iterations has a big influence on the relative performance cost of the involved functions (notably <code class="literal">cumsum</code> here).</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec109"/>How it works...</h2></div></div></div><p>Python's profiler creates a <a id="id441" class="indexterm"/>detailed report of the execution time of our code, function<a id="id442" class="indexterm"/> by function. Here, we can observe the number of <a id="id443" class="indexterm"/>calls of the functions <code class="literal">histogram</code>, <code class="literal">cumsum</code>, <code class="literal">step</code>, <code class="literal">sort</code>, and <code class="literal">rand</code>, and the total time spent in those functions during the code's execution. Internal<a id="id444" class="indexterm"/> functions are also profiled. For each function, we get the total number of calls, the total and cumulative times, and their per-call counterparts (division by <code class="literal">ncalls</code>). The<a id="id445" class="indexterm"/> <span class="strong"><strong>total time</strong></span> represents how long the interpreter stays in a given function, <span class="emphasis"><em>excluding</em></span> the time spent in calls to subfunctions. The <span class="strong"><strong>cumulative time</strong></span><a id="id446" class="indexterm"/> is similar but <span class="emphasis"><em>includes</em></span> the time spent in calls to subfunctions. The filename, function name, and line number are displayed in the last column.</p><p>The <code class="literal">%prun</code> and <code class="literal">%%prun</code> magic commands accept multiple optional options (type <code class="literal">%prun?</code> for more details). In the example, <code class="literal">-s</code> allows us to <span class="strong"><strong>sort</strong></span> the report by a particular column, <code class="literal">-q</code> to suppress (<span class="strong"><strong>quell</strong></span>) the pager output (which is useful when we want to integrate the output in a notebook), <code class="literal">-l</code> to <span class="strong"><strong>limit</strong></span> the number of lines displayed or to filter the results by function name (which is useful when we are interested in a particular function), and <code class="literal">-T</code> to save the report in a <span class="strong"><strong>text</strong></span> file. In addition, we can choose to save (<span class="strong"><strong>dump</strong></span>) the binary report in a file with <code class="literal">-D</code>, or to <span class="strong"><strong>return</strong></span> it in IPython with <code class="literal">-r</code>. This database-like object contains all information about the profiling and can be analyzed through Python's <code class="literal">pstats</code> module.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note20"/>Note</h3><p>Every profiler brings its own overhead that can bias the profiling results (<span class="strong"><strong>probe effect</strong></span>). In other words, a profiled program may run significantly slower than a non-profiled program. That's a point to keep in mind.</p></div></div><div class="section" title="&quot;Premature optimization is the root of all evil&quot;"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec24"/>"Premature optimization is the root of all evil"</h3></div></div></div><p>As Donald Knuth's well-known <a id="id447" class="indexterm"/>quote suggests, optimizing code prematurely is generally considered a bad practice. Code optimization should only be conducted when it's really needed, that is, when the code is really too slow in normal situations. Additionally, we should know exactly where we need to optimize your code; typically, the vast majority of the execution time comprises a relatively small part of the code. The only way to find out is by profiling your code; optimization should never be done without preliminary profiling.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip13"/>Tip</h3><p>I was once dealing with some fairly complicated code that was slower than expected. I thought I had a pretty good idea of what was causing the problem and how I could resolve it. The solution would involve significant changes in the code. Fortunately, I first profiled my code, just to be sure. My diagnostic appeared to be utterly wrong; I had written somewhere <code class="literal">max(x)</code> instead of <code class="literal">np.max(x)</code> by mistake, where <code class="literal">x</code> was a very large vector. It was Python's built-in function that was called, instead of NumPy's heavily optimized routine for arrays. If I hadn't profiled my code, I would probably have missed this mistake forever. The program was working perfectly fine, only 150 times slower!</p></div></div><p>For more general advice on<a id="id448" class="indexterm"/> programming optimization, see <a class="ulink" href="http://en.wikipedia.org/wiki/Program_optimization">http://en.wikipedia.org/wiki/Program_optimization</a>.</p></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec110"/>There's more...</h2></div></div></div><p>Profiling code<a id="id449" class="indexterm"/> in IPython<a id="id450" class="indexterm"/> is particularly simple (especially in the notebook), as we have seen in this recipe. However, it may be undesirable or difficult to execute the code that we need to profile from IPython (GUIs, for example). In this case, we can use <code class="literal">cProfile</code> directly. It is slightly less straightforward than with IPython.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, we call the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ python -m cProfile -o profresults myscript.py</strong></span>
</pre></div><p>The file <code class="literal">profresults</code> will contain the dump of the profiling results of <code class="literal">myscript.py</code>.</p></li><li class="listitem">Then, we execute the following code from Python or IPython to display the profiling results in a human-readable form:<div class="informalexample"><pre class="programlisting">import pstats
p = pstats.Stats('profresults')
p.strip_dirs().sort_stats("cumulative").print_stats()</pre></div></li></ol></div><p>Explore the <a id="id451" class="indexterm"/>documentation of the <code class="literal">cProfile</code> and <code class="literal">pstats</code> modules to discover all of the analyses that<a id="id452" class="indexterm"/> you can perform on the profiling reports.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip14"/>Tip</h3><p>The repository at <a class="ulink" href="https://github.com/rossant/easy_profiler">https://github.com/rossant/easy_profiler</a> contains a simple command-line tool that facilitates the profiling of<a id="id453" class="indexterm"/> Python scripts.</p></div></div><p>There are a few GUI tools for exploring and visualizing the output of a profiling session. For example, <span class="strong"><strong>RunSnakeRun</strong></span><a id="id454" class="indexterm"/> allows you to view profile dumps in a GUI program.</p><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Documentation <a id="id455" class="indexterm"/>of <code class="literal">cProfile </code>and <code class="literal">pstats</code>, available <a id="id456" class="indexterm"/>at <a class="ulink" href="https://docs.python.org/3/library/profile.html">https://docs.python.org/3/library/profile.html</a></li><li class="listitem" style="list-style-type: disc">RunSnakeRun, <a id="id457" class="indexterm"/>at <a class="ulink" href="http://www.vrplumber.com/programming/runsnakerun/">www.vrplumber.com/programming/runsnakerun/</a></li><li class="listitem" style="list-style-type: disc">Python profiling tools, available <a id="id458" class="indexterm"/>at <a class="ulink" href="http://blog.ionelmc.ro/2013/06/08/python-profiling-tools/">http://blog.ionelmc.ro/2013/06/08/python-profiling-tools/</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec111"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Profiling your code line-by-line with line_profiler</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Profiling your code line-by-line with line_profiler"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec36"/>Profiling your code line-by-line with line_profiler</h1></div></div></div><p>Python's native <code class="literal">cProfile</code> module and the<a id="id459" class="indexterm"/> corresponding <code class="literal">%prun</code> magic break down the<a id="id460" class="indexterm"/> execution time of code <span class="emphasis"><em>function by function</em></span>. Sometimes, we may need an even more fine-grained analysis of code performance with a <span class="emphasis"><em>line-by-line</em></span> report. Such reports can be easier to read than the reports of <code class="literal">cProfile</code>.</p><p>To profile code line-by-line, we need an external Python module named <code class="literal">line_profiler</code> created by <a id="id461" class="indexterm"/>Robert Kern, available at <a class="ulink" href="http://pythonhosted.org/line_profiler/">http://pythonhosted.org/line_profiler/</a>. In this recipe, we will demonstrate how to use this module within IPython.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec112"/>Getting ready</h2></div></div></div><p>To install <code class="literal">line_profiler</code>, type <code class="literal">pip install line_profiler</code> in a terminal, or type <code class="literal">!pip install line_profiler</code> in IPython (you need a C compiler).</p><p>On Windows, you can use Chris Gohlke's unofficial package available at <a class="ulink" href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#line_profiler">www.lfd.uci.edu/~gohlke/pythonlibs/#line_profiler</a>.</p></div><div class="section" title="How do to it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec113"/>How do to it...</h2></div></div></div><p>We will profile the same simulation code as in the previous recipe, line-by-line:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, let's import NumPy and the <code class="literal">line_profiler</code> IPython extension module that comes with the package:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
In [2]: %load_ext line_profiler</pre></div></li><li class="listitem">This IPython <a id="id462" class="indexterm"/>extension module provides a <code class="literal">%lprun</code> magic <a id="id463" class="indexterm"/>command to profile a Python function line-by-line. It works best when the function is defined in a file and not in the interactive namespace or in the notebook. Therefore, here we write our code in a Python script using the <code class="literal">%%writefile</code> cell magic:<div class="informalexample"><pre class="programlisting">In [3]: %%writefile simulation.py
        import numpy as np 
        def step(*shape):
            # Create a random n-vector with +1 or -1 
            # values.
            return (2 * (np.random.random_sample(shape) 
                         &lt; .5) - 1)
        def simulate(iterations, n=10000): 
            s = step(iterations, n)
            x = np.cumsum(s, axis=0)
            bins = np.arange(-30, 30, 1)
            y = np.vstack([np.histogram(x[i,:], bins)[0] 
                           for i in range(iterations)])
            return y</pre></div></li><li class="listitem">Now, let's import this script into the interactive namespace so that we can execute and profile our code:<div class="informalexample"><pre class="programlisting">In [4]: import simulation</pre></div></li><li class="listitem">We execute the function under the control of the line profiler. The functions to be profiled need to be explicitly specified in the <code class="literal">%lprun</code> magic command. We also save the report in a file, <code class="literal">lprof0</code>:<div class="informalexample"><pre class="programlisting">In [5]: %lprun -T lprof0 -f simulation.simulate simulation.simulate(50)</pre></div></li><li class="listitem">Let's display the report (the following output is a stripped-down version that fits in the page):<div class="informalexample"><pre class="programlisting">In [6]: print(open('lprof0', 'r').read())
File: simulation.py
Function: simulate at line 7
Total time: 0.114508 s
Line #   % Time  Line Contents
     7           def simulate(iterations, n=10000):
     8    36.3       s = step(iterations, n)
     9     5.6       x = np.cumsum(s, axis=0)
    10     0.1       bins = np.arange(-30, 30, 1)
    11    58.1       y = np.vstack([np.histogram(...)])
    12     0.0       return y</pre></div></li><li class="listitem">If we perform the<a id="id464" class="indexterm"/> same analysis with 10 times the previous<a id="id465" class="indexterm"/> number of iterations (<code class="literal">simulation.simulate(500)</code>), we get the following report:<div class="informalexample"><pre class="programlisting">Total time: 1.28704 s
     7           def simulate(iterations, n=10000):
     8    29.2       s = step(iterations, n)
     9    30.9       x = np.cumsum(s, axis=0)
    10     0.0       bins = np.arange(-30, 30, 1)
    11    39.9       y = np.vstack([np.histogram(...)])
    12     0.0       return y</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec114"/>How it works...</h2></div></div></div><p>The <code class="literal">%lprun</code> command<a id="id466" class="indexterm"/> accepts a Python statement as its main argument. The functions to profile need to be explicitly specified with <code class="literal">-f</code>. Other optional arguments include <code class="literal">-D</code>, <code class="literal">-T</code>, and <code class="literal">-r</code>, and they work in a similar way to their <code class="literal">%prun</code> magic command counterparts.</p><p>The <code class="literal">line_profiler</code> module displays the time spent on each line of the profiled functions, either in timer units or as a fraction of the total execution time. These details are essential when we are looking for hotspots in our code.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec115"/>There's more...</h2></div></div></div><p>As in the<a id="id467" class="indexterm"/> previous <a id="id468" class="indexterm"/>recipe, there may be a need to run the line-by-line profiler on a standalone Python program that cannot be launched easily from IPython. The procedure is a bit convoluted.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We download the <code class="literal">kernprof</code> file<a id="id469" class="indexterm"/> from <a class="ulink" href="https://github.com/rkern/line_profiler/blob/master/kernprof.py">https://github.com/rkern/line_profiler/blob/master/kernprof.py</a>, and save it in your code's directory.</li><li class="listitem">In the code, we decorate the functions we wish to profile with <code class="literal">@profile</code>. We need to remove these decorators at the end of the profiling session, as they will raise <code class="literal">NameError</code> exceptions if the code is executed normally (that is, not under the control of the line profiler):<div class="informalexample"><pre class="programlisting">@profile
def thisfunctionneedstobeprofiled():
    pass</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip15"/>Tip</h3><p>See also the <a class="ulink" href="http://stackoverflow.com/questions/18229628/python-profiling-using-line-profiler-clever-way-to-remove-profile-statements">http://stackoverflow.com/questions/18229628/python-profiling-using-line-profiler-clever-way-to-remove-profile-statements</a> link for a clever way to remove profile statements.</p></div></div></li><li class="listitem">We execute the following command in a terminal:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python -m kernprof -l -v myscript.py &gt; lprof.txt</strong></span>
</pre></div><p>The <code class="literal">myscript.py</code> script will be executed, and the report will be saved in <code class="literal">lprof.txt</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip16"/>Tip</h3><p>The repository at <a class="ulink" href="https://github.com/rossant/easy_profiler">https://github.com/rossant/easy_profiler</a> offers a slightly simpler way of using the line-by-line profiler.</p></div></div></li></ol></div><div class="section" title="Tracing the step-by-step execution of a Python program"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec25"/>Tracing the step-by-step execution of a Python program</h3></div></div></div><p>Let's also talk about <span class="strong"><strong>tracing</strong></span> tools<a id="id470" class="indexterm"/> for Python, which can be useful for profiling or debugging a program, or for educational purposes.</p><p>Python's <code class="literal">trace</code> module allows us to trace program execution of Python code. That's extremely useful during in-depth debugging and profiling sessions. We can follow the entire sequence of instructions executed by the Python interpreter. More information on the trace module<a id="id471" class="indexterm"/> is available at <a class="ulink" href="https://docs.python.org/3/library/trace.html">https://docs.python.org/3/library/trace.html</a>.</p><p>In addition, the <span class="strong"><strong>Online Python Tutor</strong></span><a id="id472" class="indexterm"/> is an online interactive educational tool that can <a id="id473" class="indexterm"/>help us understand what the Python interpreter is doing step-by-step as it executes a program's source code. The Online Python Tutor is <a id="id474" class="indexterm"/>available at <a class="ulink" href="http://pythontutor.com/">http://pythontutor.com/</a>.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec116"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Profiling your code easily with cProfile and IPython</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Profiling the memory usage of your code with memory_profiler</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Profiling the memory usage of your code with memory_profiler"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec37"/>Profiling the memory usage of your code with memory_profiler</h1></div></div></div><p>The methods described in the previous<a id="id475" class="indexterm"/> recipe were about CPU<a id="id476" class="indexterm"/> time profiling. That may be the most obvious factor when it comes to code profiling. However, memory is also a critical factor. For instance, running <code class="literal">np.zeros(500000000)</code> is likely to instantaneously crash your computer! This command may allocate more memory than is available on your system; your computer will then reach a nonresponsive state within seconds.</p><p>Writing memory-optimized code is not trivial and can really make your program faster. This is particularly important when dealing with large NumPy arrays, as we will see later in this chapter.</p><p>In this recipe, we will look at a simple memory profiler. This library, unsurprisingly called <code class="literal">memory_profiler</code>, was created by Fabian Pedregosa. Its usage is very similar to <code class="literal">line_profiler</code>, and it can be conveniently used from IPython. You can <a id="id477" class="indexterm"/>download it from <a class="ulink" href="https://pypi.python.org/pypi/memory_profiler">https://pypi.python.org/pypi/memory_profiler</a>.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec117"/>Getting ready</h2></div></div></div><p>You can install <code class="literal">memory_profiler</code> with <code class="literal">pip install memory_profiler</code>. </p><p>On Windows, you also need <code class="literal">psutil</code>, which is<a id="id478" class="indexterm"/> available at <a class="ulink" href="https://pypi.python.org/pypi/psutil">https://pypi.python.org/pypi/psutil</a>. You can install it with <code class="literal">pip install psutil</code>, or by downloading the package from <a class="ulink" href="https://code.google.com/p/psutil/">https://code.google.com/p/psutil/</a>. You can also download an installer from <a id="id479" class="indexterm"/>Chris Gohlke's webpage at <a class="ulink" href="http://www.lfd.uci.edu/~gohlke/pythonlibs/">www.lfd.uci.edu/~gohlke/pythonlibs/</a>.</p><p>The example in this recipe is the continuation of the previous recipe.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec118"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Assuming that the <a id="id480" class="indexterm"/>simulation <a id="id481" class="indexterm"/>code has been loaded as shown in the previous recipe, we load the memory profiler IPython extension:<div class="informalexample"><pre class="programlisting">In [9]: %load_ext memory_profiler</pre></div></li><li class="listitem">Now, let's run the code under the control of the memory profiler:<div class="informalexample"><pre class="programlisting">In [10]: %mprun -T mprof0 -f simulation.simulate simulation.simulate(50)</pre></div></li><li class="listitem">Let's show the results:<div class="informalexample"><pre class="programlisting">In [11]: print(open('mprof0', 'r').read())
Filename: simulation.py
Line #    Mem usage  Increment   Line Contents
     7    39.672 MB  0.000 MB   def simulate(...):
     8    41.977 MB  2.305 MB       s = step(iterations, n)
     9    43.887 MB  1.910 MB       x = np.cumsum(...)
    10    43.887 MB  0.000 MB       bins = np.arange(...)
    11    43.887 MB  0.000 MB       y = np.vstack(...)
    12    43.887 MB  0.000 MB       return y</pre></div></li><li class="listitem">Finally, here is the report with 500 iterations:<div class="informalexample"><pre class="programlisting">Line #    Mem usage Increment   Line Contents
     7    40.078 MB  0.000 MB   def simulate(...):
     8    59.191 MB 19.113 MB       s = step(iterations, n)
     9    78.301 MB 19.109 MB       x = np.cumsum(...)
    10    78.301 MB  0.000 MB       bins = np.arange(...)
    11    78.301 MB  0.000 MB       y = np.vstack(...)
    12    78.301 MB  0.000 MB       return y</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec119"/>How it works...</h2></div></div></div><p>The <code class="literal">memory_profiler</code> package<a id="id482" class="indexterm"/> checks the memory usage of the interpreter at every line. The <span class="strong"><strong>increment</strong></span> column allows us to spot those places in the code where large amounts of memory are allocated. This is especially important when working with arrays. Unnecessary array creations and copies can considerably slow down a program. We will tackle this issue in the next few recipes.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec120"/>There's more...</h2></div></div></div><p>We can use <code class="literal">memory_profiler</code> without IPython, and we can also use a quick memory benchmark in IPython for single commands.</p><div class="section" title="Using memory_profiler for standalone Python programs"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec26"/>Using memory_profiler for standalone Python programs</h3></div></div></div><p>Using the memory profiler with <a id="id483" class="indexterm"/>standalone Python programs is<a id="id484" class="indexterm"/> similar but slightly simpler than with <code class="literal">line_profiler</code>.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, in our Python scripts, we decorate the functions we wish to profile with <code class="literal">@profile</code>.</li><li class="listitem">Then, we run:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ python -m memory_profiler myscript.py &gt; mprof.txt</strong></span>
</pre></div><p>The profiling report will be saved in <code class="literal">myprof.txt</code>.</p></li></ol></div></div><div class="section" title="Using the %memit magic command in IPython"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec27"/>Using the %memit magic command in IPython</h3></div></div></div><p>The <code class="literal">memory_profiler</code> IPython extension <a id="id485" class="indexterm"/>also comes with a <code class="literal">%memit</code> magic command that lets us benchmark the memory used by a single Python statement. Here<a id="id486" class="indexterm"/> is a simple example:</p><div class="informalexample"><pre class="programlisting">In [14]: %memit np.random.randn(1000, 1000)
maximum of 1: 46.199219 MB per loop</pre></div></div><div class="section" title="Other tools"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec28"/>Other tools</h3></div></div></div><p>There are other tools to monitor the memory usage of a Python program, notably <a id="id487" class="indexterm"/>Guppy-PE (<a class="ulink" href="http://guppy-pe.sourceforge.net/">http://guppy-pe.sourceforge.net/</a>), PySizer<a id="id488" class="indexterm"/> (<a class="ulink" href="http://pysizer.8325.org/">http://pysizer.8325.org/</a>), and Pympler<a id="id489" class="indexterm"/> (<a class="ulink" href="https://code.google.com/p/pympler/">https://code.google.com/p/pympler/</a>). Used in conjunction with IPython and Python's introspection capabilities, these tools allow you to analyze the memory usage of a namespace or a particular object.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec121"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Profiling your code line-by-line with line_profiler</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Understanding the internals of NumPy to avoid unnecessary array copying</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Understanding the internals of NumPy to avoid unnecessary array copying"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec38"/>Understanding the internals of NumPy to avoid unnecessary array copying</h1></div></div></div><p>We can achieve significant performance speedups with NumPy over native Python code, particularly when our computations follow the <span class="strong"><strong>Single Instruction, Multiple Data</strong></span> (<span class="strong"><strong>SIMD</strong></span>)<a id="id490" class="indexterm"/> paradigm. However, it is also possible to unintentionally write non-optimized code with NumPy.</p><p>In the next few recipes, we will<a id="id491" class="indexterm"/> see some tricks that can help us write optimized NumPy code. In this recipe, we will see how to avoid unnecessary array copies in order to save memory. In that respect, we will need to dig into the internals of NumPy.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec122"/>Getting ready</h2></div></div></div><p>First, we need a way to check whether two arrays share the same underlying data buffer in memory. Let's define a function <code class="literal">id()</code> that returns the memory location of the underlying data buffer:</p><div class="informalexample"><pre class="programlisting">def id(x):
    # This function returns the memory
    # block address of an array.
    return x.__array_interface__['data'][0]</pre></div><p>Two arrays with the same data location (as returned by <code class="literal">id</code>) share the same underlying data buffer. However, the opposite is true only if the arrays have the same <span class="strong"><strong>offset</strong></span><a id="id492" class="indexterm"/> (meaning that they have the same first element). Two shared arrays with different offsets will have slightly different memory locations, as shown in the following example:</p><div class="informalexample"><pre class="programlisting">In [1]: id(a), id(a[1:])
Out[1]: (71211328, 71211336)</pre></div><p>In the next few recipes, we'll make sure to use this method with arrays that have the same offset. Here is a more general and reliable solution for finding out whether two arrays share the same data:</p><div class="informalexample"><pre class="programlisting">In [2]: def get_data_base(arr):
            """For a given Numpy array, finds the
            base array that "owns" the actual data."""
            base = arr
            while isinstance(base.base, np.ndarray):
                base = base.base
            return base
        
        def arrays_share_data(x, y):
            return get_data_base(x) is get_data_base(y)

In [3]: print(arrays_share_data(a,a.copy()),
              arrays_share_data(a,a[1:]))
False True</pre></div><p>Thanks to Michael Droettboom for pointing this out and proposing this alternative solution.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec123"/>How to do it...</h2></div></div></div><p>Computations with NumPy arrays may involve internal copies between blocks of memory. These copies are not always necessary, in which case they should be avoided, as we will see in the following tips:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We may sometimes need to make a copy of an array; for instance, if we need to manipulate an array while keeping an original copy in memory:<div class="informalexample"><pre class="programlisting">In [3]: a = np.zeros(10); aid = id(a); aid
Out[3]: 65527008L
In [4]: b = a.copy(); id(b) == aid
Out[4]: False</pre></div></li><li class="listitem">Array <a id="id493" class="indexterm"/>computations can involve in-place operations (the first example in the following code: the array is modified) or implicit-copy operations (the second example: a new array is created):<div class="informalexample"><pre class="programlisting">In [5]: a *= 2; id(a) == aid
Out[5]: True
In [6]: a = a*2; id(a) == aid
Out[6]: False</pre></div><p>Be sure to choose the type of operation you actually need. Implicit-copy operations are significantly slower, as shown here:</p><div class="informalexample"><pre class="programlisting">In [7]: %%timeit a = np.zeros(10000000)
        a *= 2
10 loops, best of 3: 23.9 ms per loop
In [8]: %%timeit a = np.zeros(10000000)
        a = a*2
10 loops, best of 3: 77.9 ms per loop</pre></div></li><li class="listitem">Reshaping an array may or may not involve a copy. The reasons will be explained in the <span class="emphasis"><em>How it works...</em></span> section. For instance, reshaping a 2D matrix does not involve a copy, unless it is<a id="id494" class="indexterm"/> transposed (or more generally, <span class="strong"><strong>non-contiguous</strong></span>):<div class="informalexample"><pre class="programlisting">In [9]: a = np.zeros((10, 10)); aid = id(a)
In [10]: b = a.reshape((1, -1)); id(b) == aid
Out[10]: True
In [11]: c = a.T.reshape((1, -1)); id(c) == aid
Out[11]: False</pre></div><p>Therefore, the latter instruction will be significantly slower than the former.</p></li><li class="listitem">Both the <code class="literal">flatten</code> and the <code class="literal">ravel</code> methods of an array reshape it into a 1D vector (a flattened array). However, the <code class="literal">flatten</code> method always returns a copy, and the <code class="literal">ravel</code> method returns a copy only if necessary (thus it's faster, especially with large arrays).<div class="informalexample"><pre class="programlisting">In [12]: d = a.flatten(); id(d) == aid
Out[12]: False
In [13]: e = a.ravel(); id(e) == aid
Out[13]: True
In [14]: %timeit a.flatten()
1000000 loops, best of 3: 1.65 µs per loop
In [15]: %timeit a.ravel()
1000000 loops, best of 3: 566 ns per loop</pre></div></li><li class="listitem"><span class="strong"><strong>Broadcasting rules</strong></span> <a id="id495" class="indexterm"/>allow us to<a id="id496" class="indexterm"/> make computations on arrays with different but compatible shapes. In other words, we don't always need to reshape or tile our arrays to make their shapes match. The following example illustrates two ways of doing an <span class="strong"><strong>outer product</strong></span> between two vectors: the first method involves array tiling, the second one (faster) involves broadcasting:<div class="informalexample"><pre class="programlisting">In [16]: n = 1000
In [17]: a = np.arange(n)
         ac = a[:, np.newaxis]  # Column vector.
         ar = a[np.newaxis, :]  # Row vector.
In [18]: %timeit np.tile(ac, (1, n)) * np.tile(ar, (n, 1))
10 loops, best of 3: 25 ms per loop
In [19]: %timeit ar * ac
100 loops, best of 3: 4.63 ms per loop</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec124"/>How it works...</h2></div></div></div><p>In this section, we will see what happens under the hood when using NumPy, and how this knowledge allows us to understand the tricks given in this recipe.</p><div class="section" title="Why are NumPy arrays efficient?"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec29"/>Why are NumPy arrays efficient?</h3></div></div></div><p>A NumPy array<a id="id497" class="indexterm"/> is basically described by metadata (notably the number of dimensions, the shape, and the data type) and the actual data. The data is stored in a homogeneous and contiguous block of memory, at a particular address in system<a id="id498" class="indexterm"/> memory (<span class="strong"><strong>Random Access Memory</strong></span>, or <span class="strong"><strong>RAM</strong></span>). This block of memory is called the <a id="id499" class="indexterm"/><span class="strong"><strong>data buffer</strong></span>. This is the main difference when compared to a pure Python structure, such as a list, where the items are scattered across the system memory. This aspect is the critical feature that makes NumPy arrays so efficient.</p><p>Why is this so important? Here are the <a id="id500" class="indexterm"/>main reasons:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Computations on arrays can be written very efficiently in a low-level language such as C (and a large part of NumPy is actually written in C). Knowing the address of the memory block and the data type, it is just simple arithmetic to loop over all items, for example. There would be a significant overhead to do that in Python with a list.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Spatial locality</strong></span><a id="id501" class="indexterm"/> in memory access patterns results in performance gains notably due to the CPU cache. Indeed, the cache loads bytes in chunks from RAM to the CPU registers. Adjacent items are then loaded very efficiently (<span class="strong"><strong>sequential locality</strong></span>, <a id="id502" class="indexterm"/>or <a id="id503" class="indexterm"/><span class="strong"><strong>locality of reference</strong></span>).</li><li class="listitem" style="list-style-type: disc">Finally, the fact that items are stored contiguously in memory allows NumPy to take advantage of <span class="strong"><strong>vectorized instructions</strong></span><a id="id504" class="indexterm"/> of modern CPUs, such as Intel's <span class="strong"><strong>SSE</strong></span><a id="id505" class="indexterm"/> and <a id="id506" class="indexterm"/><span class="strong"><strong>AVX</strong></span>, AMD's XOP, and so on. For example, multiple consecutive floating point numbers can be loaded in 128, 256, or 512 bits registers for vectorized arithmetical computations implemented as CPU instructions.<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note21"/>Note</h3><p>Additionally, NumPy can be linked to highly optimized linear algebra libraries such as <span class="strong"><strong>BLAS</strong></span><a id="id507" class="indexterm"/> and <span class="strong"><strong>LAPACK</strong></span><a id="id508" class="indexterm"/> through <span class="strong"><strong>ATLAS</strong></span><a id="id509" class="indexterm"/> or the <span class="strong"><strong>Intel Math Kernel Library</strong></span> (<span class="strong"><strong>MKL</strong></span>)<a id="id510" class="indexterm"/>. A few specific matrix computations may also be multithreaded, taking advantage of the power of modern multicore processors.</p></div></div></li></ul></div><p>In conclusion, storing data in a contiguous block of memory ensures that the architecture of modern CPUs is used optimally, in terms of memory access patterns, CPU cache, and vectorized instructions.</p></div><div class="section" title="What is the difference between in-place and implicit-copy operations?"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec30"/>What is the difference between in-place and implicit-copy operations?</h3></div></div></div><p>Let's explain the example in<a id="id511" class="indexterm"/> step 2. An expression<a id="id512" class="indexterm"/> such as <code class="literal">a *= 2</code> corresponds to an in-place operation, where all values of the array are multiplied by two. By contrast, <code class="literal">a = a*2</code> means that a new array containing the values of <code class="literal">a*2</code> is created, and the variable <code class="literal">a</code> now points to this new array. The old array becomes unreferenced and will be deleted by the garbage collector. No memory allocation happens in the first case, contrary to the second case.</p><p>More generally, expressions such as <code class="literal">a[i:j]</code> are <span class="strong"><strong>views</strong></span><a id="id513" class="indexterm"/> to parts of an array; they point to the memory buffer containing the data. Modifying them with in-place operations changes the original array. Hence, <code class="literal">a[:] = a*2</code> results in an in-place operation, unlike <code class="literal">a = a*2</code>.</p><p>Knowing this subtlety of NumPy can help you fix some bugs (where an array is implicitly and unintentionally modified because of an operation on a view), and optimize the speed and memory consumption of your code by reducing the number of unnecessary copies.</p></div><div class="section" title="Why can't some arrays be reshaped without a copy?"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec31"/>Why can't some arrays be reshaped without a copy?</h3></div></div></div><p>We explain the example in step 3 here, where a transposed 2D matrix cannot be flattened without a copy. A 2D matrix contains items indexed by two numbers (row and column), but it is stored internally as a 1D contiguous block of memory, accessible with a single number. There is more than one way of storing matrix items in a 1D block of memory: we can put the elements of the first row first, then the second row, and so on, or the elements of the first column first, then the second column, and so on. The first method is called<a id="id514" class="indexterm"/> <span class="strong"><strong>row-major order</strong></span>, whereas the latter is called <a id="id515" class="indexterm"/><span class="strong"><strong>column-major order</strong></span>. Choosing between the two methods is only a matter of internal convention: NumPy uses the row-major order, like C, but unlike FORTRAN.</p><div class="mediaobject"><img src="images/4818OS_04_01.jpg" alt="Why can't some arrays be reshaped without a copy?"/><div class="caption"><p>Internal array layouts: row-major and column-major orders</p></div></div><p>More generally, NumPy uses the notion of <code class="literal">strides</code> to convert between a multidimensional index and the memory location of the underlying (1D) sequence of elements. The specific mapping between <code class="literal">array[i1, i2]</code> and the relevant byte address of the internal data is given by:</p><div class="informalexample"><pre class="programlisting">offset = array.strides[0] * i1 + array.strides[1] * i2</pre></div><p>When reshaping an array, NumPy avoids copies when possible by modifying the <code class="literal">strides</code> attribute. For example, when transposing a matrix, the order of strides is reversed, but the underlying data remains identical. However, flattening a transposed array cannot be accomplished simply by modifying strides (try it!), so a copy is needed (thanks to Chris Beaumont for clarifying an earlier version of this paragraph).</p><p>Internal array layout can also explain some unexpected performance discrepancies between very similar NumPy operations. As a small exercise, can you explain the following benchmarks?</p><div class="informalexample"><pre class="programlisting">In [20]: a = np.random.rand(5000, 5000)
In [21]: %timeit a[0,:].sum()
100000 loops, best of 3: 17.9 µs per loop
In [22]: %timeit a[:,0].sum()
10000 loops, best of 3: 60.6 µs per loop</pre></div></div><div class="section" title="What are NumPy broadcasting rules?"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec32"/>What are NumPy broadcasting rules?</h3></div></div></div><p>Broadcasting rules <a id="id516" class="indexterm"/>describe how arrays with different dimensions and/or shapes can be used for computations. The general rule is that <span class="emphasis"><em>two dimensions are compatible when they are equal, or when one of them is 1</em></span>. NumPy uses this rule to compare the shapes of the two arrays element-wise, starting with the trailing dimensions and working its way forward. The smallest dimension is internally stretched to match the other dimension, but this operation does not involve any memory copy.</p></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec125"/>There's more...</h2></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Broadcasting rules and <a id="id517" class="indexterm"/>examples, available at <a class="ulink" href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a></li><li class="listitem" style="list-style-type: disc">Array interface in <a id="id518" class="indexterm"/>NumPy, at <a class="ulink" href="http://docs.scipy.org/doc/numpy/reference/arrays.interface.html">http://docs.scipy.org/doc/numpy/reference/arrays.interface.html</a></li><li class="listitem" style="list-style-type: disc">Locality of<a id="id519" class="indexterm"/> reference, at <a class="ulink" href="http://en.wikipedia.org/wiki/Locality_of_reference">http://en.wikipedia.org/wiki/Locality_of_reference</a></li><li class="listitem" style="list-style-type: disc">Internals of NumPy in the SciPy lectures notes, available at <a class="ulink" href="http://scipy-lectures.github.io/advanced/advanced_numpy/">http://scipy-lectures.github.io/advanced/advanced_numpy/</a></li><li class="listitem" style="list-style-type: disc">100 NumPy exercises by Nicolas Rougier, available<a id="id520" class="indexterm"/> at <a class="ulink" href="http://www.loria.fr/~rougier/teaching/numpy.100/index.html">www.loria.fr/~rougier/teaching/numpy.100/index.html</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec126"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Using stride tricks with NumPy</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Using stride tricks with NumPy"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec39"/>Using stride tricks with NumPy</h1></div></div></div><p>In this recipe, we will dig deeper into the internals of NumPy arrays, by generalizing the notion of row-major and column-major orders to multidimensional arrays. The general notion is that of <a id="id521" class="indexterm"/><span class="strong"><strong>strides</strong></span>, which<a id="id522" class="indexterm"/> describe how the items of a multidimensional <a id="id523" class="indexterm"/>array are organized within a one-dimensional data buffer. Strides are mostly an implementation detail, but they can also be used in specific situations to optimize some algorithms.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec127"/>Getting ready</h2></div></div></div><p>We suppose that NumPy has been imported and that the <code class="literal">id</code> function has been defined (see the previous recipe, <span class="emphasis"><em>Understanding the internals of NumPy to avoid unnecessary array copying</em></span>).</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec128"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Strides are integer numbers describing the byte step in the contiguous block of memory for each dimension.<div class="informalexample"><pre class="programlisting">In [3]: x = np.zeros(10); x.strides
Out[3]: (8,)</pre></div><p>This vector <code class="literal">x</code> contains double-precision floating point numbers (float64, 8 bytes); one needs to go <span class="emphasis"><em>8 bytes forward</em></span> to go from one item to the next.</p></li><li class="listitem">Now, let's look at the strides of a 2D array:<div class="informalexample"><pre class="programlisting">In [4]: y = np.zeros((10, 10)); y.strides
Out[4]: (80, 8)</pre></div><p>In the first dimension (vertical), one needs to go <span class="emphasis"><em>80 bytes</em></span> (10 float64 items) <span class="emphasis"><em>forward</em></span> to go from one item to the next, because the items are internally stored in row-major order. In the second dimension (horizontal), one needs to go <span class="emphasis"><em>8 bytes</em></span> <span class="emphasis"><em>forward</em></span> to go from one item to the next.</p></li><li class="listitem">Let's show how <a id="id524" class="indexterm"/>we can revisit the broadcasting rules<a id="id525" class="indexterm"/> from the previous recipe using strides:<div class="informalexample"><pre class="programlisting">In [5]: n = 1000; a = np.arange(n)</pre></div><p>We will create a new array, <code class="literal">b</code>, pointing to the same memory block as <code class="literal">a</code>, but with a different shape and different strides. This new array will look like a vertically-tiled version of <code class="literal">a</code>. We use a special function in NumPy to change the strides of an array:</p><div class="informalexample"><pre class="programlisting">In [6]: b = np.lib.stride_tricks.as_strided(a, (n, n), 
                                           (0, 4))
Out[7]: array([[ 0, 1, 2, ..., 997, 998, 999],
             ..., 
             [ 0, 1, 2, ..., 997, 998, 999]])
In [8]: b.size, b.shape, b.nbytes
Out[8]: (1000000, (1000, 1000), 4000000)</pre></div><p>NumPy believes that this array contains one million different elements, whereas the data buffer actually contains the same 1000 elements as <code class="literal">a</code>.</p></li><li class="listitem">We can now perform an efficient outer product using the same principle as with broadcasting rules:<div class="informalexample"><pre class="programlisting">In [9]: %timeit b * b.T
100 loops, best of 3: 5.03 ms per loop
In [10]: %timeit np.tile(a, (n, 1)) \
               * np.tile(a[:, np.newaxis], (1, n))
10 loops, best of 3: 28 ms per loop</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec129"/>How it works...</h2></div></div></div><p>Every array has<a id="id526" class="indexterm"/> a number of dimensions, a shape, a data<a id="id527" class="indexterm"/> type, and strides. Strides describe how the items of a multidimensional array are organized in the data buffer. There are many different schemes for arranging the items of a multidimensional array in a one-dimensional block. NumPy implements a<a id="id528" class="indexterm"/> <span class="strong"><strong>strided indexing scheme</strong></span>, where the position of any element is a <span class="strong"><strong>linear combination</strong></span><a id="id529" class="indexterm"/> of the dimensions, the coefficients being the strides. In other words, strides describe, in any dimension, how many bytes we need to jump over in the data buffer to go from one item to the next.</p><p>The position of any element in a multidimensional array is given by a linear combination of its indices, as follows:</p><div class="mediaobject"><img src="images/4818OS_04_02.jpg" alt="How it works..."/></div><p>Artificially changing the strides allows us to make some array operations more efficient than with standard methods, which may involve array copies. Internally, that's how broadcasting works in NumPy.</p><p>The <code class="literal">as_strided</code> method<a id="id530" class="indexterm"/> takes an array, a shape, and strides as arguments. It creates a new array, but uses the same data buffer as the original array. The only thing that changes is the metadata. This trick lets us manipulate NumPy arrays as usual, except that they may take much less memory than what NumPy thinks. Here, using 0 in the strides implies that any array item can be addressed by many multidimensional indices, resulting in memory savings.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip17"/>Tip</h3><p>Be careful with strided arrays! The <code class="literal">as_strided</code> function does not check whether you stay inside the memory block bounds. This means that you need to handle edge effects manually; otherwise, you may end up with garbage values in your arrays.</p></div></div><p>We will see a more useful application of stride tricks in the next recipe.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec130"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Implementing an efficient rolling average algorithm with stride tricks</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Implementing an efficient rolling average algorithm with stride tricks"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec40"/>Implementing an efficient rolling average algorithm with stride tricks</h1></div></div></div><p>Stride tricks can be <a id="id531" class="indexterm"/>useful for local <a id="id532" class="indexterm"/>computations on arrays, when the computed value at a given position depends on the neighboring values. Examples include dynamical systems, digital filters, and cellular automata.</p><p>In this recipe, we will implement an efficient <span class="strong"><strong>rolling average</strong></span><a id="id533" class="indexterm"/> algorithm (a particular type of convolution-based linear filter) with NumPy stride tricks. A rolling average of a 1D vector contains, at each position, the average of the elements around this position in the original vector. Roughly speaking, this process filters out the noisy components of a signal so as to keep only the slower components.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec131"/>Getting ready</h2></div></div></div><p>Make sure to reuse the <code class="literal">id()</code> function from the <span class="emphasis"><em>Understanding the internals of NumPy to avoid unnecessary array copying</em></span> recipe. This function returns the memory address of the internal data buffer of a NumPy array.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec132"/>How to do it...</h2></div></div></div><p>The idea is to start from a 1D vector, and make a <span class="emphasis"><em>virtual</em></span> 2D array where each line is a shifted version of the previous line. When using stride tricks, this process is very efficient as it does not involve any copy.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's generate a 1D vector:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        from numpy.lib.stride_tricks import as_strided
In [2]: n = 5; k = 2
In [3]: a = np.linspace(1, n, n); aid = id(a)</pre></div></li><li class="listitem">Let's change the strides of <code class="literal">a</code> to add shifted rows:<div class="informalexample"><pre class="programlisting">In [4]: as_strided(a, (k, n), (8, 8))
Out[4]: array([[ 1e+00,  2e+00,  3e+00,  4e+00,  5e+00],
               [ 2e+00,  3e+00,  4e+00,  5e+00, -1e-23]])</pre></div><p>The last value indicates an out-of-bounds problem: stride tricks can be dangerous as memory access is not checked. Here, we should take edge effects into account by limiting the shape of the array.</p><div class="informalexample"><pre class="programlisting">In [5]: as_strided(a, (k, n-k+1), (8, 8))
Out[5]: array([[ 1.,  2.,  3.,  4.],
               [ 2.,  3.,  4.,  5.]])</pre></div></li><li class="listitem">Now, let's implement the computation of the rolling average. The first version (standard method) involves explicit array copies, whereas the second version uses stride tricks:<div class="informalexample"><pre class="programlisting">In [6]: def shift1(x, k):
            return np.vstack([x[i:n-k+i+1] for i in 
                              range(k)])
In [7]: def shift2(x, k):
            return as_strided(x, (k, n-k+1), 
                              (x.itemsize,)*2)</pre></div></li><li class="listitem">These two functions return the same result, except that the array returned by the second function refers to the original data buffer:<div class="informalexample"><pre class="programlisting">In [8]: b = shift1(a, k); b, id(b) == aid
Out[8]: (array([[ 1.,  2.,  3.,  4.],
                [ 2.,  3.,  4.,  5.]]), False) 
In [9]: c = shift2(a, k); c, id(c) == aid
Out[9]: (array([[ 1.,  2.,  3.,  4.],
                [ 2.,  3.,  4.,  5.]]), True)</pre></div></li><li class="listitem">Let's <a id="id534" class="indexterm"/>generate <a id="id535" class="indexterm"/>a signal:<div class="informalexample"><pre class="programlisting">In [10]: n, k = 100, 10
         t = np.linspace(0., 1., n)
         x = t + .1 * np.random.randn(n)</pre></div></li><li class="listitem">We compute the signal rolling average by creating the shifted version of the signal, and averaging along the vertical dimension. The result is shown in the next figure:<div class="informalexample"><pre class="programlisting">In [11]: y = shift2(x, k)
         x_avg = y.mean(axis=0)</pre></div><div class="mediaobject"><img src="images/4818OS_04_03.jpg" alt="How to do it..."/><div class="caption"><p>A signal and its rolling average</p></div></div></li><li class="listitem">Let's evaluate <a id="id536" class="indexterm"/>the time <a id="id537" class="indexterm"/>taken by the first method:<div class="informalexample"><pre class="programlisting">In [15]: %timeit shift1(x, k)
10000 loops, best of 3: 163 µs per loop
In [16]: %%timeit y = shift1(x, k)
         z = y.mean(axis=0)
10000 loops, best of 3: 63.8 µs per loop</pre></div></li><li class="listitem">And by the second method:<div class="informalexample"><pre class="programlisting">In [17]: %timeit shift2(x, k)
10000 loops, best of 3: 23.3 µs per loop
In [18]: %%timeit y = shift2(x, k)
         z = y.mean(axis=0)
10000 loops, best of 3: 35.8 µs per loop</pre></div><p>In the first version, most of the time is spent in the array copy, whereas in the stride trick version, most of the time is instead spent in the computation of the average.</p></li></ol></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec133"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Using stride tricks with NumPy</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Making efficient array selections in NumPy"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec41"/>Making efficient array selections in NumPy</h1></div></div></div><p>NumPy offers several ways<a id="id538" class="indexterm"/> of<a id="id539" class="indexterm"/> selecting slices of arrays. <span class="strong"><strong>Array views</strong></span><a id="id540" class="indexterm"/> refer to the original data buffer of an array, but with different offsets, shapes, and strides. They only permit strided selections (that is, with linearly spaced indices). NumPy also offers specific functions to make arbitrary selections along one axis. Finally, fancy indexing is the most general selection method, but it is also the slowest as we will see in this recipe. Faster alternatives should be chosen when possible.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec134"/>Getting ready</h2></div></div></div><p>We suppose that NumPy has been imported and that the <code class="literal">id</code> function has been defined (see the <span class="emphasis"><em>Understanding the internals of NumPy to avoid unnecessary array copying</em></span> recipe).</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec135"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's create an array with a large number of rows. We will select slices of this array along the first dimension:<div class="informalexample"><pre class="programlisting">In [3]: n, d = 100000, 100
In [4]: a = np.random.random_sample((n, d)); aid = id(a)</pre></div></li><li class="listitem">Let's select one row from every 10 rows, using two different methods (array view and fancy indexing):<div class="informalexample"><pre class="programlisting">In [5]: b1 = a[::10]
        b2 = a[np.arange(0, n, 10)]
In [6]: np.array_equal(b1, b2)
Out[6]: True</pre></div></li><li class="listitem">The view refers to the original data buffer, whereas fancy indexing yields a copy:<div class="informalexample"><pre class="programlisting">In [7]: id(b1) == aid, id(b2) == aid
Out[7]: (True, False)</pre></div></li><li class="listitem">Let's compare the performance of both methods:<div class="informalexample"><pre class="programlisting">In [8]: %timeit a[::10]
100000 loops, best of 3: 2.03 µs per loop
In [9]: %timeit a[np.arange(0, n, 10)]
10 loops, best of 3: 46.3 ms per loop</pre></div><p>Fancy indexing is several orders of magnitude slower as it involves copying a large array.</p></li><li class="listitem">When nonstrided selections need to be done along one dimension, array views are not an option. However, alternatives to fancy indexing still exist in this case. Given a list of indices, NumPy's <code class="literal">take()</code> function performs a selection along one axis:<div class="informalexample"><pre class="programlisting">In [10]: i = np.arange(0, n, 10)
In [11]: b1 = a[i]
         b2 = np.take(a, i, axis=0)
In [12]: np.array_equal(b1, b2)
Out[12]: True</pre></div><p>The second method is faster:</p><div class="informalexample"><pre class="programlisting">In [13]: %timeit a[i]
10 loops, best of 3: 50.2 ms per loop
In [14]: %timeit np.take(a, i, axis=0)
100 loops, best of 3: 11.1 ms per loop</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip18"/>Tip</h3><p>Performance of fancy indexing has been improved in recent versions of NumPy; this trick is especially useful on older versions of NumPy.</p></div></div></li><li class="listitem">When the indices <a id="id541" class="indexterm"/>to select along one axis are specified by <a id="id542" class="indexterm"/>a vector of Boolean masks, the <code class="literal">compress()</code> function is an alternative to fancy indexing:<div class="informalexample"><pre class="programlisting">In [15]: i = np.random.random_sample(n) &lt; .5
In [16]: b1 = a[i]
         b2 = np.compress(i, a, axis=0) 
In [17]: np.array_equal(b1, b2)
Out[17]: True
In [18]: %timeit a[i]
1 loops, best of 3: 286 ms per loop
In [19]: %timeit np.compress(i, a, axis=0)
10 loops, best of 3: 41.3 ms per loop</pre></div><p>The second method is also faster than fancy indexing.</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec136"/>How it works...</h2></div></div></div><p>Fancy indexing is the most general way of making completely arbitrary selections of an array. However, more specific and faster methods often exist and should be preferred when possible.</p><p>Array views should be used whenever strided selections have to be done, but we need to be careful about the fact that views refer to the original data buffer.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec137"/>There's more...</h2></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The complete list of NumPy routines<a id="id543" class="indexterm"/> is available in the NumPy Reference Guide, at <a class="ulink" href="http://docs.scipy.org/doc/numpy/reference/">http://docs.scipy.org/doc/numpy/reference/</a></li><li class="listitem" style="list-style-type: disc">The list of indexing routines is available<a id="id544" class="indexterm"/> at <a class="ulink" href="http://docs.scipy.org/doc/numpy/reference/routines.indexing.html">http://docs.scipy.org/doc/numpy/reference/routines.indexing.html</a></li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Processing huge NumPy arrays with memory mapping"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec42"/>Processing huge NumPy arrays with memory mapping</h1></div></div></div><p>Sometimes, we need to deal with NumPy arrays that are too big to fit in the system memory. A common solution is to use <span class="strong"><strong>memory mapping</strong></span><a id="id545" class="indexterm"/> and implement <a id="id546" class="indexterm"/><span class="strong"><strong>out-of-core computations</strong></span>. The array is stored in a file on the hard <a id="id547" class="indexterm"/>drive, and we create a <a id="id548" class="indexterm"/>memory-mapped object to this file that can be used as a regular NumPy array. Accessing a portion of the array results in the corresponding data being automatically fetched from the hard drive. Therefore, we only consume what we use.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec138"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's create a memory-mapped array:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
In [2]: nrows, ncols = 1000000, 100
In [3]: f = np.memmap('memmapped.dat', dtype=np.float32, 
                      mode='w+', shape=(nrows, ncols))</pre></div></li><li class="listitem">Let's feed the array with random values, one column at a time because our system's memory is limited!<div class="informalexample"><pre class="programlisting">In [4]: for i in range(ncols):
            f[:,i] = np.random.rand(nrows)</pre></div><p>We save the last column of the array:</p><div class="informalexample"><pre class="programlisting">In [5]: x = f[:,-1]</pre></div></li><li class="listitem">Now, we flush memory changes to disk by deleting the object:<div class="informalexample"><pre class="programlisting">In [6]: del f</pre></div></li><li class="listitem">Reading a memory-mapped array from disk involves the same <code class="literal">memmap</code> function. The data type and the shape need to be specified again, as this information is not stored in the file:<div class="informalexample"><pre class="programlisting">In [7]: f = np.memmap('memmapped.dat', dtype=np.float32,
                      shape=(nrows, ncols))
In [8]: np.array_equal(f[:,-1], x)
Out[8]: True
In [9]: del f</pre></div></li></ol></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip19"/>Tip</h3><p>This method is not the most adapted for long-term storage of data and data sharing. The following recipes in this chapter will show a better way based on the HDF5 file format.</p></div></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec139"/>How it works...</h2></div></div></div><p>Memory mapping lets you work with huge arrays almost as if they were regular arrays. Python code that accepts a NumPy array as input will also accept a <code class="literal">memmap</code> array. However, we need to ensure that the array is used efficiently. That is, the array is never loaded as a whole (otherwise, it would waste system memory and would dismiss any advantage of the technique).</p><p>Memory<a id="id549" class="indexterm"/> mapping is also useful when you<a id="id550" class="indexterm"/> have a huge file containing raw data in a homogeneous binary format with a known data type and shape. In this case, an alternative solution is to use NumPy's <code class="literal">fromfile()</code> function with a file handle created with Python's native <code class="literal">open()</code> function. Using <code class="literal">f.seek()</code> lets you position the cursor at any location and load a given number of bytes into a NumPy array.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec140"/>There's more...</h2></div></div></div><p>Another way of dealing with huge NumPy matrices is to use <span class="strong"><strong>sparse matrices</strong></span><a id="id551" class="indexterm"/> through SciPy's <span class="strong"><strong>sparse</strong></span> subpackage. It is adapted when our matrices contain mostly zeros, as is often the case with simulations of partial differential equations, graph algorithms, or specific machine learning applications. Representing matrices as dense structures can be a waste of memory, and sparse matrices offer a more efficient compressed representation.</p><p>Using sparse matrices in SciPy is not straightforward as multiple implementations exist. Each implementation is best for a particular kind of application. Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">SciPy lecture notes about sparse matrices, available at <a class="ulink" href="http://scipy-lectures.github.io/advanced/scipy_sparse/index.html">http://scipy-lectures.github.io/advanced/scipy_sparse/index.html</a></li><li class="listitem" style="list-style-type: disc">Reference documentation<a id="id552" class="indexterm"/> on sparse matrices, at <a class="ulink" href="http://docs.scipy.org/doc/scipy/reference/sparse.html">http://docs.scipy.org/doc/scipy/reference/sparse.html</a></li><li class="listitem" style="list-style-type: disc">Documentation of memmap, at <a class="ulink" href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.memmap.html">http://docs.scipy.org/doc/numpy/reference/generated/numpy.memmap.html</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec141"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Manipulating large arrays with HDF5 and PyTables</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Manipulating large heterogeneous tables with HDF5 and PyTables</em></span> recipe</li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Manipulating large arrays with HDF5 and PyTables"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec43"/>Manipulating large arrays with HDF5 and PyTables</h1></div></div></div><p>NumPy arrays can be <a id="id553" class="indexterm"/>persistently saved on disk using built-in functions in <a id="id554" class="indexterm"/>NumPy such as <code class="literal">np.savetxt</code>, <code class="literal">np.save</code>, or <code class="literal">np.savez</code>, and loaded<a id="id555" class="indexterm"/> in memory using analogous<a id="id556" class="indexterm"/> functions. These methods are best when the arrays contain less than a few million points. For larger arrays, these methods suffer from two major problems: they become too slow, and they require the arrays to be fully loaded in memory. Arrays containing billions of points can be too big to fit in system memory, and alternative methods are required.</p><p>These alternative methods<a id="id557" class="indexterm"/> rely on <span class="strong"><strong>memory mapping</strong></span>: the array resides on the hard drive, and chunks of the array are selectively loaded in memory as soon as the CPU needs them. This technique is memory-efficient, at the expense of a slight overhead due to hard drive access. Cache mechanisms and optimizations can mitigate this issue.</p><p>The previous recipe showed a basic memory mapping technique using NumPy. In this recipe, we will use a package named <a id="id558" class="indexterm"/><span class="strong"><strong>PyTables</strong></span>, which is specifically designed to deal with very large datasets. It implements fast memory-mapping techniques via a widely-used and open file format specification called <span class="strong"><strong>Hierarchical Data Format</strong></span>, or <span class="strong"><strong>HDF5</strong></span>. An HDF5<a id="id559" class="indexterm"/> file contains one or several datasets (arrays or heterogeneous tables) organized into a POSIX-like hierarchy. Any part of the datasets can be accessed efficiently and easily without unnecessarily wasting the system memory.</p><p>As we will see later in this recipe, an alternative for PyTables is <a id="id560" class="indexterm"/><span class="strong"><strong>h5py</strong></span>. It is more lightweight and more adapted than PyTables in some situations.</p><p>In this recipe, we will see how to manipulate large arrays using HDF5 and PyTables. The next recipe will be about pandas-like heterogeneous tables.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec142"/>Getting ready</h2></div></div></div><p>You need PyTables 3.0+ for this recipe and the next one. With Anaconda, you can install PyTables<a id="id561" class="indexterm"/> using <code class="literal">conda install tables</code>. You will also find binary installers at <a class="ulink" href="http://pytables.github.io/usersguide/installation.html">http://pytables.github.io/usersguide/installation.html</a>. Windows users can find installers on <a class="ulink" href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#pytables">www.lfd.uci.edu/~gohlke/pythonlibs/#pytables</a>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip20"/>Tip</h3><p>Prior to version 3.0, PyTables used a camel case convention for the names of attributes and methods. The latest versions use the more standard Python convention using underscores. So, for example, <code class="literal">tb.open_file</code> is <code class="literal">tb.openFile</code> in versions prior to 3.0.</p></div></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec143"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, we<a id="id562" class="indexterm"/> need to import NumPy and<a id="id563" class="indexterm"/> PyTables (the package's name is <code class="literal">tables</code>):<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import tables as tb</pre></div></li><li class="listitem">Let's create a new empty HDF5 file:<div class="informalexample"><pre class="programlisting">In [2]: f = tb.open_file('myfile.h5', 'w')</pre></div></li><li class="listitem">We create a new top-level group named <code class="literal">experiment1</code>:<div class="informalexample"><pre class="programlisting">In [3]: f.create_group('/', 'experiment1')
Out[3]: /experiment1 (Group) u''
  children := []</pre></div></li><li class="listitem">Let's also add some metadata to this group:<div class="informalexample"><pre class="programlisting">In [4]: f.set_node_attr('/experiment1', 'date', '2014-09-01')</pre></div></li><li class="listitem">In this group, we create a 1000*1000 array named <code class="literal">array1</code>:<div class="informalexample"><pre class="programlisting">In [5]: x = np.random.rand(1000, 1000)
        f.create_array('/experiment1', 'array1', x)
Out[5]: /experiment1/array1 (Array(1000L, 1000L))</pre></div></li><li class="listitem">Finally, we <a id="id564" class="indexterm"/>need to close the file to commit the<a id="id565" class="indexterm"/> changes on disk:<div class="informalexample"><pre class="programlisting">In [6]: f.close()</pre></div></li><li class="listitem">Now, let's open this file. We could have done this in another Python session since the array has been saved in the HDF5 file.<div class="informalexample"><pre class="programlisting">In [7]: f = tb.open_file('myfile.h5', 'r')</pre></div></li><li class="listitem">We can retrieve an attribute by giving the group path and the attribute name:<div class="informalexample"><pre class="programlisting">In [8]: f.get_node_attr('/experiment1', 'date')
Out[8]: '2014-09-01'</pre></div></li><li class="listitem">We can access any item in the file using attributes, replacing slashes with dots in the paths, and starting with <code class="literal">root </code>(corresponding to the path <code class="literal">/</code>). IPython's tab completion is particularly useful in this respect when exploring a file interactively.<div class="informalexample"><pre class="programlisting">In [9]: y = f.root.experiment1.array1  
        # /experiment1/array1
        type(y)
Out[9]: tables.array.Array</pre></div></li><li class="listitem">The array can be used as a NumPy array, but an important distinction is that it is stored on disk instead of system memory. Performing a computation on this array automatically loads the requested section of the array into memory, thus it is more efficient to access only the array's views.<div class="informalexample"><pre class="programlisting">In [10]: np.array_equal(x[0,:], y[0,:])
Out[10]: True</pre></div></li><li class="listitem">It is also possible to get a node from its absolute path, which is useful when the path is only known at runtime:<div class="informalexample"><pre class="programlisting">In [11]: f.get_node('/experiment1/array1')
Out[11]: /experiment1/array1 (Array(1000, 1000))</pre></div></li><li class="listitem">We're done for this recipe, so let's do some clean-up:<div class="informalexample"><pre class="programlisting">In [12]: f.close()
In [13]: import os
         os.remove('myfile.h5')</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec144"/>How it works...</h2></div></div></div><p>In this recipe, we <a id="id566" class="indexterm"/>stored a single array in the file, but HDF5 is <a id="id567" class="indexterm"/>especially useful when many arrays need to be <a id="id568" class="indexterm"/>saved in a single file. HDF5 is generally used in <a id="id569" class="indexterm"/>big projects, when large arrays have to be organized within a hierarchical structure. For example, it is largely used at NASA, NOAA, and many other scientific institutions (see <a class="ulink" href="http://www.hdfgroup.org/users.html">www.hdfgroup.org/users.html</a>). Researchers can store recorded data across multiple devices, multiple trials, and multiple experiments.</p><p>In HDF5, the data is organized within a tree. Nodes are either <a id="id570" class="indexterm"/><span class="strong"><strong>groups</strong></span> (analogous to folders in a file system) or <span class="strong"><strong>datasets</strong></span><a id="id571" class="indexterm"/> (analogous to files). A group can contain subgroups and datasets, whereas datasets only contain data. Both groups and datasets can contain attributes (metadata) that have a basic data type (integer or floating point number, string, and so on). HDF5 also supports internal and external links; a given path can refer to another group or dataset within the same file, or within another file. This feature may be useful if you need different files for the same experiment or project.</p><p>Being able to access a<a id="id572" class="indexterm"/> chunk of a single array without loading the rest <a id="id573" class="indexterm"/>of the array and the file in memory is quite <a id="id574" class="indexterm"/>convenient. Moreover, a loaded array can be <a id="id575" class="indexterm"/>polymorphically accessed using standard NumPy's slicing syntax. Code that accepts a NumPy array as an argument can, in principle, accept a PyTables array object as an argument as well.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec145"/>There's more...</h2></div></div></div><p>In this recipe, we created a PyTables <code class="literal">Array</code> object to store our NumPy array. Other similar types of objects include <code class="literal">CArrays</code> that store large arrays in chunks and support lossless compression. Also, an <code class="literal">EArray</code> object is extendable in at most one dimension, which is useful when the dimensions of the array are not known when creating the array in the file. A common use case is recording data during an online experiment.</p><p>The other main type of HDF5 object is <code class="literal">Table</code>, which stores tabular data in a two-dimensional table with heterogeneous data types. In PyTables, a <code class="literal">Table</code> is to an <code class="literal">Array</code> what a pandas <code class="literal">DataFrame</code> is to a NumPy <code class="literal">ndarray</code>. We will see those in the next recipe.</p><p>An interesting feature of HDF5 files is that they are not tied to PyTables. As HDF5 is an open format specification, libraries exist in most languages (C, FORTRAN, MATLAB, and many others), so it's easy to open an HDF5 file in these languages.</p><p>In HDF5, a dataset may be stored in a <span class="strong"><strong>contiguous</strong></span> block<a id="id576" class="indexterm"/> of memory, or in chunks. <span class="strong"><strong>Chunks</strong></span><a id="id577" class="indexterm"/> are atomic objects and HDF5/PyTables can only read and write entire chunks. Chunks are internally organized within a tree data structure called a <a id="id578" class="indexterm"/><span class="strong"><strong>B-tree</strong></span>. When we create a new array or table, we can specify the<a id="id579" class="indexterm"/> <span class="strong"><strong>chunk shape</strong></span>. It is an internal detail, but it can greatly affect performance when writing and reading parts of the dataset.</p><p>The optimal chunk shape depends on how we plan to access the data. There is a trade-off between many small chunks (large overhead due to managing lots of chunks) and a few large chunks (inefficient disk I/O). In general, the chunk size is recommended to be smaller than 1 MB. The chunk cache is also an important parameter that may affect performance.</p><p>Relatedly, we should specify as an optional argument the expected number of rows when we create an <code class="literal">EArray</code> or a <code class="literal">Table</code> object so as to optimize the internal structure of the file. You can find more information in the PyTables users guide section on optimization (see the link mentioned in the following references), which is a must-read if you plan to do anything slightly complex on large HDF5 files (more than 100 MB).</p><p>Finally, we should mention another HDF5 library in Python named <span class="strong"><strong>h5py</strong></span>. This lightweight library offers an easy interface to HDF5 files, with emphasis on arrays rather than tables. It provides very natural access to HDF5 arrays from NumPy, and may be sufficient if you do not need the database-like features of PyTables. For more information on h5py<a id="id580" class="indexterm"/>, refer to <a class="ulink" href="http://www.h5py.org">www.h5py.org</a>.</p><p>Here are a few <a id="id581" class="indexterm"/>references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">HDF5 chunking, at <a class="ulink" href="http://www.hdfgroup.org/HDF5/doc/Advanced/Chunking/">www.hdfgroup.org/HDF5/doc/Advanced/Chunking/</a></li><li class="listitem" style="list-style-type: disc">PyTables optimization guide, available at <a class="ulink" href="http://pytables.github.io/usersguide/optimization.html">http://pytables.github.io/usersguide/optimization.html</a></li><li class="listitem" style="list-style-type: disc">Difference between PyTables<a id="id582" class="indexterm"/> and h5py, from the perspective of <a id="id583" class="indexterm"/>h5py, at <a class="ulink" href="https://github.com/h5py/h5py/wiki/FAQ#whats-the-difference-between-h5py-and-pytables">https://github.com/h5py/h5py/wiki/FAQ#whats-the-difference-between-h5py-and-pytables</a></li><li class="listitem" style="list-style-type: disc">Difference between PyTables and h5py, from the perspective of PyTables, at <a class="ulink" href="http://www.pytables.org/moin/FAQ#HowdoesPyTablescomparewiththeh5pyproject.3F">www.pytables.org/moin/FAQ#HowdoesPyTablescomparewiththeh5pyproject.3F</a></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec146"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Processing huge NumPy arrays with memory mapping</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Manipulating large heterogeneous tables with HDF5 and PyTables</em></span> recipe</li><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Ten tips for conducting reproducible interactive computing experiments</em></span> recipe in <a class="link" href="ch02.html" title="Chapter 2. Best Practices in Interactive Computing">Chapter 2</a>, <span class="emphasis"><em>Best Practices in Interactive Computing</em></span></li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Manipulating large heterogeneous tables with HDF5 and PyTables"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec44"/>Manipulating large heterogeneous tables with HDF5 and PyTables</h1></div></div></div><p>PyTables can store homogeneous blocks of data as NumPy-like arrays in HDF5 files. It can also store heterogeneous tables, as we will see in this recipe.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec147"/>Getting ready</h2></div></div></div><p>You need PyTables <a id="id584" class="indexterm"/>for this <a id="id585" class="indexterm"/>recipe (see the previous recipe for installation instructions).</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec148"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's import<a id="id586" class="indexterm"/> NumPy and <a id="id587" class="indexterm"/>PyTables:<div class="informalexample"><pre class="programlisting">In [1]: import numpy as np
        import tables as tb</pre></div></li><li class="listitem">Let's create a new HDF5 file:<div class="informalexample"><pre class="programlisting">In [2]: f = tb.open_file('myfile.h5', 'w')</pre></div></li><li class="listitem">We will <a id="id588" class="indexterm"/>create an HDF5 table with <a id="id589" class="indexterm"/>two columns: the name of a city (a string with 64 characters at most), and its population (a 32-bit integer). We can specify the columns by creating a complex data type <a id="id590" class="indexterm"/>with NumPy:<div class="informalexample"><pre class="programlisting">In [3]: dtype = np.dtype([('city','S64'),
                          ('population', 'i4')])</pre></div></li><li class="listitem">Now, we <a id="id591" class="indexterm"/>create the table in <code class="literal">/table1</code>:<div class="informalexample"><pre class="programlisting">In [4]: table = f.create_table('/', 'table1', dtype)</pre></div></li><li class="listitem">Let's add a few rows:<div class="informalexample"><pre class="programlisting">In [5]: table.append([('Brussels', 1138854),
                      ('London',   8308369),
                      ('Paris',    2243833)])</pre></div></li><li class="listitem">After adding rows, we need to flush the table to commit the changes on disk:<div class="informalexample"><pre class="programlisting">In [6]: table.flush()</pre></div></li><li class="listitem">There are many ways to access the data from a table. The easiest but not particularly efficient way is to load the entire table in memory, which returns a NumPy array:<div class="informalexample"><pre class="programlisting">In [7]: table[:]
Out[7]: array([('Brussels', 1138854),
               ('London', 8308369), 
               ('Paris', 2243833)], 
               dtype=[('city', 'S64'), 
                      ('population', '&lt;i4')])</pre></div></li><li class="listitem">It is also possible to load a particular column (with all rows):<div class="informalexample"><pre class="programlisting">In [8]: table.col('city')
Out[8]: array(['Brussels', 'London', 'Paris'],
              dtype='|S64')</pre></div></li><li class="listitem">When dealing with a large number of rows, we can make a SQL-like query in the table to load all rows that satisfy particular conditions:<div class="informalexample"><pre class="programlisting">In [9]: [row['city'] for row in 
         table.where('population&gt;2e6')]
Out[9]: ['London', 'Paris']</pre></div></li><li class="listitem">Finally, if their indices are known, we can access specific rows:<div class="informalexample"><pre class="programlisting">In [10]: table[1]
Out[10]: ('London', 8308369)</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec149"/>How it works...</h2></div></div></div><p>A table can be created from scratch like in this recipe, or from either an existing NumPy array or a pandas <code class="literal">DataFrame</code>. In the first case, the description of the columns can be given with a NumPy data type as shown here, with a dictionary, or with a class deriving from <code class="literal">IsDescription</code>. In the second case, the table description will be automatically inferred from the given array or table.</p><p>Rows can be <a id="id592" class="indexterm"/>added efficiently at the end of the <a id="id593" class="indexterm"/>table using <code class="literal">table.append()</code>. To <a id="id594" class="indexterm"/>add a single row, first get a new row<a id="id595" class="indexterm"/> instance with <code class="literal">row = table.row</code>, set the fields of the row as if it were a dictionary, and then call <code class="literal">row.append()</code> to add the new row at the end of the table. Calling <code class="literal">flush()</code> after a set of writing operations ensures that these changes are synchronized on disk. PyTables uses complex cache mechanisms to ensure maximum performance when writing and reading data in a table; thus, new rows are not immediately written to the disk.</p><p>PyTables supports highly efficient SQL-like queries called <a id="id596" class="indexterm"/><span class="strong"><strong>in-kernel queries</strong></span>. The string containing the query expression is compiled and evaluated on all rows. A less-efficient method consists of iterating over all rows with <code class="literal">table.iterrows()</code> and using an <code class="literal">if</code> statement on the rows' fields.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec150"/>There's more...</h2></div></div></div><p>Here are a few references:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">In-kernel queries, <a id="id597" class="indexterm"/>at <a class="ulink" href="http://pytables.github.io/usersguide/condition_syntax.html">http://pytables.github.io/usersguide/condition_syntax.html</a>.</li><li class="listitem" style="list-style-type: disc">An alternative to PyTables and HDF5 might come from the Blaze project, still in early development at the time of writing. For more information on <a id="id598" class="indexterm"/>Blaze, refer to <a class="ulink" href="http://blaze.pydata.org">http://blaze.pydata.org</a>.</li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec151"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <span class="emphasis"><em>Manipulating large arrays with HDF5 and PyTables</em></span> recipe</li></ul></div></div></div></div>
</body></html>