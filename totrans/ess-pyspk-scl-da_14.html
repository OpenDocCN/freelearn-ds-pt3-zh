<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer068">
			<h1 id="_idParaDest-188"><a id="_idTextAnchor188"/>Chapter 11: Data Visualization with PySpark</h1>
			<p>So far, from <a href="B16736_01_Final_JM_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">,</em><em class="italic"> Distributed Computing Primer</em><em class="italic">,</em> through <a href="B16736_09_Final_JM_ePub.xhtml#_idTextAnchor164"><em class="italic">Chapter 9</em></a>,<em class="italic"> Machine Learning Life Cycle Management</em>, you have learned how to ingest, integrate, and cleanse data, as well as how to make data conducive for analytics. You have also learned how to make use of clean data for practical business applications using data science and machine learning. This chapter will introduce you to the basics of deriving meaning out of data using data visualizations.</p>
			<p>In this chapter, we're going to cover the following main topics: </p>
			<ul>
				<li>Importance of data visualization</li>
				<li>Techniques for visualizing data using PySpark</li>
				<li>Considerations for PySpark to pandas conversion</li>
			</ul>
			<p>Data visualization is the process of graphically representing data using visual elements such as charts, graphs, and maps. Data visualization helps you understand patterns within data in a visual manner. In the big data world, with massive amounts of data, it is even more important to make use of data visualizations to derive meaning out of such data and present it in a simple and easy-to-understand way to business users; this helps them make data-driven decisions.</p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor189"/>Technical requirements</h1>
			<p>In this chapter, we will be using Databricks Community Edition to run our code. </p>
			<ul>
				<li>Databricks Community Edition can be accessed at <a href="https://community.cloud.databricks.com">https://community.cloud.databricks.com</a>. </li>
				<li>Sign-up instructions can be found at <a href="https://databricks.com/try-databricks">https://databricks.com/try-databricks</a>. </li>
				<li>The code and data for this chapter can be downloaded from <a href="https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter11">https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/tree/main/Chapter11</a>.</li>
			</ul>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor190"/>Importance of data visualization</h1>
			<p>Data visualization is the process of translating data into a pictorial representation in the form of graphs, charts, or maps. This makes it easier for the human mind to comprehend complex information. Typically, data visualization is the final stage of business analytics <a id="_idIndexMarker786"/>and the first step of any data science process. Though there are professionals who deal solely with data visualizations, any data professional needs to be able to understand and produce data visualizations. They help convey complex patterns that are hidden within data in an easy-to-understand way to business users. Every business needs information for optimal performance, and data visualization helps businesses make easier data-driven decisions by representing relationships between datasets in a visual way and surfacing actionable insights. With the advent of big data, there has been an explosion of both structured and unstructured data, and it is difficult to make sense of it without the help of visual aids. Data visualization helps in accelerating the decision-making process by surfacing key business information and helps business users act on those insights quickly. Data visualization also aids the storytelling process by helping convey the right message to the right audience.</p>
			<p>A data visualization can be a simple graph representing a single aspect of the current state of the business, a complex sales report, or a dashboard that gives a holistic view of an organization's performance. Data visualization tools are key to unlocking the power of data visualizations. We will explore the different types of data visualization tools that are available in the following section.</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor191"/>Types of data visualization tools</h2>
			<p>Data visualization tools provide us with an easier way to create data visualizations. They allow data <a id="_idIndexMarker787"/>analysts and data scientists to create data visualizations conveniently by providing a graphical user interface, database connections, and sometimes data manipulation tools in a single, unified interface. There are different types of data visualizations tools, and each serves a slightly different purpose. We will explore them in this section. </p>
			<h3>Business intelligence tools</h3>
			<p><strong class="bold">Business intelligence</strong> <strong class="bold">(BI)</strong> tools are <a id="_idIndexMarker788"/>typically <a id="_idIndexMarker789"/>enterprise-grade tools that help organizations track and visually represent their <strong class="bold">Key Performance Indicators</strong> (<strong class="bold">KPIs</strong>). BI tools typically include provisions for creating complex logical data models and contain data cleansing and integration mechanisms. BI tools also include connectors <a id="_idIndexMarker790"/>to a myriad of data sources and built-in data visualizations with drag-and-drop features to help business users quickly create data visualizations, operational and performance dashboards, and scorecards to track the performance of an individual department or the entire organization. The primary users of BI tools are business analysts and business executives involved in making tactical and strategic decisions. </p>
			<p>BI tools <a id="_idIndexMarker791"/>traditionally use data <a id="_idIndexMarker792"/>warehouses as their data sources, but modern BI tools support RDMS, NoSQL databases, and data lakes as data sources. Some examples of prominent <a id="_idIndexMarker793"/>BI tools include <strong class="bold">Tableau</strong>, <strong class="bold">Looker</strong>, <strong class="bold">Microsoft Power BI</strong>, <strong class="bold">SAP Business Objects</strong>, <strong class="bold">MicroStrategy</strong>, <strong class="bold">IBM Cognos</strong>, and <strong class="bold">Qlikview</strong>, to name a few. BI tools <a id="_idIndexMarker794"/>can connect <a id="_idIndexMarker795"/>to Apache Spark and consume <a id="_idIndexMarker796"/>data stored in Spark SQL tables using an ODBC connection. These <a id="_idIndexMarker797"/>concepts <a id="_idIndexMarker798"/>will be explored in detail in <a href="B16736_13_Final_JM_ePub.xhtml#_idTextAnchor214"><em class="italic">Chapter 13</em></a>, <em class="italic">Integrating External Tools with Spark SQL</em>. A class of data visualization <a id="_idIndexMarker799"/>tools with all the necessary data visualization and data connectivity components, minus any data processing capabilities such as Redash, also exist and they can also connect to Apache Spark via an ODBC connection.</p>
			<h3>Observability tools</h3>
			<p>Observability is the process of constantly monitoring and understanding what's happening <a id="_idIndexMarker800"/>in highly distributed systems. Observability helps us understand what is slow or broken, as well as what needs to be <a id="_idIndexMarker801"/>fixed to improve performance. However, since modern cloud environments are dynamic and constantly increasing in scale and complexity, most problems are neither known nor monitored. Observability addresses common issues with modern cloud environments that are dynamic and ever-increasing in scale by enabling you to continuously monitor and surface any issues that might arise. Observability tools help businesses continuously monitor systems and applications and enable a business to receive actionable insights into system behavior, as well as predict outages or problems before they occur. Data visualization is an important component of observability tools; a few popular examples include Grafana and Kibana.</p>
			<p>Data teams are typically not responsible for monitoring and maintaining the health of data processing <a id="_idIndexMarker802"/>systems – this is usually handled by specialists such as <strong class="bold">DevOps</strong> engineers. Apache <a id="_idIndexMarker803"/>Spark doesn't have any direct <a id="_idIndexMarker804"/>integrations with any observability <a id="_idIndexMarker805"/>tools out of the box, but it can be integrated with popular observability platforms such as <strong class="bold">Prometheus</strong> and <strong class="bold">Grafana</strong>. Apache Spark's integration <a id="_idIndexMarker806"/>with observability tools is outside the scope of this book, so we won't discuss this here.</p>
			<h3>Notebooks</h3>
			<p>Notebooks are interactive computing tools that are used to execute code, visualize results, and share insights. Notebooks are indispensable tools in the data science process and <a id="_idIndexMarker807"/>are becoming prominent in the entire data analytics development life cycle, as you have witnessed throughout this book. Notebooks are also excellent data visualization tools as they help you convert your Python or SQL code into easy-to-understand interactive data visualizations. Some notebooks, such as Databricks, Jupyter, and Zeppelin notebooks can also be used as standalone dashboards. The remainder of this chapter will focus on how notebooks can be used as data visualization tools when using PySpark.</p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor192"/>Techniques for visualizing data using PySpark</h1>
			<p>Apache Spark is a unified data processing engine and doesn't come out of the box with a graphical <a id="_idIndexMarker808"/>user interface, per se. As discussed in the previous sections, data that's been processed by Apache Spark can be stored in <a id="_idIndexMarker809"/>data warehouses and visualized using BI tools or natively visualized using notebooks. In this section, we will focus on how to leverage notebooks to interactively process and visualize data <a id="_idIndexMarker810"/>using PySpark. As we have done throughout <a id="_idIndexMarker811"/>this book, we will be making use of notebooks that <a id="_idIndexMarker812"/>come with <strong class="bold">Databricks Community Edition</strong>, though <strong class="bold">Jupyter</strong> and <strong class="bold">Zeppelin</strong> notebooks can also be used.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor193"/>PySpark native data visualizations</h2>
			<p>There aren't <a id="_idIndexMarker813"/>any data visualization libraries that can work with PySpark DataFrames natively. However, the notebook implementations of cloud-based Spark distributions such as Databricks and Qubole support natively visualizing Spark DataFrames using the built-in <strong class="source-inline">display()</strong> function. Let's see how we can use the <strong class="source-inline">display()</strong> function to visualize PySpark DataFrames in Databricks Community Edition. </p>
			<p>We will use the cleansed, integrated, and wrangled dataset that we produced toward the end of <a href="B16736_06_Final_JM_ePub.xhtml#_idTextAnchor107"><em class="italic">Chapter 6</em></a>, <em class="italic">Feature Engineering – Extraction, Transformation, and Selection</em>, here, as shown in the following code snippet:</p>
			<p class="source-code">retail_df = spark.read.table("feature_store.retail_features")</p>
			<p class="source-code">viz_df = retail_df.select("invoice_num", "description", </p>
			<p class="source-code">                          "invoice_date", "invoice_month", </p>
			<p class="source-code">                          "country_code", "quantity", </p>
			<p class="source-code">                          "unit_price", "occupation", </p>
			<p class="source-code">                          "gender")</p>
			<p class="source-code">viz_df.display()</p>
			<p>In the previous code snippet, we read a table into a Spark DataFrame and selected the columns that we intend to visualize. Then, we called the <strong class="source-inline">display()</strong> method on the Spark DataFrame. The result is a grid display in the notebook, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="Images/B16736_11_01.jpg" alt="Figure 11.1 – The grid widget&#13;&#10;" width="1263" height="264"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 – The grid widget</p>
			<p>The previous screenshot shows the result of calling the <strong class="source-inline">display()</strong> function on a Spark DataFrame within a Databricks notebook. This way, any Spark DataFrame can be visualized in a tabular format within Databricks notebooks. The tabular grid supports sorting arbitrary columns. Databricks notebooks also support charts and graphs that can be used from within the notebooks. </p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Databricks's <strong class="source-inline">display()</strong> method supports all of Spark's programming APIs, including Python, Scala, R, and SQL. In addition, the <strong class="source-inline">display()</strong> method can also render Python pandas DataFrames.</p>
			<p>We can <a id="_idIndexMarker814"/>use the same grid display and convert it into a graph by clicking on the graph icon and choosing the desired graph from the list, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="Images/B16736_11_02.jpg" alt="Figure 11.2 – Graph options&#13;&#10;" width="315" height="228"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – Graph options</p>
			<p>As we can see, the graph menu has multiple chart options, with the bar chart being the first on the list. If you choose the bar chart, plot options can be used to configure the chart's key, value, and series grouping options. Similarly, we can use a line graph or a pie chart, as shown here:</p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="Images/B16736_11_03.jpg" alt="Figure 11.3 – Pie chart&#13;&#10;" width="730" height="552"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 – Pie chart</p>
			<p>Here, the <strong class="source-inline">display()</strong> function can be used to display various kinds of charts within the notebook <a id="_idIndexMarker815"/>and help configure various graph options. Databricks notebooks also support a rudimentary map widget that can visualize metrics on a world map, as illustrated in the following screenshot:</p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="Images/B16736_11_04.jpg" alt="Figure 11.4 – World map&#13;&#10;" width="551" height="345"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4 – World map</p>
			<p>The previous <a id="_idIndexMarker816"/>screenshot shows metrics on a world map. Since our dataset only contains a few European countries, France and the UK have been shaded in on the map widget. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">For this widget, the values should be either country codes in ISO 3166-1 alpha-3 format ("GBR") or US state abbreviations ("TX").</p>
			<p>In addition <a id="_idIndexMarker817"/>to basic bars and charts, Databricks notebooks also support <a id="_idIndexMarker818"/>scientific <a id="_idIndexMarker819"/>visualizations such as <strong class="bold">scatter plots</strong>, <strong class="bold">histograms</strong>, <strong class="bold">quantile plots</strong>, and <strong class="bold">Q-Q</strong> plots, as <a id="_idIndexMarker820"/>illustrated in the following figure:</p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="Images/B16736_11_05.jpg" alt="Figure 11.5 – Quantile plot&#13;&#10;" width="526" height="365"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.5 – Quantile plot</p>
			<p>A quantile plot, as illustrated in the previous figure, helps determine whether two datasets <a id="_idIndexMarker821"/>have a common distribution. Quantile plots are available in Databricks notebooks via the graph menu, and plot properties such as keys, values, and series groupings are available via the plot options menu.</p>
			<p>We can use the following code to make Databricks notebooks display images:</p>
			<p class="source-code">image_df = spark.read.format("image").load("/FileStore/FileStore/shared_uploads/images")</p>
			<p>The previous code snippet uses Apache Spark's built-in image data source to load images from a directory on persistent storage such as a data lake:</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="Images/B16736_11_06.jpg" alt="Figure 11.6 – Image data &#13;&#10;" width="334" height="166"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.6 – Image data </p>
			<p>This image is rendered in a notebook using Databricks's <strong class="source-inline">display()</strong> function as it is capable of displaying image previews.</p>
			<p>Databricks <a id="_idIndexMarker822"/>notebooks are also capable of rendering <a id="_idIndexMarker823"/>machine learning-specific <a id="_idIndexMarker824"/>visualizations such as <strong class="bold">residuals</strong>, <strong class="bold">ROC curves</strong>, and <strong class="bold">decision trees</strong>. In <a href="B16736_07_Final_JM_ePub.xhtml#_idTextAnchor128"><em class="italic">Chapter 7</em></a><em class="italic">, Supervised Machine Learning</em>, we used <a id="_idIndexMarker825"/>the <strong class="source-inline">display()</strong> function to visualize a <strong class="bold">decision tree</strong> model that we had trained, as illustrated in the following screenshot:</p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="Images/B16736_11_07.jpg" alt="Figure 11.7 – Decision tree model&#13;&#10;" width="1263" height="705"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.7 – Decision tree model</p>
			<p>The previous screenshot shows the decision tree model that we built using Spark ML, rendered in a Databricks notebook.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">More information on rendering machine learning-specific visualizations using Databricks notebooks <a id="_idIndexMarker826"/>can be found in Databricks's public documentation here: <a href="https://docs.databricks.com/notebooks/visualizations/index.html#machine-learning-visualizations">https://docs.databricks.com/notebooks/visualizations/index.html#machine-learning-visualizations</a>.</p>
			<h3>Interactive visuals using JavaScript and HTML</h3>
			<p>Databricks notebooks also support <strong class="bold">HTML</strong>, <strong class="bold">JavaScript</strong>, and <strong class="bold">CSS</strong> for interactive visualizations via <strong class="bold">D3.js</strong> and <strong class="bold">SVG</strong> and the <strong class="source-inline">displayHTML()</strong> function. You can pass any arbitrary <a id="_idIndexMarker827"/>HTML code to <strong class="source-inline">displayHTML()</strong> and have it rendered in a notebook, as shown in the following code snippet:</p>
			<p class="source-code">displayHTML("&lt;a href ='/files/image.jpg'&gt;Arbitrary Hyperlink&lt;/a&gt;")</p>
			<p>The <a id="_idIndexMarker828"/>preceding code snippet displays an arbitrary HTML <a id="_idIndexMarker829"/>hyperlink in a notebook. Other HTML <a id="_idIndexMarker830"/>elements such as paragraphs, headings, images, and <a id="_idIndexMarker831"/>more can also be used with the <strong class="source-inline">displayHTML()</strong> function.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">HTML blocks such as hyperlinks, images, and tables can be used to make your notebooks more descriptive and interactive for end users and can aid in the storytelling process.</p>
			<p>Similarly, SVG graphics can also be rendered using the <strong class="source-inline">displayHTML()</strong> function, as shown in the following code block:</p>
			<p class="source-code">displayHTML("""&lt;svg width="400" height="400"&gt;</p>
			<p class="source-code">  &lt;ellipse cx="300" cy="300" rx="100" ry="60" style="fill:orange"&gt;</p>
			<p class="source-code">    &lt;animate attributeType="CSS" attributeName="opacity" from="1" to="0" dur="5s" repeatCount="indefinite" /&gt;</p>
			<p class="source-code">  &lt;/ellipse&gt;</p>
			<p class="source-code">&lt;/svg&gt;""")</p>
			<p>The preceding code renders an orange-colored, animated ellipse that fades in and out. Far more complex SVG graphics can also be rendered and data from a Spark DataFrame can be passed along. Similarly, the popular HTML and JavaScript-based visualization library can also be leveraged with Databricks notebooks, as shown here:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="Images/B16736_11_08.jpg" alt="Figure 11.8 – Word cloud using D3.js&#13;&#10;" width="552" height="274"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.8 – Word cloud using D3.js</p>
			<p>Here, we <a id="_idIndexMarker832"/>have taken the <strong class="source-inline">description</strong> column from the <strong class="source-inline">retail_sales</strong> Delta <a id="_idIndexMarker833"/>table that we created during <a id="_idIndexMarker834"/>our data processing steps in the previous <a id="_idIndexMarker835"/>chapters, and then we extracted individual words from the item description column. Then, we rendered the words using a word <a id="_idIndexMarker836"/>cloud visualization by using HTML, CSS, and JavaScript. After, we used the popular D3.js JavaScript library to manipulate the HTML documents based on data. The code for this visualization can be found at <a href="https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/blob/main/Chapter11/databricks-charts-graphs.py">https://github.com/PacktPublishing/Essential-PySpark-for-Scalable-Data-Analytics/blob/main/Chapter11/databricks-charts-graphs.py</a>.</p>
			<p>So far, you have seen some of the basic and statistical graphs that are available via the Databricks notebook interface, which can work natively with Spark DataFrames. However, sometimes, you may need some additional graphs and charts that aren't available within the notebook, or you may need more control over your graph. In these instances, popular visualization libraries for Python such as <strong class="source-inline">matplotlib</strong>, <strong class="source-inline">plotly</strong>, <strong class="source-inline">seaborn</strong>, <strong class="source-inline">altair</strong>, <strong class="source-inline">bokeh</strong>, and so on can be used with PySpark. We will explore some of these visualization libraries in the next section.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor194"/>Using Python data visualizations with PySpark</h2>
			<p>As you learned in the previous section, PySpark doesn't inherently have any visualization capabilities, but you can choose to use Databricks notebook capabilities to visualize data in Spark <a id="_idIndexMarker837"/>DataFrames. In situations where using Databricks notebooks is not possible, you can use popular Python-based visualizations <a id="_idIndexMarker838"/>libraries to visualize your data using any notebook interface that you are comfortable with. In this section, we will explore some prominent Python visualization libraries and how to use them for data visualization in Databricks notebooks.</p>
			<h3>Creating two-dimensional plots using Matplotlib</h3>
			<p><strong class="bold">Matplotlib</strong> is one <a id="_idIndexMarker839"/>of the oldest and most popular Python-based visualization libraries for creating two-dimensional plots from data stored in arrays. Matplotlib <a id="_idIndexMarker840"/>comes pre-installed in Databricks Runtime, though it can also be easily installed from the <strong class="source-inline">PyPI</strong> repository using a package manager such as <strong class="source-inline">pip</strong>. The following code example shows how Matplotlib can be used with PySpark:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">retail_df = spark.read.table("feature_store.retail_features")</p>
			<p class="source-code">viz_df = retail_df.select("invoice_num", "description", </p>
			<p class="source-code">                          "invoice_date", "invoice_month", </p>
			<p class="source-code">                          "country_code", "quantity", </p>
			<p class="source-code">                          "unit_price", "occupation", </p>
			<p class="source-code">                          "gender")</p>
			<p class="source-code">pdf = viz_df.toPandas()</p>
			<p class="source-code">pdf['quantity'] = pd.to_numeric(pdf['quantity'], </p>
			<p class="source-code">                                errors='coerce')</p>
			<p class="source-code">pdf.plot(kind='bar', x='invoice_month', y='quantity',</p>
			<p class="source-code">         color='orange')</p>
			<p>In <a id="_idIndexMarker841"/>the previous code snippet, we did the following:</p>
			<ol>
				<li>First, we imported the <strong class="source-inline">pandas</strong> and <strong class="source-inline">matplotlib</strong> libraries, assuming they are already installed in the notebook. </li>
				<li>Then, we generated a Spark DataFrame with the required columns using the online retail dataset that we have created during the data processing steps in the previous chapters.</li>
				<li>Since Python-based visualization libraries cannot directly use Spark DataFrames, we converted the Spark DataFrame into a pandas DataFrame.</li>
				<li>Then, we converted the quantity column into a numeric data type so that we could plot it.</li>
				<li>After that, we defined a plot on the pandas DataFrame using the <strong class="source-inline">plot()</strong> method of the Matplotlib library, specified the type of plot to be generated as a bar graph, and passed the x-axis and y-axis column names.</li>
				<li>Some notebook environments may require you to explicitly call the <strong class="source-inline">display()</strong> function for the plot to be displayed. </li>
			</ol>
			<p>This way, Matplotlib can be used with any Spark DataFrame if we convert it into a pandas DataFrame. The plot that was generated looks as follows:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="Images/B16736_11_09.jpg" alt="Figure 11.9 – Matplotlib visualization&#13;&#10;" width="497" height="350"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.9 – Matplotlib visualization</p>
			<p>The <a id="_idIndexMarker842"/>previous graph depicts the number of items that have been sold over a certain period.</p>
			<h3>Scientific visualizations using Seaborn</h3>
			<p><strong class="bold">Seaborn</strong> is a visualization library based on Matplotlib and has close integration with pandas. It has <a id="_idIndexMarker843"/>a high-level interface for easily plotting statistical data. Seaborn <a id="_idIndexMarker844"/>comes pre-installed with Databricks, though it can easily be installed from PyPI using <strong class="source-inline">pip</strong>. The following code sample shows how Seaborn can be used with PySpark DataFrames:</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">import seaborn as sns</p>
			<p class="source-code">data = retail_df.select("unit_price").toPandas()["unit_price"]</p>
			<p class="source-code">plt.figure(figsize=(10, 3))</p>
			<p class="source-code">sns.boxplot(data)</p>
			<p>In the previous code snippet, we did the following:</p>
			<ol>
				<li value="1">First, we imported the <strong class="source-inline">matplotlib</strong> and <strong class="source-inline">seaborn</strong> libraries.</li>
				<li>Next, we converted the Spark DataFrame, which contains a single column called <strong class="source-inline">unit_price</strong>, into a pandas DataFrame using the <strong class="source-inline">toPandas()</strong> PySpark function.</li>
				<li>Then, we defined our plot dimensions using the <strong class="source-inline">plot.figure()</strong> Matplotlib method.</li>
				<li>Finally, we plotted a boxplot by invoking the <strong class="source-inline">seaborn.boxplot()</strong> method and passing <a id="_idIndexMarker845"/>the pandas DataFrame with a single column. The resultant plot is shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="Images/B16736_11_10.jpg" alt="Figure 11.10 – Seaborn boxplot visualization&#13;&#10;" width="827" height="297"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.10 – Seaborn boxplot visualization</p>
			<p>The previous screenshot depicts how the <strong class="bold">unit_price</strong> column can be distributed as a box plot using its minimum, first quartile, median, third quartile, and maximum values.</p>
			<h3>Interactive visualizations using Plotly</h3>
			<p><strong class="bold">Plotly</strong> is a <a id="_idIndexMarker846"/>JavaScript-based visualization library that enables Python users to <a id="_idIndexMarker847"/>create interactive web-based visualizations that can be displayed in notebooks or saved to standalone HTML files. Plotly comes pre-installed with Databricks and can be used like so:</p>
			<p class="source-code">import plotly.express as plot</p>
			<p class="source-code">df = viz_df.toPandas()</p>
			<p class="source-code">fig = plot.scatter(df, x="fin_wt", y="quantity",</p>
			<p class="source-code">                   size="unit_price", color="occupation",</p>
			<p class="source-code">                   hover_name="country_code", log_x=True, </p>
			<p class="source-code">                   size_max=60)</p>
			<p class="source-code">fig.show()</p>
			<p>In the <a id="_idIndexMarker848"/>previous code snippet, we did the following actions:</p>
			<ol>
				<li value="1">First, we imported the <strong class="source-inline">matplotlib</strong> and <strong class="source-inline">seaborn</strong> libraries.</li>
				<li>Next, we converted the Spark DataFrame, along with the required columns, into a pandas DataFrame.</li>
				<li>Then, we defined the Plotly plot parameters using the <strong class="source-inline">plot.scatter()</strong> method. This method configures a scatter plot with three dimensions.</li>
				<li>Finally, we rendered the plot using the <strong class="source-inline">fig.show()</strong> method. The resultant plot is shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="Images/B16736_11_11.jpg" alt="Figure 11.11 – Plotly bubble chart visualization&#13;&#10;" width="1460" height="446"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.11 – Plotly bubble chart visualization</p>
			<p>The preceding screenshot shows a bubble graph that depicts three metrics along three dimensions. The <a id="_idIndexMarker849"/>plot is interactive, and information is provided when you hover your mouse over various parts of the graph.</p>
			<h3>Declarative visualizations using Altair</h3>
			<p><strong class="bold">Altair</strong> is a declarative <a id="_idIndexMarker850"/>statistical visualization library for Python. Altair is based on an open source, declarative <a id="_idIndexMarker851"/>grammar engine called <strong class="bold">Vega</strong>. Altair also offers a concise visualization grammar that enables users to build a wide range <a id="_idIndexMarker852"/>of visualizations quickly. It can be installed using the following command:</p>
			<p class="source-code">%pip install altair </p>
			<p>The previous command installs Altair in the notebook's local Python kernel and restarts it. Once Altair has been successfully installed, it can be invoked using the usual Python <strong class="source-inline">import</strong> statements, as shown in the following code sample:</p>
			<p class="source-code">import altair as alt</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">source = (viz_df.selectExpr("gender as Gender", "trim(occupation) as Occupation").where("trim(occupation) in ('Farming-fishing', 'Handlers-cleaners', 'Prof-specialty', 'Sales', 'Tech-support') and cust_age &gt; 49").toPandas())</p>
			<p>In the previous code snippet, we imported the Altair and pandas libraries. Then, we selected the required columns from the Spark table and convert them into a pandas DataFrame. Once we have data in Python in a pandas DataFrame, Altair can be used to create a plot, as shown here:</p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="Images/B16736_11_12.jpg" alt="Figure 11.12 – Altair isotype visualization &#13;&#10;" width="689" height="408"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.12 – Altair isotype visualization </p>
			<p>The preceding figure depicts an isotype visualization that shows the distribution of occupations <a id="_idIndexMarker853"/>by gender, across countries. Other open source libraries such as <strong class="source-inline">bokeh</strong>, <strong class="source-inline">pygal</strong>, and <strong class="source-inline">leather</strong> can also be used to visualize PySpark DataFrames. Bokeh is another popular data visualization library in Python that provides high-performance interactive charts and plots. Bokeh is based on JavaScript and HTML and unlike Matplotlib, it lets users create custom visualizations. Information on using Bokeh in Databricks notebooks can be found in Databricks's public documentation at <a href="https://docs.databricks.com/notebooks/visualizations/bokeh.html#bokeh">https://docs.databricks.com/notebooks/visualizations/bokeh.html#bokeh</a>.</p>
			<p>So far, you have learned how to use some of the popular visualizations that are available for Python with Spark DataFrames by converting PySpark DataFrames into pandas DataFrames. However, there are some performance considerations and limitations you must consider when converting PySpark DataFrames into pandas DataFrames. We will look at these in the next section.</p>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor195"/>Considerations for PySpark to pandas conversion</h1>
			<p>This section will introduce <strong class="bold">pandas</strong>, demonstrate the differences between pandas and PySpark, and the <a id="_idIndexMarker854"/>considerations that need to be kept in mind while <a id="_idIndexMarker855"/>converting datasets between PySpark and pandas.</p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor196"/>Introduction to pandas</h2>
			<p><strong class="bold">pandas</strong> is one of the <a id="_idIndexMarker856"/>most widely used open source data analysis libraries for Python. It contains a diverse set of utilities for processing, manipulating, cleaning, munging, and wrangling data. pandas is much easier to work with than Pythons lists, dictionaries, and loops. In some ways, pandas is like other statistical data analysis tools such as R or SPSS, which makes it very popular with data science and machine learning enthusiasts.</p>
			<p>The primary abstractions of pandas are <strong class="bold">Series</strong> and <strong class="bold">DataFrames</strong>, with the former essentially being a one-dimensional array and the latter a two-dimensional array. One of the fundamental differences between pandas and PySpark is that pandas represents its datasets as one- and two-dimensional <strong class="bold">NumPy</strong> arrays, while PySpark DataFrames are collections of <strong class="bold">Row</strong> and <strong class="bold">Column</strong> objects, based on Spark SQL. While pandas DataFrames can only be manipulated using pandas DSL, PySpark DataFrames can be manipulated using Spark's DataFrame DSL, as well as SQL. Owing to this difference, developers familiar with manipulating pandas might find PySpark to be different and may face a learning curve when working with the platform. The Apache Spark community realized this difficulty and launched a new open source project called Koalas. Koalas implements a pandas-like API on top of Spark DataFrames to try and overcome the previously mentioned difference between pandas and PySpark. More information on using Koalas will be presented in <a href="B16736_10_Final_JM_ePub.xhtml#_idTextAnchor176"><em class="italic">Chapter 10</em></a>, <em class="italic">Scaling Out Single-Node Machine Learning Using PySpark</em>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">NumPy is a Python package for scientific computing that provides a multi-dimensional array and a <a id="_idIndexMarker857"/>set of routines for fast operations on arrays. More information about NumPy can be found here: <a href="https://numpy.org/doc/stable/user/whatisnumpy.html">https://numpy.org/doc/stable/user/whatisnumpy.html</a>.</p>
			<p>The other fundamental difference, in the context of big data and processing massive amounts of data at big data scale, is that pandas was designed to process data on a single machine and PySpark, by design, is distributed and can process data on multiple machines in <a id="_idIndexMarker858"/>a massively parallel manner. This brings up an important limitation of pandas compared to PySpark, as well as some important considerations that the developer needs to keep in mind while converting from pandas into PySpark. We will look at these in the following section. </p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor197"/>Converting from PySpark into pandas</h2>
			<p>The PySpark API comes with a handy utility function called <strong class="source-inline">DataFrame.toPandas()</strong> that converts <a id="_idIndexMarker859"/>PySpark DataFrames into pandas DataFrames. This <a id="_idIndexMarker860"/>function has been demonstrated throughout this chapter. If you recall our discussions from <a href="B16736_01_Final_JM_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>,<strong class="bold"> </strong><em class="italic">Distributed Computing Primer</em>, especially the <em class="italic">Spark's cluster architecture</em> section, a Spark cluster consists of a <strong class="bold">Driver</strong> process and a set of executor processes on worker machines, with the Driver being responsible for compiling user code, passing it on to the workers, managing and communicating with the workers, and if required, aggregating and collecting data from the workers. The Spark workers are responsible for all the data processing tasks. However, pandas is not based on the distributed computing paradigm and works solely on a single computing machine. Thus, when you execute pandas code on a Spark cluster, it executes on the Driver or the Master node, as depicted in the following diagram:</p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="Images/B16736_11_13.jpg" alt="Figure 11.13 – PySpark architecture&#13;&#10;" width="1263" height="576"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.13 – PySpark architecture</p>
			<p>As we can see, Python and <strong class="bold">JVM</strong> are two separate processes running inside the <strong class="bold">Driver</strong>. The communication between the <strong class="bold">Python kernel</strong> and the <strong class="bold">JVM</strong> happens through another process called Py4J, which is the link between the Spark context and the Python kernel inside the Driver. In the case of pandas, it runs inside the Python kernel, completely independent and oblivious of any knowledge of Spark and its DataFrames. Whenever the <strong class="source-inline">toPandas()</strong> function is called on a Spark DataFrame, it collects rows from all the Spark workers and then creates a pandas DataFrame locally on the Driver inside the Python kernel.</p>
			<p>The first issue with this is that <strong class="source-inline">toPandas()</strong> practically collects all the data from the workers and brings it back to the Driver. This may cause the Driver to run out of memory if the dataset being collected is too large. Another issue with this process is that by default, the <strong class="bold">Row</strong> objects of the Spark DataFrame are collected on the Driver as a <strong class="bold">list</strong> of <strong class="bold">tuples</strong>, and then converted to a pandas DataFrame. This ends up using a large amount of memory and sometimes even data that's twice the size of the Spark DataFrame being collected.</p>
			<p>To mitigate <a id="_idIndexMarker861"/>some of the memory issues during PySpark to <a id="_idIndexMarker862"/>pandas conversion, Apache Arrow can be used. Apache Arrow is an in-memory, columnar data format that is similar to Spark's internal representation of datasets and is efficient at transferring data between the JVM and Python processes. Apache Arrow is not enabled by default in Spark and needs to be enabled by setting the <strong class="source-inline">spark.sql.execution.arrow.enabled</strong> Spark configuration to <strong class="source-inline">true</strong>. </p>
			<p class="callout-heading">Note </p>
			<p class="callout">PyArrow, the Python binding of Apache Arrow, is pre-installed on Databricks Runtime. However, you might need to install a version of PyArrow that's appropriate for the Spark and Python versions of your Spark cluster. </p>
			<p>Apache Arrow helps mitigate some of the memory issues that might arise from using <strong class="source-inline">toPandas()</strong>. Despite this optimization, the conversion operation still results in all records in the Spark DataFrame being collected by the Driver, so you should only perform the conversion on a small subset of the original data. Thus, by making use of the PyArrow format and taking care to sample down datasets, you can still use all the open source visualizations that are compatible with standard Python to visualize your PySpark DataFrames in a notebook environment.</p>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor198"/>Summary</h1>
			<p>In this chapter, you learned about the importance of using data visualization to convey meaning from complex datasets in a simple way, as well as to easily surface patterns among data to business users. Various strategies for visualizing data with Spark were introduced. You also learned how to use data visualizations with PySpark natively using Databricks notebooks. We also looked at techniques for using plain Python visualization libraries to visualize data with Spark DataFrames. A few of the prominent open source visualization libraries, such as Matplotlib, Seaborn, Plotly, and Altair, were introduced, along with practical examples of their usage and code samples. Finally, you learned about the pitfalls of using plain Python visualizations with PySpark, the need for PySpark conversion, and some strategies to overcome these issues. </p>
			<p>The next chapter will cover the topic of connecting various BI and SQL analysis tools to Spark, which will help you perform ad hoc data analysis and build complex operational and performance dashboards.</p>
		</div>
	</div></body></html>