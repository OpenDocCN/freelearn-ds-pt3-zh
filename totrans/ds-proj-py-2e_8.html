<html><head></head><body>
		<div class="Content" id="_idContainer226">
			<h1 id="_idParaDest-148"><a id="_idTextAnchor147"/>Appendix</h1>
		</div>
		<div class="Content" id="_idContainer249">
			<h1 id="_idParaDest-149"><a id="_idTextAnchor148"/>1. Data Exploration and Cleaning</h1>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor149"/>Activity 1.01: Exploring the Remaining Financial Features in the Dataset</h2>
			<p><strong class="bold">Solution:</strong></p>
			<p>Before beginning, set up your environment and load in the cleaned dataset as follows:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">import matplotlib.pyplot as plt #import plotting package</p>
			<p class="source-code">#render plotting automatically</p>
			<p class="source-code">%matplotlib inline</p>
			<p class="source-code">import matplotlib as mpl #additional plotting functionality</p>
			<p class="source-code">mpl.rcParams['figure.dpi'] = 400 #high resolution figures</p>
			<p class="source-code">mpl.rcParams['font.size'] = 4 #font size for figures</p>
			<p class="source-code">from scipy import stats</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">df = pd.read_csv('../../Data/Chapter_1_cleaned_data.csv')</p>
			<ol>
				<li>Create lists of feature names for the remaining financial features.<p>These fall into two groups, so we will make lists of feature names as before, to facilitate analyzing them together. You can do this with the following code:</p><p class="source-code">bill_feats = ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', \</p><p class="source-code">              'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']</p><p class="source-code">pay_amt_feats = ['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', \</p><p class="source-code">                 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']</p></li>
				<li>Use <strong class="source-inline">.describe()</strong> to examine statistical summaries of the bill amount features. Reflect on what you see. Does it make sense?<p>Use the following code to view the summary:</p><p class="source-code">df[bill_feats].describe()</p><p>The output should appear as follows:</p><div class="IMG---Figure" id="_idContainer227"><img alt="Figure 1.47: Statistical description of bill amounts for the past 6 months&#13;&#10;" src="image/B16392_01_47.jpg"/></div><p class="figure-caption">Figure 1.47: Statistical description of bill amounts for the past 6 months</p><p>We see that the average monthly bill is roughly 40,000 to 50,000 NT dollars. You are encouraged to examine the conversion rate to your local currency. For example, 1 US dollar ~= 30 NT dollars. Do the conversion and ask yourself, is this a reasonable monthly payment? We should also confirm this with the client, but it seems reasonable.</p><p>We also notice there are some negative bill amounts. This seems reasonable because of the possible overpayment of the previous month's bill, perhaps in anticipation of a purchase that would show up on the current month's bill. A scenario like this would leave that account with a negative balance, in the sense of a credit to the account holder.</p></li>
				<li>Visualize the bill amount features using a 2 by 3 grid of histogram plots using the following code:<p class="source-code">df[bill_feats].hist(bins=20, layout=(2,3))</p><p>The graph should look like this:</p><div class="IMG---Figure" id="_idContainer228"><img alt="Figure 1.48: Histograms of bill amounts&#13;&#10;" src="image/B16392_01_48.jpg"/></div><p class="figure-caption">Figure 1.48: Histograms of bill amounts</p><p>The histogram plots in <em class="italic">Figure 1.48</em> make sense in several respects. Most accounts have relatively small bills. There is a steady decrease in the number of accounts as the amount of the bill increases. It also appears that the distribution of billed amounts is roughly similar month to month, so we don't notice any data inconsistency issues as we did with the payment status features. This feature appears to pass our data quality inspection. Now, we'll move on to the final set of features.</p></li>
				<li>Use the <strong class="source-inline">.describe()</strong> method to obtain a summary of the payment amount features using the following code: <p class="source-code">df[pay_amt_feats].describe()</p><p>The output should appear thus:</p><div class="IMG---Figure" id="_idContainer229"><img alt="Figure 1.49: Statistical description of bill payment amounts for the past 6 months&#13;&#10;" src="image/B16392_01_49.jpg"/></div><p class="figure-caption">Figure 1.49: Statistical description of bill payment amounts for the past 6 months</p><p>The average payment amounts are about an order of magnitude (power of 10) lower than the average bill amounts we summarized earlier in the activity. This means that the "average case" is an account that is not paying off its entire balance from month to month. This makes sense in light of our exploration of the <strong class="source-inline">PAY_1</strong> feature, for which the most prevalent value was 0 (the account made at least the minimum payment but did not pay off the whole balance). There are no negative payments, which also seems right.</p></li>
				<li>Plot a histogram of the bill payment features similar to the bill amount features, but also apply some rotation to the <em class="italic">x-axis</em> labels with the <strong class="source-inline">xrot</strong> keyword argument so that they don't overlap. Use the <strong class="source-inline">xrot=&lt;angle&gt;</strong> keyword argument to rotate the <em class="italic">x-axis</em> labels by a given angle in degrees using the following code:<p class="source-code">df[pay_amt_feats].hist(layout=(2,3), xrot=30)</p><p>In our case, we found that 30 degrees of rotation worked well. The plot should look like this:</p><div class="IMG---Figure" id="_idContainer230"><img alt="Figure 1.50: Histograms of raw payment amount data&#13;&#10;" src="image/B16392_01_50.jpg"/></div><p class="Normal">``````</p><p class="figure-caption">Figure 1.50: Histograms of raw payment amount data</p><p>A quick glance at this figure indicates that this is not a very informative graphic; there is only one bin in most of the histograms that is of a noticeable height. This is not an effective way to visualize this data. It appears that the monthly payment amounts are mainly in a bin that includes 0. How many are in fact 0?</p></li>
				<li>Use a Boolean mask to see how much of the payment amount data is exactly equal to 0 using the following code: Do this with the following code:<p class="source-code">pay_zero_mask = df[pay_amt_feats] == 0</p><p class="source-code">pay_zero_mask.sum()</p><p>The output should look like this:</p><div class="IMG---Figure" id="_idContainer231"><img alt="Figure 1.51: Counts of bill payments equal to 0&#13;&#10;" src="image/B16392_01_51.jpg"/></div><p class="figure-caption">Figure 1.51: Counts of bill payments equal to 0</p><p><strong class="bold">Does this data make sense given the histogram in the previous step?</strong></p><p>The first line here creates a new DataFrame called <strong class="source-inline">pay_zero_mask</strong>, which is a DataFrame of <strong class="source-inline">True</strong> and <strong class="source-inline">False</strong> values according to whether the payment amount is equal to 0. The second line takes the column sums of this DataFrame, interpreting <strong class="source-inline">True</strong> as 1 and <strong class="source-inline">False</strong> as 0, so the column sums indicate how many accounts have a value of 0 for each feature.</p><p>We see that a substantial portion, roughly around 20-25% of accounts, have a bill payment equal to 0 in any given month. However, most bill payments are above 0. So, why can't we see them in the histogram? This is due to the <strong class="bold">range</strong> of values for bill payments relative to the values of the majority of the bill payments.</p><p>In the statistical summary, we can see that the maximum bill payment in a month is typically 2 orders of magnitude (100 times) larger than the average bill payment. It seems likely there are only a small number of these very large bill payments. But, because of the way the histogram is created, using equal-sized bins, nearly all the data is lumped into the smallest bin, and the larger bins are nearly invisible because they have so few accounts. We need a strategy to effectively visualize this data.</p></li>
				<li>Ignoring the payments of 0 using the mask you created in the previous step, use pandas' <strong class="source-inline">.apply()</strong> and NumPy's <strong class="source-inline">np.log10()</strong> method to plot histograms of logarithmic transformations of the non-zero payments. You can use <strong class="source-inline">.apply()</strong> to apply any function, including <strong class="source-inline">log10</strong>, to all the elements of a DataFrame. Use the following code for this:<p class="source-code">df[pay_amt_feats][~pay_zero_mask].apply(np.log10)\</p><p class="source-code">                                 .hist(layout=(2,3))</p><p>This is a relatively advanced use of pandas, so don't worry if you couldn't figure it out by yourself. However, it's good to start to get an impression of how you can do a lot in pandas with relatively little code. </p><p>The output should be as follows:</p><div class="IMG---Figure" id="_idContainer232"><img alt="Figure 1.52: Base-10 logs of non-zero bill payment amounts&#13;&#10;" src="image/B16392_01_52.jpg"/></div></li>
			</ol>
			<p class="figure-caption">Figure 1.52: Base-10 logs of non-zero bill payment amounts</p>
			<p>While we could have tried to create variable-width bins for better visualization of the payment amounts, a more convenient approach that is often used to visualize, and sometimes even model, data that has a few values on a much different scale than most of the values is a logarithmic transformation, or <strong class="bold">log transform</strong>. We used a base-10 log transform. Roughly speaking, this transform tells us the number of zeros in a value. In other words, a balance of at least 1 million dollars, but less than 10 million, would have a log transform of at least 6 but less than 7, because 106 = 1,000,000 (and conversely log10(1,000,000) = 6) while 107 = 10,000,000.</p>
			<p>To apply this transformation to our data, first, we needed to mask out the zero payments, because <strong class="source-inline">log10(0)</strong> is undefined (another common approach in this case is to add a very small number to all values, such as 0.01, so there are no zeros). We did this with the Python logical <strong class="source-inline">not</strong> operator <strong class="source-inline">~</strong> and the zero mask we created already. Then we used the pandas <strong class="source-inline">.apply()</strong> method, which applies any function we like to the data we have selected. In this case, we wished to apply a base-10 logarithm, calculated by <strong class="source-inline">np.log10</strong>. Finally, we made histograms of these values.</p>
			<p>The result is a more effective data visualization: the values are spread in a more informative way across the histogram bins. We can see that the most commonly occurring bill payments are in the range of thousands (<strong class="source-inline">log10(1,000) = 3</strong>), which matches what we observed for the mean bill payment in the statistical summary. There are some pretty small bill payments, and also a few pretty large ones. Overall, the distribution of bill payments appears pretty consistent from month to month, so we don't see any potential issues with this data.</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor150"/>2. Introduction to Scikit-Learn and Model Evaluation</h1>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor151"/>Activity 2.01: Performing Logistic Regression with a New Feature and Creating a Precision-Recall Curve</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Use scikit-learn's <strong class="source-inline">train_test_split</strong> to make a new set of training and test data. This time, instead of <strong class="source-inline">EDUCATION</strong>, use <strong class="source-inline">LIMIT_BAL</strong>, the account's credit limit, as the feature.<p>Execute the following code to do this:</p><p class="source-code">X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split\</p><p class="source-code">                                          (df['LIMIT_BAL']\</p><p class="source-code">                                           .values\</p><p class="source-code">                                           .reshape(-1,1),\</p><p class="source-code">                                           df['default'\</p><p class="source-code">                                              'payment next'\</p><p class="source-code">                                              'month'].values,\</p><p class="source-code">                                           test_size=0.2,\</p><p class="source-code">                                           random_state=24))</p><p>Notice here we create new training and test splits, with new variable names.</p></li>
				<li>Train a logistic regression model using the training data from your split.<p>The following code does this:</p><p class="source-code">example_lr.fit(X_train_2, y_train_2)</p><p>You can reuse the same model object you used earlier, <strong class="source-inline">example_lr</strong>, if you're running the whole chapter in a single notebook. You can <strong class="bold">re-train</strong> this object to learn the relationship between this new feature and the response. You could even try a different train/test split, if you wanted to, without creating a new model object. The existing model object has been updated <strong class="bold">in-place</strong> in these scenarios.</p></li>
				<li>Create the array of predicted probabilities for the test data.<p>Here is the code for this step:</p><p class="source-code">y_test_2_pred_proba = example_lr.predict_proba(X_test_2)</p></li>
				<li>Calculate the ROC AUC using the predicted probabilities and the true labels of the test data. Compare this to the ROC AUC from using the <strong class="source-inline">EDUCATION</strong> feature.<p>Run this code for this step:</p><p class="source-code">metrics.roc_auc_score(y_test_2, y_test_2_pred_proba[:,1])</p><p>The output is as follows:</p><p class="source-code">0.6201990844642832</p><p>Notice that we index the predicted probabilities array in order to get the predicted probability of the positive class from the second column. How does this compare to the ROC AUC from the <strong class="source-inline">EDUCATION</strong> logistic regression? The AUC is higher. This may be because now we are using a feature that has something to do with an account's financial status (credit limit), to predict something else related to the account's financial status (whether or not it will default), instead of using something less directly related to finances.</p></li>
				<li>Plot the ROC curve.<p>Here is the code to do this; it's similar to the code we used in the previous exercise:</p><p class="source-code">fpr_2, tpr_2, thresholds_2 = metrics.roc_curve\</p><p class="source-code">                             (y_test_2, \</p><p class="source-code">                              y_test_2_pred_proba[:,1])</p><p class="source-code">plt.plot(fpr_2, tpr_2, '*-')</p><p class="source-code">plt.plot([0, 1], [0, 1], 'r--')</p><p class="source-code">plt.legend(['Logistic regression', 'Random chance'])</p><p class="source-code">plt.xlabel('FPR')</p><p class="source-code">plt.ylabel('TPR')</p><p class="source-code">plt.title('ROC curve for logistic regression with '\</p><p class="source-code">          'LIMIT_BAL feature')</p><p>The plot should appear as follows:</p><div class="IMG---Figure" id="_idContainer233"><img alt="Figure 2.30: ROC curve for the LIMIT_BAL logistic regression&#13;&#10;" src="image/B16392_02_30.jpg"/></div><p class="figure-caption">Figure 2.30: ROC curve for the LIMIT_BAL logistic regression</p><p>This looks a little closer to an ROC curve that we'd like to see: it's a bit further from the random chance line than the model using only <strong class="source-inline">EDUCATION</strong>. Also notice that the variation in pairs of true and false positive rates is a little smoother over the range of thresholds, reflective of the larger number of distinct values of the <strong class="source-inline">LIMIT_BAL</strong> feature.</p></li>
				<li>Calculate the data for the precision-recall curve on the test data using scikit-learn's functionality.<p>Precision is often considered in tandem with recall. We can use <strong class="source-inline">precision_recall_curve</strong> in <strong class="source-inline">sklearn.metrics</strong> to automatically vary the threshold and calculate pairs of precision and recall values at each threshold value. Here is the code to retrieve these values, which is similar to <strong class="source-inline">roc_curve</strong>:</p><p class="source-code">precision, recall, thresh_3 = metrics.precision_recall_curve\</p><p class="source-code">                              (y_test_2,\</p><p class="source-code">                               y_test_2_pred_proba[:,1])</p></li>
				<li>Plot the precision-recall curve using matplotlib: we can do this with the following code.<p>Note that we put recall on the <strong class="source-inline">x</strong>-axis and precision on the <strong class="source-inline">y</strong>-axis, and we set the axes' limits to the range [0, 1]:</p><p class="source-code">plt.plot(recall, precision, '-x')</p><p class="source-code">plt.xlabel('Recall')</p><p class="source-code">plt.ylabel('Precision')</p><p class="source-code">plt.title('Precision and recall for the logistic'\</p><p class="source-code">          'regression 'with LIMIT_BAL')</p><p class="source-code">plt.xlim([0, 1])</p><p class="source-code">plt.ylim([0, 1])</p><div class="IMG---Figure" id="_idContainer234"><img alt="Figure 2.31: Plot of the precision-recall curve&#13;&#10;" src="image/B16392_02_31.jpg"/></div><p class="figure-caption">Figure 2.31: Plot of the precision-recall curve</p></li>
				<li>Use scikit-learn to calculate the area under the precision-recall curve.<p>Here is the code for this:</p><p class="source-code">metrics.auc(recall, precision)</p><p>You will obtain the following output:</p><p class="source-code">0.31566964427378624</p><p>We saw that the precision-recall curve shows that precision is generally fairly low for this model; for nearly all of the range of thresholds, the precision, or portion of positive classifications that are correct, is less than half. We can calculate the area under the precision-recall curve as a way to compare this classifier with other models or feature sets we may consider.</p><p>Scikit-learn offers functionality for calculating an AUC for any set of <strong class="source-inline">x-y</strong> data, using the trapezoid rule, which you may recall from calculus: <strong class="source-inline">metrics.auc</strong>. We used this functionality to get the area under the precision-recall curve.</p></li>
				<li>Now recalculate the ROC AUC, except this time do it for the training data. How is this different, conceptually and quantitatively, from your earlier calculation?<p>First, we need to calculate predicted probabilities using the training data, as opposed to the test data. Then we can calculate the ROC AUC using the training data labels. Here is the code:</p><p class="source-code">y_train_2_pred_proba = example_lr.predict_proba(X_train_2)</p><p class="source-code">metrics.roc_auc_score(y_train_2, y_train_2_pred_proba[:,1])</p><p>You should obtain the following output:</p><p class="source-code">0.6182918113358344</p></li>
			</ol>
			<p>Quantitatively, we can see that this AUC is not all that different from the test data ROC AUC we calculated earlier. Both are about 0.62. Conceptually, what is the difference? When we calculate this metric on the training data, we are measuring the model's skill in predicting the same data that "taught" the model how to make predictions. We are seeing <em class="italic">how well the model fits the data</em>. On the other hand, test data metrics indicate performance on out-of-sample data the model hasn't "seen" before. If there was much of a difference in these scores, which usually would come in the form of a higher training score than the test score, it would indicate that while the model fits the data well, the trained model does not generalize well to new, unseen data.</p>
			<p>In this case, the training and test scores are similar, meaning the model does about as well on out-of-sample data as it does on the same data used in model training. We will learn more about the insights we can gain by comparing training and test scores in <em class="italic">Chapter 4,</em> <em class="italic">The Bias-Variance Trade-Off</em>.</p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor152"/>3. Details of Logistic Regression and Feature Exploration</h1>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor153"/>Activity 3.01: Fitting a Logistic Regression Model and Directly Using the Coefficients</h2>
			<p><strong class="bold">Solution:</strong></p>
			<p>The first few steps are similar to things we've done in previous activities:</p>
			<ol>
				<li value="1">Create a train/test split (80/20) with <strong class="source-inline">PAY_1</strong> and <strong class="source-inline">LIMIT_BAL</strong> as features:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split(</p><p class="source-code">    df[['PAY_1', 'LIMIT_BAL']].values,</p><p class="source-code">    df['default payment next month'].values,</p><p class="source-code">    test_size=0.2, random_state=24)</p></li>
				<li>Import <strong class="source-inline">LogisticRegression</strong>, with the default options, but set the solver to <strong class="source-inline">'liblinear'</strong>:<p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">lr_model = LogisticRegression(solver='liblinear')</p></li>
				<li>Train on the training data and obtain predicted classes, as well as class probabilities, using the test data:<p class="source-code">lr_model.fit(X_train, y_train)</p><p class="source-code">y_pred = lr_model.predict(X_test)</p><p class="source-code">y_pred_proba = lr_model.predict_proba(X_test)</p></li>
				<li>Pull out the coefficients and intercept from the trained model and manually calculate predicted probabilities. You'll need to add a column of ones to your features, to multiply by the intercept.<p>First, let's create the array of features, with a column of ones added, using horizontal stacking:</p><p class="source-code">ones_and_features = np.hstack\</p><p class="source-code">                    ([np.ones((X_test.shape[0],1)), X_test])</p><p>Now we need the intercept and coefficients, which we reshape and concatenate from scikit-learn output:</p><p class="source-code">intercept_and_coefs = np.concatenate\</p><p class="source-code">                      ([lr_model.intercept_.reshape(1,1), \</p><p class="source-code">                        lr_model.coef_], axis=1)</p><p>To repeatedly multiply the intercept and coefficients by all the rows of <strong class="source-inline">ones_and_features</strong>, and take the sum of each row (that is, find the linear combination), you could write this all out using multiplication and addition. However, it's much faster to use the dot product:</p><p class="source-code">X_lin_comb = np.dot(intercept_and_coefs,\</p><p class="source-code">                    np.transpose(ones_and_features))</p><p>Now <strong class="source-inline">X_lin_comb</strong> has the argument we need to pass to the sigmoid function we defined, in order to calculate predicted probabilities:</p><p class="source-code">y_pred_proba_manual = sigmoid(X_lin_comb)</p></li>
				<li>Using a threshold of <strong class="source-inline">0.5</strong>, manually calculate predicted classes. Compare this to the class predictions outputted by scikit-learn.<p>The manually predicted probabilities, <strong class="source-inline">y_pred_proba_manual</strong>, should be the same as <strong class="source-inline">y_pred_proba</strong>; we'll check that momentarily. First, manually predict the classes with the threshold:</p><p class="source-code">y_pred_manual = y_pred_proba_manual &gt;= 0.5</p><p>This array will have a different shape than <strong class="source-inline">y_pred</strong>, but it should contain the same values. We can check whether all the elements of two arrays are equal like this:</p><p class="source-code">np.array_equal(y_pred.reshape(1,-1), y_pred_manual)</p><p>This should return a logical <strong class="source-inline">True</strong> if the arrays are equal.</p></li>
				<li>Calculate ROC AUC using both scikit-learn's predicted probabilities and your manually predicted probabilities, and compare them.<p>First, import the following:</p><p class="source-code">from sklearn.metrics import roc_auc_score</p><p>Then, calculate this metric on both versions, taking care to access the correct column, or reshape as necessary:</p><div class="IMG---Figure" id="_idContainer235"><img alt="Figure 3.37: Calculating the ROC AUC from predicted probabilities&#13;&#10;" src="image/B16392_03_37.jpg"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.37: Calculating the ROC AUC from predicted probabilities</p>
			<p>The AUCs are, in fact, the same. What have we done here? We've confirmed that all we really need from this fitted scikit-learn model is three numbers: the intercept and the two coefficients. Once we have these, we could create model predictions using a few lines of code, with mathematical functions, that are equivalent to the predictions directly made from scikit-learn.</p>
			<p>This is good to confirm your understanding, but otherwise, why would you ever want to do this? We'll talk about <strong class="bold">model deployment</strong> in the final chapter. However, depending on your circumstances, you may be in a situation where you don't have access to Python in the environment where new features will need to be input into the model for prediction. For example, you may need to make predictions entirely in SQL. While this is a limitation in general, with logistic regression you can use mathematical functions that are available in SQL to re-create the logistic regression prediction, only needing to copy and paste the intercept and coefficients somewhere in your SQL code. The dot product may not be available, but you can use multiplication and addition to accomplish the same purpose.</p>
			<p>Now, what about the results themselves? What we've seen here is that we can slightly boost model performance above our previous efforts: using just <strong class="source-inline">LIMIT_BAL</strong> as a feature in the previous chapter's activity, the ROC AUC was a bit less at 0.62, instead of 0.63 here. In the next chapter, we'll learn advanced techniques with logistic regression that we can use to further improve performance.</p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor154"/>4. The Bias-Variance Trade-Off</h1>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor155"/>Activity 4.01: Cross-Validation and Feature Engineering with the Case Study Data</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Select out the features from the DataFrame of the case study data. <p>You can use the list of feature names that we've already created in this chapter, but be sure not to include the response variable, which would be a very good (but entirely inappropriate) feature:</p><p class="source-code">features = features_response[:-1]</p><p class="source-code">X = df[features].values</p></li>
				<li>Make a training/test split using a random seed of 24: <p class="source-code">X_train, X_test, y_train, y_test = \</p><p class="source-code">train_test_split(X, df['default payment next month'].values,</p><p class="source-code">                 test_size=0.2, random_state=24)</p><p>We'll use this going forward and reserve this test data as the unseen test set. By specifying the random seed, we can easily create separate notebooks with other modeling approaches using the same training data.</p></li>
				<li>Instantiate <strong class="source-inline">MinMaxScaler</strong> to scale the data, as shown in the following code:<p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">min_max_sc = MinMaxScaler()</p></li>
				<li>Instantiate a logistic regression model with the <strong class="source-inline">saga</strong> solver, L1 penalty, and set <strong class="source-inline">max_iter</strong> to <strong class="source-inline">1000</strong>, as we'd like to allow the solver enough iterations to find a good solution:<p class="source-code">lr = LogisticRegression(solver='saga', penalty='l1',</p><p class="source-code">                        max_iter=1000)</p></li>
				<li>Import the <strong class="source-inline">Pipeline</strong> class and create a pipeline with the scaler and the logistic regression model, using the names <strong class="source-inline">'scaler'</strong> and <strong class="source-inline">'model'</strong> for the steps, respectively:<p class="source-code">from sklearn.pipeline import Pipeline</p><p class="source-code">scale_lr_pipeline = Pipeline(</p><p class="source-code">    steps=[('scaler', min_max_sc), ('model', lr)])</p></li>
				<li>Use the <strong class="source-inline">get_params</strong> and <strong class="source-inline">set_params</strong> methods to see how to view the parameters from each stage of the pipeline and change them (execute each of the following lines in a separate cell in your notebook and observe the output):<p class="source-code">scale_lr_pipeline.get_params()</p><p class="source-code">scale_lr_pipeline.get_params()['model__C']</p><p class="source-code">scale_lr_pipeline.set_params(model__C = 2)</p></li>
				<li>Create a smaller range of <em class="italic">C</em> values to test with cross-validation, as these models will take longer to train and test with more data than our previous exercise; we recommend <em class="italic">C = [10</em><span class="superscript">2</span><em class="italic">, 10, 1, 10</em><span class="superscript">-1</span><em class="italic">, 10</em><span class="superscript">-2</span><em class="italic">, 10</em><span class="superscript">-3</span><em class="italic">]</em>:<p class="source-code">C_val_exponents = np.linspace(2,-3,6)</p><p class="source-code">C_vals = np.float(10)**C_val_exponents</p></li>
				<li>Make a new version of the <strong class="source-inline">cross_val_C_search</strong> function, called <strong class="source-inline">cross_val_C_search_pipe</strong>. Instead of the <strong class="source-inline">model</strong> argument, this function will take a <strong class="source-inline">pipeline</strong> argument. The changes inside the function will be to set the <em class="italic">C</em> value using <strong class="source-inline">set_params(model__C = &lt;value you want to test&gt;)</strong> on the pipeline, replacing the model with the pipeline for the <strong class="source-inline">fit</strong> and <strong class="source-inline">predict_proba</strong> methods, and accessing the <em class="italic">C</em> value using <strong class="source-inline">pipeline.get_params()['model__C']</strong> for the printed status update.<p>The changes are as follows:</p><p class="source-code">def cross_val_C_search_pipe(k_folds, C_vals, pipeline, X, Y):</p><p class="source-code">##[…]</p><p class="source-code">pipeline.set_params(model__C = C_vals[c_val_counter])</p><p class="source-code">##[…]</p><p class="source-code">pipeline.fit(X_cv_train, y_cv_train)</p><p class="source-code">##[…]</p><p class="source-code">y_cv_train_predict_proba = pipeline.predict_proba(X_cv_train)</p><p class="source-code">##[…]</p><p class="source-code">y_cv_test_predict_proba = pipeline.predict_proba(X_cv_test)</p><p class="source-code">##[…]</p><p class="source-code">print('Done with C = {}'.format(pipeline.get_params()\</p><p class="source-code">                                ['model__C']))</p><p class="callout-heading">Note</p><p class="callout">For the complete code, refer to <a href="https://packt.link/AsQmK">https://packt.link/AsQmK</a>. </p></li>
				<li>Run this function as in the previous exercise, but using the new range of <em class="italic">C</em> values, the pipeline you created, and the features and response variable from the training split of the case study data. You may see warnings here, or in later steps, regarding the non-convergence of the solver; you could experiment with the <strong class="source-inline">tol</strong> or <strong class="source-inline">max_iter</strong> options to try and achieve convergence, although the results you obtain with <strong class="source-inline">max_iter = 1000</strong> are likely to be sufficient. Here is the code to do this:<p class="source-code">cv_train_roc_auc, cv_test_roc_auc, cv_test_roc = \</p><p class="source-code">cross_val_C_search_pipe(k_folds, C_vals, scale_lr_pipeline,</p><p class="source-code">                        X_train, y_train)</p><p>You will obtain the following output:</p><p class="source-code">Done with C = 100.0</p><p class="source-code">Done with C = 10.0</p><p class="source-code">Done with C = 1.0</p><p class="source-code">Done with C = 0.1</p><p class="source-code">Done with C = 0.01</p><p class="source-code">Done with C = 0.001</p></li>
				<li>Plot the average training and test ROC AUC across folds, for each <em class="italic">C</em> value, using the following code:<p class="source-code">plt.plot(C_val_exponents, np.mean(cv_train_roc_auc, axis=0),</p><p class="source-code">         '-o', label='Average training score')</p><p class="source-code">plt.plot(C_val_exponents, np.mean(cv_test_roc_auc, axis=0),</p><p class="source-code">         '-x', label='Average testing score')</p><p class="source-code">plt.ylabel('ROC AUC')</p><p class="source-code">plt.xlabel('log$_{10}$(C)')</p><p class="source-code">plt.legend()</p><p class="source-code">plt.title('Cross-validation on Case Study problem')</p><p>You will obtain the following output:</p><div class="IMG---Figure" id="_idContainer236"><img alt="Figure 4.25: Cross-validation test performance&#13;&#10;" src="image/B16392_04_25.jpg"/></div><p class="figure-caption">Figure 4.25: Cross-validation test performance</p><p>You should notice that regularization does not impart much benefit here, as may be expected: for lower <em class="italic">C</em> values, which correspond to stronger regularization, model testing (as well as training) performance decreases. While we are able to increase model performance over our previous efforts by using all the features available, it appears there is no overfitting going on. Instead, the training and test scores are about the same. Instead of overfitting, it's possible that we may be underfitting. Let's try engineering some interaction features to see if they can improve performance.</p></li>
				<li>Create interaction features for the case study data and confirm that the number of new features makes sense using the following code:<p class="source-code">from sklearn.preprocessing import PolynomialFeatures</p><p class="source-code">make_interactions = PolynomialFeatures(degree=2,</p><p class="source-code">                                       interaction_only=True,</p><p class="source-code">                                       include_bias=False)</p><p class="source-code">X_interact = make_interactions.fit_transform(X)</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split(</p><p class="source-code">    X_interact, df['default payment next month'].values,</p><p class="source-code">    test_size=0.2, random_state=24)</p><p class="source-code">print(X_train.shape)</p><p class="source-code">print(X_test.shape)</p><p>You will obtain the following output:</p><p class="source-code">(21331, 153)</p><p class="source-code">(5333, 153)</p><p>From this you should see the new number of features is 153, which is <em class="italic">17 + "17 choose 2" = 17 + 136 = 153</em>. The <em class="italic">"17 choose 2"</em> part comes from choosing all possible combinations of 2 features to interact from the 17 original features.</p></li>
				<li>Repeat the cross-validation procedure and observe the model performance when using interaction features; that is, repeat <em class="italic">steps 9</em> and <em class="italic">10</em>. Note that this will take substantially more time, due to the larger number of features, but it will probably take less than 10 minutes. <p>You will obtain the following output:</p><div class="IMG---Figure" id="_idContainer237"><img alt="Figure 4.26: Improved cross-validation test performance from adding interaction features&#13;&#10;" src="image/B16392_04_26.jpg"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.26: Improved cross-validation test performance from adding interaction features</p>
			<p>So, does the average cross-validation test performance improve with the interaction features? Is regularization useful?</p>
			<p>Engineering the interaction features increases the best model test score to about <em class="italic">ROC AUC = 0.74</em> on average across the folds, from about 0.72 without including interactions. These scores happen at <em class="italic">C = 100</em>, that is, with negligible regularization. On the plot of training versus test scores for the model with interactions, you can see that the training score is a bit higher than the test score, so it could be said that some amount of overfitting is going on. However, we cannot increase the test score through regularization here, so this may not be a problematic instance of overfitting. In most cases, whatever strategy yields the highest test score is the best strategy.</p>
			<p>In summary, adding interaction features improved cross-validation performance, and regularization appears not to be useful for the case study at this point, using a logistic regression model. We will reserve the step of fitting on all the training data for later when we've tried other models in cross-validation to find the best model.</p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor156"/>5. Decision Trees and Random Forests</h1>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor157"/>Activity 5.01: Cross-Validation Grid Search with Random Forest</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Create a dictionary representing the grid for the <strong class="source-inline">max_depth</strong> and <strong class="source-inline">n_estimators</strong> hyperparameters that will be searched. Include depths of 3, 6, 9, and 12, and 10, 50, 100, and 200 trees. Leave the other hyperparameters at their defaults. Create the dictionary using this code:<p class="source-code">rf_params = {'max_depth':[3, 6, 9, 12],</p><p class="source-code">             'n_estimators':[10, 50, 100, 200]}</p><p class="callout-heading">Note</p><p class="callout">There are many other possible hyperparameters to search over. In particular, the scikit-learn documentation for random forest indicates that "The main parameters to adjust when using these methods are <strong class="source-inline">n_estimators</strong> and <strong class="source-inline">max_features</strong>" and that "Empirical good default values are … <strong class="source-inline">max_features=sqrt(n_features)</strong> for classification tasks."</p><p class="callout">Source: https://scikit-learn.org/stable/modules/ensemble.html#parameters</p><p class="callout">For the purposes of this book, we will use <strong class="source-inline">max_features='auto'</strong> (which is equal to <strong class="source-inline">sqrt(n_features)</strong>) and limit our exploration to <strong class="source-inline">max_depth</strong> and <strong class="source-inline">n_estimators</strong> for the sake of a shorter runtime. In a real-world situation, you should explore other hyperparameters according to how much computational time you can afford. Remember that in order to search in especially large parameter spaces, you can use <strong class="source-inline">RandomizedSearchCV</strong> to avoid exhaustively calculating metrics for every combination of hyperparameters in the grid.</p></li>
				<li>Instantiate a <strong class="source-inline">GridSearchCV</strong> object using the same options that we have previously used in this chapter, but with the dictionary of hyperparameters created in step 1 here. Set <strong class="source-inline">verbose=2</strong> to see the output for each fit performed. You can reuse the same random forest model object, <strong class="source-inline">rf</strong>, that we have been using or create a new one. Create a new random forest object and instantiate the <strong class="source-inline">GridSearchCV</strong> class using this code:<p class="source-code">rf = RandomForestClassifier(n_estimators=10,\</p><p class="source-code">                            criterion='gini',\</p><p class="source-code">                            max_depth=3,\</p><p class="source-code">                            min_samples_split=2,\</p><p class="source-code">                            min_samples_leaf=1,\</p><p class="source-code">                            min_weight_fraction_leaf=0.0,\</p><p class="source-code">                            max_features='auto',\</p><p class="source-code">                            max_leaf_nodes=None,\</p><p class="source-code">                            min_impurity_decrease=0.0,\</p><p class="source-code">                            min_impurity_split=None,\</p><p class="source-code">                            bootstrap=True,\</p><p class="source-code">                            oob_score=False,\</p><p class="source-code">                            n_jobs=None,</p><p class="source-code">                            random_state=4,\</p><p class="source-code">                            verbose=0,\</p><p class="source-code">                            warm_start=False,\</p><p class="source-code">                            class_weight=None)</p><p class="source-code">cv_rf = GridSearchCV(rf, param_grid=rf_params,\</p><p class="source-code">                     scoring='roc_auc',\</p><p class="source-code">                     n_jobs=-1,\</p><p class="source-code">                     refit=True,\</p><p class="source-code">                     cv=4,\</p><p class="source-code">                     verbose=2,\</p><p class="source-code">                     error_score=np.nan,\</p><p class="source-code">                     return_train_score=True)</p></li>
				<li>Fit the <strong class="source-inline">GridSearchCV</strong> object on the training data. Perform the grid search using this code:<p class="source-code">cv_rf.fit(X_train, y_train)</p><p>Because we chose the <strong class="source-inline">verbose=2</strong> option, you will see a relatively large amount of output in the notebook. There will be output for each combination of hyperparameters and, for each fold, as it is fitted and tested. Here are the first few lines of output:</p><div class="IMG---Figure" id="_idContainer238"><img alt="Figure 5.22: The verbose output from cross-validation&#13;&#10;" src="image/B16392_05_22.jpg"/></div><p class="figure-caption">Figure 5.22: The verbose output from cross-validation</p><p>While it's not necessary to see all this output for shorter cross-validation procedures, for longer ones, it can be reassuring to see that the cross-validation is working and to give you an idea of how long the fits are taking for various combinations of hyperparameters. If things are taking too long, you may want to interrupt the kernel by pushing the stop button (square) at the top of the notebook and choosing hyperparameters that will take less time to run, or use a more limited set of hyperparameters.</p><p>When this is all done, you should see the following output:</p><div class="IMG---Figure" id="_idContainer239"><img alt="Figure 5.22: The cross-validation output upon completion&#13;&#10;" src="image/B16392_05_23.jpg"/></div><p class="figure-caption">Figure 5.23: The cross-validation output upon completion</p><p>This cross-validation job took about 2 minutes to run. As your jobs grow, you may wish to explore parallel processing with the <strong class="source-inline">n_jobs</strong> parameter to see whether it's possible to speed up the search. Using <strong class="source-inline">n_jobs=-1</strong> for parallel processing, you should be able to achieve shorter runtimes than with serial processing. However, with parallel processing, you won't be able to see the output of each individual model fitting operation, as shown in <em class="italic">Figure 5.23</em>.</p></li>
				<li>Put the results of the grid search in a pandas DataFrame. Use this code to put the results in a DataFrame:<p class="source-code">cv_rf_results_df = pd.DataFrame(cv_rf.cv_results_)</p></li>
				<li>Create a <strong class="source-inline">pcolormesh</strong> visualization of the mean testing score for each combination of hyperparameters. Here is the code to create a mesh graph of cross-validation results. It's similar to the example graph that we created previously, but with annotation that is specific to the cross-validation we performed here:<p class="source-code">ax_rf = plt.axes()</p><p class="source-code">pcolor_graph = ax_rf.pcolormesh\</p><p class="source-code">               (xx_rf, yy_rf,\</p><p class="source-code">                cv_rf_results_df['mean_test_score']\</p><p class="source-code">                .values.reshape((4,4)), cmap=cm_rf)</p><p class="source-code">plt.colorbar(pcolor_graph, label='Average testing ROC AUC')</p><p class="source-code">ax_rf.set_aspect('equal')</p><p class="source-code">ax_rf.set_xticks([0.5, 1.5, 2.5, 3.5])</p><p class="source-code">ax_rf.set_yticks([0.5, 1.5, 2.5, 3.5])</p><p class="source-code">ax_rf.set_xticklabels\</p><p class="source-code">([str(tick_label) for tick_label in rf_params['n_estimators']])</p><p class="source-code">ax_rf.set_yticklabels\</p><p class="source-code">([str(tick_label) for tick_label in rf_params['max_depth']])</p><p class="source-code">ax_rf.set_xlabel('Number of trees')</p><p class="source-code">ax_rf.set_ylabel('Maximum depth')</p><p>The main change from our previous example is that instead of plotting the integers from 1 to 16, we're plotting the mean testing scores that we've retrieved and reshaped with <strong class="source-inline">cv_rf_results_df['mean_test_score'].values.reshape((4,4))</strong>. The other new things here are that we are using list comprehensions to create lists of strings for tick labels, based on the numerical values of hyperparameters in the grid. We access them from the dictionary that we defined, and then convert them individually to the <strong class="source-inline">str</strong> (string) data type within the list comprehension, for example, <strong class="source-inline">ax_rf.set_xticklabels([str(tick_label) for tick_label in rf_params['n_estimators']])</strong>. We have already set the tick locations to the places where we want the ticks using <strong class="source-inline">set_xticks</strong>. Also, we make a square-shaped graph using <strong class="source-inline">ax_rf.set_aspect('equal')</strong>. The graph should appear as follows:</p><div class="IMG---Figure" id="_idContainer240"><img alt="Figure 5.24: Results of cross-validation of a random forest &#13;&#10;over a grid with two hyperparameters&#13;&#10;" src="image/B16392_05_24.jpg"/></div><p class="figure-caption">Figure 5.24: Results of cross-validation of a random forest over a grid with two hyperparameters</p></li>
				<li>Conclude which set of hyperparameters to use.<p>What can we conclude from our grid search? There certainly seems to be an advantage to using trees with a depth of more than 3. Of the parameter combinations that we tried, <strong class="source-inline">max_depth=9</strong> with 200 trees yields the best average testing score, which you can look up in the DataFrame and confirm is ROC AUC = 0.776. </p><p>This is the best model we've found from all of our efforts so far.</p><p>In a real-world scenario, we'd likely do a more thorough search. Some good next steps would be to try a larger number of trees and not spend any more time with <strong class="source-inline">n_estimators</strong> &lt; 200, since we know that we need at least 200 trees to get the best performance. You may search a more granular space of <strong class="source-inline">max_depth</strong> instead of jumping by 3s, as we've done here, and try a couple of other hyperparameters, such as <strong class="source-inline">max_features</strong>. For our purposes, however, we will assume that we've found the optimal hyperparameters here and move forward..</p></li>
			</ol>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor158"/>6. Gradient Boosting, XGBoost, and SHAP Values</h1>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor159"/>Activity 6.01: Modeling the Case Study Data with XGBoost and Explaining the Model with SHAP </h2>
			<p><strong class="bold">Solution:</strong></p>
			<p>In this activity, we'll take what we've learned in this chapter with a synthetic dataset and apply it to the case study data. We'll see how an XGBoost model performs on a validation set and explain the model predictions using SHAP values. We have prepared the dataset for this activity by replacing the samples that had missing values for the <strong class="source-inline">PAY_1</strong> feature, that we had previously ignored, while maintaining the same train/test split for the samples with no missing values. You can see how the data was prepared in the Appendix to the notebook for this activity.</p>
			<ol>
				<li value="1">Load the case study data that has been prepared for this exercise. The file path is <strong class="source-inline">../../Data/Activity_6_01_data.pkl</strong> and the variables are: <strong class="source-inline">features_response, X_train_all, y_train_all, X_test_all, y_test_all</strong>:<p class="source-code">with open('../../Data/Activity_6_01_data.pkl', 'rb') as f:</p><p class="source-code">    features_response, X_train_all, y_train_all, X_test_all,\</p><p class="source-code">    y_test_all = pickle.load(f)</p></li>
				<li>Define a validation set to train XGBoost with early stopping:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">X_train_2, X_val_2, y_train_2, y_val_2 = \</p><p class="source-code">train_test_split(X_train_all, y_train_all,\</p><p class="source-code">                 test_size=0.2, random_state=24)</p></li>
				<li>Instantiate an XGBoost model. We'll use the <strong class="source-inline">lossguide</strong> grow policy and examine validation set performance for several values of <strong class="source-inline">max_leaves</strong>:<p class="source-code">xgb_model_4 = xgb.XGBClassifier(</p><p class="source-code">    n_estimators=1000,</p><p class="source-code">    max_depth=0,</p><p class="source-code">    learning_rate=0.1,</p><p class="source-code">    verbosity=1,</p><p class="source-code">    objective='binary:logistic',</p><p class="source-code">    use_label_encoder=False,</p><p class="source-code">    n_jobs=-1,</p><p class="source-code">    tree_method='hist',</p><p class="source-code">    grow_policy='lossguide')</p></li>
				<li>Search values of <strong class="source-inline">max_leaves</strong> from 5 to 200, counting by 5's:<p class="source-code">max_leaves_values = list(range(5,205,5))</p></li>
				<li>Create the evaluation set for early stopping:<p class="source-code">eval_set_2 = [(X_train_2, y_train_2), (X_val_2, y_val_2)]</p></li>
				<li>Loop through hyperparameter values and create a list of validation ROC AUCs, using the same technique as in <em class="italic">Exercise 6.01: Randomized Grid Search for Tuning XGBoost Hyperparameters</em>:<p class="source-code">%%time</p><p class="source-code">val_aucs = []</p><p class="source-code">for max_leaves in max_leaves_values:</p><p class="source-code">    #Set parameter and fit model</p><p class="source-code">    xgb_model_4.set_params(**{'max_leaves':max_leaves})</p><p class="source-code">    xgb_model_4.fit(X_train_2, y_train_2,\</p><p class="source-code">                    eval_set=eval_set_2,\</p><p class="source-code">                    eval_metric='auc',\</p><p class="source-code">                    verbose=False,\</p><p class="source-code">                    early_stopping_rounds=30)</p><p class="source-code">    #Get validation score</p><p class="source-code">    val_set_pred_proba = xgb_model_4.predict_proba(X_val_2)[:,1]</p><p class="source-code">    val_aucs.append(roc_auc_score(y_val_2, val_set_pred_proba))</p></li>
				<li>Create a data frame of the hyperparameter search results and plot the validation AUC against <strong class="source-inline">max_leaves</strong>:<p class="source-code">max_leaves_df_2 = \</p><p class="source-code">pd.DataFrame({'Max leaves':max_leaves_values,\</p><p class="source-code">              'Validation AUC':val_aucs})</p><p class="source-code">mpl.rcParams['figure.dpi'] = 400</p><p class="source-code">max_leaves_df_2.set_index('Max leaves').plot()</p><p>The plot should look something like this:</p><div class="IMG---Figure" id="_idContainer241"><img alt="Figure 6.15: Validation AUC versus max_leaves for the case study data&#13;&#10;" src="image/B16392_06_15.jpg"/></div><p class="figure-caption">Figure 6.15: Validation AUC versus <strong class="source-inline">max_leaves</strong> for the case study data</p><p>Although the relationship is somewhat noisy, we see that in general, lower values of <strong class="source-inline">max_leaves</strong> result in a higher validation set ROC AUC. This is because limiting the complexity of trees by allowing fewer leaves results in less overfitting, and increases the validation set score.</p></li>
				<li>Observe the number of <strong class="source-inline">max_leaves</strong> corresponding to the highest ROC AUC on the validation set:<p class="source-code">max_auc_2 = max_leaves_df_2['Validation AUC'].max()</p><p class="source-code">max_auc_2</p><p class="source-code">max_ix_2 = max_leaves_df_2['Validation AUC'] == max_auc_2</p><p class="source-code">max_leaves_df_2[max_ix_2]</p><p>The result should be as follows:</p><div class="IMG---Figure" id="_idContainer242"><img alt="Figure 6.16: Optimal max_leaves and validation set AUC for the case study data&#13;&#10;" src="image/B16392_06_16.jpg"/></div><p class="figure-caption">Figure 6.16: Optimal <strong class="source-inline">max_leaves</strong> and validation set AUC for the case study data</p><p>We would like to interpret these results in light of our previous efforts in modeling the case study data. This is not a perfect comparison, because here we have missing values in the training and validation data, while previously we ignored them, and here we only have one validation set, as opposed to the k-folds cross-validation used earlier (although the interested reader could try using k-folds cross-validation for multiple training/validation splits in XGBoost with early stopping).</p><p>However, even given these limitations, the validation results here should provide a measure of out-of-sample performance similar to the k-folds cross-validation we performed earlier. We note that the validation ROC AUC here of 0.779 here is a bit higher than the 0.776 obtained previously with random forest in <em class="italic">Activity 5.01</em>, <em class="italic">Cross-Validation Grid Search with Random Forest</em>, from <em class="italic">Chapter 5, Decision Trees and Random Forests</em>. These validation scores are fairly similar and it would probably be fine to use either model in practice. We'll now move forward with the XGBoost model.</p></li>
				<li>Refit the XGBoost model with the optimal hyperparameter:<p class="source-code">xgb_model_4.set_params(**{'max_leaves':40})</p><p class="source-code">xgb_model_4.fit(X_train_2, y_train_2, eval_set=eval_set_2,</p><p class="source-code">                eval_metric='auc',</p><p class="source-code">                verbose=False, early_stopping_rounds=30)</p></li>
				<li>So that we can examine SHAP values for the validation set, make a data frame of this data:<p class="source-code">X_val_2_df = pd.DataFrame(data=X_val_2,</p><p class="source-code">                          columns=features_response[:-1])</p></li>
				<li>Create an SHAP explainer for our new model using the validation data as the background dataset, obtain the SHAP values, and make a summary plot:<p class="source-code">explainer_2 = shap.explainers.Tree(xgb_model_4, data=X_val_2_df)</p><p class="source-code">shap_values_2 = explainer_2(X_val_2_df)</p><p class="source-code">mpl.rcParams['figure.dpi'] = 75</p><p class="source-code">shap.summary_plot(shap_values_2.values, X_val_2_df)</p><p>The plot should look like this:</p><div class="IMG---Figure" id="_idContainer243"><img alt="Figure 6.17: SHAP values for the XGBoost model of the case study data on the validation set&#13;&#10;" src="image/B16392_06_17.jpg"/></div><p class="figure-caption">Figure 6.17: SHAP values for the XGBoost model of the case study data on the validation set</p><p>From <em class="italic">Figure 6.17</em>, we can see that the most important features in the XGBoost model are somewhat different from those in the random forest model we explored in <em class="italic">Chapter 5</em>, <em class="italic">Decision Trees and Random Forests</em> (<em class="italic">Figure 5.15</em>). No longer is <strong class="source-inline">PAY_1</strong> the most important feature, although it is still quite important at number 3. <strong class="source-inline">LIMIT_BAL</strong>, the borrower's credit limit, is now the most important feature. This makes sense as an important feature as the lender has likely based the credit limit on how risky a borrower is, so it should be a good predictor of the risk of default.</p><p>Let's explore whether <strong class="source-inline">LIMIT_BAL</strong> has any interesting SHAP interactions with other features. Instead of specifying which feature to color the scatter plot by, we can let the <strong class="source-inline">shap</strong> package pick the feature that has the most interaction by not indexing the explainer object for the color argument.</p></li>
				<li>Make a scatter plot of <strong class="source-inline">LIMIT_BAL</strong> SHAP values, colored by the feature with the strongest interaction:<p class="source-code">shap.plots.scatter(shap_values_2[:,'LIMIT_BAL'],</p><p class="source-code">                   color=shap_values_2)</p><p>The plot should look like this:</p><div class="IMG---Figure" id="_idContainer244"><img alt="Figure 6.18: Scatter plot of SHAP values of LIMIT_BAL and the &#13;&#10;feature with the strongest interaction&#13;&#10;" src="image/B16392_06_18.jpg"/></div><p class="figure-caption">Figure 6.18: Scatter plot of SHAP values of <strong class="source-inline">LIMIT_BAL</strong> and the feature with the strongest interaction</p><p><strong class="source-inline">BILL_AMT2</strong>, the amount of the bill from two months previous, has the strongest interaction with <strong class="source-inline">LIMIT_BAL</strong>. We can see that for most values of <strong class="source-inline">LIMIT_BAL</strong>, if the bill was particularly high, this leads to more positive SHAP values, meaning an increased risk of default. This can be observed by noting that most of the reddest colored dots appear along the top of the band of dots in <em class="italic">Figure 6.18</em>. This makes intuitive sense: even if a borrower was given a large credit limit, if their bill becomes very large, this may signal an increased risk of default.</p><p>Finally, we will save the model along with the training and test data for analysis and delivery to our business partner. We accomplish this using Python's <strong class="source-inline">pickle</strong> functionality.</p></li>
				<li>Save the trained model along with the training and test data to a file:<p class="source-code">with open('../Data/xgb_model_w_data.pkl', 'wb') as f:</p><p class="source-code">    pickle.dump([X_train_all, y_train_all,\</p><p class="source-code">                 X_test_all, y_test_all,\</p><p class="source-code">                 xgb_model_4], f)</p></li>
			</ol>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor160"/>7. Test Set Analysis, Financial Insights, and Delivery to the Client</h1>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor161"/>Activity 7.01: Deriving Financial Insights</h2>
			<p><strong class="bold">Solution:</strong></p>
			<ol>
				<li value="1">Using the testing set, calculate the cost of all defaults if there were no counseling program.<p>Use this code for the calculation:</p><p class="source-code">cost_of_defaults = np.sum(y_test_all * X_test_all[:,5])</p><p class="source-code">cost_of_defaults </p><p>The output should be this:</p><p class="source-code">60587763.0</p></li>
				<li>Calculate by what percent the cost of defaults can be decreased by the counseling program.<p>The potential decrease in the cost of default is the greatest possible net savings of the counseling program, divided by the cost of all defaults in the absence of a program:</p><p class="source-code">net_savings[max_savings_ix]/cost_of_defaults</p><p>The output should be this:</p><p class="source-code">0.2214260658542551</p><p>Results indicate that we can decrease the cost of defaults by 22% using a counseling program, guided by predictive modeling.</p></li>
				<li>Calculate the net savings per account (considering all accounts it might be possible to counsel, in other words relative to the whole test set) at the optimal threshold.<p>Use this code for the calculation:</p><p class="source-code">net_savings[max_savings_ix]/len(y_test_all)</p><p>The output should be as follows:</p><p class="source-code">2259.2977433479286</p><p>Results like these help the client scale the potential amount of savings they could create with the counseling program, to as many accounts as they serve.</p></li>
				<li>Plot the net savings per account against the cost of counseling per account for each threshold.<p>Create the plot with this code:</p><p class="source-code">plt.plot(total_cost/len(y_test_all),</p><p class="source-code">         net_savings/len(y_test_all))</p><p class="source-code">plt.xlabel\</p><p class="source-code">('Upfront investment: cost of counselings per account (NT$)')</p><p class="source-code">plt.ylabel('Net savings per account (NT$)')</p><p>The resulting plot should appear like this:</p><div class="IMG---Figure" id="_idContainer245"><img alt="Figure 7.14: The initial cost of the counseling program needed &#13;&#10;to achieve a given amount of savings&#13;&#10;" src="image/B16392_07_14.jpg"/></div><p class="figure-caption">Figure 7.14: The initial cost of the counseling program needed to achieve a given amount of savings</p><p>This indicates how much money the client needs to budget to the counseling program in a given month, to achieve a given amount of savings. It looks like the greatest benefit can be created by budgeting up to about NT$1300 per account (you could find the exact budgeted amount corresponding to maximum net savings using <strong class="source-inline">np.argmax</strong>). However, net savings are relatively flat for upfront investments between NT$1000 and 2000, being lower outside that range. The client may not actually be able to budget this much for the program. However, this graphic gives them evidence to argue for a larger budget if they need to.</p><p>This result corresponds to our graphic from the previous exercise. Although we've shown the optimal threshold is 0.36, it may be fine for the client to use a higher threshold up to about 0.5, thus making fewer positive predictions, offering counseling to fewer account holders, and having a smaller upfront program cost. <em class="italic">Figure 7.14</em> shows how this plays out in terms of cost and net savings per account.</p></li>
				<li>Plot the fraction of accounts predicted as positive (this is called the "flag rate") at each threshold.<p>Use this code to plot the flag rate against the threshold:</p><p class="source-code">plt.plot(thresholds, n_pos_pred/len(y_test_all))</p><p class="source-code">plt.ylabel('Flag rate')</p><p class="source-code">plt.xlabel('Threshold')</p><p>The plot should appear as follows:</p><div class="IMG---Figure" id="_idContainer246"><img alt="Figure 7.15: Flag rate against threshold for the credit counseling program&#13;&#10;" src="image/B16392_07_15.jpg"/></div><p class="figure-caption">Figure 7.15: Flag rate against threshold for the credit counseling program</p><p>This plot shows the fraction of people who will be predicted to default and therefore will be recommended outreach at each threshold. It appears that at the optimal threshold of 0.36, only about 20% of accounts will be flagged for counseling. This shows how using a model to prioritize accounts for counseling can help focus on the right accounts and reduce wasted resources. Higher thresholds, which may result in nearly optimal savings up to a threshold of about 0.5 as shown in <em class="italic">Figure 7.12</em> (<em class="italic">Chapter 7</em>, <em class="italic">Test Set Analysis, Financial Insights, and Delivery to the Client</em>) result in lower flag rates.</p></li>
				<li>Plot a precision-recall curve for the testing data using the following code:<p class="source-code">plt.plot(n_true_pos/sum(y_test_all),\</p><p class="source-code">         np.divide(n_true_pos, n_pos_pred)) </p><p class="source-code">plt.xlabel('Recall')</p><p class="source-code">plt.ylabel('Precision')</p><p>The plot should look like this:</p><div class="IMG---Figure" id="_idContainer247"><img alt="Figure 7.16: Precision-recall curve&#13;&#10;" src="image/B16392_07_16.jpg"/></div><p class="figure-caption">Figure 7.16: Precision-recall curve</p><p><em class="italic">Figure 7.16</em> shows that in order to start getting a true positive rate (that is, recall) much above 0, we need to accept a precision of about 0.8 or lower.</p><p>Precision and recall have a direct link to the cost and savings of the program: the more precise our predictions are, the less money we are wasting on counseling due to incorrect model predictions. And, the higher the recall, the more savings we can create by successfully identifying accounts that would default. Compare the code in this step to the code used to calculate costs and savings in the previous exercise to see this.</p><p>To see the connection of precision and recall with the threshold used to define positive and negative predictions, it can be instructive to plot them separately.</p></li>
				<li>Plot precision and recall separately on the <em class="italic">y</em>-axis against threshold on the <em class="italic">x</em>-axis.<p>Use this code to produce the plot:</p><p class="source-code">plt.plot(thresholds, np.divide(n_true_pos, n_pos_pred),</p><p class="source-code">         label='Precision')</p><p class="source-code">plt.plot(thresholds, n_true_pos/sum(y_test_all),</p><p class="source-code">         label='Recall')</p><p class="source-code">plt.xlabel('Threshold')</p><p class="source-code">plt.legend()</p><p>The plot should appear as follows:</p><div class="IMG---Figure" id="_idContainer248"><img alt="Figure 7.17: Precision and recall plotted separately against the threshold&#13;&#10;" src="image/B16392_07_17.jpg"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.17: Precision and recall plotted separately against the threshold</p>
			<p>This plot sheds some light on why the optimal threshold turned out to be 0.36. While the optimal threshold also depends on the financial analysis of costs and savings, we can see here that the steepest part of the initial increase in precision, which represents the correctness of positive predictions and is therefore a measure of how cost-effective the model-guided counseling can be, happens up to a threshold of about 0.36.</p>
		</div>
	

		<div>
			<div id="_idContainer250">
				<img alt="Author" src="image/Author_Page.png"/>
			</div>
		</div>
		<div class="Content" id="_idContainer252">
			<h2 id="_idParaDest-163"><a id="_idTextAnchor162"/>Hey!</h2>
			<p>I am Stephen Klosterman, the author of this book. I really hope you enjoyed reading my book and found it useful.</p>
			<p>It would really help me (and other potential readers!) if you could leave a review on Amazon sharing your thoughts on <em class="italic">Data Science Projects with Python</em>, <em class="italic">Second Edition</em>.</p>
			<p>Go to the link <a href="https://packt.link/r/1800564481">https://packt.link/r/1800564481</a>.</p>
			<p>OR</p>
			<p>Scan the QR code to leave your review.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer251">
					<img alt="Barcode&#13;&#10;" src="image/Barcode.jpg"/>
				</div>
			</div>
			<p>Your review will help me to understand what's worked well in this book and what could be improved upon for future editions, so it really is appreciated.</p>
			<p>Best wishes,</p>
			<p>Stephen Klosterman</p>
		</div>
	</body></html>