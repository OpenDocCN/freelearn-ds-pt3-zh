["```py\n…\n# Code in cell 5\ndf = spark.read.csv(\n    \"file:///\" + SparkFiles.get(DATASET_FILE),\n    header=True, sep=\";\", inferSchema=True)\ndf.cache()\ndf.count()\n…\n# Code in cell 7\ndf = df.withColumn('Time', F.date_format('Time', 'HH:mm:ss'))\n# Create timestamp column\ndf = df.withColumn('timestamp', F.concat(df.Date, F.lit(\" \"), df.Time))\ndf = df.withColumn(\n    'timestamp',\n    F.to_timestamp(df.timestamp, 'yyyy-MM-dd HH:mm:ss'))\n# Fix data types\ndf = df \\\n    .withColumn('Global_active_power',\n    df.Global_active_power.cast('double')) \\\n…\nprint(\"Schema:\")\ndf.spark.read option inferSchema. The data types before conversion, displayed with printSchema(), are shown in *Figure 6**.1*.\n![](img/B18568_06_01.jpg)\n\nFigure 6.1: Inferred schema with data types\nThe updated schema is as per *Figure 6**.2*, showing the converted data types.\n![](img/B18568_06_02.jpg)\n\nFigure 6.2: Updated schema with converted data types\nWe are now ready to profile the data.\nData profiling\nData profiling involves analyzing the dataset’s structure, quality, and statistical properties. This helps to identify anomalies, missing values, and outliers, ensuring data integrity. This process can also be comprehensive, including the analysis of trends, seasonal patterns, and correlations, guiding more accurate forecasting and modeling.\nNote\nData profiling can also guide preprocessing steps such as normalization and transformation, covered in [*Chapter 5*](B18568_05.xhtml#_idTextAnchor103).\nApache Spark provides the convenient `summary()` function, as per the following code, for summary statistics:\n\n```", "```py\n\n This generates the following outcome:\n![](img/B18568_06_03.jpg)\n\nFigure 6.3: Summary statistics\nWhile these summary statistics are useful, they are usually not sufficient. A data profiling tool such as YData Profiling, which we will look at next, provides more extensive analysis and reporting.\nThe following code extract shows how to launch a Profile Report with YData. Notable here is the use of a Pandas DataFrame, `pdf`, and of the time series mode (`tsmode` parameter), with the `sortby` parameter to sort by timestamp. We also want correlations to be included in the report. After the report is generated, it is converted to HTML for display with the `to_html()` function.\n\n```", "```py\n\n The generated report contains an **Overview** section, as per *Figure 6**.4*, with an indication, among other things, of the number of variables (columns), observations (rows), and missing values and duplicate counts.\n![](img/B18568_06_04.jpg)\n\nFigure 6.4: Data profile report – Overview\nScrolling down from **Overview**, we can see column-specific statistics, as shown in *Figure 6**.5*, such as the minimum, maximum, mean, number of zeros, and number of distinct values.\n![](img/B18568_06_05.jpg)\n\nFigure 6.5: Data profile report – Details\nThis section has further sub-sections, such as **Histogram**, showing the distribution of values, and **Gap analysis**, as per *Figure 6**.6*, with indications of data gaps for the column.\n![](img/B18568_06_06.jpg)\n\nFigure 6.6: Data profile report – Gap analysis\nWith the time series mode specified earlier, we also get a basic **Time Series** part of the report, shown in *Figure 6**.7*\n![](img/B18568_06_07.jpg)\n\nFigure 6.7: Data profile report – Time Series\nOther sections of the report cover **Alerts**, shown in *Figure 6**.8*, with outcomes of tests run on the dataset, including time-series-specific ones, and a **Reproduction** section with details on the profiling run.\n![](img/B18568_06_08.jpg)\n\nFigure 6.8: Data profile report – Time Series\nThis section provided an example of how to perform data profiling on time series data using YData Profiling and Apache Spark. Further information on YData Profiling can be found here: [https://github.com/ydataai/ydata-profiling](https://github.com/ydataai/ydata-profiling).\nWe will now drill down further in our understanding of the data, by analyzing the gaps in the dataset.\nGap analysis\nIn the previous section, we mentioned gap analysis for gaps in value for a specific column. Another consideration for time series data is gaps in the timeline itself, as in the following example with the household energy consumption dataset, where we are expecting values every minute.\nIn this case, we first calculate the time difference between consecutive timestamps using `diff()`, as in the following code, with a pandas DataFrame, `pdf`. If this is greater than `1 minute`, we can flag the timestamp as having a prior gap:\n\n```", "```py\n\n As *Figure 6**.9* shows, we found 3 gaps of 2 minutes each in this example.\n![](img/B18568_06_09.jpg)\n\nFigure 6.9: Gap analysis\nDepending on the size of the gap and the nature of the dataset, we can adopt one of the following approaches:\n\n*   Ignore the gap\n*   Aggregate, for example, use the mean value at a higher interval\n*   Use one of the missing-value handling techniques we saw in [*Chapter 5*](B18568_05.xhtml#_idTextAnchor103), such as forward filling\n\nRegular or irregular time series\nThe gap analysis presented here assumes a regular time series. The approach is slightly different in detecting gaps in the timeline of irregular time series. The previous example of checking for the absence of values at every minute interval is not applicable for an irregular time series. We will have to look at the distribution of the count of values over the timeline of the irregular time series and make reasonable assumptions about how regularly we expect values in the irregular time series. For instance, if we are considering the energy consumption of a household, the time series may be irregular at minute intervals, but based on historical data, we expect energy use every hour or daily. In this case, not having a data point on a given hour or day can be indicative of a gap. Once we have identified a gap, we can use the same approaches as discussed for regular time series, that is, forward filling or similar imputation, aggregation at higher intervals, or just ignoring the gap.\nWe discussed here the specific problem of gaps in the time series. We mentioned that, to identify gaps, we can look at the distribution of the data, which will be covered next.\nDistribution analysis\nDistribution analysis of time series provides an understanding of the underlying patterns and characteristics of the data, such as skewness, kurtosis, and outliers. This helps detect deviations from normal distribution, trends, and seasonal patterns, and visualize the variability of the time series. This understanding then feeds into choosing the appropriate statistical models and forecasting methods. This is required as models are built on assumptions of the distribution of the time series. Done correctly, distribution analysis ensures that model assumptions are met. This also improves the accuracy and reliability of the predictions.\nIn this section, we will examine a few examples of distribution analysis, starting with the profiling output of *Figure 6**.5*, which shows a kurtosis of 2.98 and a skewness of 1.46\\. Let’s explain what this means by first defining these terms.\n**Kurtosis** indicates how peaked or flat a distribution is compared to a normal distribution. A value greater than 2, as in our example in *Figure 6**.5*, indicates the distribution is too peaked. Less than -2 means too flat.\n**Skewness** indicates how centered and symmetric the distribution is compared to a normal distribution. A value between -1 and 1 is considered near normal, between -2 and 2, as in the example in *Figure 6**.5*, is acceptable, and below -2 or above 2 is not normal.\nWhen both kurtosis and skewness are zero, we have a perfectly normal distribution, which is quite unlikely to be seen with real data.\nLet’s now do some further distribution analysis with the following code extract. We want to understand the frequency distribution of `Global_active_power`, the distribution by day of the week, `dayOfWeek`, and the hour of the day. We will use the Seaborn (`sns`) visualization library for the plots, with the pandas DataFrame, `pdf`, passed as a parameter:\n\n```", "```py\n\n We can see the frequency of occurrence of the different values of `Global_active_power` in *Figure 6**.10*, with the skewness to the left.\n![](img/B18568_06_10.jpg)\n\nFigure 6.10: Distribution by frequency\nIf we look at the distribution by day of the week, as in *Figure 6**.11*, power consumption during the weekends is higher, as can be expected for a household, with 1 on the *x* axis representing Sundays and 7 Saturdays. The distribution is also over a broader range of values these days.\n![](img/B18568_06_11.jpg)\n\nFigure 6.11: Distribution by day of the week\nThe distribution by hour of the day, as in *Figure 6**.12*, shows higher power consumption during the morning and evening, again as can be expected for a household.\n![](img/B18568_06_12.jpg)\n\nFigure 6.12: Distribution by hour of the day\nYou will also notice in the distribution plots the values that are flagged as outliers, lying beyond the whiskers. These are at a 1.5 **inter-quartile range** (**IQR**) above the third quartile. You can use other thresholds for outliers, as in [*Chapter 5*](B18568_05.xhtml#_idTextAnchor103), where we used a cutoff on the z-score value.\nVisualizations\nAs we have seen so far in this book and, more specifically, this chapter, visualizations play an important role in time series analysis. By providing us with an intuitive and immediate understanding of the data’s underlying patterns, they help to identify seasonal variations, trends, and anomalies that might not otherwise be seen from raw data alone. Furthermore, visualizations facilitate the detection of correlations, cycles, and structural changes over time, contributing to better forecasting and decision-making.\nFundamentally, (and this is not only true for time series analysis) visualizations aid in communicating complex insights to stakeholders and, in doing so, improve their ability to understand and act accordingly.\nBuilding on the techniques for statistical analysis seen in this chapter, we will now move on to other important techniques to consider while analyzing  time series—resampling, decomposition, and stationarity.\nResampling, decomposition, and stationarity\nThis section details additional techniques used in time series analysis, introduced in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016). We will see code examples of how to implement these techniques.\nResampling and aggregation\nResampling and aggregation are used in time series analysis to transform and analyze data at different time scales. **Resampling** is changing the frequency of the time series, such as converting hourly data to daily data, which can reveal trends and patterns at different time frequencies. **Aggregation**, on the other hand, is the summarizing of data over specified intervals and is used in conjunction with resampling to calculate the resampled value. This can reduce noise, handle missing values, and convert an irregular time series to a regular series.\nThe following code extract shows the resampling at different intervals, together with the aggregation. The original dataset has data every minute. With `resample('h').mean()` applied to the pandas DataFrame, `pdf`, we resample this value to the mean over the hour:\n\n```", "```py\n\n *Figure 6**.13* shows the outcome of the hourly resampling.\n![](img/B18568_06_13.jpg)\n\nFigure 6.13: Resampled hourly\n*Figure 6**.14* shows the outcome of the daily resampling.\n![](img/B18568_06_14.jpg)\n\nFigure 6.14: Resampled daily\n*Figure 6**.15* shows the outcome of the weekly resampling.\n![](img/B18568_06_15.jpg)\n\nFigure 6.15: Resampled weekly\nWith these examples, we have resampled and aggregated time series data using Apache Spark. We will next expand on the time series decomposition of the resampled time series.\nDecomposition\nAs introduced in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016), decomposition breaks down the time series into its fundamental components: trend, seasonality, and residuals. This separation helps uncover underlying patterns within the data more clearly. The trend shows long-term movement, while seasonal components show repeating patterns. Residuals highlight any deviation from the trend and seasonal components. This decomposition allows for each component to be analyzed and addressed individually.\nThe following code extract shows the decomposition of time series using `seasonal_decompose` from the `statsmodels` library. In [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016), we used a different library, `Prophet`.\n\n```", "```py\n\n *Figure 6**.16* shows the components of the hourly resampled time series. The seasonal component shows a pattern, with each repeating pattern corresponding to a day, and the ups in power consumption every morning and evening are visible.\n![](img/B18568_06_16.jpg)\n\nFigure 6.16: Decomposition of hourly data\n*Figure 6**.17* shows the components of the daily resampled time series. The seasonal component shows a pattern, with each repeating pattern corresponding to a week, and the ups in power consumption every weekend are visible.\n![](img/B18568_06_17.jpg)\n\nFigure 6.17: Decomposition of daily data\nNow that we have performed time series decomposition using Apache Spark and `statsmodels` for time series at different resampling intervals, let's discuss the next technique. \nStationarity\nAnother key concept related to time series data, introduced in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016), stationarity concerns the statistical properties of the series, such as mean, variance, and autocorrelation remaining constant over time. This is an assumption on which time series models, such as **AutoRegressive Integrated Moving Average** (**ARIMA**) are built. A series must be identified and converted to stationary before using such models. In general, stationary time series facilitate analysis and improve model accuracy.\nThe first step in handling non-stationarity is to check the time series, which we will look at next.\nCheck\nThe **Augmented Dickey-Fuller** (**ADF**) test and the **Kwiatkowski-Phillips-Schmidt-Shin** (**KPSS**) test are commonly used statistical tests to check for stationarity. Without going into the details of these tests, we can say they calculate a value, which is called the p-value. A value of p < 0.05 for ADF means that the series is stationary. Additionally, we can check for stationarity by visual inspection of the time series plot and **autocorrelation function** (**ACF**) plots, and by comparing summary statistics over different time periods. Mean, variance, and autocorrelation remaining constant across time suggest stationarity. Significant changes indicate non-stationarity.\nThe following example code checks for stationarity using the ADF test, `adfuller`, from the `statsmodels` library. We will use the hourly resampled data in this example.\n\n```", "```py\n\n In this case, the p-value, as shown in *Figure 6**.18*, is less than 0.05, and we can conclude the time series is stationary from the ADF test.\n![](img/B18568_06_18.jpg)\n\nFigure 6.18: ADF test results – Power consumption dataset\nRunning the ADF test on the dataset for the annual mean temperature of Mauritius, used in [*Chapter 1*](B18568_01.xhtml#_idTextAnchor016), gives a p-value greater than 0.05, as shown in *Figure 6**.19*. In this case, we can conclude that the time series is non-stationary.\n![](img/B18568_06_19.jpg)\n\nFigure 6.19: ADF test results – Annual mean temperature dataset\nAs we now have a non-stationary series, we will next consider converting it to a stationary series using differencing.\nDifferencing\nThe following code extract shows the conversion of a non-stationary time series to a stationary one. We’ll use differencing, a common method to remove trends and seasonality, which can make the time series stationary. By using a combination of the `Window` function and `lag` of 1, we can find the difference between an annual mean and the previous year’s value.\n\n```", "```py\n\n We can see the original time series compared to the differenced time series in *Figure 6**.20*. The removal of the trend is visible.\n![](img/B18568_06_20.jpg)\n\nFigure 6.20: Differencing – Annual mean temperature dataset\nRunning the ADF test after differencing, gives a p-value less than 0.05, as shown in *Figure 6**.21*. We can conclude that the difference in time series is stationary.\n![](img/B18568_06_21.jpg)\n\nFigure 6.21: ADF test results – Differenced annual mean temperature dataset\nBuilding on our understanding of techniques for exploratory analysis learned in this section, we will now move on to the last section of this chapter, which is about correlation of  time series data.\nCorrelation analysis\nCorrelation measures the relationship between two variables. This relationship can be causal, whether one is the result of the other. This section will explore the different types of correlation applicable to time series.\nAutocorrelation\nThe **AutoCorrelation Function** (**ACF**) measures the relationship between a time series and its past values. High autocorrelation indicates that past values have a strong influence on future values. This information can then be used to build predictive models, for instance, in selecting the right parameters for models such as ARIMA, thereby enhancing the robustness of the analysis. Understanding autocorrelation also helps in identifying seasonal effects and cycles.\nThe **Partial AutoCorrelation Function** (**PACF**) similarly measures the relationship between a variable and its past values, but contrary to the ACF, with the PACF we discount the effect of values of the time series at all shorter lags.\nCheck\nThe following code shows how you can check for autocorrelation and partial autocorrelation using Apache Spark and `plot_acf` and `plt_pacf` from the `statsmodels` library.\n\n```", "```py\n\n The resulting ACF and PACF plots are shown in *Figure 6**.22*.\n![](img/B18568_06_22.jpg)\n\nFigure 6.22: ACF and PACF plots\nThe outcomes of ACF and PACF indicate the nature of the time series and guide the selection of the appropriate models and parameters for forecasting. Let’s now make sense of these plots and how we can use their outcome.\nInterpretation of ACF\nWe will consider the peaks and the decay from the ACF plot to interpret the outcome, using the upper graph in *Figure 6**.22* as an example.\nPeaks in the autocorrelation plot outside the confidence interval indicate notable autocorrelations. Regular intervals point to seasonality. From the example, we can see autocorrelation at lags 1, 2, and 3 and seasonality at lags 12 and 24, which correspond to a 12- and 24-hour interval.\nA slow decay in the autocorrelation plot suggests that the series is non-stationary with a trend. In this case, we can convert the series to stationary by differencing it, as discussed in the previous section on *Differencing*. This, however, is not the case in our example in *Figure 6**.22*, as there is no slow decay.\nThe outcome of the ACF can be used to define the `q` of an ARIMA model. Major peaks at lags 1, 2 and 3 in our example, means q=1, q=2, and q=3.\nInterpretation of PACF\nWe will consider the peaks and the cut-off from the PACF plot to interpret the outcome, using the lower graph in *Figure 6**.22* as an example.\nPeaks in the partial autocorrelation plot outside the confidence interval indicate notable partial autocorrelations. In the example, this is seen at lags 1, 12, and 24.\nAn immediate cut-off after some lags indicates an **autoregressive** (**AR**) component. In the example, this is after lag 1.\nThe outcome of the PACF can be used to define the AR parameter `p` of an ARIMA model. Major peaks at lag 1 in our example, means p=1.\nModel parameters\nBased on the interpretation of the ACF and PACF plots in *Figure 6**.22*, we can consider the following candidate ARIMA(p, d, q) models, where p is the PACF cut-off point, d is the order of differencing, and q is the ACF autocorrelation lag:\n\n*   ARIMA(1, 0, 1)\n*   ARIMA(1, 0, 2)\n*   ARIMA(1, 0, 3)\n\nWe will discuss model selection and parameters in detail in the next chapter. The depth of our discussion here is just enough to conclude the discussion on ACF and PACF. Let’s move on to other lag analysis methods.\nLag analysis\nIn addition to ACF and PACF plots seen previously, we will explore another lag analysis method in this section.\nWe’ll start by calculating the different lag values of interest, as per the following code extract, using the `Window` and `lag` functions we have seen previously.\n\n```", "```py\n\n This creates the lag columns, as shown in *Figure 6**.23*.\n![](img/B18568_06_23.jpg)\n\nFigure 6.23: Lag values\nWe then calculate the correlation of the current values with their lag values, as in the following code, using the `stat.corr()` function.\n\n```", "```py\n\n *Figure 6**.24* shows the autocorrelation values, significant at lag 1, 2, and 24, as we saw on the ACF plot previously.\n![](img/B18568_06_24.jpg)\n\nFigure 6.24: Autocorrelation at different lag values\nFinally, by plotting the current and lag values together, we can see in *Figure 6**.25* how they compare to each other. We can visually confirm here the greater correlation at lag 1, 2, and 24.\n![](img/B18568_06_25.jpg)\n\nFigure 6.25: Comparison of current and lag values\nThis concludes the section on autocorrelation, where we looked at ACF and PACF, and how to calculate lagged features and their correlation using Apache Spark. While the lag analysis methods in this section have been used for autocorrelation, they can also be used for cross-correlation, which we will cover next, as another type of correlation, this time between different time series.\nCross-correlation\nCross-correlation measures the relationship between two different time series. One series may influence or predict the other over different time lags, in what is called a **lead-lag relationship**. Cross-correlation is used for multivariate time series modeling and causality analysis.\nGoing back to the profiling report we saw earlier, we can see a graph of the correlation of the different columns of the example dataset included in the report, as in *Figure 6**.26*.\n![](img/B18568_06_26.jpg)\n\nFigure 6.26: Cross-correlation heatmap\nWe can calculate the cross-correlation directly with the following code.\n\n```", "```py\n\n The cross-correlation calculation yields the value in *Figure 6**.26*. As this correlation is at the same lag, it does not have predictive value, in the sense that we are not using the past to predict the future. However, this pair of attributes is still worth further analysis at different lags, due to the significant cross-correlation.\n![](img/B18568_06_27.jpg)\n\nFigure 6.27: Cross-correlation value\nNote\nWe know that P=IV, where P is electrical power, I is current, and V is voltage, indicates how power and voltage are related. Hence, these two time series are not independent of each other. Even if there is no further insight into the P and V relationship, we will continue this analysis as an example of cross-correlation analysis.\nAs cross-correlation at the same lag does not help much for prediction, we will now look at using different lag values with the following code. This uses the cross-correlation `ccf()` function, which calculates the cross-correlation at different lag values.\n\n```", "```py\n\n This generates the plot in *Figure 6**.27*, which shows the correlation of the two attributes at different lags.\n![](img/B18568_06_28.jpg)\n\nFigure 6.28: Cross-correlation function\nTo conclude, this section showed how to perform cross-correlation analysis by creating lagged features, and calculating and visualizing cross-correlation.\nSummary\nIn this chapter, we used exploratory data analysis to uncover patterns and insights in time series data. Starting with statistical analysis techniques, where we profiled the data and analyzed its distribution, we then resampled and decomposed the series into its components. To understand the nature of the time series, we also checked for stationarity, autocorrelation, and cross-correlation. By this point, we have gathered enough information on time series to guide us into the next step of building predictive models for time series.\nIn the next chapter, we will dive into the core topic of this book, which is developing and testing models for time series analysis.\nJoin our community on Discord\nJoin our community’s Discord space for discussions with the authors and other readers:\n[https://packt.link/ds](https://packt.link/ds)\n![](img/ds_(1).jpg)\n\n```"]