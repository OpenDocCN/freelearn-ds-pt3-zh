<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer046">
			<h1 id="_idParaDest-150"><a id="_idTextAnchor150"/>Chapter 8: Unsupervised Machine Learning</h1>
			<p>In the previous two chapters, you were introduced to the supervised learning class of machine learning algorithms, their real-world applications, and how to implement them at scale using Spark MLlib. In this chapter, you will be introduced to the unsupervised learning category of machine learning, where you will learn about parametric and non-parametric unsupervised algorithms. A few real-world applications of <strong class="bold">clustering</strong> and <strong class="bold">association</strong> algorithms will be presented to help you understand the applications of <strong class="bold">unsupervised learning</strong> to solve real-life problems. You will gain basic knowledge and understanding of clustering and association problems when using unsupervised machine learning. We will also look at the implementation details of a few clustering algorithms in Spark ML, such as <strong class="bold">K-means clustering</strong>, <strong class="bold">hierarchical clustering</strong>, <strong class="bold">latent Dirichlet allocation</strong>, and an association algorithm called <strong class="bold">alternating least squares</strong>. </p>
			<p>In this chapter, we're going to cover the following main topics: </p>
			<ul>
				<li>Introduction to unsupervised learning</li>
				<li>Clustering using machine learning</li>
				<li>Building association using machine learning</li>
				<li>Real-world applications of unsupervised learning</li>
			</ul>
			<p>By the end of this chapter, you should have gained sufficient knowledge and practical understanding of the clustering and association types of unsupervised machine learning algorithms, their practical applications, and skills to implement these types of algorithms at scale using Spark MLlib.</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor151"/>Technical requirements</h1>
			<p>In this chapter, we will be using Databricks Community Edition to run our code (<a href="https://community.cloud.databricks.com">https://community.cloud.databricks.com</a>). </p>
			<ul>
				<li>Sign-up instructions can be found at <a href="https://databricks.com/try-databricks">https://databricks.com/try-databricks</a>. </li>
				<li>The code for this chapter can be downloaded from <a href="https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter08">https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter08</a>. </li>
				<li>The datasets for this chapter can be found at<a href="https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data"> https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data</a>.</li>
			</ul>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor152"/>Introduction to unsupervised machine learning</h1>
			<p>Unsupervised learning is a machine learning technique where no guidance is available to the learning algorithm in the form of known label values in the training data. Unsupervised learning<a id="_idIndexMarker628"/> is useful in categorizing unknown data points into groups based on patterns, similarities, or differences that are inherent within the data, without any prior knowledge of the data. </p>
			<p>In supervised learning, a model is trained on known data, and then inferences are drawn from the model using new, unseen data. On the other hand, in unsupervised learning, the model training process in itself is the end goal, where patterns hidden within the training data are discovered during the model training process. Unsupervised learning is harder compared to supervised learning since it is difficult to ascertain if the results of an unsupervised learning algorithm are meaningful without any external evaluation, especially without access to any correctly labeled data.</p>
			<p>One of the advantages of unsupervised learning is that it helps interpret very large datasets where labeling existing data would not be practical. Unsupervised learning is also useful for tasks such as predicting the number of classes within a dataset, or grouping and clustering data before applying a supervised learning algorithm. It is also very useful in solving classification problems as unsupervised learning can work well with unlabelled data, and there is no need for any manual intervention either. Unsupervised learning can be classified into two major learning techniques, known as clustering and association. These will be presented in the following sections.</p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor153"/>Clustering using machine learning</h1>
			<p>In machine learning, clustering<a id="_idIndexMarker629"/> deals with identifying patterns or structures within uncategorized data without needing any external guidance. Clustering algorithms parse given data to identify clusters or groups with matching patterns that exist in the dataset. The result of clustering algorithms are clusters of data that can be defined as a collection of objects that are similar in a certain way. The following diagram illustrates how clustering works:</p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="Images/B16736_08_01.jpg" alt="Figure 8.1 – Clustering&#13;&#10;" width="653" height="267"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Clustering</p>
			<p>In the previous diagram, an uncategorized dataset is being passed through a clustering algorithm, resulting in the data being categorized into smaller clusters or groups of data, based on a data point's proximity to another data point in a two-dimensional Euclidian space.</p>
			<p>Thus, the clustering<a id="_idIndexMarker630"/> algorithm groups data based on the Euclidean distance between the data on a two-dimensional plane. Clustering algorithms consider the Euclidean distance between data points in the training dataset in that, within a cluster, the distance between the data points should be small, while outside the cluster, the distance between the data points should be large. A few types of clustering techniques that are available in Spark MLlib will be presented in the following sections.</p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor154"/>K-means clustering</h2>
			<p><strong class="bold">K-means</strong> is the most popular<a id="_idIndexMarker631"/> clustering algorithm and one of the simplest<a id="_idIndexMarker632"/> of the unsupervised learning algorithms as well. The K-means clustering algorithm works iteratively on the provided dataset to categorize it into <em class="italic">k</em> groups. The larger the value of <em class="italic">k</em>, the smaller the size of the clusters, and vice versa. Thus, with K-means, the user can control the number of clusters that are identified within the given dataset. </p>
			<p>In K-means clustering, each cluster is defined by creating a center for each cluster. These centroids are placed as far away as possible from each other. Then, K-means associates each data point with the given dataset to its nearest centroid, thus forming the first group of clusters. K-means then iteratively recalculates the centroids' position within the dataset so that it's as close to the center of the identified clusters. This process stops when the centroids don't need to be moved anymore.</p>
			<p>The following code<a id="_idIndexMarker633"/> block illustrates how to implement K-means<a id="_idIndexMarker634"/> clustering using Spark MLlib:</p>
			<p class="source-code">from pyspark.ml.clustering import KMeans</p>
			<p class="source-code">from pyspark.ml.evaluation import ClusteringEvaluator</p>
			<p class="source-code">retail_features = spark.read.table("retail_features")</p>
			<p class="source-code">train_df = retail_features.selectExpr("selected_features as features")</p>
			<p class="source-code">kmeans = KMeans(k=3, featuresCol='features')</p>
			<p class="source-code">kmeans_model = kmeans.fit(train_df)</p>
			<p class="source-code">predictions = kmeans_model.transform(train_df)</p>
			<p class="source-code">evaluator = ClusteringEvaluator()</p>
			<p class="source-code">silhouette = evaluator.evaluate(predictions)</p>
			<p class="source-code">print("Silhouette measure using squared Euclidean distance = " + str(silhouette))</p>
			<p class="source-code">cluster_centers = kmeans_model.clusterCenters()</p>
			<p class="source-code">print(cluster_centers)</p>
			<p>In the preceding code snippet, we did the following:</p>
			<ol>
				<li>First, we used <strong class="source-inline">import</strong> to import the appropriate MLlib packages related to clustering and clustering evaluation.</li>
				<li>Then, we imported the already existing feature vector that we derived during the feature engineering process into a Spark DataFrame and stored it in the data lake in Delta format. </li>
				<li>Next, a new <strong class="source-inline">KMeans</strong> object was initialized by us passing in the number of desired clusters and the column name for the feature vector.</li>
				<li>The <strong class="source-inline">fit()</strong> method was called on the training DataFrame to kick off the learning process. A model object was generated as a result.</li>
				<li>Predictions on the original training dataset were generated by calling the <strong class="source-inline">transform()</strong> method on the model object.</li>
				<li>Next, we invoked Spark MLlib's <strong class="source-inline">ClusteringEvaluator()</strong> helper function, which is useful for evaluating clustering algorithms, and applied it to the predictions DataFrame we generated in the previous step. This resulted in a value referred to as <strong class="source-inline">silhouette</strong>, which is a measure of consistency within clusters and is calculated based on the Euclidean distance measure between data points. A <strong class="source-inline">silhouette</strong> value closer to <strong class="source-inline">1</strong> means that the points within a cluster are close together and that points outside the cluster are far apart. The closer the <strong class="source-inline">silhouette</strong> value is to <strong class="source-inline">1</strong>, the more performant the learned model is.</li>
				<li>Finally, we printed<a id="_idIndexMarker635"/> the centroids of each of the categorized clusters.</li>
			</ol>
			<p>This way, using<a id="_idIndexMarker636"/> just a few lines of code, uncategorized data can easily be clustered using Spark's implementation of the K-means clustering algorithm.</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor155"/>Hierarchical clustering using bisecting K-means</h2>
			<p><strong class="bold">Hierarchical clustering</strong> is a type of clustering technique where all the data<a id="_idIndexMarker637"/> points start within a single cluster. They are then recursively<a id="_idIndexMarker638"/> split into smaller clusters by moving them down<a id="_idIndexMarker639"/> a hierarchy. Spark ML implements<a id="_idIndexMarker640"/> this kind of divisive hierarchical clustering via the bisecting K-means algorithm. The following example illustrates how to implement bisecting K-means clustering using Spark MLlib:</p>
			<p class="source-code">from pyspark.ml.clustering import BisectingKMeans</p>
			<p class="source-code">from pyspark.ml.evaluation import ClusteringEvaluator</p>
			<p class="source-code">retail_features = spark.read.table("retail_features")</p>
			<p class="source-code">train_df = retail_features.selectExpr("selected_features as features")</p>
			<p class="source-code">bkmeans = BisectingKMeans(k=3, featuresCol='features')</p>
			<p class="source-code">bkmeans_model = kmeans.fit(train_df)</p>
			<p class="source-code">predictions = bkmeans_model.transform(train_df)</p>
			<p class="source-code">evaluator = ClusteringEvaluator()</p>
			<p class="source-code">silhouette = evaluator.evaluate(predictions)</p>
			<p class="source-code">print("Silhouette measure using squared euclidean distance = " + str(silhouette))</p>
			<p class="source-code">cluster_centers = kmeans_model.clusterCenters()</p>
			<p class="source-code">print(cluster_centers)</p>
			<p>In the previous<a id="_idIndexMarker641"/> code snippet, we did the following:</p>
			<ol>
				<li value="1">First, we<a id="_idIndexMarker642"/> initialized a new <strong class="source-inline">BisectingKMeans</strong> object by passing<a id="_idIndexMarker643"/> in the number of desired clusters and the column name for the feature column.</li>
				<li>The <strong class="source-inline">fit()</strong> method was called on the training DataFrame to start the learning process. A model object was generated as a result.</li>
				<li>Next, predictions on the original training dataset were generated by calling the <strong class="source-inline">transform()</strong> method on the model object.</li>
				<li>After, we invoked Spark MLlib's <strong class="source-inline">ClusteringEvaluator()</strong> helper function, which is useful for evaluating clustering algorithms, and applied it to the predictions DataFrame we generated in the previous step. This results in the <strong class="source-inline">silhouette</strong> value, which is a measure of consistency within clusters and is calculated based on the Euclidean distance measure between data points. </li>
				<li>Finally, we printed the centroids of each of the clusters.</li>
			</ol>
			<p>Now that we have learned a clustering technique, let's find out about a learning technique in the next section. </p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor156"/>Topic modeling using latent Dirichlet allocation</h2>
			<p><strong class="bold">Topic modeling</strong> is a learning technique<a id="_idIndexMarker644"/> where you categorize documents. Topic modeling is not the same as topic classification since topic classification is a supervised<a id="_idIndexMarker645"/> learning technique where<a id="_idIndexMarker646"/> the learning model tries to classify unseen documents based on some previously labeled data. On the other hand, topic modeling categorizes documents containing text or natural language in the same way as clustering groups categorize numeric data without any external guidance. Thus, topic modeling is an unsupervised learning problem.</p>
			<p><strong class="bold">Latent Dirichlet allocation</strong> (<strong class="bold">LDA</strong>) is a popular topic modeling technique. The goal of LDA is to associate a given document<a id="_idIndexMarker647"/> with a particular topic based on the keywords found within the document. Here, the topics are unknown and hidden within the documents, thus the latent part of LDA. LDA works by assuming each word within a document belongs to a different topic and assigns a probability score to each word. Once the probability of each word belonging to a particular topic is estimated, LDA tries to pick all the words belonging to a topic by setting a threshold and choosing every word that meets or exceeds that threshold value. LDA also considers each document to be just a bag of words, without placing any importance on the grammatical role played by the individual words. Also, stop words in a language such as articles, conjunctions, and interjections need to be removed before LDA is applied as these words do not carry any topic information.</p>
			<p>The following code example illustrates how LDA can be implemented using Spark MLlib:</p>
			<p class="source-code">from pyspark.ml.clustering import LDA</p>
			<p class="source-code">train_df = spark.read.table("retail_features").selectExpr("selected_features as features")</p>
			<p class="source-code">lda = LDA(k=3, maxIter=1)</p>
			<p class="source-code">lda_model = lda.fit(train_df)</p>
			<p class="source-code">topics = lda_model.describeTopics(3)</p>
			<p class="source-code">topics.show()</p>
			<p class="source-code">transformed = lda_model.transform(dataset)</p>
			<p class="source-code">transformed.show()</p>
			<p>In the previous code<a id="_idIndexMarker648"/> snippet, we did the following:</p>
			<ol>
				<li value="1">First, we imported the<a id="_idIndexMarker649"/> appropriate MLlib packages related to LDA.</li>
				<li>Next, we imported the already existing feature vector that had been derived during the feature engineering process into a Spark DataFrame and stored it in the data lake in Delta format. </li>
				<li>After, we initialized a new <strong class="source-inline">LDA</strong> object by passing in the number of clusters and the maximum number of iterations.</li>
				<li>Next, the <strong class="source-inline">fit()</strong> method was called on the training DataFrame to start the learning process. A model object was generated as a result.</li>
				<li>The topics that were modeled by the LDA algorithm can be shown by using the <strong class="source-inline">describeTopics()</strong> method on the model object.</li>
			</ol>
			<p>As we have seen, by using Apache Spark's implementation of the LDA algorithm, topic modeling can be implemented at scale.</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor157"/>Gaussian mixture model</h2>
			<p>One of the disadvantages of K-means clustering<a id="_idIndexMarker650"/> is that it will associate every data point with exactly one cluster. This way, it is not possible to get the probability of a data point belonging to a particular cluster. The <strong class="bold">Gaussian mixture model</strong> (<strong class="bold">GSM</strong>) attempts to solve this hard clustering problem of K-means clustering. </p>
			<p>GSM is a probabilistic model for representing a subset of a sample within an overall sample of data points. A GSM represents a mixture of several Gaussian distributions of data points, where a data point is drawn from one of the <em class="italic">K</em> Gaussian distributions and has a probability score of it belonging to one of those distributions.</p>
			<p>The following code example describes the implementation details of a GSM using Spark ML:</p>
			<p class="source-code">from pyspark.ml.clustering import GaussianMixture</p>
			<p class="source-code">train_df = spark.read.table("retail_features").selectExpr("selected_features as features")</p>
			<p class="source-code">gmm = GaussianMixture(k=3, featuresCol='features')</p>
			<p class="source-code">gmm_model = gmm.fit(train_df)</p>
			<p class="source-code">gmm_model.gaussiansDF.display()</p>
			<p>In the preceding code block, we initialized a new <strong class="source-inline">GaussianMixture</strong> object after importing the appropriate libraries from the <strong class="source-inline">pyspark.ml.clustering</strong> package. Then, we passed in some hyperparameters, including the number of clusters and the name of the column containing the feature vector. Then, we trained the model using the <strong class="source-inline">fit()</strong> method and displayed the results of the trained model using the model's <strong class="source-inline">gaussianDF</strong> attribute.</p>
			<p>So far, you have seen different<a id="_idIndexMarker651"/> kinds of clustering and topic modeling techniques and their implementations when using Spark MLlib. In the following section, you will learn about another type of unsupervised learning algorithm called <strong class="bold">association rules</strong>.</p>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor158"/>Building association rules using machine learning</h1>
			<p><strong class="bold">Association rules</strong> is a data mining technique<a id="_idIndexMarker652"/> where the goal is identifying relationships<a id="_idIndexMarker653"/> between various entities within<a id="_idIndexMarker654"/> a given dataset by identifying entities that occur frequently together. Association rules are useful in making new item recommendations based on the relationship between existing items that frequently appear together. In data mining association, rules are implemented using a series of <strong class="source-inline">if-then-else</strong> statements that help show the probability of relationships between entities. The association rules technique is widely used in recommender systems, market basket analysis, and affinity analysis problems.</p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor159"/>Collaborative filtering using alternating least squares</h2>
			<p>In machine learning, <strong class="bold">collaborative filtering</strong> is more commonly used for <strong class="bold">recommender systems</strong>. A recommender system is a technique<a id="_idIndexMarker655"/> that's used to filter information by considering<a id="_idIndexMarker656"/> user preference. Based on user preference and taking into consideration<a id="_idIndexMarker657"/> their past behavior, recommender systems can make predictions on items that the user might like. Collaborative filtering performs information filtering by making use of historical user behavior data and their preferences to build a user-item association matrix. Spark ML uses the <strong class="bold">alternating least squares</strong> algorithm to implement the collaborative filtering technique.</p>
			<p>The following code example demonstrates Spark MLlib's implementation of the alternating least squares algorithm:</p>
			<p class="source-code">from pyspark.ml.evaluation import RegressionEvaluator</p>
			<p class="source-code">from pyspark.ml.recommendation import ALS</p>
			<p class="source-code">from pyspark.sql import Row</p>
			<p class="source-code">ratings_df = (spark.read.table("retail_features").selectExpr(</p>
			<p class="source-code">    "CAST(invoice_num AS INT) as user_id",</p>
			<p class="source-code">    "CAST(stock_code AS INT) as item_id",</p>
			<p class="source-code">    "CAST(quantity AS INT) as rating")</p>
			<p class="source-code">    .where("user_id is NOT NULL AND item_id is NOT NULL"))</p>
			<p class="source-code">df.display()</p>
			<p class="source-code">(train_df, test_df) = ratings_df.randomSplit([0.7, 0.3])</p>
			<p class="source-code">als = ALS(maxIter=3, regParam=0.03, userCol="user_id", </p>
			<p class="source-code">          itemCol="item_id", ratingCol="rating", </p>
			<p class="source-code">          coldStartStrategy="drop")</p>
			<p class="source-code">als_model = als.fit(train_df)</p>
			<p class="source-code">predictions = model.transform(test_df)</p>
			<p class="source-code">evaluator = RegressionEvaluator(metricName="rmse", </p>
			<p class="source-code">                                labelCol="rating", </p>
			<p class="source-code">                                predictionCol="prediction")</p>
			<p class="source-code">rmse = evaluator.evaluate(predictions)</p>
			<p class="source-code">print("Root-mean-square error = " + str(rmse))</p>
			<p class="source-code">user_recs = als_model.recommendForAllUsers(5)</p>
			<p class="source-code">user_recs.show()</p>
			<p class="source-code">item_recs = als_model.recommendForAllItems(5)</p>
			<p class="source-code">item_recs.show()</p>
			<p>In the previous code<a id="_idIndexMarker658"/> block, we did the following:</p>
			<ol>
				<li value="1">First, we generated<a id="_idIndexMarker659"/> the ratings dataset as a Spark DataFrame using the feature dataset stored in the Delta Lake. A few of the columns that the ALS algorithm required, such as <strong class="source-inline">user_id</strong>, <strong class="source-inline">item_id</strong>, and <strong class="source-inline">ratings</strong> were not in the required integer format. Thus, we used the <strong class="source-inline">CAST</strong> Spark SQL method to convert them into the required data format.</li>
				<li>Next, we initialized an ALS object with the desired parameters and split our training dataset into two random parts using the <strong class="source-inline">randomSplit()</strong> method.</li>
				<li>After, we started the learning process by calling the <strong class="source-inline">fit()</strong> method on the training dataset.</li>
				<li>Then, we evaluated the accuracy metric's <strong class="source-inline">RMSE</strong> using the evaluator provided by Spark MLlib.</li>
				<li>Finally, we gathered the predictions for the top 5 item recommendations for each user and the top 5 user recommendations per item using the built-in <strong class="source-inline">recommendForAllUsers()</strong> and <strong class="source-inline">recommendForAllItems()</strong> methods, respectively.</li>
			</ol>
			<p>This way, you can leverage alternating<a id="_idIndexMarker660"/> least squares to build recommender systems for use cases such as movie recommendations for a <strong class="bold">video on demand</strong> platform, product recommendations, or <strong class="bold">market basket analysis</strong> for an e-tailer application. Spark MLlib helps you implement<a id="_idIndexMarker661"/> this scale with only a few lines of code.</p>
			<p>In addition to clustering<a id="_idIndexMarker662"/> and association rules, Spark MLlib also allows you to implement <strong class="bold">dimensionality reduction</strong> algorithms such as <strong class="bold">singular value decomposition</strong> (<strong class="bold">SVD</strong>) and <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>). Dimensionality reduction is the process of reducing the number of random variables under consideration. Though an<a id="_idIndexMarker663"/> unsupervised learning<a id="_idIndexMarker664"/> method, dimensionality reduction is useful for feature extraction and selection. A detailed<a id="_idIndexMarker665"/> discussion of this topic is beyond<a id="_idIndexMarker666"/> the scope of this book, and Spark MLlib only has the dimensionality reduction algorithm's implementation available for the RDD API. More details on<a id="_idIndexMarker667"/> dimensionality reduction can be found in Apache Spark's public documentation at <a href="https://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html">https://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html</a>.</p>
			<p>In the next section, we will delve into a few more real-life applications of unsupervised learning algorithms that are in use today by various businesses.</p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor160"/>Real-world applications of unsupervised learning </h1>
			<p>Unsupervised learning algorithms are being used today to solve some real-world business challenges. We will take a look at a few such challenges in this section.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor161"/>Clustering applications</h2>
			<p>This section presents<a id="_idIndexMarker668"/> some of the real-world business applications of clustering algorithms.</p>
			<h3>Customer segmentation</h3>
			<p>Retail marketing teams, as well as business-to-customer organizations, are always trying to optimize their<a id="_idIndexMarker669"/> marketing spends. Marketing teams in particular are concerned with one specific metric called <strong class="bold">cost per acquisition</strong> (<strong class="bold">CPA</strong>). CPA is indicative of the amount<a id="_idIndexMarker670"/> that an organization needs to spend to acquire a single customer, and an optimal CPA means a better return on marketing investments. The best way to optimize CPA is via customer segmentation as this improves the effectiveness of marketing campaigns. Traditional customer segmentation<a id="_idIndexMarker671"/> takes standard customer features such as demographic, geographic, and social information into consideration, along with historical transactional data, to define standard customer segments. This traditional way of customer segmentation is time-consuming and involves a lot of manual work and is prone to errors. However, machine learning algorithms can be leveraged to find hidden patterns and associations among data sources. Also, in recent years. the number of customer touchpoints has increased, and it is not practical and intuitive to identify patterns among all those customer touchpoints to identify patterns manually. However, machine learning algorithms can easily parse through millions of records and surface insights that can be leveraged promptly by marketing teams to meet their customers where they want, when they want. Thus, by leveraging clustering algorithms, marketers can improve the efficacy of their marketing campaigns via refined customer segmentation.</p>
			<h3>Retail assortment optimization</h3>
			<p>Retailers with brick-and-mortar<a id="_idIndexMarker672"/> stores have limited store space. Thus, they need to ensure that their store space is utilized optimally by placing only those products that are highly likely to sell. A classic example of assortment optimization<a id="_idIndexMarker673"/> is that of a hardware retailer, stocking up on lawnmowers during the deep winter season in the midwestern parts of the United States, when it is highly likely to snow through the season. In this example, the store space is being sub-optimally utilized by having lawnmowers in there, which have a very small chance of selling during the snow season. A better choice would have been space heaters, snow shovels, or other winter season equipment. To overcome this problem, retailers usually employ analysts, who take historical transactional data, seasonality, and current trends into consideration to make recommendations on the optimal assortment of products that are appropriate for the season and location of the store. However, what if we increase the scale of this problem to a much larger retailer, with thousands of warehouses and tens of thousands of retail outlets? At such a scale, manually planning optimal assortments of products becomes impractical and very time-consuming, reducing the time to value drastically. Assortment optimization can be treated as a clustering problem, and clustering algorithms can be applied to help plan<a id="_idIndexMarker674"/> how these clusters will be sorted. Here, several more data points must be taken into consideration, including historical consumer<a id="_idIndexMarker675"/> buying patterns, seasonality, trends on social media, search patterns on search engines, and more. This not only helps with better assortment optimization but also in increased revenues, a decrease in product waste, and faster time to market for businesses.</p>
			<h3>Customer churn analysis</h3>
			<p>It is becoming increasingly difficult for businesses to acquire customers because of ever-changing customer preferences<a id="_idIndexMarker676"/> and fierce competition in the marketplace. Thus,  businesses need to retain existing customers. <strong class="bold">Customer churn rate</strong> is one of the prominent metrics<a id="_idIndexMarker677"/> that business executives want to minimize. Machine learning classification algorithms can be used to predict if a particular customer will churn. However, having an understanding of the factors that affect churn would be useful, so that they can change or improve their operations to increase customer satisfaction. Clustering algorithms can be used not only to identify which group of customers are likely to churn, but also to further the analysis by identifying a set of factors that are affecting churn. Businesses can then act on these churn factors to either bring new products into the mix or improve churn to improve customer satisfaction and, in turn, decrease customer churn. </p>
			<h3>Insurance fraud detection</h3>
			<p>Insurance companies<a id="_idIndexMarker678"/> traditionally use manual inspection, along with rules engines, to flag insurance claims as fraudulent. However, as the size of the data increases, the traditional methods might miss a sizeable portion of the claims since manual inspection is time-consuming and error-prone, and fraudsters are constantly innovating and devising new ways of committing fraud. Machine learning clustering algorithms can be used to group new claims with existing fraud clusters, and classification algorithms can be used to classify whether these claims are fraudulent. This way, by leveraging machine learning and clustering algorithms, insurance companies can constantly detect and prevent insurance fraud.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor162"/>Association rules and collaborative filtering applications</h2>
			<p>Association rules<a id="_idIndexMarker679"/> and collaborative filtering are techniques<a id="_idIndexMarker680"/> that are used for building recommender systems. This section will explore some practical use cases of recommendation systems for practical business applications.</p>
			<h3>Recommendation systems</h3>
			<p>Recommendation systems are employed by e-retailers to perform market basket analysis, where the system makes product recommendations to users based on their preferences, as well as items already in their cart. Recommendation systems<a id="_idIndexMarker681"/> can also be used for location- or proximity-based recommendations, such as displaying ads or coupons when a customer is near a particular store. Recommendation systems are also used in marketing, where marketers can get recommendations regarding users who are likely to buy an item, which helps with the effectiveness of marketing campaigns.</p>
			<p>Recommendation systems are also heavily employed by online music and video service providers for user content personalization. Here, recommendation systems are used to make new music or video recommendations to users based on their preferences and historical usage patterns.</p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor163"/>Summary</h1>
			<p>This chapter introduced you to unsupervised learning algorithms, as well as how to categorize unlabeled data and identify associations between data entities. Two main areas of unsupervised learning algorithms, namely clustering and association rules, were presented. You were introduced to the most popular clustering and collaborative filtering algorithms. You were also presented with working code examples of clustering algorithms such as K-means, bisecting K-means, LDA, and GSM using code in Spark MLlib. You also saw code examples for building a recommendation engine using the alternative least-squares algorithm in Spark MLlib. Finally, a few real-world business applications of unsupervised learning algorithms were presented. We looked at several concepts, techniques, and code examples surrounding unsupervised learning algorithms so that you can train your models at scale using Spark MLlib.</p>
			<p>So far, in this and the previous chapter, you have only explored the data wrangling, feature engineering, and model training parts of the machine learning process. In the next chapter, you will be introduced to machine learning life cycle management, where you will explore concepts such as model performance tuning, tracking machine learning experiments, storing machine learning models in a central repository, and operationalizing ML models before putting them in production applications. Finally, an open end-to-end ML life cycle management tool called MLflow will also be introduced and explored.</p>
		</div>
	</div></body></html>