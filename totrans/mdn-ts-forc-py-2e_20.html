<html><head></head><body>
  <div id="_idContainer1125" class="Basic-Text-Frame">
    <h1 class="chapterNumber">17</h1>
    <h1 id="_idParaDest-414" class="chapterTitle">Probabilistic Forecasting and More</h1>
    <p class="normal">Throughout the book, we have learned different techniques to generate a forecast, including some classical methods, using machine learning, and some deep learning architectures as well. But we have been focusing on one typical type of forecasting problem—generating a point forecast for continuous time series with no hierarchy and a good amount of history. We have been doing that because this is the most popular kind of problem you will face. But in this chapter, we will take some time to look at a few niche topics that, although less popular, are no less important.</p>
    <p class="normal">In this chapter, we will focus on these topics:</p>
    <ul>
      <li class="bulletList">Probabilistic forecasting<ul>
          <li class="bulletList level-2">Probability Density Functions</li>
          <li class="bulletList level-2">Quantile functions</li>
          <li class="bulletList level-2">Monte Carlo Dropout</li>
          <li class="bulletList level-2">Conformal Prediction</li>
        </ul>
      </li>
      <li class="bulletList">Intermittent/sparse time series forecasting</li>
      <li class="bulletList">Interpretability</li>
      <li class="bulletList">Cold-start forecasting<ul>
          <li class="bulletList level-2">Pre-trained models like TimeGPT</li>
          <li class="bulletList level-2">Similarity-based forecasting</li>
        </ul>
      </li>
      <li class="bulletList">Hierarchical forecasting</li>
    </ul>
    <h1 id="_idParaDest-415" class="heading-1">Probabilistic forecasting</h1>
    <p class="normal">So far, we <a id="_idIndexMarker1382"/>have been talking about the forecast as a single number. We have been projecting our DL models to a single dimension or training our machine learning models to output a single number. Subsequently, we were training the model using a loss, such as mean squared loss. This paradigm is what we call a <em class="italic">point forecast</em>. But we are not considering one important aspect. We are using the history to train our model to make the best guess. But how sure is the model about its prediction? Those of you who are aware of machine learning and classification problems would recognize that for classification problems, besides getting a prediction of which class the sample belongs to, we also get a notion of the uncertainty of the model. But our forecasting is a regression problem and we don’t get the uncertainty for free.</p>
    <p class="normal">But why is quantifying uncertainty important in forecasting? Any forecast is created for some purpose, some downstream task for which the forecasted information is being used. In other words, there is some decision that has to be taken using the forecast we generate. And when making a decision, we usually would like to have the maximum amount of information available to us.</p>
    <p class="normal">Let’s look at an example to really drive home the point. You have recorded your monthly grocery consumption for the last 5 years and, using the techniques in the book, created a super accurate forecast and an app that tells you how much to shop for in any month. You open up the app and it tells you that you need to buy two bread loaves this month. You head out to the supermarket, bag two loaves of bread, and return home. And a week before the end of the month, the bread loaves ran out, and you starved the rest of the time. At that peak of starvation, you started questioning your decisions and the forecast. You analyzed the data to figure out where you went wrong and realized that your consumption of bread per month varied a lot. In some months you consumed 4 loaves and, in some other months, it was just 1. So, there is a good chance that this forecast will leave you with no bread for a few months and with excess bread for a few other months.</p>
    <p class="normal">Then, you read this chapter and convert your forecast to a probabilistic forecast and now it tells you that 50% of the time your next month’s consumption of bread is 2 loaves. But now, there is an additional feature in the app, which asks you whether you prefer to starve or have excess bread at the end of the day. So, depending on your appetite to starve or save money, you decide on an option. Let’s say you don’t want to starve, but are okay if bread runs out like 10% of the time. Once you enter this preference into the app, it revises its forecast and tells you that you should get 3 loaves of bread, and you never starve again (also because you smartened up and bought other stuff from the supermarket).</p>
    <p class="normal">The ability to <a id="_idIndexMarker1383"/>use the uncertainty in the forecast and revise it according to our appetite for risk is one of the main utilities for probabilistic forecasting. It also helps our forecast to be more transparent and trustworthy to the users.</p>
    <p class="normal">Now, let’s quickly look at the types of uncertainty one would have in a prediction problem using learned models.</p>
    <h2 id="_idParaDest-416" class="heading-2">Types of Predictive Uncertainty</h2>
    <p class="normal">We <a id="_idIndexMarker1384"/>saw in <em class="chapterRef">Chapter 5</em> that supervised machine learning is nothing but learning a function, <img src="../Images/B22389_17_001.png" alt=""/>, where <em class="italic">h</em>, along with <img src="../Images/B22389_07_003.png" alt=""/>, is the model that we learn and is the input data. So, if we think about the sources of uncertainty, it can be from two of these components.</p>
    <p class="normal">The model, <em class="italic">h</em>, we learned is an approximation using a dataset, <em class="italic">X</em>, which may or may not cover all the cases completely and some uncertainty can be introduced to the system from this. We<a id="_idIndexMarker1385"/> call this <strong class="keyWord">Epistemic Uncertainty</strong>. In<a id="_idIndexMarker1386"/> the context of machine learning, epistemic uncertainty can occur when the model has not been exposed to enough data, the model itself is insufficient to learn the complexity of the problem, or the data it has been trained on does not represent all possible scenarios. This is also known as systemic or reducible uncertainty because this is the portion of the total predictive uncertainty that can be actively reduced by having better models, better data, and so on; in other words, by gaining more knowledge about the system. Let’s see a few examples to make the concept clearer:</p>
    <ul>
      <li class="bulletList">If a weather forecast model has less data from a particular region (maybe because of a faulty sensor), there is going to be less knowledge about that region, and this increases uncertainty.</li>
      <li class="bulletList">If a linear model was used for a non-linear problem, we would be introducing some uncertainty because of the simpler model having less knowledge about the system.</li>
      <li class="bulletList">If an economic forecasting model is not trained using a few key influencing factors like change in economic policies, climate change influencing the decisions, and so on, this also creates some uncertainty.</li>
    </ul>
    <p class="normal">The good thing about this kind of uncertainty is that this is something that we can actively reduce by collecting better data, training better models, and so on.</p>
    <p class="normal">Now there is another kind of uncertainty in the total predictive uncertainty—<strong class="keyWord">Aleatoric Uncertainty</strong>. This <a id="_idIndexMarker1387"/>refers to the inherent randomness in the data<a id="_idIndexMarker1388"/> which cannot be explained away. This is also known as statistical or irreducible uncertainty. Although our universe appears deterministic to us, there is an ever-prevalent layer of uncertainty just beneath the surface.</p>
    <p class="normal">For example, the motion of the celestial objects can be calculated accurately (thanks to General Relativity and Einstein), but still, a random asteroid can hit any of the bodies and cause alterations to the calculated trajectory. This kind of irreducible and unavoidable uncertainty is referred to as aleatoric uncertainty. Let’s see a few examples:</p>
    <ul>
      <li class="bulletList">Weather predictions, no matter how accurate the measurements and the models are, are still variable. There is an inherent randomness in weather that we might never be able to completely explain.</li>
      <li class="bulletList">The performance of an athlete, no matter how much they have trained and followed the rules, is still not completely deterministic. There are a lot of factors, like weather, health, and other random events during or before the game, which will affect the performance of the athlete.</li>
    </ul>
    <p class="normal">Now that we understand the different types of uncertainty, and why we need uncertainty quantification, let’s see what it means in the context of forecasting.</p>
    <h2 id="_idParaDest-417" class="heading-2">What are probabilistic forecasts and Prediction Intervals?</h2>
    <p class="normal">A probabilistic forecast <a id="_idIndexMarker1389"/>is when the forecast, instead of having a single-point prediction, captures the uncertainty of that forecast as well. Probabilistic forecasting is a method of predicting future events or outcomes by providing a <a id="_idIndexMarker1390"/>range of possible values along with associated probabilities or confidence levels. This approach captures the uncertainty in the prediction process.</p>
    <p class="normal">In the econometrics and classical time series world, the prediction intervals were already baked into the formulation. The statistical grounding and strong assumptions of those methods made sure that the output of those models was readily interpreted in a probabilistic way as well (so long as you could satisfy the assumptions that were stipulated by those models). But in the modern machine learning/deep learning world, probabilistic forecasting is not an afterthought. A combination of factors, such as fewer rigid assumptions and the way we train the models, leads to this predicament.</p>
    <p class="normal">There are <a id="_idIndexMarker1391"/>different methods with which we can add the probabilistic dimension to our forecast, and we will go through a few of them in this chapter. But before that, let’s also understand one of the most useful manifestations of <a id="_idIndexMarker1392"/>probabilistic forecasts—<strong class="keyWord">Prediction Intervals</strong>.</p>
    <p class="normal">A Prediction Interval is a range within which a future observation is expected to fall with a specified probability. For instance, if we have a 95% prediction interval for a time step of <code class="inlineCode">[5,8]</code>, we say that 95% of the time, the actual value will lie between 5 and 8. Let’s take an example of a normal distribution with mean, <img src="../Images/B22389_17_003.png" alt=""/>, and variance, <img src="../Images/B22389_17_004.png" alt=""/> as the forecast at timestep, <em class="italic">t</em> (one of the techniques we will talk about gives us just that). So, the prediction interval at time, <em class="italic">t</em>, with a significance level (the probability with which the forecast can fall outside the interval), <img src="../Images/B22389_04_009.png" alt=""/>, can be written as:</p>
    <p class="center"><img src="../Images/B22389_17_006.png" alt=""/></p>
    <p class="normal">where <em class="italic">z</em> is the z-score of the normal distribution.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Prediction Interval</strong> (<strong class="keyWord">PI</strong>) vs <strong class="keyWord">Confidence Interval</strong> (<strong class="keyWord">CI</strong>)</p>
      <p class="normal">One of the most <a id="_idIndexMarker1393"/>confusing topics is prediction intervals and confidence intervals. Let’s demystify them here. These are both ways we quantify uncertainty, but they serve different purposes and are interpreted differently in the context of forecasting. A confidence interval also provides a range, but for a population parameter (like the mean) of the sample data, whereas a prediction interval focuses on providing a range for a future observation. One of the key differences between the two is that CIs are, typically, narrower than PIs because PIs also account for the uncertainty of the new point. On the other hand, CIs only account for the uncertainty accounted to the model parameters. So, we use PIs when we need to give a range for when a future observation is likely to fall, like the next month’s sales. CIs are used when we need to provide a range for an estimated parameter, like the estimated average demand over a year.</p>
    </div>
    <p class="normal">There are a few terms and concepts we need to clarify before we begin the discussion in detail.</p>
    <h2 id="_idParaDest-418" class="heading-2">Confidence levels, error rates, and quantiles</h2>
    <p class="normal">When working with prediction intervals, it’s crucial to understand the relationship between <a id="_idIndexMarker1394"/>confidence levels, error rates, and quantiles. These concepts help in determining the range within which future observations are expected to fall with a certain probability.</p>
    <p class="normal"><em class="italic">Error Rate</em> (<img src="../Images/B22389_04_009.png" alt=""/>) is the allowable probability that<a id="_idIndexMarker1395"/> the <a id="_idIndexMarker1396"/>prediction interval will not contain the future observation. It is typically expressed in percentages or as a decimal between 0 and 1. If we say <img src="../Images/B22389_17_008.png" alt=""/> or <img src="../Images/B22389_17_009.png" alt=""/>, it means that there is a 10% chance that the future observation will not be in the prediction interval.</p>
    <p class="normal"><em class="italic">Confidence Level</em> (<img src="../Images/B22389_17_010.png" alt=""/>) is the complement of <a id="_idIndexMarker1397"/>error rate and is the probability that the prediction interval contains the future observation. <img src="../Images/B22389_17_011.png" alt=""/>. If we say that the error rate is 10%, the confidence level would be 90%.</p>
    <p class="normal"><em class="italic">Quantiles</em> are <a id="_idIndexMarker1398"/>points that divide the data into intervals with <a id="_idIndexMarker1399"/>equal probabilities. In simpler terms, a quantile shows the value below which a certain percentage of data falls. For instance, the 5th percentile or 0.05th quantile marks the point where 5% of data lies below it. Therefore, we can use quantiles as well to define the prediction intervals where we don’t have an analytical way to get the prediction intervals based on distributional assumption.</p>
    <p class="normal">In <em class="italic">Figure 17.1</em>, we show the prediction intervals for a standard normal distribution.</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.1: Prediction intervals for a standard normal distribution</p>
    <p class="normal">There is a <a id="_idIndexMarker1400"/>strong link between error rates, confidence levels, and quantiles. Error rates and confidence levels are direct complements to each other and can be used interchangeably to define the confidence that we want our prediction intervals to have or the error rate we are okay with from the prediction intervals. Another way to look at it is by the area under the curve. In <em class="italic">Figure 17.1</em>, the area of the green-shaded region denotes the confidence level, and the area of the red area denotes the error rate. </p>
    <p class="normal">For a standard normal distribution, we can directly get the prediction intervals by using the analytical formula:</p>
    <p class="center"><img src="../Images/B22389_17_012.png" alt=""/></p>
    <p class="normal">where <img src="../Images/B22389_03_003.png" alt=""/> is the mean of the distribution, <img src="../Images/B22389_03_004.png" alt=""/> is the standard deviation of the distribution, and <img src="../Images/B22389_17_015.png" alt=""/> is the critical value from the standard normal distribution corresponding to the desired confidence level, <img src="../Images/B22389_17_016.png" alt=""/>. <img src="../Images/B22389_17_017.png" alt=""/> is taken because we are allowing for the error rate to be spread on both sides (red shaded area on both sides of the curve in <em class="italic">Figure 17.1</em>).</p>
    <p class="normal">Now let’s<a id="_idIndexMarker1401"/> look at how error rates and confidence levels are linked with quantiles because if we don’t know what the distribution is (and we don’t want to assume any distribution), we can’t go by the analytical formula to get prediction intervals. In such cases, we can use quantiles to get the same. Just like we did with the analytical formula, the error rate, <img src="../Images/B22389_04_009.png" alt=""/>, should be equally divided across both sides. And therefore, the prediction interval would be:</p>
    <p class="center"><img src="../Images/B22389_17_019.png" alt=""/></p>
    <p class="normal">where <img src="../Images/B22389_17_020.png" alt=""/> is the <em class="italic">t</em><sup class="superscript-italic" style="font-style: italic;">th</sup> quantile. So, from the definition of quantile, we know <img src="../Images/B22389_17_021.png" alt=""/> would have <img src="../Images/B22389_17_017.png" alt=""/> % of data below it and <img src="../Images/B22389_17_023.png" alt=""/> would have <img src="../Images/B22389_17_017.png" alt=""/> % of data above it, thus making the area outside the intervals to be <img src="../Images/B22389_04_009.png" alt=""/>.</p>
    <p class="normal">Using this relation, we can go from error rates to quantiles or confidence levels to quantiles. If the error rate is <img src="../Images/B22389_04_009.png" alt=""/> we have already seen what the corresponding quantiles denoting the prediction interval are. Let’s also see one more quick formula to go from confidence levels (declared as percentages) to quantiles:</p>
    <p class="center"><img src="../Images/B22389_17_027.png" alt=""/></p>
    <p class="normal">In Python code, this is simply:</p>
    <pre class="programlisting code"><code class="hljs-code">level = <span class="hljs-number">95</span>  <span class="hljs-comment"># Confidence levels</span>
qs = [<span class="hljs-number">50</span> - level / <span class="hljs-number">2</span>, <span class="hljs-number">50</span> + level / <span class="hljs-number">2</span>] <span class="hljs-comment"># Quantiles</span>
</code></pre>
    <p class="normal">Now, let’s look at how to measure the goodness of prediction intervals.</p>
    <h2 id="_idParaDest-419" class="heading-2">Measuring the goodness of prediction intervals</h2>
    <p class="normal">We<a id="_idIndexMarker1402"/> know what a probabilistic forecast is and what prediction intervals are. But before we look at techniques to generate prediction intervals, we need a way to measure the goodness of such an interval. Standard metrics like Mean Absolute Error or Mean Squared Error hold no more because they are point forecast measuring metrics.</p>
    <p class="normal">What do we <a id="_idIndexMarker1403"/>want from a prediction interval? If we have a prediction interval with 90% confidence, we would expect the data points to lie between the interval at least 90% of the time. This can easily be obtained by having very wide prediction intervals, but that again becomes a useless prediction interval. So, we want our prediction interval to be as narrow as possible and still have the 90% confidence criteria respected. To measure these two distinct aspects, we can use two metrics—<strong class="keyWord">Coverage</strong> and <strong class="keyWord">Average Length of Prediction Intervals</strong>.</p>
    <p class="normal"><strong class="keyWord">Coverage</strong> is the<a id="_idIndexMarker1404"/> proportion of true values that fall within the prediction intervals. Mathematically, if we denote the Prediction Interval by <img src="../Images/B22389_17_028.png" alt=""/> for each observation <em class="italic">i</em>, and true value by <img src="../Images/B22389_17_029.png" alt=""/>, coverage can be defined as:</p>
    <p class="center"><img src="../Images/B22389_17_030.png" alt=""/></p>
    <p class="normal">where <img src="../Images/B22389_17_031.png" alt=""/> is an indicator function that equals 1 if the condition inside is true and 0 otherwise, and <em class="italic">N</em> is the total number of observations. A coverage metric close to the desired confidence level (e.g., 95% for a 95% prediction interval) indicates that the model’s uncertainty estimates are well-calibrated.</p>
    <p class="normal"><strong class="keyWord">Average Length of Prediction Intervals</strong> is<a id="_idIndexMarker1405"/> calculated by averaging the lengths of the prediction intervals across all observations. Using the same notations as above, it can be mathematically written as:</p>
    <p class="center"><img src="../Images/B22389_17_032.png" alt=""/></p>
    <p class="normal">This metric helps in understanding the trade-off between the coverage of the intervals and their precision. Let’s also look at Python functions for both of these metrics:</p>
    <p class="normal"><strong class="keyWord">Coverage</strong>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">def</span> <span class="hljs-title">coverage</span>(<span class="hljs-params">y_true, lower_bounds, upper_bounds</span>):
    <span class="hljs-string">"""</span>
<span class="hljs-string">    Calculate the coverage of prediction intervals.</span>
<span class="hljs-string">   </span>
<span class="hljs-string">    Parameters:</span>
<span class="hljs-string">    y_true (array-like): True values.</span>
<span class="hljs-string">    lower_bounds (array-like): Lower bounds of prediction intervals.</span>
<span class="hljs-string">    upper_bounds (array-like): Upper bounds of prediction intervals.</span>
<span class="hljs-string">   </span>
<span class="hljs-string">    Returns:</span>
<span class="hljs-string">    float: Coverage metric.</span>
<span class="hljs-string">    """</span>
    y_true = np.array(y_true)
    lower_bounds = np.array(lower_bounds)
    upper_bounds = np.array(upper_bounds)
    <span class="hljs-comment"># Check if true values fall within the prediction intervals</span>
    coverage = np.mean((y_true &gt;= lower_bounds) &amp; (y_true &lt;= upper_bounds))
    <span class="hljs-keyword">return</span> coverage
</code></pre>
    <p class="normal"><strong class="keyWord">Average Length</strong>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">average_length</span>(<span class="hljs-params">lower_bounds, upper_bounds</span>):
    <span class="hljs-string">"""</span>
<span class="hljs-string">    Calculate the average length of prediction intervals.</span>
<span class="hljs-string">   </span>
<span class="hljs-string">    Parameters:</span>
<span class="hljs-string">    lower_bounds (array-like): Lower bounds of prediction intervals.</span>
<span class="hljs-string">    upper_bounds (array-like): Upper bounds of prediction intervals.</span>
<span class="hljs-string">   </span>
<span class="hljs-string">    Returns:</span>
<span class="hljs-string">    float: Average length of prediction intervals.</span>
<span class="hljs-string">    """</span>
    lower_bounds = np.array(lower_bounds)
    upper_bounds = np.array(upper_bounds)
   
    <span class="hljs-comment"># Calculate the length of each prediction interval</span>
    lengths = upper_bounds - lower_bounds
    <span class="hljs-comment"># Calculate the average length</span>
    average_length = np.mean(lengths)
    <span class="hljs-keyword">return</span> average_length
</code></pre>
    <p class="normal">Both of these<a id="_idIndexMarker1406"/> Python functions can be found in <code class="inlineCode">src/utils/ts_utils.py</code> and we will be using them to measure the quality of prediction intervals generated in the chapter.</p>
    <p class="normal">Now, let’s look at different techniques we can use to get probabilistic forecasts and how we can use them practically.</p>
    <h2 id="_idParaDest-420" class="heading-2">Probability Density Function (PDF)</h2>
    <p class="normal">This is <a id="_idIndexMarker1407"/>one of the most common<a id="_idIndexMarker1408"/> techniques for probabilistic forecasting, especially in the deep learning space, because of the ease of implementation. The forecast at time <em class="italic">t</em>, <img src="../Images/B22389_17_033.png" alt=""/>, can be seen as the realization of a probability distribution, <img src="../Images/B22389_17_034.png" alt=""/>. And instead of estimating <img src="../Images/B22389_17_033.png" alt=""/>, we can estimate <img src="../Images/B22389_17_034.png" alt=""/>. If we assume <img src="../Images/B22389_17_034.png" alt=""/> is one of the parametrized distributions, <img src="../Images/B22389_17_038.png" alt=""/>, with parameters <img src="../Images/B22389_17_039.png" alt=""/>, then we can estimate the parameters <img src="../Images/B22389_17_039.png" alt=""/>, instead of <img src="../Images/B22389_17_033.png" alt=""/>, directly.</p>
    <p class="normal">For instance, if we assume the forecast is drawn from normal distribution, we can model</p>
    <p class="center"><img src="../Images/B22389_17_042.png" alt=""/></p>
    <p class="normal">So, instead of getting our model to output <img src="../Images/B22389_17_033.png" alt=""/>, we can get it to output <img src="../Images/B22389_17_003.png" alt=""/> and <img src="../Images/B22389_17_045.png" alt=""/>. And with <img src="../Images/B22389_17_003.png" alt=""/> and <img src="../Images/B22389_17_045.png" alt=""/>, we can easily calculate the prediction intervals at the given <img src="../Images/B22389_04_009.png" alt=""/>:</p>
    <p class="center"><img src="../Images/B22389_17_049.png" alt=""/></p>
    <p class="center"><img src="../Images/B22389_17_050.png" alt=""/></p>
    <p class="normal">where <em class="italic">Z</em> is the critical value from the standard normal distribution corresponding to the desired confidence level. For a 90% confidence level, <img src="../Images/B22389_17_051.png" alt=""/>. Simple enough, right? Not so fast!</p>
    <p class="normal">Now that we are modeling the parameters of a distribution, how do we train the model? We still have the actual point forecast as the target. In the normal distribution case, the targets are still actual <img src="../Images/B22389_17_052.png" alt=""/> and not the means and standard deviations. We get over this problem by using a loss function like <strong class="keyWord">log likelihood</strong>, instead of losses like Squared Error.</p>
    <p class="normal">For instance, let’s say we have a set of <em class="italic">i.i.d</em> observations (in our case, the target), <img src="../Images/B22389_17_053.png" alt=""/>. With the predicted parameters of the assumed distribution (<img src="../Images/B22389_17_054.png" alt=""/>), we will be able to calculate the probability of each of the target, <img src="../Images/B22389_17_055.png" alt=""/>. The <em class="italic">i.i.d</em> assumption means each sample is independent of each other. And high-school mathematics tells us that when two independent<a id="_idIndexMarker1409"/> events happen, we <a id="_idIndexMarker1410"/>can calculate their joint probability by multiplying the two independent probabilities together. Using the same logic, we can calculate the joint probability or likelihood of all <em class="italic">n</em> <em class="italic">i.i.d</em> observations (probability that all these events occur) by just multiplying all the individual probabilities together.</p>
    <p class="center"><img src="../Images/B22389_17_056.png" alt=""/></p>
    <p class="normal">Maximizing the likelihood helps the model learn the right parameters for each sample such that the probability under the assumed distribution maximizes. We can intuitively think about this as follows. For an assumed distribution like normal distribution, maximizing the likelihood makes sure that the target falls in the center of the distribution defined by the predicted parameters for each sample.</p>
    <p class="normal">However, this operation is not numerically stable. Since probabilities are <img src="../Images/B22389_17_057.png" alt=""/>, multiplying them together makes the result progressively smaller and can soon lead to numerical underflow issues. Therefore, we use the log likelihood, which is nothing but the likelihood but log transformed. We do it because:</p>
    <ul>
      <li class="bulletList">Being a strictly monotonic transformation, optimizing a function is equivalent to optimizing the log transform of the function. Therefore, optimizing likelihood and log likelihood is the same thing.</li>
      <li class="bulletList">Log transformation converts the multiplication into an addition, which is a much more numerically stable operation.</li>
    </ul>
    <p class="center"><img src="../Images/B22389_17_058.png" alt=""/></p>
    <p class="normal">The major disadvantage of this approach is that this relies on parametrized probability distributions whose log likelihood computation is tractable. Therefore, we are forced to make assumptions about the output and pick a distribution that might fit ahead of time. </p>
    <p class="normal">This is a double-edged sword. On one hand, we can inject some domain knowledge into the problem and regularize the model training, but on the other hand, if we aren’t clear on whether <a id="_idIndexMarker1411"/>choosing a normal distribution is the right <a id="_idIndexMarker1412"/>choice, it can lead to an unnecessarily constrained model.</p>
    <p class="normal">Many popular distributions, such as Normal, Poisson, Negative Binomial, Exponential, LogNormal, Tweedie, and so on, can be used for generating probabilistic forecasts with this technique.</p>
    <p class="normal">Now, we have all the components for training and learning a model and can make it predict a full probabilistic distribution instead of a point forecast. With all that theory set, let’s switch gears and see how we can use this technique.</p>
    <h3 id="_idParaDest-421" class="heading-3">Forecasting with PDF—machine learning models</h3>
    <p class="normal">We have <a id="_idIndexMarker1413"/>seen how to use standard machine learning models for forecasting in <em class="italic">Part 2</em>, <em class="italic">Machine Learning for Time Series</em>. But all of that was for point forecast. Can we easily convert all of that into probabilistic forecasts using the PDF approach? In theory, yes. But practically, it’s not that easy. All the popular implementations of machine learning models like <code class="inlineCode">sci-kit learn</code>, <code class="inlineCode">xgboost</code>, <code class="inlineCode">lightgbm</code>, and so on take a point prediction paradigm. And as users of such open-source libraries, it isn’t easy for us to tweak and re-write the code to make it optimize the log likelihood as a regression loss. But fear not, it is not impossible. <code class="inlineCode">NGBoost</code> is a distant cousin of the very popular gradient boosting models, like <code class="inlineCode">xgboost</code> and <code class="inlineCode">lightgbm</code>, and it is implemented such that it predicts the PDF, instead of the point prediction.</p>
    <div class="note">
      <p class="normal">There are other techniques like Quantile Forecast or Conformal Prediction, which are more widely applicable (and recommended) to the machine learning models we discussed in the book if the end goal is to have a prediction interval. NGBoost is discussed for completeness and for the cases where a full probability distribution is needed as an output.</p>
    </div>
    <p class="normal">We aren’t going too deep into what NGBoost is and how it differs from the regular gradient boosting models here, but just know that it is a model that predicts probability distributions instead of point predictions.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research paper by Duan et.al., which proposed <code class="inlineCode">NGBoost</code>, is cited in <em class="italic">References</em> under reference <em class="italic">1</em>.</p>
    </div>
    <p class="normal"><em class="italic">Further</em><em class="italic"><a id="_idIndexMarker1414"/></em><em class="italic"> reading</em> has a link to a blog about NGBoost, which goes into a bit more depth on what the model is.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">To follow along with the complete code, use the notebook named <code class="inlineCode">01-NGBoost_prediction_intervals.ipynb</code> in the <code class="inlineCode">Chapter17</code> folder.</p>
    </div>
    <p class="normal">Let’s use a sample of eight time series from the M4 competition (which has 100,000 time series, references <em class="italic">5</em>) for the probabilistic forecasts. The data is easily available online and the download script is included in the notebook. We are using this simpler dataset than the one we have been working on because I want to avoid complicating the narrative with exogenous variables and such. Here is the plot of the last 100 time steps of the eight sampled time series.</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_02_01.png" alt=""/></figure>
    <p class="packt_figref"><img src="../Images/B22389_17_02_02.png" alt=""/></p>
    <p class="packt_figref">Figure 17.2: Last 100 timesteps of 8 sampled time series from the M4 Competition. Test period is drawn in a dotted purple line.</p>
    <p class="normal">Let’s use <a id="_idIndexMarker1415"/>mlforecast to create some features quickly in order to convert this into a regression problem. We had a bonus notebook back in <em class="chapterRef">Chapter 6</em>, which showed how to use <code class="inlineCode">mlforecast</code> as an alternative for the feature engineering that is included in the book’s repository. For the detailed code, refer to the full notebook, but for now, let’s assume that we have a dataframe called <code class="inlineCode">data</code>, which has all the features necessary to run a machine learning model. We have split it into <code class="inlineCode">train</code> and <code class="inlineCode">val</code> and then subsequently to <code class="inlineCode">X_train</code>, <code class="inlineCode">y_train</code>, <code class="inlineCode">X_val</code>, and <code class="inlineCode">y_val</code>. Now, let’s see how we can train a model, assuming the output is a Normal distribution.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> ngboost <span class="hljs-keyword">import</span> NGBRegressor
<span class="hljs-keyword">from</span> ngboost.distns <span class="hljs-keyword">import</span> Normal
<span class="hljs-comment"># Training the model</span>
ngb = NGBRegressor(Dist=Normal).fit(X_train, Y_train)
</code></pre>
    <p class="normal">NGBoost doesn’t have a lot<a id="_idIndexMarker1416"/> of parameters to tune and therefore isn’t as flexible as other <strong class="keyWord">gradient-boosting decision trees</strong> (<strong class="keyWord">GBDT</strong>). And it’s also not as fast as the other GBDTs. This is a model that you use only for special use cases when you need probabilistic outputs.</p>
    <p class="normal">These are a few<a id="_idIndexMarker1417"/> parameters that NGBoost has:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">Dist</code>: This is the assumed distributional form of the output. The package currently supports <code class="inlineCode">Normal</code>, <code class="inlineCode">LogNormal</code>, and <code class="inlineCode">Exponential</code>—all of which can be imported from <code class="inlineCode">ngboost.distns</code></li>
      <li class="bulletList"><code class="inlineCode">Score</code>: This is any valid scoring function that is used to compare the predicted distributions to observations. The log likelihood score that we discussed earlier is called <code class="inlineCode">LogScore</code> in the package and is the default value. All scoring functions can be imported from <code class="inlineCode">ngboost.scores</code>.</li>
      <li class="bulletList"><code class="inlineCode">n_estimators</code>: This is the number of estimators that are used in the boosted trees.</li>
      <li class="bulletList"><code class="inlineCode">learning_rate</code>: This is the learning rate used for combining the boosted trees.</li>
      <li class="bulletList"><code class="inlineCode">mini_batch_frac</code>: The percent of rows which is sub-sampled for each iteration. This is set to <code class="inlineCode">1.0</code> as the default value.</li>
    </ul>
    <p class="normal">Now that we have a trained NGBoost model, let’s see how we can use it to generate predictions and prediction intervals.</p>
    <p class="normal">To get point predictions, the syntax is exactly the same as the sci-kit learn API.</p>
    <pre class="programlisting code"><code class="hljs-code">y_pred = ngb.predict(X_val)
</code></pre>
    <p class="normal">This is nothing <a id="_idIndexMarker1418"/>but a wrapper method that calculates the location parameter of the assumed distribution. For instance, for the Normal distribution, the mean of the predicted distribution is the point prediction.</p>
    <p class="normal">Now to get the underlying probabilistic prediction and subsequently the prediction intervals, we need to use a different method:</p>
    <pre class="programlisting code"><code class="hljs-code">y_pred_dists = ngb.pred_dist(X_val)
</code></pre>
    <p class="normal">Each point in <code class="inlineCode">y_pred_dists</code> is a complete distribution. If we want to instead the first five predicted points, we can do the following:</p>
    <pre class="programlisting code"><code class="hljs-code">y_pred_dists[<span class="hljs-number">0</span>:<span class="hljs-number">5</span>].params
</code></pre>
    <p class="normal">Now, to get the prediction interval, we can use <code class="inlineCode">y_pred_dist</code> and call a method, giving the level of confidence we expect. This, in turn, calls the <code class="inlineCode">scipy</code> distribution (like <code class="inlineCode">scipy.stats.norm</code>), which has a method, <code class="inlineCode">interval</code>, to get the intervals given the confidence level.</p>
    <pre class="programlisting code"><code class="hljs-code">y_pred_lower, y_pred_upper = y_pred_dists.dist.interval(<span class="hljs-number">0.95</span>)
</code></pre>
    <p class="normal">Now, we have a window around <code class="inlineCode">y_pred</code> wide enough to envelope the uncertainty expected at each data point—the prediction interval. Let’s look at the forecast and the metrics.</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_03_01.png" alt=""/></figure>
    <p class="packt_figref"><img src="../Images/B22389_17_03_02.png" alt=""/></p>
    <p class="packt_figref">Figure 17.3: Forecast with prediction intervals from NGBoost</p>
    <p class="normal">And below <a id="_idIndexMarker1419"/>are the metrics we calculated for these eight time series (Mean Absolute Error, Coverage, and Average Length):</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.4: Metrics for NGBoost</p>
    <p class="normal">Although for some time series, the intervals seem to be good, some others (like time series H103) seem to have way too narrow a prediction interval, which is also evident in the low coverage as well.</p>
    <h3 id="_idParaDest-422" class="heading-3">Forecasting with PDF—deep learning models</h3>
    <p class="normal">Unlike <a id="_idIndexMarker1420"/>machine learning models, it’s very easy to convert all the deep learning models we have learned about in the book to their PDF version. Remember the discussion we had before we started the practical application? The major changes we needed to do were these:</p>
    <ul>
      <li class="bulletList">Instead of predicting point forecast (single number), we predict the parameters of a probability distribution (one or more numbers).</li>
      <li class="bulletList">Instead of using a point loss like Mean Squared Error, use a probabilistic scoring function like log likelihood.</li>
    </ul>
    <p class="normal">In the deep learning paradigm, these are very simple changes to make, aren’t they? In almost all the deep learning models we have learned to use in the book, there is a linear projection at the end, which projects the output into the required dimensions. Conceptually, it’s simple enough to make these output multiple numbers (parameters of the assumed distribution) by changing the linear projection. Similarly, it’s simple enough to change the loss function as well. Notably, <em class="italic">DeepAR </em>(Reference <em class="italic">3</em>) is a well-known deep learning model that uses this technique for probabilistic forecasting.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">To follow along with the complete code, use the notebook named <code class="inlineCode">02-NeuralForecast_prediction_intervals_PDF.ipynb</code> in the <code class="inlineCode">Chapter17</code> folder.</p>
    </div>
    <p class="normal">Let’s see how we can do this in <code class="inlineCode">neuralforecast</code> (the library we were using <em class="italic">Chapter 16</em>). In our example here, we will take a simple model like an LSTM, but we can do the same with any model because all we are doing is switching the loss function to <code class="inlineCode">DistributionLoss</code>. </p>
    <p class="normal">And, for this example, we are going to use the M4 competition dataset, which is freely available (the code to download the dataset is included in the notebook).</p>
    <p class="normal">Let’s start from the point where we have the data formatted the way <code class="inlineCode">neuralforecast</code> expects in <code class="inlineCode">Y_train_df</code> and <code class="inlineCode">Y_test_df</code>. The first thing we need to do is import the necessary classes.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> neuralforecast <span class="hljs-keyword">import</span> NeuralForecast
<span class="hljs-keyword">from</span> neuralforecast.models <span class="hljs-keyword">import</span> LSTM
<span class="hljs-keyword">from</span> neuralforecast.losses.pytorch <span class="hljs-keyword">import</span> DistributionLoss
</code></pre>
    <p class="normal">The only <a id="_idIndexMarker1421"/>class that we haven’t looked at before in the book is <code class="inlineCode">DistributionLoss</code>. This is a class that wraps <code class="inlineCode">torch.distribution</code> classes and implements the negative log likelihood loss we discussed earlier. At the time of writing this book, the <code class="inlineCode">DistributionLoss</code> class supports these underlying distributions:</p>
    <ul>
      <li class="bulletList">Poisson</li>
      <li class="bulletList">Normal</li>
      <li class="bulletList">StudentT</li>
      <li class="bulletList">NegativeBinomial</li>
      <li class="bulletList">Tweedie</li>
      <li class="bulletList">Bernoulli (Temporal Classifiers)</li>
      <li class="bulletList">ISQF (Incremental Spline Quantile Function)</li>
    </ul>
    <p class="normal">The choice between these different distributions is totally up to the modeler and is a key assumption in the model. If the output we are modeling is expected to be in a normal distribution, then we can choose <code class="inlineCode">Normal</code>. These are the major parameters of <code class="inlineCode">DistributionLoss</code>:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">distribution</code>: This is a string that identifies which distribution we are assuming. It can be any one from the list we saw before.</li>
      <li class="bulletList"><code class="inlineCode">level</code>: This is a list of floats that defines the different confidence levels we are interested in modeling. For instance, if we want to model 80% and 90% confidence, we should give the values as <code class="inlineCode">[80,90]</code>.</li>
      <li class="bulletList"><code class="inlineCode">quantiles</code>: This is an alternate way of defining the level. Instead of 95% confidence, you can define them in quantiles → <code class="inlineCode">[0.1, 0.9]</code>.<div class="note">
          <p class="normal"><strong class="keyWord">Practitioner’s tip</strong>:</p>
          <p class="normal">Making a distributional assumption requires a deep study of the domain and data. But if you are not completely familiar with these distributions, <code class="inlineCode">Normal</code> or <code class="inlineCode">StudentT</code> is a good starting point as a lot of data resembles normal distribution. But before you jump into using <code class="inlineCode">Normal</code> distributions everywhere, you should do a bit of literature study in the domain of the problem and choose the distribution accordingly. For instance, intermittent demand or sparse demand, which is very common in retail, is better modeled using a <code class="inlineCode">Poisson</code> distribution. If the forecast is a count data (positive integers), <code class="inlineCode">Negative Binomial</code> is a good option.</p>
        </div>
      </li>
    </ul>
    <p class="normal">Now let’s<a id="_idIndexMarker1422"/> set a horizon, the levels we need, and a few hyperparameters for LSTM (we’ve chosen some small and simple hyperparameters to make the training faster. In real-world problems, it is advisable to do a hyperparameter search to find the best parameters).</p>
    <pre class="programlisting code"><code class="hljs-code">horizon = <span class="hljs-number">48</span>
levels = [<span class="hljs-number">80</span>, <span class="hljs-number">90</span>]
lstm_config = <span class="hljs-built_in">dict</span>(input_size=<span class="hljs-number">3</span>*horizon, encoder_hidden_size=<span class="hljs-number">8</span>, decoder_hidden_size=<span class="hljs-number">8</span>)
</code></pre>
    <p class="normal">Now we need to define the models we are going to use and the <code class="inlineCode">NeuralForecast</code> class. Let’s define two models—one using <code class="inlineCode">Normal</code> and another using <code class="inlineCode">StudentT</code>.</p>
    <pre class="programlisting code"><code class="hljs-code">models = [
    LSTM(
        h=horizon,
        loss=DistributionLoss(distribution=<span class="hljs-string">"StudentT"</span>, level=levels),
        alias=<span class="hljs-string">"LSTM_StudentT"</span>,
        **lstm_config
    ),
    LSTM(
        h=horizon,
        loss=DistributionLoss(distribution=<span class="hljs-string">"Normal"</span>, level=levels),
        alias=<span class="hljs-string">"LSTM_Normal"</span>,
        **lstm_config
    ),
]
<span class="hljs-comment"># Setting freq=1 because the ds column is not date, but instead a sequentially increasing number</span>
nf = NeuralForecast(models=models, freq=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Notice how the syntax is exactly the same as for point forecast, except for the Distribution Loss we chose. Now all that’s left is to train the models.</p>
    <pre class="programlisting code"><code class="hljs-code">nf.fit(df=Y_train_df)
</code></pre>
    <p class="normal">Once the model is trained, we can predict using the predict method. This output will have the point forecast under the alias we have defined and the high and low intervals for all the levels we have defined.</p>
    <pre class="programlisting code"><code class="hljs-code">Y_hat_df = nf.predict()
</code></pre>
    <p class="normal">Now <a id="_idIndexMarker1423"/>that we have the probabilistic forecasts, let’s take a look at them and also calculate the metrics.</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_05_01.png" alt=""/></figure>
    <p class="packt_figref"><img src="../Images/B22389_17_05_02.png" alt=""/></p>
    <p class="packt_figref">Figure 17.5: Forecast with prediction intervals from LSTM with StudentT distribution as output</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.6: Metrics for the LSTM with Normal and StudentT distribution outputs</p>
    <p class="normal">If we compare the coverages with the ones with NGBoost, we can see that the deep learning approach increased the coverage, but also has wider than necessary intervals in most cases (as evidenced by larger average widths).</p>
    <p class="normal">The <a id="_idIndexMarker1424"/>biggest disadvantage of this method is that we restrict the output to one of the parametrized distributions. In many real-world cases, the data might not conform to any parametric distributions.</p>
    <p class="normal">Now, let’s see a method that doesn’t require the assumption of any parametric distribution, but still gets the prediction intervals.</p>
    <h2 id="_idParaDest-423" class="heading-2">Quantile function</h2>
    <p class="normal">If our only<a id="_idIndexMarker1425"/> aim is to get prediction intervals, we <a id="_idIndexMarker1426"/>can also do the same using quantiles. Let’s look at the PDF method in a slightly different light. In the PDF method, we have a full probability distribution as the output at each timestep and we use that distribution to get the quantiles, which are the prediction intervals. Although for most parametric distributions, there are analytical formulae to get the quantiles, we can also find the quantiles numerically. We just draw N samples, where N is sufficiently large, and then calculate the quantiles of the drawn samples. The point is that even though we have a full probability distribution, for prediction intervals, all we need are the quantiles. And calculating quantiles given N samples is not dependent on the kind of distribution.</p>
    <p class="normal">So, what if we can train our models to predict the specified quantiles directly, having no assumption on the underlying probability distribution? This is exactly what we do with quantile functions.</p>
    <p class="normal">Before talking about quantile<a id="_idIndexMarker1427"/> functions, let’s spend a minute on the <strong class="keyWord">Cumulative Distribution Function</strong> (<strong class="keyWord">CDF</strong>). This, again, is high school probability. In simple words, a CDF returns the probability of some random variable, <em class="italic">X</em> being less than or equal to some value, <em class="italic">x</em>:</p>
    <p class="center"><img src="../Images/B22389_17_059.png" alt=""/></p>
    <p class="normal">where <em class="italic">F</em> is the CDF. This function takes in an input, <em class="italic">x</em>, and returns a value between 0 and 1. Let’s call this value <img src="../Images/B22389_16_143.png" alt=""/>.</p>
    <p class="normal">A <em class="italic">quantile function</em> is an inverse of CDF. This function tells you what value of <em class="italic">x</em> would make <img src="../Images/B22389_17_061.png" alt=""/> return a particular value, <img src="../Images/B22389_16_143.png" alt=""/>.</p>
    <p class="center"><img src="../Images/B22389_17_063.png" alt=""/></p>
    <p class="normal">This function, <img src="../Images/B22389_17_064.png" alt=""/>, is the <em class="italic">quantile function</em>.</p>
    <p class="normal">From the <a id="_idIndexMarker1428"/>implementation perspective, for models which are capable of multi-output prediction (like the deep learning models), we can use a single model and predict all the quantiles we want to by changing the output layer. And for models that are restricted to a single output (like machine learning models), we can learn separate quantile models for each quantile.</p>
    <p class="normal">Now, just like before, we can’t use point losses like the mean squared error. We got over this problem in PDFs by using the log likelihood function. And here, we can use quantile loss or pinball loss.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The paper that proposed quantile loss and regression is cited in the <em class="italic">References</em> under reference <em class="italic">4</em>.</p>
    </div>
    <p class="normal">The quantile loss can be defined as below:</p>
    <p class="center"><img src="../Images/B22389_17_065.png" alt=""/></p>
    <p class="normal">where <img src="../Images/B22389_17_052.png" alt=""/> is the target value at time <em class="italic">t</em>, <img src="../Images/B22389_17_067.png" alt=""/> is the quantile forecast, and <em class="italic">q</em> is the quantile we are forecasting. The formula looks daunting, but bear with me for a second; it’s quite simple. Let’s try and get some intuitions about the loss. We know the median is the 0.5 quantile, and that is a measure of central tendency. But if we want our predictions to approximate the 75<sup class="superscript">th</sup> percentile or 0.75 quantile, then we would have to urge the model to overestimate, right? And if we want the model to overestimate, we need to penalize the model more if it’s underestimating. And vice versa, if we want to predict the 0.25 quantile, we need to underestimate. Quantile loss does exactly that:</p>
    <ul>
      <li class="bulletList">For <img src="../Images/B22389_17_068.png" alt=""/> (under estimation), the loss is <img src="../Images/B22389_17_069.png" alt=""/></li>
      <li class="bulletList">For <img src="../Images/B22389_17_070.png" alt=""/> (over estimation), the loss is <img src="../Images/B22389_17_071.png" alt=""/></li>
    </ul>
    <p class="normal">The asymmetry<a id="_idIndexMarker1429"/> is derived from the term <em class="italic">q</em> or 1 - <em class="italic">q</em>. The other term is just the difference between the actual and predicted values.</p>
    <p class="normal">Let’s try to understand this with an example. Suppose we have the true value <img src="../Images/B22389_17_072.png" alt=""/>, and we want to estimate 0.75 quantile (<img src="../Images/B22389_17_073.png" alt=""/>).</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Case 1: Overestimation</strong>: <img src="../Images/B22389_17_074.png" alt=""/>. Since <img src="../Images/B22389_17_070.png" alt=""/>, our quantile loss would be:</li>
    </ul>
    <p class="center"><img src="../Images/B22389_17_076.png" alt=""/></p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Case 2: Underestimation</strong>: <img src="../Images/B22389_17_077.png" alt=""/>. Since <img src="../Images/B22389_17_078.png" alt=""/>, our quantile loss would be:</li>
    </ul>
    <p class="center"><img src="../Images/B22389_17_079.png" alt=""/></p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">To follow along with the complete code, use the notebook named <code class="inlineCode">03-Understanding_Quantile_Loss.ipynb</code> in the <code class="inlineCode">Chapter17</code> folder.</p>
    </div>
    <p class="normal">Therefore, by varying the value of <em class="italic">q</em>, we can make the loss more or less asymmetric and toward either side. <em class="italic">Figure 17.7</em> below shows the loss curves for <em class="italic">q</em> = 0.5 and <em class="italic">q</em> = 0.75 with these example predictions marked on it.</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.7: Quantile loss curves for q=0.5 and q=0.75</p>
    <p class="normal">We can see <a id="_idIndexMarker1430"/>that the quantile loss for <em class="italic">q</em> = 0.5 is symmetric as it represents the median, whereas the loss curve for <em class="italic">q</em> = 0.75 is asymmetric, penalizing underestimation a lot more than overestimation.</p>
    <p class="normal">Although the formula has a branching structure, a maximum operation can be easily avoided when implementing it in code. The quantile loss in Python looks like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">quantile_loss</span>(<span class="hljs-params">q, y, y_hat_q</span>):
    <span class="hljs-string">""" Calculate the quantile loss for a given quantile.</span>
<span class="hljs-string">    Args:</span>
<span class="hljs-string">    q (float): The quantile to be evaluated, e.g., 0.5 for median.</span>
<span class="hljs-string">    y (float): The target value.</span>
<span class="hljs-string">    y_hat_q (float): The quantile forecast.</span>
<span class="hljs-string">    """</span>
    error = y - y_hat_q
    <span class="hljs-keyword">return</span> np.maximum(q * error, (q - <span class="hljs-number">1</span>) * error)
</code></pre>
    <p class="normal">Now, let’s get into the practical side of things and learn how to use quantile loss with the different techniques (both machine learning and deep learning) we have covered in this book.</p>
    <h3 id="_idParaDest-424" class="heading-3">Forecasting with quantile loss (machine learning)</h3>
    <p class="normal">Regular<a id="_idIndexMarker1431"/> machine learning models are typically capable of modeling one output. Therefore, we will need to train a different model for each of the quantiles we are interested in using quantile loss. So, if we want to predict the 0.5, 0.05, and 0.95 quantiles for a problem, we will have to train three separate models, one for each quantile.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">To follow along with the complete code, use the notebook named <code class="inlineCode">04-LightGBM_Prediction_Interval_Quantile_Loss.ipynb </code>in the <code class="inlineCode">Chapter17 </code>folder.</p>
    </div>
    <p class="normal">Let’s see how we can do that. Just like in the PDF section, we are using <code class="inlineCode">mlforecast</code> to quickly whip up a synthetic problem and create some features. For the detailed code, refer to the full notebook, but for now, let’s assume that we have a dataframe <code class="inlineCode">data</code>, which has all the features necessary to run a machine learning model. We have split it into <code class="inlineCode">train</code> and <code class="inlineCode">val</code> and then subsequently into <code class="inlineCode">X_train</code>, <code class="inlineCode">y_train</code>, <code class="inlineCode">X_val</code>, and <code class="inlineCode">y_val</code>.</p>
    <p class="normal">The first step is to import <a id="_idIndexMarker1432"/>the <strong class="keyWord">LGBMRegressor</strong> and set some parameters and quantiles we want to train.</p>
    <pre class="programlisting code"><code class="hljs-code">params = {
    <span class="hljs-string">'objective'</span>: <span class="hljs-string">'quantile'</span>,
    <span class="hljs-string">'metric'</span>: <span class="hljs-string">'quantile'</span>,
    <span class="hljs-string">'max_depth'</span>: <span class="hljs-number">4</span>,
    <span class="hljs-string">'num_leaves'</span>: <span class="hljs-number">15</span>,
    <span class="hljs-string">'learning_rate'</span>: <span class="hljs-number">0.1</span>,
    <span class="hljs-string">'n_estimators'</span>: <span class="hljs-number">100</span>,
    <span class="hljs-string">'boosting_type'</span>: <span class="hljs-string">'gbdt'</span>
}
<span class="hljs-comment"># converting levels to quantiles</span>
<span class="hljs-comment"># For 90% Confidence - 0.05 for lower, 0.5 for median, and 0.95 for upper</span>
quantiles = [<span class="hljs-number">0.5</span>] + <span class="hljs-built_in">sum</span>([level_to_quantiles(l) <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> levels], [])
</code></pre>
    <p class="normal">The key parameter to note here is that the <code class="inlineCode">objective</code> is set as <code class="inlineCode">quantile</code> and the <code class="inlineCode">metric</code> is also set as <code class="inlineCode">quantile</code>. The rest of the LightGBM parameters can be tuned and tweaked for each use case. Now, let’s train all the quantile models.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Training a model each for the quantiles</span>
quantile_models = {}
<span class="hljs-keyword">for</span> q <span class="hljs-keyword">in</span> quantiles:
    model = LGBMRegressor(alpha=q, **params)
    model = model.fit(X_train, Y_train)
    quantile_models[q] = model
</code></pre>
    <p class="normal">Now that the <a id="_idIndexMarker1433"/>model is trained, we can get the point prediction and prediction intervals from the quantile models.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Point Forecast using the 0.5 quantile model</span>
y_pred = quantile_models[<span class="hljs-number">0.5</span>].predict(X_val)
<span class="hljs-comment"># Prediction Intervals using the 0.1 and 0.9 quantile models</span>
y_pred_lower = quantile_models[<span class="hljs-number">0.1</span>].predict(X_val)
y_pred_upper = quantile_models[<span class="hljs-number">0.9</span>].predict(X_val)
</code></pre>
    <p class="normal">Now, let’s see what the forecast and metrics look like.</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_08_01.png" alt=""/></figure>
    <p class="packt_figref"><img src="../Images/B22389_17_08_02.png" alt=""/></p>
    <p class="packt_figref">Figure 17.8: Forecast with prediction intervals using Quantile Regression with LightGBM</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.9: Metrics for Quantile Regression with LightGBM</p>
    <p class="normal">The <a id="_idIndexMarker1434"/>disadvantage of this is that we are training a model for each quantile. This can become unwieldy quickly. Instead of training one model, training three models would make the total training time increase. Another issue is that since all three models are trained differently, they might have different properties, and the way they have learned to solve the problem can also be very different. And because of this inconsistency, the prediction intervals can also suffer from some issues. We can see it clearly in the jagged prediction intervals in many of the time series in <em class="italic">Figure 17.7</em>; they seem disconnected from the median prediction. This is a problem that we don’t have in the deep learning world.</p>
    <h3 id="_idParaDest-425" class="heading-3">Forecasting with quantile loss (deep learning)</h3>
    <p class="normal">In the <a id="_idIndexMarker1435"/>deep learning models, we use a common learning structure and just use different linear projections on the shared projection for the different quantiles. This ensures that the underlying representation and learning are common across all the quantiles and can cause a more coherent set of quantile predictions. So, for all the deep learning models we have learned about in the book, we can make them into a Quantile Forecast model by doing two things:</p>
    <ol>
      <li class="numberedList" value="1">Instead of predicting point forecast (single number), we predict parameters of a probability distribution (one or more numbers).</li>
      <li class="numberedList">Instead of using a point loss like Mean Squared Error, use a probabilistic scoring function like log likelihood.</li>
    </ol>
    <p class="normal">And just like we did in the PDF section, all we need to do is to switch out the loss function in <code class="inlineCode">neuralforecast</code>.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">To follow along with the complete code, use the notebook named <code class="inlineCode">05-NeuralForecast_prediction_intervals_Quantile_Loss.ipynb</code> in the <code class="inlineCode">Chapter17</code> folder.</p>
    </div>
    <p class="normal">Let’s see <a id="_idIndexMarker1436"/>how we can do this in <code class="inlineCode">neuralforecast</code> (the library we were using <em class="italic">Chapter 16</em>). Just like before, we will take a simple model like an LSTM and the M4 competition dataset, but we can do the same with any model or any dataset because all we are doing is switching the loss function to <code class="inlineCode">MQLoss</code> (multi-quantile loss).</p>
    <p class="normal">Let’s start from the point where we have the data formatted the way <code class="inlineCode">neuralforecast</code> expects in a <code class="inlineCode">Y_train_df</code> and <code class="inlineCode">Y_test_df</code>. The first thing we need to do is import the necessary classes.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> neuralforecast <span class="hljs-keyword">import</span> NeuralForecast
<span class="hljs-keyword">from</span> neuralforecast.models <span class="hljs-keyword">import</span> LSTM
<span class="hljs-keyword">from</span> neuralforecast.losses.pytorch <span class="hljs-keyword">import</span> MQLoss
</code></pre>
    <p class="normal">The only class that we haven’t looked at before is <code class="inlineCode">MQLoss</code>. This class just calculates the quantile loss we discussed just now for multiple quantiles (which is how you would want to typically train the model). These are the major parameters of <code class="inlineCode">MQLoss</code>:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">level</code>: This is a list of floats that defines the different confidence levels we are interested in modeling. For instance, if we want to model 80% and 90% confidence, we should give the values as <code class="inlineCode">[80,90]</code>.</li>
      <li class="bulletList"><code class="inlineCode">quantiles</code>: This is an alternate way of defining the level. Instead of 95% confidence, you can define them in quantiles → <code class="inlineCode">[0.1, 0.9]</code>.</li>
    </ul>
    <p class="normal">Now, let’s set a horizon, the levels we need, and a few hyperparameters for LSTM.</p>
    <pre class="programlisting code"><code class="hljs-code">horizon = <span class="hljs-number">48</span>
levels = [<span class="hljs-number">80</span>, <span class="hljs-number">90</span>]
lstm_config = <span class="hljs-built_in">dict</span>(input_size=<span class="hljs-number">3</span>*horizon)
</code></pre>
    <p class="normal">Now, we need to define the models we are going to use and the <code class="inlineCode">NeuralForecast</code> class. Let’s define just one model.</p>
    <pre class="programlisting code"><code class="hljs-code">models = [LSTM(h=horizon, loss=MQLoss(level=levels), **lstm_config)]
<span class="hljs-comment"># Setting freq=1 because the ds column is not date, but instead a sequentially increasing number</span>
nf = NeuralForecast(models=models, freq=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Notice <a id="_idIndexMarker1437"/>how the syntax is exactly the same as the point forecast, except for the multi-quantile loss we chose. Now, all that’s left is to train the models.</p>
    <pre class="programlisting code"><code class="hljs-code">nf.fit(df=Y_train_df)
</code></pre>
    <p class="normal">Once the model is trained, we can predict using the <code class="inlineCode">predict</code> method. This output will have the point forecast under the alias we have defined and the high and low intervals for all the levels we have defined.</p>
    <pre class="programlisting code"><code class="hljs-code">Y_hat_df = nf.predict()
</code></pre>
    <p class="normal">Now, we have a way of getting prediction intervals without making an assumption about the output distribution and that is valuable in real-world cases where we are not sure what the underlying output distribution would be. Let’s look at the generated probabilistic forecasts and its metrics.</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_10_01.png" alt=""/></figure>
    <p class="packt_figref"><img src="../Images/B22389_17_10_02.png" alt=""/></p>
    <p class="packt_figref">Figure 17.10: Forecast with prediction intervals using Quantile Regression (deep learning)</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.11: Metrics for the Quantile Regression (deep learning)</p>
    <p class="normal">We can see<a id="_idIndexMarker1438"/> that the prediction intervals are quite in sync with each other and the median prediction and not disconnected like separate models with LightGBM. This is because the same learning is happening for all the quantiles, just the final projection heads are different. The coverage, in general, is also better.</p>
    <p class="normal">There is another way to get prediction intervals that are dead simple, but not that easy to implement because of the way PyTorch is typically used. That’s what we are going to see next.</p>
    <h2 id="_idParaDest-426" class="heading-2">Monte Carlo Dropout</h2>
    <p class="normal">Dropout is <a id="_idIndexMarker1439"/>a hugely popular regularization<a id="_idIndexMarker1440"/> layer in deep learning. Without going into details, dropout regularization is when we randomly make some part of the weights of the network while training (dropout is turned off during inference). Intuitively, this forces the model to not rely on a few weights but rather to distribute the relevance of the weights across the network. From another perspective, we are applying a sort of regularization (very similar to Ridge regularization), which makes sure none of the weights are single-handedly too high to influence the output drastically.</p>
    <p class="normal">Technically, besides making a random part of the weights zero, we also debias each layer by normalizing by the fraction of nodes/weights that were retained (not zeroed out). Let’s formalize this layer now. If the probability of dropout is <img src="../Images/B22389_16_143.png" alt=""/>, and is applied on an intermediate activation, <em class="italic">h</em> then the activation after dropout, <img src="../Images/B22389_17_081.png" alt=""/> will be:</p>
    <p class="center"><img src="../Images/B22389_17_082.png" alt=""/></p>
    <div class="note">
      <p class="normal">Why do we need to scale/normalize the output when dropout is applied? The intuitive answer is to make sure the output has the same scale during training (when dropout is active) and during inference (when dropout is turned off). The long answer is as follows.</p>
      <p class="normal">Let’s say without dropout, the output of a node is <em class="italic">h</em>. Now, with dropout, with a probability of <img src="../Images/B22389_17_083.png" alt=""/>, this output becomes 0 and with a probability of <img src="../Images/B22389_17_084.png" alt=""/>, it is <em class="italic">h</em>. Therefore, the expected value of the node would be: <img src="../Images/B22389_17_085.png" alt=""/>. This means the average value of the output is reduced by a factor of <img src="../Images/B22389_17_084.png" alt=""/>, which is not desirable as this would change the scale of the values during training and inference. Therefore, the solution is to scale the output of the nodes that are retained by dropout by <img src="../Images/B22389_17_084.png" alt=""/>.</p>
    </div>
    <p class="normal">Now, we <a id="_idIndexMarker1441"/>know what dropout is. But remember that we were using dropout only during training as a regularization. In 2015, Yarin Gal et al. proposed that the good old dropout also doubles as a <em class="italic">Bayesian Approximation of Gaussian Processes</em>. That’s a lot of terms that we haven’t come across in a single phrase. Let’s take a short detour to understand these terms at a high level, and I’ll include other links in <em class="italic">Further reading</em>.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The paper by Yarin Gal et al. about Monte Carlo Dropout is cited in <em class="italic">References</em> under reference <em class="italic">2</em>.</p>
    </div>
    <p class="normal"><em class="italic">Bayesian inference</em> is a statistical method that updates the probability of a hypothesis as more evidence or information becomes available. It is based on Bayes’ theorem, which mathematically expresses the relationship between the prior probability, the likelihood, and the posterior probability. Formally, Bayes’ theorem is given by:</p>
    <p class="center"><img src="../Images/B22389_17_088.png" alt=""/></p>
    <p class="normal">where <img src="../Images/B22389_17_089.png" alt=""/> is the <em class="italic">posterior probability</em> of hypothesis <img src="../Images/B22389_04_016.png" alt=""/> given the data <em class="italic">D</em>, <img src="../Images/B22389_17_091.png" alt=""/> is the <em class="italic">likelihood</em> (probability of observing the data <em class="italic">D</em> given hypothesis <img src="../Images/B22389_04_016.png" alt=""/>), <img src="../Images/B22389_17_093.png" alt=""/> is the <em class="italic">prior probability</em> of the hypothesis <img src="../Images/B22389_04_016.png" alt=""/> before observing the data, and <img src="../Images/B22389_17_095.png" alt=""/> is the marginal likelihood or <em class="italic">evidence</em> (the total probability of observing the data under all possible hypotheses).</p>
    <p class="normal">Although it has some specific terminology, this is very intuitive and provides a structured way to update our prior beliefs in the face of evidence or data. We start with a prior distribution <img src="../Images/B22389_17_096.png" alt=""/> that represents our initial beliefs about the parameters. As we observe data <em class="italic">D</em>, we update our beliefs getting the posterior distribution <img src="../Images/B22389_17_089.png" alt=""/>. This posterior distribution combines the prior information and the likelihood of the observed data, providing a new, updated belief about the parameters. <em class="italic">Further reading</em> has a more detailed<a id="_idIndexMarker1442"/> explanation for those who are interested. It also has a page from <em class="italic">Seeing Theory</em> that helps you visualize these in an intuitive way.</p>
    <p class="normal">Now, let’s move on to <a id="_idIndexMarker1443"/>the <strong class="keyWord">Gaussian Process</strong> (<strong class="keyWord">GP</strong>). As we have seen in <em class="chapterRef">Chapter 5</em>, supervised learning is learning a function <img src="../Images/B22389_17_098.png" alt=""/> where <img src="../Images/B22389_05_001.png" alt=""/> is the quantity we are interested in predicting, <em class="italic">h</em>, is the function we learn, <em class="italic">X</em> is the input data, and <img src="../Images/B22389_07_003.png" alt=""/> represents the model parameters. So, GPs assume this function as a probability distribution and use Bayesian inference to update the posterior of the function using the data we have available for training. It’s a drastically different way of learning from data and is inherently probabilistic.</p>
    <p class="normal">There is one more term left, which is approximation. In many cases, the complex posterior distributions in Bayesian models make them intractable. So, we have a technique called <em class="italic">Variational Inference</em> in which we use a known parametric distribution family, <em class="italic">q</em>, and find a member that is closest to the true posterior. Getting into details of Variational Inference and GPs is out of the scope of the book, but I have added a few links in <em class="italic">Further reading</em> for those of you who are interested.</p>
    <p class="normal">So, coming back to dropouts, Yarin Gal et al. showed that a neural network defined with dropout layers before each of the weight layers is in fact a Bayesian approximation of a GP. So, if the model with dropout is a GP and GP is a posterior over functions, this should give us a probabilistic output, right? But here, we don’t have a well-defined parametric probability distribution like the Normal distribution for us to analytically calculate the properties (like the mean, standard deviation, or quantiles) of the distribution. How do we do that?</p>
    <p class="normal">Remember the discussion at the start of the Quantile Function section where we said that if we have <em class="italic">N</em> samples drawn from a distribution and if that <em class="italic">N</em> is sufficiently large, we can approximate the properties of the distribution? We have a name for that, and it’s called <em class="italic">Monte Carlo sampling</em>. <em class="italic">Monte Carlo sampling</em> is a computational technique used to estimate the statistical properties of a distribution by generating many random samples from that distribution. Bringing this idea to the dropout-enabled neural network, we can assess the properties of the posterior probability distribution of the function using Monte Carlo sampling, which means we need to keep dropout turned on during inference and sample from the posterior by executing the forward pass <em class="italic">N</em> times.</p>
    <p class="normal">Theoretical justification allows us to apply dropout to any neural network, getting uncertainty estimates through a simple operation. Isn’t the simplicity of that beautiful?</p>
    <p class="normal">So, all of that <a id="_idIndexMarker1444"/>boils down to these simple steps to get prediction intervals for our forecasts:</p>
    <ol>
      <li class="numberedList" value="1">Pick any deep learning architecture.</li>
      <li class="numberedList">Insert Dropout Layers before every major operation and set them to a value <img src="../Images/B22389_16_143.png" alt=""/>, where <img src="../Images/B22389_17_102.png" alt=""/>.</li>
      <li class="numberedList">After training the model, with dropout enabled, do <em class="italic">N</em> forward passes.</li>
      <li class="numberedList">Using the <em class="italic">N</em> samples, estimate the median (for the point prediction), and the quantiles corresponding to the defined confidence levels for prediction intervals.</li>
    </ol>
    <p class="normal">This sounds simple enough, but it’s complicated for just one reason. Both <code class="inlineCode">PyTorch</code> and <code class="inlineCode">Tensorflow</code> are designed in a way that dropouts are turned off during inference. In <code class="inlineCode">PyTorch</code>, we can indicate if the model is in the training phase or inference phase by doing <code class="inlineCode">model.train()</code> or <code class="inlineCode">model.eval()</code>, respectively. And most popular implementations that wrap <code class="inlineCode">PyTorch</code> to make training easy and automated (like <code class="inlineCode">PyTorch Lightning</code>) do this <code class="inlineCode">model.eval()</code> step in the backend before predicting. So, when using libraries like <code class="inlineCode">neuralforecast</code> (which uses <code class="inlineCode">PyTorch Lightning</code> in the background), turning on dropout during prediction isn’t easy.</p>
    <p class="normal">Before we learn how to implement MC Dropout for neuralforecast models, let’s take a slight detour and learn how to define custom models in neuralforecast. It is useful when you want to tweak any model for your use case. And we are doing it here because of two reasons:</p>
    <ol>
      <li class="numberedList" value="1">I wanted to show you how to define a new model in neuralforecast.</li>
      <li class="numberedList">I wanted a model that is ideal for the MC Dropout technique, i.e., the model needs to have dropouts before every weight/layer.</li>
    </ol>
    <h3 id="_idParaDest-427" class="heading-3">Creating a custom model in neuralforecast</h3>
    <p class="normal">Now it’s <a id="_idIndexMarker1445"/>time to have some fun and define a custom <code class="inlineCode">PyTorch</code> model that works with <code class="inlineCode">neuralforecast</code>. To keep things simple, let’s use a model that is a small tweak on <strong class="keyWord">D-Linear</strong> we learned about in <em class="italic">Chapter 16</em>. Besides the linear trend and seasonality, we also add a component for <a id="_idIndexMarker1446"/>non-linear trend. Let’s give it a wacky name as well—<strong class="keyWord">D-NonLinear</strong>. The architecture <a id="_idIndexMarker1447"/>would be something like this:</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.12: D-NonLinear model architecture</p>
    <p class="normal">Now, let’s understand how to write a model that would work with <code class="inlineCode">neuralforecast</code>. All models in <code class="inlineCode">neuralforecast</code> are inherited from one of three classes—<code class="inlineCode">BaseWindows</code>, <code class="inlineCode">BaseRecurrent</code>, or <code class="inlineCode">BaseMultivariate</code>. The documentation clearly explains the purpose of <code class="inlineCode">BaseWindows</code>, which <a id="_idIndexMarker1448"/>is exactly what we require for our use case. We need to sample windows from a time series while training. </p>
    <p class="normal">One more thing we need to keep in mind is that <code class="inlineCode">neuralforecast</code> uses <code class="inlineCode">PyTorch Lightning</code> under the hood for training. This link has more details on how to define a new model for <code class="inlineCode">neuralforecast</code>: <a href="https://nixtlaverse.nixtla.io/neuralforecast/docs/tutorials/adding_models.html"><span class="url">https://nixtlaverse.nixtla.io/neuralforecast/docs/tutorials/adding_models.html</span></a>.</p>
    <div class="note">
      <p class="normal">If you aren’t aware of <strong class="keyWord">Object-Oriented Programming</strong> (<strong class="keyWord">OOP</strong>) and <strong class="keyWord">inheritance</strong>, then it<a id="_idIndexMarker1449"/> might be difficult for you to understand what we are doing here. Inheritance allows a child class to inherit all the attributes and methods defined in a parent class. This allows developers to define common functionalities in a base class and then inherit that class to get all the functionality and then add on top of that any specific functionality you want to add to a class. It is highly recommended that you understand inheritance, not only for this example but also to become a better developer in general. There are hundreds of tutorials on the internet, and I’m linking one here: <a href="https://ioflood.com/blog/python-inheritance"><span class="url">https://ioflood.com/blog/python-inheritance</span></a>.</p>
    </div>
    <p class="normal">The full code for the model can be found in <code class="inlineCode">src/dl/nf_models.py</code>, but we will look at key portions of the model definition right here.</p>
    <p class="normal">Let’s start by defining the <code class="inlineCode">__init__</code> function (only including relevant portions here; refer to the Python file for the full class definition).</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">DNonLinear</span>(<span class="hljs-title">BaseWindows</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(
<span class="hljs-params">        self,</span>
<span class="hljs-params">        </span><span class="hljs-comment"># Inherited hyperparameters with no defaults</span>
<span class="hljs-params">        h,</span>
<span class="hljs-params">        input_size,</span>
<span class="hljs-params">        </span><span class="hljs-comment"># Model specific hyperparameters</span>
<span class="hljs-params">        </span><span class="hljs-comment"># Window over which the moving average operates for trend extraction</span>
<span class="hljs-params">        moving_avg_window=</span><span class="hljs-number">3</span><span class="hljs-params">,</span>
<span class="hljs-params">        dropout=</span><span class="hljs-number">0.1</span><span class="hljs-params">,</span>
<span class="hljs-params">        </span><span class="hljs-comment"># Inhereted hyperparameters with defaults</span>
<span class="hljs-params">        ...</span>
<span class="hljs-params">        **trainer_kwargs,</span>
<span class="hljs-params">    </span>):
        <span class="hljs-built_in">super</span>(DropoutDNonLinear, <span class="hljs-variable">self</span>).__init__(
            h=h,
            ...
            **trainer_kwargs,
        )
        <span class="hljs-comment"># Model specific hyperparameters</span>
        <span class="hljs-variable">self</span>.moving_avg_window = moving_avg_window
        <span class="hljs-variable">self</span>.dropout = dropout
        <span class="hljs-comment"># Model initialization to follow</span>
</code></pre>
    <p class="normal">We have<a id="_idIndexMarker1450"/> now defined part of the <code class="inlineCode">__init__</code> function. Now, let’s initialize the different layers needed in the rest of the method. We have a series decomposition layer that uses a moving average to split the input into a trend and seasonal component, a linear trend predictor and seasonality predictor that takes the linear trend and seasonality and projects it into the future, and a non-linear predictor that takes in the original input and projects into the future.</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-comment"># Defining a decomposition Layer</span>
        <span class="hljs-variable">self</span>.decomp = SeriesDecomp(<span class="hljs-variable">self</span>.moving_avg_window)
        <span class="hljs-comment"># Defining a non-linear trend predictor with dropout</span>
        <span class="hljs-variable">self</span>.non_linear_block = nn.Sequential(
            nn.Dropout(<span class="hljs-variable">self</span>.dropout),
            nn.Linear(<span class="hljs-variable">self</span>.input_size, <span class="hljs-number">100</span>),
            nn.ReLU(),
            nn.Dropout(<span class="hljs-variable">self</span>.dropout),
            nn.Linear(<span class="hljs-number">100</span>, <span class="hljs-number">100</span>),
            nn.ReLU(),
            nn.Dropout(<span class="hljs-variable">self</span>.dropout),
            nn.Linear(<span class="hljs-number">100</span>, <span class="hljs-variable">self</span>.h),
        )
        <span class="hljs-comment"># Defining a linear trend predictor with dropout</span>
        <span class="hljs-variable">self</span>.linear_trend = nn.Sequential(
            nn.Dropout(<span class="hljs-variable">self</span>.dropout),
            nn.Linear(<span class="hljs-variable">self</span>.input_size, <span class="hljs-variable">self</span>.h),
        )
        <span class="hljs-comment"># Defining a seasonality predictor with dropout</span>
        <span class="hljs-variable">self</span>.seasonality = nn.Sequential(
            nn.Dropout(<span class="hljs-variable">self</span>.dropout),
            nn.Linear(<span class="hljs-variable">self</span>.input_size, <span class="hljs-variable">self</span>.h),
        )
</code></pre>
    <p class="normal">Now, let’s define the <code class="inlineCode">forward</code> method. The <code class="inlineCode">forward</code> method should have just one argument that is a dictionary of different inputs:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">insample_y</code>: The context window of the target time series we have to predict</li>
      <li class="bulletList"><code class="inlineCode">futr_exog</code>: The exogenous variables for the future</li>
      <li class="bulletList"><code class="inlineCode">hist_exog</code>: The exogenous variables for the context window</li>
      <li class="bulletList"><code class="inlineCode">stat_exog</code>: The static variables</li>
    </ul>
    <p class="normal">For this <a id="_idIndexMarker1451"/>use case, we only need the <code class="inlineCode">insample_y</code> since our model doesn’t use any other information. So, this is the <code class="inlineCode">forward</code> method implementation:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, windows_batch</span>):
        <span class="hljs-comment"># Parse windows_batch</span>
        insample_y = windows_batch[
            <span class="hljs-string">"insample_y"</span>
        ].clone()  <span class="hljs-comment"># --&gt; (batch_size, input_size)</span>
        seasonal_init, trend_init = <span class="hljs-variable">self</span>.decomp(
            insample_y
        )  <span class="hljs-comment"># --&gt; (batch_size, input_size)</span>
        <span class="hljs-comment"># Non-linear block</span>
        non_linear_part = <span class="hljs-variable">self</span>.non_linear_block(
            insample_y
        )  <span class="hljs-comment"># --&gt; (batch_size, horizon)</span>
        <span class="hljs-comment"># Linear trend block</span>
        trend_part = <span class="hljs-variable">self</span>.linear_trend(trend_init)  <span class="hljs-comment"># --&gt; (batch_size, horizon)</span>
        <span class="hljs-comment"># Seasonality block</span>
        seasonal_part = <span class="hljs-variable">self</span>.seasonality(
            seasonal_init
        )  <span class="hljs-comment"># --&gt; (batch_size, horizon)</span>
        <span class="hljs-comment"># Combine the components</span>
        forecast = (
            trend_part + seasonal_part + non_linear_part
        )  <span class="hljs-comment"># --&gt; (batch_size, horizon)</span>
        <span class="hljs-comment"># Map the forecast to the domain of the target</span>
        forecast = <span class="hljs-variable">self</span>.loss.domain_map(forecast)
        <span class="hljs-keyword">return</span> forecast
</code></pre>
    <p class="normal">The code is pretty straightforward. The only thing we need to ensure to align to <code class="inlineCode">neuralforecast</code> models is to <a id="_idIndexMarker1452"/>take the data we need from the input dictionary and call <code class="inlineCode">self.loss.domain_map</code> at the end so that it is mapped to the right output size depending on the loss. Now, this model will function just like any other model in the neuralforecast library.</p>
    <p class="normal">Now, let’s get back to MC Dropout and its implementation.</p>
    <h3 id="_idParaDest-428" class="heading-3">Forecasting with MC Dropout (neuralforecast)</h3>
    <p class="normal">We said it <a id="_idIndexMarker1453"/>wasn’t easy to implement MC Dropout in frameworks like <code class="inlineCode">neuralforecast</code> and <code class="inlineCode">PyTorch Lightning</code>, but just because something isn’t easy shouldn’t stop us from doing it. All we need to do is to make sure the dropouts are enabled during prediction and take multiple samples. If you are writing your own <code class="inlineCode">PyTorch</code> training code, then it’s as simple as not calling <code class="inlineCode">model.eval()</code> before predicting. But the best practice is to just make the dropouts into train mode and not the whole model. There may be layers like batch normalization, which also behave differently during inference, which might be affected. Let’s see a handy method that makes all dropout layers into train mode.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">enable_dropout</span>(<span class="hljs-params">model</span>):
    <span class="hljs-string">"""Function to enable the dropout layers during test-time"""</span>
    <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> model.modules():
        <span class="hljs-keyword">if</span> m.__class__.__name__.startswith(<span class="hljs-string">"Dropout"</span>):
            m.train()
</code></pre>
    <p class="normal">For <code class="inlineCode">neuralforecast</code>, we have put together a recipe with which we can use MC Dropout for any of their models (provided they have enough dropouts). Now, we are going to use the custom <code class="inlineCode">DNonLinear</code> model we just defined. Note that each component in the definition starts with a dropout layer so that we can apply MC Dropout with no qualms.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">To follow along with the complete code, use the notebook named <code class="inlineCode">06-Prediction_Intervals_MCDropout.ipynb</code> in the <code class="inlineCode">Chapter17</code> folder.</p>
    </div>
    <p class="normal">If you remember, we used <code class="inlineCode">PyTorch Lightning</code> back in <em class="chapterRef">Chapter 13</em> and explained that it’s pretty much standard <code class="inlineCode">PyTorch</code> code, but organized in a specified form—<code class="inlineCode">training_step</code>, <code class="inlineCode">validation_step</code>, <code class="inlineCode">predict_step</code>, <code class="inlineCode">configure_optimizers</code>, etc. For a refresher, head back to <em class="chapterRef">Chapter 13</em> and <em class="italic">Further reading</em> in the chapter to learn more about how to migrate from <code class="inlineCode">PyTorch</code> to <code class="inlineCode">PyTorch Lightning</code>. Since neuralforecast is already using <code class="inlineCode">PyTorch Lightning</code> in the backend, the <code class="inlineCode">BaseWindows</code> that we are inheriting is already a <code class="inlineCode">PyTorch Lightning</code> model. This information is essential because we need to essentially modify the <code class="inlineCode">predict_step</code> method to implement our MC Dropout.</p>
    <p class="normal">Using the <a id="_idIndexMarker1454"/>same inheritance we used to inherit <code class="inlineCode">BaseWindows</code>, we can inherit the <code class="inlineCode">DNonLinear</code> class we defined earlier and make a few changes so that it becomes an MC Dropout model. And for that, all we need to re-define is the <code class="inlineCode">predict_step</code> method. The <code class="inlineCode">predict_step</code> method is the method <code class="inlineCode">PyTorch Lightning</code> calls every time it has to get a prediction for a batch. So, instead of taking the predictions as is, we need to keep the dropout enabled, take <em class="italic">N</em> samples from <em class="italic">N</em> forward passes, calculate the prediction intervals and median (point forecast), and return it.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">MCDropoutDNonLinear</span>(<span class="hljs-title">DNonLinear</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title">predict_step</span>(<span class="hljs-params">self, batch, batch_idx</span>):
        enable_dropout(<span class="hljs-variable">self</span>)
        pred_samples = []
        <span class="hljs-comment"># num_samples and levels will be saved to the model in MCNeuralForecast predict method</span>
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable">self</span>.num_samples):
            y_hat = <span class="hljs-built_in">super</span>().predict_step(batch, batch_idx)
            pred_samples.append(y_hat)
        <span class="hljs-comment"># Stack the samples</span>
        pred_samples = torch.stack(pred_samples, dim=<span class="hljs-number">0</span>)
        <span class="hljs-comment"># Calculate the median and the quantiles</span>
        y_hat = [pred_samples.quantile(<span class="hljs-number">0.5</span>, dim=<span class="hljs-number">0</span>)]
        <span class="hljs-keyword">if</span> <span class="hljs-variable">self</span>.levels <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
            <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-variable">self</span>.levels:
                lo, hi = level_to_quantiles(l)
                y_hat_lo = pred_samples.quantile(lo, dim=<span class="hljs-number">0</span>)
                y_hat_hi = pred_samples.quantile(hi, dim=<span class="hljs-number">0</span>)
                y_hat.extend([y_hat_lo, y_hat_hi])
        <span class="hljs-comment"># Stack the results</span>
        y_hat = torch.stack(y_hat, dim=-<span class="hljs-number">1</span>)
        <span class="hljs-keyword">return</span> y_hat
</code></pre>
    <p class="normal">Are we done yet? Not quite. There is just one little thing to be done. In <em class="chapterRef">Chapter 16</em>, we used a class called <code class="inlineCode">NeuralForecast</code> for all the fitting and predicting of <code class="inlineCode">neuralforecast</code> models. This is like a wrapper class that does the heavy lifting of preparing the inputs and outputs in the right way before calling the underlying models. This class has to be aware that we have tweaked the <code class="inlineCode">predict_step</code> and therefore, we need to make a small change there. The solution is more of a hack than a principled way of editing, but a hack is just as good if it achieves the purpose. I have done the snooping around the implementation to figure out the best way to hack <code class="inlineCode">NeuralForecast</code> to enable our MCDropout inference. There is no short way of explaining the hack, but just understand that I have misused the way <code class="inlineCode">neuralforecast</code> flexibly produces point forecasts and prediction intervals based <a id="_idIndexMarker1455"/>on different losses. So, here is the re-defined <code class="inlineCode">NeuralForecast</code> class with a hack in the <code class="inlineCode">predict</code> method.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">MCNeuralForecast</span>(<span class="hljs-title">NeuralForecast</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, num_samples, levels=</span><span class="hljs-literal">None</span><span class="hljs-params">, **kwargs</span>):
        <span class="hljs-built_in">super</span>().__init__(**kwargs)
        <span class="hljs-variable">self</span>.num_samples = num_samples
        <span class="hljs-variable">self</span>.levels = levels
    <span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>(
<span class="hljs-params">        self,</span>
<span class="hljs-params">        df=</span><span class="hljs-literal">None</span><span class="hljs-params">,</span>
<span class="hljs-params">        static_df=</span><span class="hljs-literal">None</span><span class="hljs-params">,</span>
<span class="hljs-params">        futr_df=</span><span class="hljs-literal">None</span><span class="hljs-params">,</span>
<span class="hljs-params">        sort_df=</span><span class="hljs-literal">True</span><span class="hljs-params">,</span>
<span class="hljs-params">        verbose=</span><span class="hljs-literal">False</span><span class="hljs-params">,</span>
<span class="hljs-params">        engine=</span><span class="hljs-literal">None</span><span class="hljs-params">,</span>
<span class="hljs-params">        **data_kwargs,</span>
<span class="hljs-params">    </span>):
        <span class="hljs-comment"># Adding model columns to loss output names</span>
        <span class="hljs-comment"># Necessary hack to get the quantiles and format it correctly</span>
        <span class="hljs-keyword">for</span> model <span class="hljs-keyword">in</span> <span class="hljs-variable">self</span>.models:
            model.loss.output_names = [<span class="hljs-string">"</span><span class="hljs-string">-median"</span>]
            <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span>(<span class="hljs-variable">self</span>.levels):
                model.loss.output_names.append(<span class="hljs-string">f"-lo-</span><span class="hljs-subst">{l}</span><span class="hljs-string">"</span>)
                model.loss.output_names.append(<span class="hljs-string">f"-hi-</span><span class="hljs-subst">{l}</span><span class="hljs-string">"</span>)
            <span class="hljs-comment"># Setting the number of samples and levels in the model</span>
            model.num_samples = <span class="hljs-variable">self</span>.num_samples
            model.levels = <span class="hljs-variable">self</span>.levels
        <span class="hljs-keyword">return</span> <span class="hljs-built_in">super</span>().predict(
            df, static_df, futr_df, sort_df, verbose, engine, **data_kwargs
        )
</code></pre>
    <p class="normal">That’s it. We have successfully “hacked” the library to do our bidding. In addition to this being an MC Dropout tutorial, it’s also a tutorial on how to hack a library to do what you want it to do. It is important to note that this doesn’t make you a “hacker,” so stop before you update your LinkedIn title.</p>
    <p class="normal">Now, on to <a id="_idIndexMarker1456"/>training the model. This is pretty much the same as you train other models in <code class="inlineCode">neuralforecast</code>, but instead of the <code class="inlineCode">NeuralForecast</code> class, you need to use the new <code class="inlineCode">MCNeuralForecast</code> class we defined.</p>
    <pre class="programlisting code"><code class="hljs-code">horizon = <span class="hljs-built_in">len</span>(Y_test_df.ds.unique()) <span class="hljs-comment"># 48</span>
levels = [<span class="hljs-number">80</span>, <span class="hljs-number">90</span>]
model = MCDropoutDNonLinear (
    h= horizon,
    input_size=WINDOW,
    moving_avg_window=horizon*<span class="hljs-number">3</span>,
    dropout=<span class="hljs-number">0.1</span>,
    max_steps=<span class="hljs-number">500</span>,
    early_stop_patience_steps=<span class="hljs-number">5</span>,
)
mcnf = MCNeuralForecast(models=[model], freq=<span class="hljs-number">1</span>, num_samples=<span class="hljs-number">100</span>, levels=levels)
mcnf.fit(Y_train_df, val_size= horizon, verbose=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">Once the training is finished, we can generate the predictions like this:</p>
    <pre class="programlisting code"><code class="hljs-code">Y_hat_df = mcnf.predict()
Y_hat_df = Y_hat_df.reset_index()
</code></pre>
    <p class="normal">The output is exactly like any other model from <code class="inlineCode">neuralforecast</code> with the prediction intervals formatted as <code class="inlineCode">&lt;ModelName&gt;-lo-&lt;level&gt;</code> and <code class="inlineCode">&lt;ModelName&gt;-hi-&lt;level&gt;</code>. The point forecast can be found under <code class="inlineCode">&lt;ModelName&gt;-median</code>. In this case, <code class="inlineCode">&lt;ModelName&gt;</code> would be <code class="inlineCode">MCDropoutDNonLinear</code>.</p>
    <p class="normal">Let’s look at the plot of the forecasts and the metrics.</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_13_01.png" alt=""/></figure>
    <p class="packt_figref"><img src="../Images/B22389_17_13_02.png" alt=""/></p>
    <p class="packt_figref">Figure 17.13: Forecast with prediction intervals using MC Dropout</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.14: Metrics for MC Dropout</p>
    <p class="normal">Our forecasting <a id="_idIndexMarker1457"/>model does decent enough on the data; it’s nothing to write home about, but decent. If we do an ablation study, we might even realize that the non-linear component we added does absolutely nothing. But, as long as we had fun doing it and learned something from it, I’m happy. Now, look at the prediction intervals. They aren’t really smooth and have quite a bit of “noise” when you look at them, right? This is because of the inherent randomness in the methodology and may be because of insufficient learning. When we do MC Dropout, we are essentially relying on <em class="italic">N</em> sub-models or sub-networks and calculating the quantiles based on these <em class="italic">N</em> forecasts. Maybe a few of these sub-networks haven’t learned very well, and those outputs can skew the quantiles and thereby the prediction intervals.</p>
    <p class="normal">There are <a id="_idIndexMarker1458"/>many criticisms of the MC Dropout method. Many in the Bayesian community don’t consider MC Dropout as Bayesian and consider the Variational Approximation that was proposed such a poor approximation that we can’t refer to what it measures as Bayesian uncertainty. There is an unpublished Arxiv paper by Loic Le Folgoc et al. called “Is MC Dropout Bayesian?” (Reference<em class="italic"> 6</em>), which claims that it isn’t. But it still doesn’t take away the fact that MC Dropout is a cheap way of getting uncertainty quantified. But when used in fields like medical studies, where uncertainty quantification is of paramount importance, we may want to take on something more principled.</p>
    <p class="normal">We can also notice that the coverage is quite bad across all time series. And this is, again, something that is exhibited across different studies. In 2023, Nicolas Dewolf et al. published a study comparing different ways of uncertainty quantification for regression problems (Reference<em class="italic"> 7</em>). They found that MC Dropout has one of the worst performances in both coverage and average length, underlying the claim that MC Dropout is a very dirty approximation of the uncertainty.</p>
    <p class="normal">Now, let’s look at another technique for probabilistic forecasting that promises theoretical guarantees for perfect coverage and has become quite the rage in the last few years.</p>
    <h2 id="_idParaDest-429" class="heading-2">Conformal Prediction</h2>
    <p class="normal">What if I tell you that <a id="_idIndexMarker1459"/>there is a technique<a id="_idIndexMarker1460"/> for generating prediction intervals that statistically guarantees perfect coverage, can work on any model, and doesn’t require us to make any assumptions about the output distribution? Conformal Prediction is just that. Conformal Prediction is a method that helps machine learning models make reliable predictions by estimating how uncertain the model is. Conformal Prediction provides robust, statistically valid measures of uncertainty for any machine learning model, ensuring reliable and trustworthy predictions in critical applications. </p>
    <p class="normal">Although it was proposed as early as 2005 by Vladmir Vovk <em class="italic">(</em>Reference<em class="italic"> 8</em>), it picked up interest in the last couple of years. Let’s first understand the basic principles of Conformal Prediction using a classification example and then see how we can do it for regression and time series examples.</p>
    <h3 id="_idParaDest-430" class="heading-3">Conformal Prediction for classification</h3>
    <p class="normal">Let’s <a id="_idIndexMarker1461"/>start with a trained model, <img src="../Images/B22389_17_103.png" alt=""/>, which outputs estimated probabilities (<em class="italic">softmax</em> scores) for <em class="italic">K</em> output classes (<img src="../Images/B22389_17_104.png" alt=""/>). It doesn’t matter what this model is; it can be a machine learning model, a deep learning model, or even a rule-based model. We have training data, <img src="../Images/B22389_17_105.png" alt=""/>, and test data, <img src="../Images/B22389_17_106.png" alt=""/>. Now, we need a small amount of additional data (other than training and test) called <em class="italic">calibration data</em>, <img src="../Images/B22389_17_107.png" alt=""/>. Now, what do we want from this? Using <img src="../Images/B22389_17_103.png" alt=""/> and <img src="../Images/B22389_17_109.png" alt=""/>, we want to create a prediction set of possible labels, <img src="../Images/B22389_17_110.png" alt=""/> that makes sure that the probability that a test data point is part of that set is almost exactly the user-defined error rate, <img src="../Images/B22389_04_009.png" alt=""/> (we will be talking about error rates throughout this discussion. A 10% error rate is a 90% confidence level). This is exactly what Conformal Prediction guarantees. It’s called the <em class="italic">marginal coverage</em> guarantee and it can be written more formally as:</p>
    <p class="center"><img src="../Images/B22389_17_112.png" alt=""/></p>
    <p class="normal">Naturally, you may have this question in your mind. What is this <img src="../Images/B22389_17_113.png" alt=""/>? This term signifies that the coverage guarantee is derived from a finite sample of size <em class="italic">n</em>. Since <em class="italic">n</em> is in the denominator, we know as and when <em class="italic">n</em> increases, this term becomes smaller and smaller. Extending this to the limit, we know that if <img src="../Images/B22389_17_114.png" alt=""/>, this term would be zero and the coverage would be exactly <img src="../Images/B22389_17_016.png" alt=""/>.</p>
    <p class="normal">We know what we want, but how do we get it? The core idea in conformal prediction is very simple and can be laid out in four steps:</p>
    <ol>
      <li class="numberedList" value="1">Identify a heuristic notion of uncertainty using the trained model. In our classification example, this can be the softmax scores.</li>
      <li class="numberedList">Define a score function, <img src="../Images/B22389_17_116.png" alt=""/>, which is also called <em class="italic">Non-Conformity Scores</em>. This can be a score that takes in the prediction, <img src="../Images/B22389_05_001.png" alt=""/>, and actual value, <img src="../Images/B22389_17_118.png" alt=""/>, and gives a score that encodes the disagreement between them. The higher the score is, the larger the disagreement is. In the classification example, this would be something as simple as <img src="../Images/B22389_17_119.png" alt=""/>. In <a id="_idIndexMarker1462"/>simple English, this means taking the softmax score of the correct class and doing <img src="../Images/B22389_17_120.png" alt=""/>.</li>
      <li class="numberedList">Compute <img src="../Images/B22389_17_121.png" alt=""/> as the <img src="../Images/B22389_17_122.png" alt=""/> quantile of the calibration scores. We use the calibration data and score function to calculate calibration scores and calculate the quantile on that data. The <img src="../Images/B22389_17_123.png" alt=""/> is the quantile calculation is again derived from the finite sample correction. As <img src="../Images/B22389_17_124.png" alt=""/> tends to infinity, the term tends to zero.</li>
      <li class="numberedList">Use this quantile to form prediction sets for new examples: <img src="../Images/B22389_17_125.png" alt=""/>. This means selecting all the items from the output set that has a score (according to the score function) greater than the threshold, <img src="../Images/B22389_17_121.png" alt=""/>.</li>
    </ol>
    <p class="normal">This simple technique will give us prediction sets that guarantee to satisfy the marginal coverage, no matter what model is used or what the distribution of the data is. Let’s see how simple this is using <code class="inlineCode">Python</code> code and assuming that the model we are talking about is a <code class="inlineCode">scikit-learn</code> classifier. </p>
    <p class="normal">We have a trained model, <code class="inlineCode">model</code>, calibration data, <code class="inlineCode">X_calib</code>, and test data, <code class="inlineCode">X_test</code>. For full code and some visualizations, check the notebook.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">To follow along with the complete code, use the notebook named <code class="inlineCode">07-Understanding_Conformal_Prediction.ipynb</code> in the <code class="inlineCode">Chapter17</code> folder.</p>
    </div>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># 1: Get conformal scores</span>
n = calib_y.shape[<span class="hljs-number">0</span>]
cal_smx = model.predict_proba(calib_x) <span class="hljs-comment"># shape (n, n_classes)</span>
<span class="hljs-comment"># scores from the softmax score for the correct class</span>
cal_scores = <span class="hljs-number">1</span> - cal_smx[np.arange(n), calib_y] <span class="hljs-comment"># shape (n,)</span>
<span class="hljs-comment"># 2: Get adjusted quantile</span>
alpha = <span class="hljs-number">0.1</span>  <span class="hljs-comment"># Confidence level (1 - alpha)</span>
q_level = np.ceil((n + <span class="hljs-number">1</span>) * (<span class="hljs-number">1</span> - alpha)) / n
qhat = np.quantile(cal_scores, q_level, method=<span class="hljs-string">'higher'</span>)
<span class="hljs-comment"># 3: Form prediction sets</span>
val_smx = model.predict_proba(test_x)
prediction_sets = val_smx &gt;= (<span class="hljs-number">1</span> - qhat)
</code></pre>
    <p class="normal">Now, let’s <a id="_idIndexMarker1463"/>think about the prediction set, <img src="../Images/B22389_17_127.png" alt=""/>. We have been defining it as set-valued with discrete classes for the classification scenario. This set becomes larger or smaller based on how confident the initial heuristic estimate of uncertainty is.</p>
    <h3 id="_idParaDest-431" class="heading-3">Conformal Prediction for regression</h3>
    <p class="normal">Let’s <a id="_idIndexMarker1464"/>extend this notion of prediction sets to regression. In regression, the output space is continuous rather than discrete, and we aim to construct continuous prediction sets, which are typically a continuous interval in <img src="../Images/B22389_17_128.png" alt=""/>. The idea is to maintain the same principle of coverage: the prediction interval should contain the true value with high probability. So, now the prediction set, <img src="../Images/B22389_17_127.png" alt=""/>, that we saw earlier is also the prediction interval in the regression context. But along with the change in the interpretation of prediction sets, we will also need to change the score function, which calculates non-conformity scores. A common score function that is used is the distance to the conditional mean, <img src="../Images/B22389_17_130.png" alt=""/> (Reference<em class="italic"> 10</em>). When we have a trained model, <img src="../Images/B22389_17_103.png" alt=""/>, we can consider the output of the model as the conditional mean, which will make this the absolute residual value for each point.</p>
    <p class="center"><img src="../Images/B22389_17_132.png" alt=""/></p>
    <p class="normal">Note that this score satisfies the condition. The larger the deviation, the larger the “heuristic” measure of uncertainty is. The rest of the procedure remains almost the same—calculating the quantile, <img src="../Images/B22389_17_121.png" alt=""/>, and forming the <a id="_idIndexMarker1465"/>prediction intervals, <img src="../Images/B22389_17_134.png" alt=""/></p>
    <p class="normal">Let’s check how the Python code changes (full code is in the notebook).</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># 1: Get conformal scores</span>
calib_preds = model.predict(calib_x)
cal_scores = np.<span class="hljs-built_in">abs</span>(calib_y - calib_preds)
<span class="hljs-comment"># 2: Get adjusted quantile</span>
qhat = … <span class="hljs-comment"># Exactly the same as classification</span>
<span class="hljs-comment"># 3: Form prediction intervals</span>
test_preds = model.predict(test_x)
lower_bounds = test_preds - qhat
upper_bounds = test_preds + qhat
</code></pre>
    <p class="normal">It’s as simple as that. We can check coverage and see that it will be greater than 90%, which is the error rate we defined with <img src="../Images/B22389_17_009.png" alt=""/>.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Practitioner’s Note</strong></p>
      <p class="normal">In many use cases, we will be training a single model for multiple entities or groups we care about. For instance, for the global forecasting models we talked about in <em class="chapterRef">Chapter 10</em>, we use a single regression model for multiple time series. In such cases, we can also run Conformal Prediction on each time series or groups of time series separately to be more adaptive to the errors in that subset. This would allow for coverage guarantees at the group/time series level.</p>
    </div>
    <p class="normal">But you might have noticed something. In this method, we have the same width of the interval all throughout. But we would expect the intervals to be tighter when the model is more confident and wider when it isn’t (let’s call it <em class="italic">adaptive prediction intervals </em>from now on). Let’s look at another technique that has this property.</p>
    <h3 id="_idParaDest-432" class="heading-3">Conformalized Quantile Regression</h3>
    <p class="normal">We learned<a id="_idIndexMarker1466"/> about Quantile Regression as one of the methods for probabilistic forecast (or regression in a general case). Quantile Regression is powerful in the sense that it does not require us to have any prior assumption about the underlying output distribution. But it does not enjoy the coverage guarantees that Conformal Prediction offers. In 2019, Yaniv Roano et al. (Reference<em class="italic"> 11</em>) brought the best of both worlds into <strong class="keyWord">Conformalized Quantile Regression</strong> (<strong class="keyWord">CQR</strong>). They proposed a way to take the quantile forecasts and conformalize them such that they have the coverage guarantees that conformal prediction assures.</p>
    <p class="normal">In this case, the <a id="_idIndexMarker1467"/>model to be used has a restriction. It should be a model that outputs quantile predictions. And, from our earlier discussion, we know that if the error rate is <img src="../Images/B22389_04_009.png" alt=""/>, then the quantiles we need for the prediction intervals are <img src="../Images/B22389_17_137.png" alt=""/> and <img src="../Images/B22389_17_138.png" alt=""/>.</p>
    <p class="normal">So, the quantile model predicts <img src="../Images/B22389_17_139.png" alt=""/> and <img src="../Images/B22389_17_138.png" alt=""/>. By definition, if <img src="../Images/B22389_17_139.png" alt=""/> and <img src="../Images/B22389_17_138.png" alt=""/> are true estimations of the real quantiles, a quantile regression alone will have perfect coverage. But the model fit may not be perfect and that would result in sub-par coverage. We will use conformal prediction to correct the quantiles based on calibration data such that we get the perfect coverage promised by conformal prediction.</p>
    <p class="normal">Yaniv Roano et al. proposed to use a new non-conformity score function for Quantile Regression.</p>
    <p class="center"><img src="../Images/B22389_17_143.png" alt=""/></p>
    <p class="normal">Let’s take a beat and explore the score function using the diagram in <em class="italic">Figure 17.15</em>.</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_15.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.15: Illustration of score function for Conformalized Quantile Regression</p>
    <p class="normal">There<a id="_idIndexMarker1468"/> are two terms in the max operator. If the true value, <img src="../Images/B22389_17_118.png" alt=""/>, is between the two quantiles, <img src="../Images/B22389_17_137.png" alt=""/> and <img src="../Images/B22389_17_138.png" alt=""/>, both the terms inside will be negative and will be the distance to the nearest prediction interval (see points B and C in <em class="italic">Figure 17.15</em>).</p>
    <p class="normal">Now, let’s look at point A (3), which is above the higher quantile. The quantiles are [1, 2]. The two terms in the max operator would be:</p>
    <p class="center"><img src="../Images/B22389_17_147.png" alt=""/></p>
    <p class="center"><img src="../Images/B22389_17_148.png" alt=""/></p>
    <p class="normal">This makes the score function 3 because of the max operator. And now, we see point D (1.6), which is below the lower quantile (quantiles are [3.5, 2]). This makes the score function max{0.4, -1.9}, which would be 0.4. So, the max operator ensures that the score is positive if it falls outside the quantiles.</p>
    <p class="normal">Therefore, what we have is a score function that assigns positive values to points where the actual value falls outside the intervals and negative to points within. And for the points that fall outside the intervals, the way the score function is constructed will choose the worse error. This satisfies our requirements from the score function. A larger score shows<a id="_idIndexMarker1469"/> larger uncertainty, and it also encodes a heuristic notion of uncertainty.</p>
    <p class="normal">Now that we have the scores, the rest of the steps are almost identical:</p>
    <ol>
      <li class="numberedList" value="1">Compute <img src="../Images/B22389_17_121.png" alt=""/> as the <img src="../Images/B22389_17_122.png" alt=""/> quantile of the calibration scores.</li>
      <li class="numberedList">Use this quantile to form prediction sets for new examples: <img src="../Images/B22389_17_151.png" alt=""/>, i.e., we widen the exiting quantiles by <img src="../Images/B22389_17_121.png" alt=""/> and get coverage guarantees.</li>
    </ol>
    <p class="normal">Quantile regression is one of the better ways of getting <em class="italic">adaptive prediction intervals</em>. Let’s look at one such technique now.</p>
    <h3 id="_idParaDest-433" class="heading-3">Conformalizing uncertainty estimates</h3>
    <p class="normal">If we think<a id="_idIndexMarker1470"/> about Conformalized Quantile Regression (from the last section) a bit deeply, we can realize that what the underlying quantile regression is doing is capturing the <em class="italic">uncertainty estimate</em> at each point in our prediction. And we conformalize those estimates for better coverage. If we can capture this <em class="italic">uncertainty estimate</em>, <img src="../Images/B22389_17_153.png" alt=""/>, we have some hope of conformalizing this to get better <em class="italic">adaptive prediction intervals</em>.</p>
    <p class="normal">Let’s say that we have a trained model, <img src="../Images/B22389_17_103.png" alt=""/>, and some uncertainty scalar, <img src="../Images/B22389_17_153.png" alt=""/>, that has high values when uncertainty is high, and vice versa. We can define our non-conformity score as:</p>
    <p class="center"><img src="../Images/B22389_17_156.png" alt=""/></p>
    <p class="normal">The natural interpretation of this score is that we are multiplying a correction factor to the standard <img src="../Images/B22389_17_157.png" alt=""/>. Once we have this new score, the rest of the process is exactly the same—taking <img src="../Images/B22389_17_121.png" alt=""/> from the scores and forming the prediction intervals as <img src="../Images/B22389_17_159.png" alt=""/>.</p>
    <p class="normal">So, what are <a id="_idIndexMarker1471"/>some ways to capture this uncertainty? (Note that this uncertainty measure should be capturing it at a data point level.)</p>
    <ul>
      <li class="bulletList">Assume a probability distribution and modeling their parameters (<strong class="keyWord">Probability Density Function</strong>). In the case of Gaussian Distribution, we would have an estimate of uncertainty as the standard deviation, <img src="../Images/B22389_17_160.png" alt=""/>, and we consider that as <img src="../Images/B22389_17_153.png" alt=""/>. <img src="../Images/B22389_17_162.png" alt=""/>.</li>
      <li class="bulletList">Use the MC Dropout technique to generate samples and calculate the standard deviation of <img src="../Images/B22389_17_163.png" alt=""/> from the samples: <img src="../Images/B22389_17_164.png" alt=""/>.</li>
      <li class="bulletList">Along with the main model prediction <img src="../Images/B22389_17_165.png" alt=""/>, train another model to predict the residuals, <img src="../Images/B22389_17_166.png" alt=""/>, and set <img src="../Images/B22389_17_167.png" alt=""/>.</li>
      <li class="bulletList">Use an ensemble of models to generate multiple predictions for each data point and take the standard deviation of different predictions at each data point: <img src="../Images/B22389_17_168.png" alt=""/>.</li>
    </ul>
    <p class="normal">Although the list above is not exhaustive, it does show that we can apply conformal prediction to almost any uncertainty estimates (including the ones we have already seen in the chapter). This makes the conformal prediction paradigm a very flexible toolkit to get coverage guarantees with a wide variety of problems. Even with this flexible nature, there are cases that mess with the coverage guarantees that the framework promises. And for our time series context, this is important to understand.</p>
    <h3 id="_idParaDest-434" class="heading-3">Exchangeability in Conformal Prediction and time series forecasting</h3>
    <p class="normal"><em class="italic">Exchangeability</em> is a <a id="_idIndexMarker1472"/>fundamental assumption in Conformal Prediction. Exchangeability means that the data points are identically distributed and their joint probability distribution does not change when the order of the data points is changed. This concept ensures that past data points can reliably predict future data points.</p>
    <p class="normal">Imagine a chocolate factory producing chocolates with consistent weights. If the production process is highly controlled, with the same conditions and ingredients, the weights of the chocolates are exchangeable because their order of production does not affect their weight. You can sample 100 chocolates, measure their weights, and calculate nonconformity scores based on deviations from the predicted weight. Using these scores, you can form prediction intervals for future chocolates. Because the chocolates are exchangeable, the sample distribution represents the future distribution, making prediction intervals reliable.</p>
    <p class="normal">However, if the production process changes over time—due to machinery wear or different ingredient batches—the weights become non-exchangeable. The order of production affects the weights, making the sample distribution unrepresentative of future weights, leading to unreliable prediction intervals.</p>
    <p class="normal">In time series data, observations are typically dependent on previous observations, violating the exchangeability assumption. For example, in a sales forecast, today’s sales may influence tomorrow’s sales due to trends, or the sales a year ago may influence tomorrow’s sales due to seasonal effects. This dependency means that the distribution of past data does not represent the distribution of future data accurately.</p>
    <p class="normal">But what does this mean for us? The most obvious answer is that our coverage guarantees will suffer. But can we still apply these techniques for time series? Of course we can. Empirically, the community has seen that this framework works for time series data as well, but with some loss in coverage guarantees. For most practical purposes, there shouldn’t be an issue in using regular conformal prediction for time series data. In 2023, Barber et al. (Reference<em class="italic"> 12</em>) studied this issue and derived theoretical coverage guarantees for non-exchangeable data (like time series). They defined the Coverage Gap as the difference between expected coverage (<img src="../Images/B22389_17_016.png" alt=""/>) and actual coverage and derived an upper bound on this gap to show how much the exchangeability assumption on the scores is violated. For this bound, they considered the scores of the calibration data with our original model, <img src="../Images/B22389_17_170.png" alt=""/>, and an alternate model, which was trained on the same data but after swapping one randomly selected datapoint in the training data with the test datapoint, <img src="../Images/B22389_17_171.png" alt=""/>. </p>
    <p class="normal">The bound was shown to be directly proportional to <img src="../Images/B22389_17_172.png" alt=""/>, which is the distributional distance <a id="_idIndexMarker1473"/>between these two scores. In most algorithms we use, swapping one data point may not change the model drastically and therefore, we can still use conformal prediction for time series data with minimal loss in coverage.</p>
    <p class="normal">But on the other hand, if we want to be really accurate with the prediction intervals, or if we are using a model that is particularly impacted by the swapping of a datapoint, then we would need some techniques to overcome this degradation due to a shift in distributions. There are many ways to deal with this, and it is an active area of research at the time of writing the book. There are two very simple methods that are worth mentioning here.</p>
    <h4 class="heading-4">Weighted conformal prediction</h4>
    <p class="normal">Suppose<a id="_idIndexMarker1474"/> we have slowly varying changes in the data distribution in a time series, <img src="../Images/B22389_17_173.png" alt=""/>, and we are using a calibration set, <img src="../Images/B22389_17_174.png" alt=""/>, taking the last <em class="italic">k</em> timesteps. And we are interested in predicting the test set, <img src="../Images/B22389_17_175.png" alt=""/>, where <em class="italic">H</em> is the horizon of forecast.</p>
    <p class="normal">So, it stands to reason that the most recent timestep in <img src="../Images/B22389_17_176.png" alt=""/> would be closest to the distribution of values we would observe in the test time period. So, what if we assign weights to the non-conformity scores in the calibration data such that the most recent time step gets higher weights, and calculates a weighted quantile instead of a regular quantile? Apparently, that’s a very good idea, with some solid theoretical backing as well.</p>
    <p class="normal">And weighing the calibration data using recency is just one of the ways we can use the weights to tackle the distribution shift. More generally, any weight schedule, <img src="../Images/B22389_17_177.png" alt=""/> can be used here. Maybe for a strongly seasonal time series, it makes sense to use seasonal periods to define the weights, or there may be some other known criteria that makes different instances of the calibration data more or less relevant to future prediction.</p>
    <p class="normal">Before we get<a id="_idIndexMarker1475"/> into the actual mechanics of this, we need to understand what weighed quantiles are. If you are already comfortable with the concept, feel free to go ahead. If you need some intuition about what it is, I strongly advise you to check out the notebook in the chapter folder named <code class="inlineCode">08-Quantiles_and_Weighted_Quantiles.ipynb</code>.</p>
    <p class="normal">Now, let’s come back to our Weighted Conformal Prediction method.</p>
    <p class="normal">So, as we discussed earlier, for any normalized weight schedule, <img src="../Images/B22389_17_178.png" alt=""/>, and calibration scores, <em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">i</sub> the weighted quantile can be formally defined as:</p>
    <p class="center"><img src="../Images/B22389_17_179.png" alt=""/></p>
    <p class="normal">where <em class="italic">inf</em> is the infimum and <img src="../Images/B22389_17_031.png" alt=""/> is an indicator function, which is 1 when the condition is true and 0 otherwise. In this context, the <em class="italic">infimum</em> is the smallest value of <em class="italic">q</em> such that the inequality holds true. This is just a more rigorous way of defining the weighted quantile that we saw in the aforementioned notebook. The rest of the process is exactly the same as before.</p>
    <p class="normal">Practically, there are a few different ways we can use this for time series problems. For instance:</p>
    <ol>
      <li class="numberedList" value="1">We can consider a moving window of length <em class="italic">K</em> and have a fixed weight vector of length <em class="italic">K</em>. Under this scheme, we will apply the weights for each point in the time series to the last <em class="italic">K</em> points and calculate the prediction interval for that point. These weights can be equal weights or even decayed weights, capturing the temporal element.</li>
      <li class="numberedList">When we model multiple time series together, we can make sure the weights reflect how close another time series is to the time series we are generating the intervals for, along with the temporal context.</li>
    </ol>
    <p class="normal">The bottom line is that we can be as creative as we can when coming up with the weights. The guideline is that the weight should reflect how different the calibration datapoints are from the datapoint you are generating the prediction interval for. Remember, we talked about the upper bound earlier, <img src="../Images/B22389_17_172.png" alt=""/>. The weight we choose would counteract this term. When we have smaller weights for datapoints that are “farther” away from the datapoint we care about, this brings down the upper bound of the coverage gap and makes it tighter.</p>
    <p class="normal">Now, let’s learn about another very simple modification to account for distribution shift.</p>
    <h4 class="heading-4">Adaptive Conformal Inference (ACI)</h4>
    <p class="normal">In 2021, Gibbs et al. (Reference <em class="italic">13</em>) proposed <a id="_idIndexMarker1476"/>another way to deal with distribution shift (especially in time series) in an online setting. Time series data usually comes in one datapoint at a time, and this way to deal with distribution shift relies on this online aspect as it proposes to keep adjusting the prediction intervals based on the data that keeps trickling in, thus making the prediction intervals adapt to changing distributions. This method, called <strong class="keyWord">Adaptive Conformal Inference</strong> (<strong class="keyWord">ACI</strong>), can <a id="_idIndexMarker1477"/>be integrated with any prediction algorithm to provide robust prediction sets under non-stationary conditions.</p>
    <p class="normal">In traditional Conformal Prediction, we have a score function, <img src="../Images/B22389_17_182.png" alt=""/>, and a quantile function, <img src="../Images/B22389_17_121.png" alt=""/>, which gives us the prediction intervals, <img src="../Images/B22389_17_184.png" alt=""/>. Note that the underlying uncertainty model, which was conformalized as <img src="../Images/B22389_17_121.png" alt=""/>, can be any way of estimating uncertainties, like quantile regression, PDF, MC Dropouts, and so on. When the data is exchangeable, the <img src="../Images/B22389_17_121.png" alt=""/> we calculated on calibration data will hold good on future test datapoints. But when the distribution shifts, this <img src="../Images/B22389_17_121.png" alt=""/> will also start to become less and less relevant. To address this, the authors propose regularly re-estimating these functions to align with the most recent data observations. Specifically, at each time point, <em class="italic">t</em>, a new score function, <img src="../Images/B22389_17_188.png" alt=""/>, and a new quantile function, <img src="../Images/B22389_17_189.png" alt=""/>(.), are fitted based on the most recent data.</p>
    <p class="normal">For this, they defined the <em class="italic">miscoverage rate</em>, <img src="../Images/B22389_17_190.png" alt=""/>, as the probability that the true label, <img src="../Images/B22389_17_191.png" alt=""/>, lies outside the prediction intervals, <img src="../Images/B22389_17_184.png" alt=""/>, where probability is calculated over the calibration data and the test datapoint. We want the <em class="italic">Miscoverage rate</em>, <img src="../Images/B22389_17_190.png" alt=""/>, to be equal to <img src="../Images/B22389_04_009.png" alt=""/> (expected error rate). But since the data distribution is shifting, <img src="../Images/B22389_17_190.png" alt=""/> is not expected to remain constant over time, and it may not equal the target level, <img src="../Images/B22389_04_009.png" alt=""/>. The authors hypothesize that for each time, <img src="../Images/B22389_16_126.png" alt=""/>, there may exist an optimal coverage level, <img src="../Images/B22389_17_198.png" alt=""/> such that the miscoverage rate, <img src="../Images/B22389_17_199.png" alt=""/>, is approximately <img src="../Images/B22389_04_009.png" alt=""/>.</p>
    <p class="normal">To <a id="_idIndexMarker1478"/>estimate this <img src="../Images/B22389_17_198.png" alt=""/>, the authors propose a simple online update equation. This update takes into consideration the empirical miscoverage rate of the previous observations and then decreases or increases our estimate of <img src="../Images/B22389_17_202.png" alt=""/>. Concretely, if we set <img src="../Images/B22389_17_203.png" alt=""/>, we can define error as:</p>
    <p class="center"><img src="../Images/B22389_17_204.png" alt=""/></p>
    <p class="normal">Now, we can define the update step recursively as:</p>
    <p class="center"><img src="../Images/B22389_17_205.png" alt=""/></p>
    <p class="normal">Here, <img src="../Images/B22389_17_206.png" alt=""/>, serves as an estimate of the historical miscoverage rate and <img src="../Images/B22389_17_207.png" alt=""/> is the step-size (a hyperparameter; more on this later). So, when <img src="../Images/B22389_17_208.png" alt=""/> (prediction was within the interval), <img src="../Images/B22389_17_209.png" alt=""/> will be positive and thus updating <img src="../Images/B22389_17_210.png" alt=""/> to be higher than <img src="../Images/B22389_17_211.png" alt=""/>. This, in turn, makes the prediction interval narrower (according to the <img src="../Images/B22389_04_011.png" alt=""/> we have defined). With the same logic, when <img src="../Images/B22389_17_213.png" alt=""/> (prediction was outside the interval), the prediction interval becomes wider.</p>
    <p class="normal">A natural alternative to this update, which also takes into account the history a bit more, is using a weighted average of past timesteps:</p>
    <p class="center"><img src="../Images/B22389_17_214.png" alt=""/></p>
    <p class="normal">where <img src="../Images/B22389_17_215.png" alt=""/> is a sequence of increasing weights with <img src="../Images/B22389_17_216.png" alt=""/>. Instead of looking at just the last time step for the estimate of miscoverage, this update looks at the recent history. This makes it slightly more robust, at least in theory. The paper reported no significant difference between the two strategies. One of the strategies they have used to decide the weights is:</p>
    <p class="center"><img src="../Images/B22389_17_217.png" alt=""/></p>
    <p class="normal">They <a id="_idIndexMarker1479"/>reported that the trajectories of prediction intervals that they obtained from the simple update and weighted update were almost the same, but the weighted one was considerably smoother with less local variation in <img src="../Images/B22389_17_211.png" alt=""/>.</p>
    <p class="normal">Now, let’s also spend some time understanding the effect of the step-size parameter, <img src="../Images/B22389_04_011.png" alt=""/>. The intuition is very similar to the learning rate in deep learning models. <img src="../Images/B22389_04_011.png" alt=""/> decides the magnitude with which we update the <img src="../Images/B22389_04_009.png" alt=""/>. The larger the value, the quicker the update, and vice versa. The paper also gives us an intuition that the greater the distributional shift greater the value of <img src="../Images/B22389_04_011.png" alt=""/>. For all their experiments, they used <img src="../Images/B22389_17_223.png" alt=""/> with a justification that they found this value to make the trajectories relatively smooth while still being large enough to allow <img src="../Images/B22389_17_211.png" alt=""/> to adapt to distributional shifts. We can see this as a parameter controlling the strength of “adapting” and letting us move in the spectrum between non-adaptive intervals and strongly adaptive intervals.</p>
    <p class="normal">Now, let’s see how to apply these in practice.</p>
    <h3 id="_idParaDest-435" class="heading-3">Forecasting with Conformal Prediction</h3>
    <p class="normal">We didn’t<a id="_idIndexMarker1480"/> find any ready-to-use implementations of all <a id="_idIndexMarker1481"/>the techniques we wanted to show here, especially one that has these properties:</p>
    <ul>
      <li class="bulletList">Complete separation of model layer and conformal prediction layer (being model-agnostic is one of the most exciting features of Conformal Prediction)</li>
      <li class="bulletList">Out-of-the box compatibility with <code class="inlineCode">neuralforecast</code> predictions</li>
      <li class="bulletList">Time series focus</li>
      <li class="bulletList">Pedagogical ease</li>
    </ul>
    <p class="normal">Therefore, we have included a file (<code class="inlineCode">src/conformal/conformal_predictions.py</code>) with the necessary implementations that would work with <code class="inlineCode">neuralforecast</code> forecasts and have a unified API. It is<a id="_idIndexMarker1482"/> also simple enough to understand. We will go through major parts of the code, but to see how it all fits together, you should just take a look at the file.</p>
    <p class="normal">All the methods <a id="_idIndexMarker1483"/>we discussed, like Conformal Prediction for Regression, Conformalized Quantile Regression, and Conformalizing uncertainty estimates, have been coded out in the same API. Let’s look at the most basic Conformal Prediction for Regression to understand the API. It can be found in the <code class="inlineCode">ConformalPrediction</code> class in the file. The rest of the techniques inherit this class and make slight tweaks. And all these classes are coded in such a way that they take in the prediction dataframe from <code class="inlineCode">neuralforecast</code> (or <code class="inlineCode">statsforecast</code>) and use the same naming conventions to conformalize those predictions. In theory, any forecast that can be made into the expected format can be used with these classes. The expected columns in the format are:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">ds</code>: This column should contain dates or the numerical equivalent of time.</li>
      <li class="bulletList"><code class="inlineCode">y</code>: For train and calibration datasets, this column is necessary and it represents the actual value of that time series.</li>
      <li class="bulletList"><code class="inlineCode">unique_id</code>: This column is the unique identifier for different time series.</li>
    </ul>
    <p class="normal">In addition to these columns, we would also have a column (or multiple columns) of forecast, named accordingly.</p>
    <p class="normal">Before we start generating conformal predictions, we also need some data and forecasts. We are using the same data that we have been using in this chapter (M4), with one additional split (calibration) created. And using the new train data, we have created these three forecasts with <code class="inlineCode">level = 90</code>:</p>
    <ol>
      <li class="numberedList" value="1">LSTM point forecast (<code class="inlineCode">LSTM</code>)</li>
      <li class="numberedList">LSTM with Quantile Regression (<code class="inlineCode">LSTM_QR</code>)</li>
      <li class="numberedList">LSTM with PDF (normal distribution) (<code class="inlineCode">LSTM_PDF</code>)</li>
    </ol>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong></p>
      <p class="normal">To follow along with the complete code, use the notebook named <code class="inlineCode">09-Conformal_Techniques.ipynb</code> in the <code class="inlineCode">Chapter17</code> folder.</p>
    </div>
    <p class="normal">The notebook <a id="_idIndexMarker1484"/>has the entire code, but we can start from the point where we have already split the data into <code class="inlineCode">Y_train_df</code>, <code class="inlineCode">Y_calib_df</code>, and <code class="inlineCode">Y_test_df</code>, and generated and stored forecasts in a dictionary, <code class="inlineCode">prediction_dict</code>. Let’s take a look at the top five rows of the <a id="_idIndexMarker1485"/>prepared data frame to see what kind of data we are working with.</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_16.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.16: Top five rows of the Y_calib_df we are working with. This is the format that the conformal prediction classes we have coded expect</p>
    <p class="normal">Now, let’s get down to business and start creating prediction intervals.</p>
    <h4 class="heading-4">Conformal prediction for regression</h4>
    <p class="normal">The <code class="inlineCode">ConformalPrediction</code> class <a id="_idIndexMarker1486"/>provides a structured way to calculate prediction intervals based on a chosen model’s predictions from a calibration dataset. It includes the following input parameters:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">model</code> (str): The name of the column with the forecast you want to conformalize. This is a required parameter.</li>
      <li class="bulletList"><code class="inlineCode">level</code> (float): The confidence level for the prediction intervals, expressed as a percentage (e.g., 95 for a 95% confidence interval). The level must be between 1 and 100. This is a required parameter.</li>
      <li class="bulletList"><code class="inlineCode">alias</code> (str, optional): An optional string to provide an alias for the model. This can be useful when working with multiple models or versions and you want to call the output something else other than the model. If not provided, <code class="inlineCode">model</code> is used.</li>
    </ul>
    <p class="normal">The major functions <a id="_idIndexMarker1487"/>for using the class are:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">fit(Y_calib_df)</code>: This method orchestrates the entire calibration process. It first calculates the calibration scores using <code class="inlineCode">calculate_scores</code> and then determines the quantiles for each <code class="inlineCode">unique_id</code> using <code class="inlineCode">get_quantile</code>. The resulting quantiles (<code class="inlineCode">q_hat</code>) are stored as an attribute of the class, making them available for subsequent prediction intervals.</li>
      <li class="bulletList"><code class="inlineCode">predict(Y_test_df)</code>: This method applies the prediction intervals to the test data. It uses the <code class="inlineCode">calc_prediction_interval</code> method to compute the intervals and then adds them to the DataFrame as new columns. The new columns are created in this format: <code class="inlineCode">f"{self.alias or self.model}-{self._mthd}-lo-{self.level}"</code>. For example, the higher interval for Conformal Prediction using LSTM would have <code class="inlineCode">LSTM-CP-hi-90</code> as the column name in the dataframe.</li>
    </ul>
    <p class="normal">These classes are the external API. Internally, there are some methods that actually define how it’s done. Let’s look at the major methods in the context of regular Conformal Prediction.</p>
    <p class="normal">The <code class="inlineCode">calculate_scores</code> method is defined below, where we just calculate the absolute residuals using the calibration dataset as the scores:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">calculate_scores</span>(<span class="hljs-params">self, Y_calib_df</span>):
    Y_calib_df = Y_calib_df.copy()
    Y_calib_df[<span class="hljs-string">"calib_scores"</span>] = np.<span class="hljs-built_in">abs</span>(Y_calib_df[<span class="hljs-string">"y"</span>] - Y_calib_df[<span class="hljs-variable">self</span>.model])
    <span class="hljs-keyword">return</span> Y_calib_df
</code></pre>
    <p class="normal">The <code class="inlineCode">get_quantile</code> method calculates the quantile using the defined <img src="../Images/B22389_04_009.png" alt=""/> for each <code class="inlineCode">unique_id</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">get_quantile</span>(<span class="hljs-params">self, Y_calib_df</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title">get_qhat</span>(<span class="hljs-params">Y_calib_df</span>):
        n_cal = <span class="hljs-built_in">len</span>(Y_calib_df)
        q_level = np.ceil((n_cal + <span class="hljs-number">1</span>) * (<span class="hljs-number">1</span> - <span class="hljs-variable">self</span>.alpha)) / n_cal
        <span class="hljs-keyword">return</span> np.quantile(
                Y_calib_df[<span class="hljs-string">"calib_scores"</span>].values, q_level, method=<span class="hljs-string">"higher"</span>
            )
    <span class="hljs-keyword">return</span> Y_calib_df.groupby(
<span class="hljs-string">"unique_id"</span>).apply(get_qhat).to_dict()
</code></pre>
    <p class="normal">The <code class="inlineCode">calc_prediction_interval</code> method uses the calculated <code class="inlineCode">q_hat</code> and mean prediction to generate the prediction intervals. For regular Conformal Prediction, it goes like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">calc_prediction_interval</span>(<span class="hljs-params">self, Y_test_df, q_hat</span>):
    <span class="hljs-keyword">return</span> (
            Y_test_df[<span class="hljs-variable">self</span>.model] - Y_test_df[<span class="hljs-string">"</span><span class="hljs-string">unique_id"</span>].<span class="hljs-built_in">map</span>(q_hat),
            Y_test_df[<span class="hljs-variable">self</span>.model] + Y_test_df[<span class="hljs-string">"unique_id"</span>].<span class="hljs-built_in">map</span>(q_hat),
        )
</code></pre>
    <p class="normal">Now, let’s<a id="_idIndexMarker1488"/> use it for forecasting.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> src.conformal.conformal_predictions <span class="hljs-keyword">import</span> ConformalPrediction
Y_calib_df, Y_test_df = prediction_dict[<span class="hljs-string">'LSTM'</span>]
<span class="hljs-comment"># Y_calib_df &amp; Y_test_df have forecasts in column named "LSTM"</span>
cp = ConformalPrediction(model=<span class="hljs-string">"LSTM"</span>, level=level)
<span class="hljs-comment"># Calibrating the model</span>
cp.fit(Y_calib_df=Y_calib_df)
<span class="hljs-comment"># Generating Prediction intervals</span>
Y_test_df_cp = cp.predict(Y_test_df=Y_test_df)
</code></pre>
    <p class="normal">The generated dataframe (<em class="italic">Figure 17.15</em>) with prediction intervals will have two columns—<code class="inlineCode">LSTM-CP-lo-90</code> and <code class="inlineCode">LSTM-CP-hi-90</code> for the lower and upper prediction interval, respectively. <code class="inlineCode">CP</code> is the method tag we have assigned the conformal prediction class. </p>
    <p class="normal">We can check the method name of any object as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">&gt;&gt; cp.method_name
'Vanilla Conformal Prediction (CP)
</code></pre>
    <p class="normal">Let’s see what the forecast dataframe looks like (CP is the method tag we have assigned the conformal prediction class. We can check the method name of any object by doing <code class="inlineCode">cp.method_name</code>):</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_17.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.17: The generated dataframe with prediction intervals</p>
    <p class="normal">Now, we also have to calculate the coverage and average length of the intervals to assess how these prediction intervals are. We use the same methods we used earlier to do that. Instead <a id="_idIndexMarker1489"/>of looking at the performance of each method, let’s save the discussion for the end and look at creating the prediction intervals for now.</p>
    <p class="normal">So, let’s move on to the next technique.</p>
    <h4 class="heading-4">Conformalized Quantile Regression</h4>
    <p class="normal">The <a id="_idIndexMarker1490"/>first condition of applying this method of conformal prediction is that there should already be a set of prediction intervals from the underlying Quantile Regression. Therefore, we use the <code class="inlineCode">LSTM_QR</code> model we trained here.</p>
    <p class="normal">The main difference between vanilla Conformal Prediction and CQR is the way the scores are calculated and the prediction intervals. So, we can inherit <code class="inlineCode">ConformalPrediction</code> and just redefine these two methods.</p>
    <p class="normal">Let’s look at the <code class="inlineCode">calculate_scores</code> method.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">calculate_scores</span>(<span class="hljs-params">self, Y_calib_df</span>):
    Y_calib_df = Y_calib_df.copy()
    lower_bounds = Y_calib_df[<span class="hljs-variable">self</span>.lower_quantile_model]
    upper_bounds = Y_calib_df[<span class="hljs-variable">self</span>.upper_quantile_model]
    Y_calib_df[<span class="hljs-string">"calib_scores"</span>] = np.maximum(
            lower_bounds - Y_calib_df[<span class="hljs-string">"y"</span>], Y_calib_df[<span class="hljs-string">"y"</span>] - upper_bounds
        )
    <span class="hljs-keyword">return</span> Y_calib_df
</code></pre>
    <p class="normal">We have<a id="_idIndexMarker1491"/> just implemented the formula we saw earlier. <code class="inlineCode">self.lower_quantile_model</code> and <code class="inlineCode">self.upper_quantile_model</code> are the column names of the already-generated intervals from CQR.</p>
    <p class="normal">Now, we also need to define the <code class="inlineCode">calc_prediction_interval</code> method.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">calc_prediction_interval</span>(<span class="hljs-params">self, Y_test_df, q_hat</span>):
    <span class="hljs-keyword">return</span> (
            Y_test_df[<span class="hljs-variable">self</span>.lower_quantile_model] - Y_test_df[<span class="hljs-string">"unique_id"</span>].<span class="hljs-built_in">map</span>(q_hat),
            Y_test_df[<span class="hljs-variable">self</span>.upper_quantile_model] + Y_test_df[<span class="hljs-string">"unique_id"</span>].<span class="hljs-built_in">map</span>(q_hat),
        )
</code></pre>
    <p class="normal"><code class="inlineCode">q_hat</code> is a dictionary with the quantiles calculated for each <code class="inlineCode">unique_id</code>. So, all we do here is take the existing prediction interval from CQR and adjust it by mapping the quantile using the <code class="inlineCode">unique_id</code> in the input dataframe.</p>
    <p class="normal">Now, let’s use this for forecasting. The API is exactly the same as before.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> src.conformal.conformal_predictions <span class="hljs-keyword">import</span> ConformalizedQuantileRegression
Y_calib_df, Y_test_df = prediction_dict[<span class="hljs-string">'LSTM_QR'</span>]
<span class="hljs-comment"># Forecast in column "LSTM_QR"</span>
cp = ConformalizedQuantileRegression(model=<span class="hljs-string">"LSTM_QR"</span>, level=level)
cp.fit(Y_calib_df=Y_calib_df)
Y_test_df_cqr = cp.predict(Y_test_df=Y_test_df)
</code></pre>
    <p class="normal">Now, let’s look at the third technique we discussed.</p>
    <h4 class="heading-4">Conformalizing uncertainty estimates</h4>
    <p class="normal">If you remember<a id="_idIndexMarker1492"/> the discussion we had earlier, to use this technique, we need an estimate of uncertainty that can be further conformalized. This is why we picked one of the other techniques we used earlier, PDF. But we can also do this with the MC Dropout just as easily. All we need is the standard deviation or something similar that captures the uncertainty at each data point.</p>
    <p class="normal">We are using the <code class="inlineCode">LSTM_PDF</code> forecast that we generated earlier for this purpose. Although the model predicts the mean and standard deviation of the normal distribution, it is used internally to generate the prediction intervals. So, the output from the PDF model we defined<a id="_idIndexMarker1493"/> earlier would be the prediction intervals, but we want the standard deviation. Fear not. We know the prediction intervals were created with the normal distribution. So, it’s not hard to re-engineer the standard deviation from the prediction intervals.</p>
    <p class="center"><img src="../Images/B22389_17_049.png" alt=""/></p>
    <p class="center"><img src="../Images/B22389_17_050.png" alt=""/></p>
    <p class="normal">Using basic math, we can derive:</p>
    <p class="center"><img src="../Images/B22389_17_228.png" alt=""/></p>
    <p class="normal">And <em class="italic">Z</em> is very straightforward to get. We can use <code class="inlineCode">scipy.stats.norm</code> for this. Below is a method that will get you the standard deviation from the prediction intervals (do keep in mind that this is only for PDFs created using a normal distribution).</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> norm
<span class="hljs-keyword">def</span> <span class="hljs-title">calculate_standard_deviation</span>(<span class="hljs-params">upper_bound, point_prediction, confidence_level</span>):
    <span class="hljs-comment"># Calculate the Z-value from the confidence level</span>
    z_value = norm.ppf((<span class="hljs-number">1</span> + confidence_level) / <span class="hljs-number">2</span>)
    <span class="hljs-comment"># Calculate the standard deviation</span>
    sigma = (upper_bound - point_prediction) / z_value
   
    <span class="hljs-keyword">return</span> sigma
<span class="hljs-keyword">def</span> <span class="hljs-title">reverse_engineer_sd</span>(<span class="hljs-params">X, model_tag, level</span>):
    X[<span class="hljs-string">"std"</span>] = calculate_standard_deviation(
        X[<span class="hljs-string">f"</span><span class="hljs-subst">{model_tag}</span><span class="hljs-string">-hi-</span><span class="hljs-subst">{level}</span><span class="hljs-string">"</span>], X[model_tag], level / <span class="hljs-number">100</span>
    )
    <span class="hljs-keyword">return</span> X
</code></pre>
    <p class="normal">Now, we add this to our <code class="inlineCode">Y_calib_df</code> and <code class="inlineCode">Y_test_df</code>.</p>
    <pre class="programlisting code"><code class="hljs-code">Y_calib_df = reverse_engineer_sd(Y_calib_df, <span class="hljs-string">"LSTM_Normal"</span>, level)
Y_test_df = reverse_engineer_sd(Y_test_df, <span class="hljs-string">"LSTM_Normal"</span>, level)
</code></pre>
    <p class="normal">Now, let’s look at how we can define the class. We need additional information in here that we didn’t need earlier—the column name of the uncertainty estimate. So, we define our <a id="_idIndexMarker1494"/>new class (still inheriting <code class="inlineCode">ConformalPrediction</code>) as below:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">ConformalizedUncertaintyEstimates</span>(<span class="hljs-title">ConformalPrediction</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(
<span class="hljs-params">        self,</span>
<span class="hljs-params">        model: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">        uncertainty_model: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">        level: </span><span class="hljs-type">Optional</span><span class="hljs-params">[</span><span class="hljs-built_in">float</span><span class="hljs-params">] = </span><span class="hljs-literal">None</span><span class="hljs-params">,</span>
<span class="hljs-params">        alias: </span><span class="hljs-built_in">str</span><span class="hljs-params"> = </span><span class="hljs-literal">None</span><span class="hljs-params">,</span>
<span class="hljs-params">    </span>):
        <span class="hljs-built_in">super</span>().__init__(model, level, alias)
        <span class="hljs-variable">self</span>.method = <span class="hljs-string">"Conformalized Uncertainty Intervals"</span>
        <span class="hljs-variable">self</span>._mthd = <span class="hljs-string">"CUE"</span>
        <span class="hljs-variable">self</span>.uncertainty_model = uncertainty_model
</code></pre>
    <p class="normal">We define an additional parameter, <code class="inlineCode">uncertainty_model</code>, and pass on the other parameters to the parent class.</p>
    <p class="normal">Now, it’s pretty straightforward. We need to define how the scores are calculated:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">calculate_scores</span>(<span class="hljs-params">self, Y_calib_df</span>):
    Y_calib_df = Y_calib_df.copy()
        uncertainty = Y_calib_df[<span class="hljs-variable">self</span>.uncertainty_model]
    Y_calib_df[<span class="hljs-string">"calib_scores"</span>] = (
            np.<span class="hljs-built_in">abs</span>(Y_calib_df[<span class="hljs-string">"y"</span>] - Y_calib_df[<span class="hljs-variable">self</span>.model]) / uncertainty
        )
    <span class="hljs-keyword">return</span> Y_calib_df
</code></pre>
    <p class="normal">And the <code class="inlineCode">calc_prediction_interval</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">calc_prediction_interval</span>(<span class="hljs-params">self, Y_test_df, q_hat:</span>
<span class="hljs-params">    uncertainty = Y_test_df[self.uncertainty_model]</span>
<span class="hljs-params">    </span><span class="hljs-keyword">return</span><span class="hljs-params"> (</span>
<span class="hljs-params">            Y_test_df[self.model] - uncertainty * Y_test_df[</span><span class="hljs-string">"unique_id"</span><span class="hljs-params">].</span><span class="hljs-built_in">map</span><span class="hljs-params">(q_hat),</span>
<span class="hljs-params">            Y_test_df[self.model] + uncertainty * Y_test_df[</span><span class="hljs-string">"unique_id"</span><span class="hljs-params">].</span><span class="hljs-built_in">map</span><span class="hljs-params">(q_hat),</span>
<span class="hljs-params">        )</span>
</code></pre>
    <p class="normal">That’s it. Now, we have a new class that conformalizes uncertainty estimates. Let’s use it to get the forecast<a id="_idIndexMarker1495"/> for the dataset we have been working with.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> src.conformal.conformal_predictions <span class="hljs-keyword">import</span> ConformalizedUncertaintyEstimates
<span class="hljs-comment"># We have saved uncertainty estimates in "std"</span>
cp = ConformalizedUncertaintyEstimates(model=<span class="hljs-string">"LSTM_Normal"</span>, uncertainty_model=<span class="hljs-string">"std"</span>, level=level)
cp.fit(Y_calib_df=Y_calib_df)
Y_test_df_pdf = cp.predict(Y_test_df=Y_test_df)
</code></pre>
    <p class="normal">Now, let’s also look at the two techniques we saw, which were more suited for time series problems where there is a distribution shift.</p>
    <h4 class="heading-4">Weighted conformal prediction</h4>
    <p class="normal">We saw earlier<a id="_idIndexMarker1496"/> that weighted conformal prediction was just about applying the right kind of weights to the calibration data such that the points similar to the test point get more weight than the dissimilar ones. And the key difference occurs only in the way the quantiles are calculated. </p>
    <p class="normal">What this means is that we can use any conformal prediction technique underneath, but instead of calculating a simple quantile, we need to calculate a weighted one. So, from an implementation perspective, we can look at this class as a wrapper class around other techniques we have defined and convert them into weighted conformal predictions.</p>
    <p class="normal">Although the weighted conformal prediction can be implemented in many ways, taking in different kinds of weights (across time, across <code class="inlineCode">unique_id</code>, and so on), we are going to implement a simpler look-back window-based weighted conformal prediction. We choose the last <em class="italic">K</em> timesteps and calculate the weighted quantile on these <em class="italic">K</em> scores using the weights given. The weights can either be simple uniform weights across all <em class="italic">K</em> steps, or have decayed weights giving the highest weightage to the most recent score. They can even have a completely custom weight.</p>
    <p class="normal">So, let’s define the <code class="inlineCode">__init__</code> of the class as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">WeightedConformalPredictor</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(
<span class="hljs-params">        self,</span>
<span class="hljs-params">        conformal_predictor: ConformalPrediction,</span>
<span class="hljs-params">        K: </span><span class="hljs-built_in">int</span><span class="hljs-params">,</span>
<span class="hljs-params">        weight_strategy: </span><span class="hljs-built_in">str</span><span class="hljs-params">,</span>
<span class="hljs-params">        custom_weights: </span><span class="hljs-built_in">list</span><span class="hljs-params"> = </span><span class="hljs-literal">None</span><span class="hljs-params">,</span>
<span class="hljs-params">        decay_factor: </span><span class="hljs-built_in">float</span><span class="hljs-params"> = </span><span class="hljs-number">0.5</span><span class="hljs-params">,</span>
<span class="hljs-params">    </span>):
    …
</code></pre>
    <p class="normal">Here, <code class="inlineCode">K</code> is the <a id="_idIndexMarker1497"/>window, and <code class="inlineCode">conformal_predictor</code> is the underlying conformal prediction class we should be using (this should be one of the three classes we have defined). We can define the weight strategy to be either <code class="inlineCode">uniform</code>, <code class="inlineCode">decay</code>, or <code class="inlineCode">custom</code> for uniform weights, decayed weights, or custom weights, respectively. <code class="inlineCode">decay_factor</code> decides how fast the decay is applied to the weights for the decayed weighting strategy, and <code class="inlineCode">custom_weights</code> lets you specify exactly the weight on these <code class="inlineCode">K</code> timesteps.</p>
    <p class="normal">While we won’t look at the entire code in the text here, we will go through the usual suspects so that you can understand what’s happening. But I would definitely urge you to take some time to digest the code in the file.</p>
    <p class="normal">First up, we have our <code class="inlineCode">fit</code> method. In this method, we just use the score calculation of the underlying conformal predictor and store the calibration dataframe.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span>(<span class="hljs-params">self, Y_calib_df</span>):
    <span class="hljs-variable">self</span>.calib_df = <span class="hljs-variable">self</span>.conformal_predictor.calculate_scores(
            Y_calib_df.sort_values([<span class="hljs-string">"unique_id"</span>, <span class="hljs-string">"ds"</span>])
        )
</code></pre>
    <p class="normal">Now, let’s look at the main parts of the <code class="inlineCode">predict</code> method.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span>(<span class="hljs-params">self, Y_test_df</span>):
    <span class="hljs-comment"># Groupby unique_id</span>
    …   
    <span class="hljs-comment"># Calculate quantiles for each unique_id</span>
    <span class="hljs-variable">self</span>.q_hat = {}
    <span class="hljs-keyword">for</span> unique_id, group <span class="hljs-keyword">in</span> grouped_calib:
        <span class="hljs-comment"># Take the last K timesteps</span>
        group = group.iloc[-<span class="hljs-variable">self</span>.K :]
        scores = group[<span class="hljs-string">"calib_scores"</span>].values
        <span class="hljs-comment"># Calculate weights based on the last K timesteps</span>
        total_timesteps = <span class="hljs-built_in">len</span>(scores)
        weights = <span class="hljs-variable">self</span>._calculate_weight(total_timesteps)
        normalized_weights = weights / weights.<span class="hljs-built_in">sum</span>()
        <span class="hljs-comment"># Calculate quantile for the current unique_id</span>
        quantile = <span class="hljs-variable">self</span>.get_weighted_quantile(
                scores, normalized_weights, <span class="hljs-variable">self</span>.conformal_predictor.alpha
            )
        <span class="hljs-variable">self</span>.q_hat[unique_id] = quantile
    <span class="hljs-comment"># Calculate prediction intervals using the underlying conformal predictor's method</span>
    lo, hi = <span class="hljs-variable">self</span>.conformal_predictor.get_prediction_interval_names()
    Y_test_df[lo], Y_test_df[hi] = (
 <span class="hljs-variable">self</span>.conformal_predictor.calc_prediction_interval(Y_test_df, <span class="hljs-variable">self</span>.q_hat)
        )
    <span class="hljs-keyword">return</span> Y_test_df
</code></pre>
    <p class="normal">Let’s now<a id="_idIndexMarker1498"/> take one of the methods we saw earlier and apply the weighted conformal predictor wrapper on it. For our example, let’s choose the simple <code class="inlineCode">ConformalPrediction</code>. Let’s see how we can use this class:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> src.conformal.conformal_predictions <span class="hljs-keyword">import</span> WeightedConformalPredictor
Y_calib_df, Y_test_df = prediction_dict[<span class="hljs-string">'LSTM'</span>]
<span class="hljs-comment"># Defining an underlying conformal predictor</span>
cp = ConformalPrediction(model=<span class="hljs-string">"</span><span class="hljs-string">LSTM"</span>, level=level)
<span class="hljs-comment"># using the defined conformal predictor in weighted version</span>
weighted_cp = WeightedConformalPredictor(
    conformal_predictor=cp,
    K=<span class="hljs-number">50</span>,
    weight_strategy=<span class="hljs-string">"uniform"</span>,
)
weighted_cp.fit(Y_calib_df=Y_calib_df)
Y_test_df_wcp = weighted_cp.predict(Y_test_df=Y_test_df)
</code></pre>
    <p class="normal">This would create the prediction intervals with the tag <code class="inlineCode">CP_Wtd</code>. We can always check the tag by doing <code class="inlineCode">weighted_cp.method_name</code>.</p>
    <p class="normal">Now, there is one small shortcoming with the implementation. Although we are considering the temporal order in the scores, we still have a fixed calibration set. So, until we “re-fit” or calibrate with the latest data points, we will still be working on the same calibrated dataset. So, if you think about it, this should ideally be applied in an online way where each time we predict the new timestep, the previous timestep (with actual value) should be added to the calibration data. We have also provided an alternative implementation that is able to do this in an online fashion. We won’t go into details of the implementation because the core logic is the same, but the API is different, making it possible to update the calibration data. The full implementation can be found in the <code class="inlineCode">OnlineWeightedConformalPredictor</code> class in the file.</p>
    <p class="normal">Let’s see <a id="_idIndexMarker1499"/>how it can be used. First, we define the setup, initialize the class, and fit the calibration.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> src.conformal.conformal_predictions <span class="hljs-keyword">import</span> OnlineWeightedConformalPredictor
cp = ConformalPrediction(model=<span class="hljs-string">"LSTM"</span>, level=level)
online_weighted_cp = OnlineWeightedConformalPredictor(
    conformal_predictor=cp,
    K=<span class="hljs-number">50</span>,
    weight_strategy=<span class="hljs-string">"uniform"</span>,
)
online_weighted_cp.fit(Y_calib_df=Y_calib_df)
joblib.dump(online_weighted_cp, "path/to/saved/file.pkl")
</code></pre>
    <p class="normal">Now, during inference, we can do something like this for each timestep:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Loading the saved model</span>
online_weighted_cp = joblib.load("path/to/saved/file.pkl")
<span class="hljs-comment"># current timestep data = current</span>
<span class="hljs-comment"># past timestep actuals = last_timestep_actuals</span>
prediction = online_weighted_cp.predict_one(current_test)
<span class="hljs-comment"># updating the calibration data using the last timestep actuals</span>
online_weighted_cp.update(last_timestep_actuals)
</code></pre>
    <p class="normal">For our special case where we are evaluating on test data where we know the actuals, there is another method that does similar online prediction for the data: <code class="inlineCode">offline_predict</code>.</p>
    <pre class="programlisting code"><code class="hljs-code">Y_test_df_wcpo = online_weighted_cp.offline_predict(Y_test_df=Y_test_df)
</code></pre>
    <p class="normal">Now, let’s look at one last method.</p>
    <h4 class="heading-4">Adaptive Conformal Inference</h4>
    <p class="normal">Lastly, we <a id="_idIndexMarker1500"/>look at the Adaptive Conformal Inference. This can also be implemented as a wrapper over other conformal prediction methods because this technique involves updating <img src="../Images/B22389_04_009.png" alt=""/> such that coverage is maintained in spite of the shift in distribution. And because of the nature of the technique, we can only apply this in an online manner, i.e., updating alpha at every time step with the available data. So, this will have the same API as the <code class="inlineCode">OnlineWeightedConformalPredictor</code> we saw earlier.</p>
    <p class="normal">The full class<a id="_idIndexMarker1501"/> is available in <code class="inlineCode">src/conformal/conformal_predictions.py</code>, but here, we will look at some main parts so that you understand it. Let’s take a look at the <code class="inlineCode">__init__</code> function first:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">OnlineAdaptiveConformalInference</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(
<span class="hljs-params">        self,</span>
<span class="hljs-params">        conformal_predictor: ConformalPrediction,</span>
<span class="hljs-params">        gamma: </span><span class="hljs-built_in">float</span><span class="hljs-params"> = </span><span class="hljs-number">0.005</span><span class="hljs-params">,</span>
<span class="hljs-params">        update_method: </span><span class="hljs-built_in">str</span><span class="hljs-params"> = </span><span class="hljs-string">"simple"</span><span class="hljs-params">,</span>
<span class="hljs-params">        momentum_bw: </span><span class="hljs-built_in">float</span><span class="hljs-params"> = </span><span class="hljs-number">0.95</span><span class="hljs-params">,</span>
<span class="hljs-params">        per_unique_id: </span><span class="hljs-built_in">bool</span><span class="hljs-params"> = </span><span class="hljs-literal">True</span><span class="hljs-params">,</span>
<span class="hljs-params">    </span>):
        …
</code></pre>
    <p class="normal">Similar to <code class="inlineCode">WeightedConformalPredictor</code>, we take in an underlying conformal predictor (<code class="inlineCode">conformal_predictor</code>). In addition, we have <code class="inlineCode">gamma</code>, which is the step size (<img src="../Images/B22389_04_011.png" alt=""/>), <code class="inlineCode">update_method</code>, which can either be <code class="inlineCode">simple</code> (taking only the last timestep for update) or <code class="inlineCode">momentum</code> (taking the running average of the trajectory of errors). Finally, we can also define <code class="inlineCode">momentum_bw</code>, which is the momentum backweight that is used to calculate the weighted average of past errors. A higher momentum (e.g., <code class="inlineCode">0.95</code>) makes the trajectory smoother, having the effect of past miscoverage rates decay slower. Going to the other extreme (<code class="inlineCode">0.05</code>) makes the weighted average more reactive and closer to the “simple” method. Lastly, we also have a parameter to do the error calculation for each <code class="inlineCode">unique_id</code> separately or pool all errors together.</p>
    <p class="normal">As usual, we have a <code class="inlineCode">fit</code> method that uses a calibration dataset to calculate the scores and keep them ready. We can also see this as a warm-up period in an online implementation. The <img src="../Images/B22389_04_009.png" alt=""/> update will use the calibration data as an initial history to start off.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">fit</span>(<span class="hljs-params">self, Y_calib_df</span>):
    <span class="hljs-string">"""</span>
<span class="hljs-string">    Fit the conformal predictor model with calibration data.</span>
<span class="hljs-string">    """</span>
    <span class="hljs-variable">self</span>.calib_df = <span class="hljs-variable">self</span>.conformal_predictor.calculate_scores(Y_calib_df)
    <span class="hljs-variable">self</span>.scores_by_id = (
        <span class="hljs-variable">self</span>.calib_df.groupby(<span class="hljs-string">"unique_id"</span>)[<span class="hljs-string">"calib_scores"</span>].apply(<span class="hljs-built_in">list</span>).to_dict()
    )
    <span class="hljs-comment"># Some more code to initialize necessary data structures     </span>
    …
    <span class="hljs-keyword">return</span> <span class="hljs-variable">self</span>
</code></pre>
    <p class="normal">Now, let’s<a id="_idIndexMarker1502"/> see the <code class="inlineCode">predict_one</code> method, which predicts one timestep.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">predict_one</span>(<span class="hljs-params">self, current_test</span>):
    unique_ids = current_test[<span class="hljs-string">"unique_id"</span>].unique()
    predictions = []
    <span class="hljs-keyword">for</span> unique_id <span class="hljs-keyword">in</span> unique_ids:
        group_scores = <span class="hljs-variable">self</span>.scores_by_id.get(unique_id, [])
        <span class="hljs-keyword">if</span> group_scores:
            <span class="hljs-comment"># Determine the appropriate alpha to use</span>
            alpha = (
                <span class="hljs-variable">self</span>.alphat[unique_id]
                <span class="hljs-keyword">if</span> <span class="hljs-variable">self</span>.per_unique_id
                <span class="hljs-keyword">else</span> <span class="hljs-variable">self</span>.alphat_global
            )
            <span class="hljs-comment"># Calculate quantile for the current unique_id</span>
            <span class="hljs-variable">self</span>.q_hat = {
                unique_id: np.quantile(group_scores, <span class="hljs-number">1</span> - alpha, method=<span class="hljs-string">"higher"</span>)
            }
            <span class="hljs-comment"># Calculating prediction intervals using conformal_predictor</span>
            …
            current_test[lo] = lower.values
            current_test[hi] = upper.values
            <span class="hljs-comment"># Storing most recent prediction</span>
            …
        <span class="hljs-comment"># Collecting and returning concatenated predictions</span>
        …
</code></pre>
    <p class="normal">The code is well-commented and you should be able to understand it. For each <code class="inlineCode">unique_id</code>, we get the scores, get the appropriate <img src="../Images/B22389_04_009.png" alt=""/>, calculate the quantile, use this information to calculate the prediction interval using the underlying conformal predictor, and store the prediction for later use.</p>
    <p class="normal">Now, we<a id="_idIndexMarker1503"/> look at the <code class="inlineCode">update</code> method, which we can use once the actual value for the timestep becomes available.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span>(<span class="hljs-params">self, new_data</span>):
    new_scores = <span class="hljs-variable">self</span>.conformal_predictor.calculate_scores(new_data)
    <span class="hljs-keyword">for</span> unique_id, score <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(
        new_scores[<span class="hljs-string">"unique_id"</span>], new_scores[<span class="hljs-string">"calib_scores"</span>]
    ):
        <span class="hljs-comment"># Updating score trajectory with new score</span>
    …
        <span class="hljs-comment"># Retrieve stored predictions and calculate adapt_err</span>
        <span class="hljs-keyword">if</span> <span class="hljs-variable">self</span>.per_unique_id:
            lower, upper = <span class="hljs-variable">self</span>.predictions[unique_id]
            actual_y = new_data.loc[new_data[<span class="hljs-string">"unique_id"</span>] == unique_id, <span class="hljs-string">"y"</span>].values[<span class="hljs-number">0</span>]
            adapt_err = <span class="hljs-built_in">int</span>(actual_y &lt; lower <span class="hljs-keyword">or</span> actual_y &gt; upper)
            <span class="hljs-comment"># Update alpha updates the alpha using simple or momentum method</span>
            <span class="hljs-variable">self</span>.update_alpha(unique_id, adapt_err)
        <span class="hljs-keyword">else</span>:
            <span class="hljs-comment"># Do the same update at a global error-pooled way</span>
</code></pre>
    <p class="normal">Let’s see how it can be used. First, we define the setup, initialize the class, and fit the calibration.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> src.conformal.conformal_predictions <span class="hljs-keyword">import</span> OnlineAdaptiveConformalInference
cp = ConformalPrediction(model=<span class="hljs-string">"LSTM"</span>, level=level)
aci_cp = OnlineAdaptiveConformalInference(
    conformal_predictor=cp,
    gamma=<span class="hljs-number">0.005</span>,
    update_method=<span class="hljs-string">"simple"</span>,
)
aci_cp.fit(Y_calib_df=Y_calib_df)
joblib.dump(aci_cp, "path/to/saved/file.pkl")
</code></pre>
    <p class="normal">Now, during inference, we can do something like below for each timestep:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Loading the saved model</span>
aci_cp = joblib.load("path/to/saved/file.pkl")
<span class="hljs-comment"># current timestep data = current</span>
<span class="hljs-comment"># past timestep actuals = last_timestep_actuals</span>
prediction = aci_cp.predict_one(current_test)
<span class="hljs-comment"># updating the calibration data using the last timestep actuals</span>
aci_cp.update(last_timestep_actuals)
</code></pre>
    <p class="normal">For our<a id="_idIndexMarker1504"/> special case where we are evaluating on test data where we know the actuals, there is another method that does similar online prediction for the data: <code class="inlineCode">offline_predict</code>.</p>
    <pre class="programlisting code"><code class="hljs-code">Y_test_df_aci = aci_cp.offline_predict(Y_test_df=Y_test_df)
</code></pre>
    <p class="normal">Now that we have seen all the techniques in action, let’s take a look at how they have been performing.</p>
    <h4 class="heading-4">Evaluating the results</h4>
    <p class="normal">If you <a id="_idIndexMarker1505"/>have been following the notebooks, you would know that we have been calculating coverage and average length for each of these techniques. To recap, coverage measures the percentage of time the actual value fell between the intervals we predicted and average length measures the average width of the intervals we predicted. For <code class="inlineCode">level=90</code>, we expect the coverage to be around 90% or 0.9 and the average length to be as small as possible, without compromising on coverage.</p>
    <p class="normal">Let’s take a look at the following figures summarizing the coverage and average length:</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_18.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.18: Coverage for all the conformal techniques (for each unique_id), colored according to how close they are to 0.9 (we had set level=90)</p>
    <figure class="mediaobject"><img src="../Images/B22389_17_19.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.19: Average length for all the conformal techniques (for each unique_id), colored according to how small they are</p>
    <p class="normal">Here’s a quick recap<a id="_idIndexMarker1506"/> of the legend to understand the different columns, which are just a combination of these tags based on the application:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">LSTM</code>: Underlying model that we trained to get point prediction</li>
      <li class="bulletList"><code class="inlineCode">LSTM_QR</code>: Underlying Quantile Regression model we trained</li>
      <li class="bulletList"><code class="inlineCode">LSTM_Normal</code>: Underlying PDF model we trained using Normal distribution assumption</li>
      <li class="bulletList"><code class="inlineCode">CP</code>: Regular Conformal Prediction</li>
      <li class="bulletList"><code class="inlineCode">CQR</code>: Conformalized Quantile Regression</li>
      <li class="bulletList"><code class="inlineCode">CUE</code>: Conformalizing uncertainty Estimates</li>
      <li class="bulletList"><code class="inlineCode">CP_Wtd</code>: Weighted corrections over Conformal Prediction</li>
      <li class="bulletList"><code class="inlineCode">CP_Wtd_O</code>: Online Weighted corrections over Conformal Prediction</li>
      <li class="bulletList"><code class="inlineCode">ACI</code>: Adaptive Conformal Inference</li>
    </ul>
    <p class="normal">Right off the bat, we can see that the techniques that corrected for distribution shift are performing the best. There are more “greens” on the right side of both tables (closer to 0.9 coverage and lower average lengths). Remember that we started with regular conformal predictions and then corrected them for distributional shifts. We can note that for most <code class="inlineCode">unique_id</code>, regular <strong class="keyWord">Conformal Prediction</strong> has wider than necessary intervals for <code class="inlineCode">level = 90</code>. The coverages are greater than 0.9, in most cases 1.0. But both <strong class="keyWord">Weighted Correction</strong> (<strong class="keyWord">CP_Wtd</strong>) and <strong class="keyWord">Adaptive Conformal Inference</strong> (<strong class="keyWord">ACI</strong>) made<a id="_idIndexMarker1507"/> the prediction intervals <a id="_idIndexMarker1508"/>narrower and got closer to our expected level. There is no clear winner between the two and <a id="_idIndexMarker1509"/>that has to be evaluated on your dataset.</p>
    <p class="normal">And this concludes our discussion about Probabilistic Forecasts. We hope that with this, you can steer into this lesser-known territory with confidence and deliver more value to whoever you are forecasting for.</p>
    <p class="normal">Now, let’s take a very high-level look at some niche topics in time series forecasting that get very little attention, but are quite relevant in many domains.</p>
    <h1 id="_idParaDest-436" class="heading-1">Road less traveled in time series forecasting</h1>
    <p class="normal">In the spirit <a id="_idIndexMarker1510"/>of Robert Frost’s <em class="italic">The Road Not Taken</em>, this section explores the lesser-known, yet highly impactful, techniques in time series forecasting. Just as Frost chooses a path that is less traveled, we delve into niche methods that, while not mainstream, offer unique insights and potential breakthroughs in various domains.</p>
    <h2 id="_idParaDest-437" class="heading-2">Intermittent or sparse demand forecasting</h2>
    <p class="normal">Intermittent<a id="_idIndexMarker1511"/> time series forecasting<a id="_idIndexMarker1512"/> handles data with sporadic, irregular events, often seen in retail, where products may have infrequent sales. It’s crucial <a id="_idIndexMarker1513"/>for managing inventory and avoiding stockouts or overstocking, especially for slow-moving items. Traditional methods struggle with these patterns because of the fact that for most of these items, the expectation of demand falls to zero, but specialized techniques are needed to improve accuracy, making them essential for retail forecasting.</p>
    <p class="normal">Here, we will quickly name-drop a few alternative algorithms that were designed for intermittent forecasting and where you can find the implementations for it.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Croston and its friends</strong>: The <a id="_idIndexMarker1514"/>Croston model, developed in 1972, is used to forecast intermittent demand by separately estimating two components: the demand rate (when sales occur) and the time interval between sales, ignoring periods with zero sales. These estimates are combined to forecast future demand, making it useful for industries with infrequent sales. However, the model doesn’t account for external factors or changes in demand patterns, which can affect its accuracy in more complex situations. Although this <a id="_idIndexMarker1515"/>was a method developed all the way back in 1972, it has stayed with us in many forms and led to many modifications and extensions. In <em class="italic">Nixtla</em>’s <code class="inlineCode">statsforecast</code>, we can find <code class="inlineCode">CrostonClassic</code>, <code class="inlineCode">CrostonOptimized</code>, <code class="inlineCode">CrostonSBA</code>, and <code class="inlineCode">TSB</code> as four models that can be used just like the other models we saw in <em class="chapterRef">Chapter 4</em>, <em class="italic">Setting a Strong Baseline Forecast</em>.</li>
      <li class="bulletList"><strong class="keyWord">ADIDA</strong>: The <strong class="keyWord">Aggregate-Disaggregate Intermittent Demand Approach</strong> (<strong class="keyWord">ADIDA</strong>) uses <strong class="keyWord">Simple Exponential Smoothing</strong> (<strong class="keyWord">SES</strong>) combined <a id="_idIndexMarker1516"/>with temporal aggregation<a id="_idIndexMarker1517"/> to forecast intermittent demand. The method starts by aggregating demand data into non-overlapping time buckets of a size equal to the <strong class="keyWord">mean inter-demand</strong> interval (<strong class="keyWord">MI</strong>). SES is <a id="_idIndexMarker1518"/>then applied to these aggregated values to generate forecasts, which are subsequently disaggregated back to the original time scale, providing demand predictions for each time period. This model is available in <code class="inlineCode">statsforecast</code> as <code class="inlineCode">ADIDA</code>.</li>
      <li class="bulletList"><strong class="keyWord">IMAPA</strong>: The <strong class="keyWord">Intermittent Multiple Aggregation Prediction Algorithm</strong> (<strong class="keyWord">IMAPA</strong>) forecasts<a id="_idIndexMarker1519"/> intermittent time series by aggregating values at regular intervals and applying optimized <strong class="keyWord">Simple exponential smoothing</strong> (<strong class="keyWord">SES</strong>) to predict <a id="_idIndexMarker1520"/>future values. IMAPA is efficient, robust to missing data, and effective across various intermittent time series, making it a practical and easy-to-implement choice. This model is available in <code class="inlineCode">statsforecast</code> as <code class="inlineCode">IMAPA</code>.</li>
    </ul>
    <h2 id="_idParaDest-438" class="heading-2">Interpretability</h2>
    <p class="normal">We directed <a id="_idIndexMarker1521"/>you toward a few interpretability techniques for machine learning models back in <em class="chapterRef">Chapter 10</em>, <em class="italic">Global Forecasting Models</em>. While some of them, such as SHAP and LIME, can still be applied to deep learning models, none of them consider the temporal aspect by design. This is because all those techniques were developed for more general purposes, such as classification and regression. That being said, there has been some work in the interpretability of DL models and time series models. Here, I’ll list a few promising papers that tackle the temporal aspect head-on:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">TimeSHAP</strong>: This is a <a id="_idIndexMarker1522"/>model-agnostic recurrent explainer that builds<a id="_idIndexMarker1523"/> upon <strong class="keyWord">KernelSHAP</strong> and extends it to the time series domain. Research paper: <a href="https://dl.acm.org/doi/10.1145/3447548.3467166"><span class="url">https://dl.acm.org/doi/10.1145/3447548.3467166</span></a>. GitHub: <a href="https://github.com/feedzai/timeshap"><span class="url">https://github.com/feedzai/timeshap</span></a>.</li>
      <li class="bulletList"><strong class="keyWord">SHAPTime</strong>: This<a id="_idIndexMarker1524"/> technique uses the Shapley Value to provide stable explanations in the temporal dimension, improving forecasting performance. ShapTime’s model-agnostic nature allows it to be deployed on any forecasting <a id="_idIndexMarker1525"/>model at a lower cost, demonstrating significant performance improvements across various datasets. Research paper: <a href="https://link.springer.com/chapter/10.1007/978-3-031-47721-8_45"><span class="url">https://link.springer.com/chapter/10.1007/978-3-031-47721-8_45</span></a>. GitHub: <a href="https://github.com/Zhangyuyi-0825/ShapTime"><span class="url">https://github.com/Zhangyuyi-0825/ShapTime</span></a>.</li>
      <li class="bulletList"><strong class="keyWord">Instance-wise Feature Importance in Time</strong> (<strong class="keyWord">FIT</strong>): This is an interpretability technique that<a id="_idIndexMarker1526"/> relies on the distribution shift between the predictive distribution and a counterfactual, where all but the feature under inspection is unobserved. Research paper: <a href="https://proceedings.neurips.cc/paper/2020/file/08fa43588c2571ade19bc0fa5936e028-Paper.pdf"><span class="url">https://proceedings.neurips.cc/paper/2020/file/08fa43588c2571ade19bc0fa5936e028-Paper.pdf</span></a>. GitHub: <a href="https://github.com/sanatonek/time_series_explainability"><span class="url">https://github.com/sanatonek/time_series_explainability</span></a>.</li>
      <li class="bulletList"><strong class="keyWord">Time Interpret</strong>: <code class="inlineCode">time_interpret</code> is a library extending <code class="inlineCode">Captum</code>, focused on temporal data, providing<a id="_idIndexMarker1527"/> feature attribution methods to explain predictions from any PyTorch model. It includes synthetic and real-world time series datasets, various PyTorch models, and evaluation methods, with some components also applicable to language model predictions. Research paper: <a href="https://arxiv.org/abs/2306.02968"><span class="url">https://arxiv.org/abs/2306.02968</span></a>. GitHub: <a href="https://github.com/josephenguehard/time_interpret"><span class="url">https://github.com/josephenguehard/time_interpret</span></a>.</li>
    </ul>
    <p class="normal">While this is not an exhaustive list, these are a few works that we feel are important and promising. This is an area of active research, and new techniques will come up as time goes on.</p>
    <h2 id="_idParaDest-439" class="heading-2">Cold-start forecasting</h2>
    <p class="normal">Cold-start forecasting<a id="_idIndexMarker1528"/> addresses the challenge of<a id="_idIndexMarker1529"/> predicting demand for products with no historical data, a common issue in industries like retail, manufacturing, and consumer goods. It arises when launching new products, onboarding brands, or expanding into new regions. Traditional statistical forecasting models like ARIMA or exponential smoothing will not be able to tackle this as they require historical data, a good amount of it as well.</p>
    <p class="normal">But all hope is not lost. We do have some ways to handle such cases:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Manual Substitution Mapping</strong>: If we<a id="_idIndexMarker1530"/> know that a new product is substituting another product, we can do manual alignment, which moves the history of the other product to the new product and uses regular techniques to forecast.</li>
      <li class="bulletList"><strong class="keyWord">Global Machine Learning Models</strong>: We <a id="_idIndexMarker1531"/>saw how we can model multiple time series using global models in <em class="chapterRef">Chapter 10</em>. There, we used some constructed features like lags, rolling aggregations, and so on. But if we train such a model without such features that rely on history and with features characterizing the item (item features that will enable us to cross-learn with other longer time series), we can utilize cross-learning to learn the forecast from similar items. But this works best for new launches that substitute another product.</li>
      <li class="bulletList"><strong class="keyWord">Launch Profile Models</strong>: If <a id="_idIndexMarker1532"/>the new product is brand-new, we can use a very similar setup to Global Machine Learning Models but with a small tweak. We can convert our time series into a date/time agnostic manner and consider each time series to start from the date of launch. For instance, the first timestep after the launch of every product can be 1, the next can be 2, and so on. Once<a id="_idIndexMarker1533"/> we convert all our time series in this manner, we have a dataset that considers how a new item is launched and ramps up. And this can be used to train models that consider the initial ramp-up during launch.</li>
      <li class="bulletList"><strong class="keyWord">Foundational Time Series Models</strong>: Foundation<a id="_idIndexMarker1534"/> models for time series leverage large-scale pre-trained models to capture generalized patterns across various time series tasks. These models are highly effective in applications like cold start forecasting, where little or no historical data is available. By using foundational models, practitioners can apply pre-trained knowledge to new scenarios, improving accuracy in forecasting tasks across industries such as retail, healthcare, and finance. The models offer flexibility, allowing <a id="_idIndexMarker1535"/>fine-tuning or zero-shot applications, making them particularly useful for complex, sparse, or intermittent data. Many foundational models for time series are already available for use—some are commercial and some others are open source. This is a nice survey of the foundational models at the time of writing the book: Foundation Models for Time Series Analysis: A Tutorial and Survey: <a href="https://arxiv.org/abs/2403.14735"><span class="url">https://arxiv.org/abs/2403.14735</span></a>. The performance of such models is average at best, but in the absence of any data (cold-start), this presents as a good option to <a id="_idIndexMarker1536"/>try out. A few popular ways of doing this practically are:<ul>
          <li class="bulletList level-2"><em class="italic">TimeGPT</em> (Commercial) from Nixtla: <a href="https://nixtlaverse.nixtla.io/nixtla/docs/getting-started/introduction.html"><span class="url">https://nixtlaverse.nixtla.io/nixtla/docs/getting-started/introduction.html</span></a></li>
          <li class="bulletList level-2"><em class="italic">Chronos</em> from Amazon: <a href="https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-chronos.html"><span class="url">https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-chronos.html</span></a></li>
          <li class="bulletList level-2"><em class="italic">Moirai</em> from Salesforce: <a href="https://huggingface.co/Salesforce/moirai-1.0-R-large"><span class="url">https://huggingface.co/Salesforce/moirai-1.0-R-large</span></a></li>
          <li class="bulletList level-2"><em class="italic">TimesFM</em> from Google: <a href="https://github.com/google-research/timesfm"><span class="url">https://github.com/google-research/timesfm</span></a></li>
        </ul>
      </li>
    </ul>
    <h2 id="_idParaDest-440" class="heading-2">Hierarchical forecasting</h2>
    <p class="normal">Hierarchical forecasting<a id="_idIndexMarker1537"/> deals with time<a id="_idIndexMarker1538"/> series that can be disaggregated into nested levels, such as product categories or geographic regions. These structures require forecasts that maintain coherence, meaning predictions at lower levels should sum up to higher levels, reflecting the aggregation. Grouped time series, which combine different hierarchies (e.g., product type and geography), add complexity. The goal is to produce consistent, accurate forecasts that align with the data’s natural aggregation, making hierarchical forecasting essential for businesses managing large collections of time series across multiple dimensions.</p>
    <p class="normal">There are some techniques of disaggregating, aggregating, or reconciling all forecasts so that they add up in a logical manner. <em class="chapterRef">Chapter 10</em> from Rob Hyndman’s free Bible of time series forecasting (<em class="italic">Forecasting: Principles and Practice</em>) talks about this at length and is a very good<a id="_idIndexMarker1539"/> resource to get up to speed (fair warning: it’s quite math-intensive). You can find the chapter here: <a href="https://otexts.com/fpp2/hierarchical.html"><span class="url">https://otexts.com/fpp2/hierarchical.html</span></a>. For a more practical approach, you can check out Nixtla’s <code class="inlineCode">hierarchicalforecast</code> library here: <a href="https://nixtlaverse.nixtla.io/hierarchicalforecast/index.html"><span class="url">https://nixtlaverse.nixtla.io/hierarchicalforecast/index.html</span></a>.</p>
    <p class="normal">And with that, one of the longest chapters of the book comes to an end. Congrats on making it and digesting all the information. Do feel free to use the notebooks and play around with the code, different options, and so on to get a better grasp of what’s happening.</p>
    <h1 id="_idParaDest-441" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we explored different techniques for generating probabilistic forecasts like Probability Density Functions, Quantile functions, Monte Carlo Dropout, and Conformal Prediction. We deep-dived into each of them and learned how to apply them to real data. For Conformal Predictions, an actively researched field, we learned different ways to conformalize prediction intervals with different underlying mechanisms, like Conformalized Quantile Regression, conformalizing uncertainty estimates, and so on. And finally, we saw a few tweaks to make conformal prediction work even better for time series problems.</p>
    <p class="normal">To wrap it up, we also looked at a few topics that are on the road less traveled, like intermittent demand forecasting, interpretability, cold-start forecasting, and hierarchical forecasting.</p>
    <p class="normal">In the next part of this book, we will look at a few mechanics of forecasting, such as multi-step forecasting, cross-validation, and evaluation.</p>
    <h1 id="_idParaDest-442" class="heading-1">References</h1>
    <p class="normal">The following are the references for this chapter:</p>
    <ol>
      <li class="numberedList" value="1">Tony Duan, A. Avati, D. Ding, S. Basu, A. Ng, and Alejandro Schuler. (2019). <em class="italic">NGBoost: Natural Gradient Boosting for Probabilistic Prediction</em>. International Conference on Machine Learning. <a href="https://proceedings.mlr.press/v119/duan20a/duan20a.pdf"><span class="url">https://proceedings.mlr.press/v119/duan20a/duan20a.pdf</span></a>.</li>
      <li class="numberedList">Y. Gal and Zoubin Ghahramani. (2015). <em class="italic">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</em>. International Conference on Machine Learning. <a href="https://proceedings.mlr.press/v48/gal16.html"><span class="url">https://proceedings.mlr.press/v48/gal16.html</span></a>.</li>
      <li class="numberedList">Valentin Flunkert, David Salinas, and Jan Gasthaus. (2017). <em class="italic">DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks</em>. International Journal of Forecasting. <a href="https://www.sciencedirect.com/science/article/pii/S0169207019301888"><span class="url">https://www.sciencedirect.com/science/article/pii/S0169207019301888</span></a>.</li>
      <li class="numberedList">Koenker, Roger. (2005). <em class="italic">Quantile Regression</em>. Cambridge University Press. pp. 146–7. ISBN 978-0-521-60827-5. <a href="http://www.econ.uiuc.edu/~roger/research/rq/QRJEP.pdf"><span class="url">http://www.econ.uiuc.edu/~roger/research/rq/QRJEP.pdf</span></a>.</li>
      <li class="numberedList">Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos. (2020). <em class="italic">The M4 Competition: 100,000 time series and 61 forecasting methods</em>. International Journal of Forecasting. <a href="https://www.sciencedirect.com/science/article/pii/S0169207019301128"><span class="url">https://www.sciencedirect.com/science/article/pii/S0169207019301128</span></a>.</li>
      <li class="numberedList">Loic Le Folgoc and Vasileios Baltatzis and Sujal Desai and Anand Devaraj and Sam Ellis and Octavio E. Martinez Manzanera and Arjun Nair and Huaqi Qiu and Julia Schnabel and Ben Glocker. (2021). <em class="italic">Is MC Dropout Bayesian?</em>. arXiv preprint arXiv: Arxiv-2110.04286. <a href="https://arxiv.org/abs/2110.04286"><span class="url">https://arxiv.org/abs/2110.04286</span></a>.</li>
      <li class="numberedList">Nicolas Dewolf and Bernard De Baets and Willem Waegeman. (2023). <em class="italic">Valid prediction intervals for regression problems</em>. Artif. Intell. Rev. <a href="https://doi.org/10.1007/s10462-022-10178-5"><span class="url">https://doi.org/10.1007/s10462-022-10178-5</span></a>.</li>
      <li class="numberedList">V. Vovk, A. Gammerman, and G. Shafer. (2005). <em class="italic">Algorithmic learning in a random world</em>. Springer. <a href="https://link.springer.com/book/10.1007/b106715"><span class="url">https://link.springer.com/book/10.1007/b106715</span></a>.</li>
      <li class="numberedList">Anastasios N. Angelopoulos and Stephen Bates. (2021). <em class="italic">A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification</em>. arXiv preprint arXiv: Arxiv-2107.07511. <a href="https://arxiv.org/abs/2107.07511"><span class="url">https://arxiv.org/abs/2107.07511</span></a>.</li>
      <li class="numberedList">Harris Papadopoulos, Kostas Proedrou, Volodya Vovk, and Alex Gammerman. (2002). <em class="italic">Inductive confidence machines for regression</em>. Machine Learning: ECML 2002. ECML 2002. Lecture Notes in Computer Science(), vol 2430. Springer, Berlin, Heidelberg. <a href="https://doi.org/10.1007/3-540-36755-1_29"><span class="url">https://doi.org/10.1007/3-540-36755-1_29</span></a>.</li>
      <li class="numberedList">Romano, Yaniv and Patterson, Evan and Candes, Emmanuel. (2019). <em class="italic">Conformalized Quantile Regression</em>. Advances in Neural Information Processing Systems. <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/5103c3584b063c431bd1268e9b5e76fb-Paper.pdf"><span class="url">https://proceedings.neurips.cc/paper_files/paper/2019/file/5103c3584b063c431bd1268e9b5e76fb-Paper.pdf</span></a>.</li>
      <li class="numberedList">R. Barber, E. Candès, Aaditya Ramdas, and R. Tibshirani. (2022). <em class="italic">Conformal prediction beyond exchangeability</em>. Annals of Statistics. <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-51/issue-2/Conformal-prediction-beyond-exchangeability/10.1214/23-AOS2276.full"><span class="url">https://projecteuclid.org/journals/annals-of-statistics/volume-51/issue-2/Conformal-prediction-beyond-exchangeability/10.1214/23-AOS2276.full</span></a>.</li>
      <li class="numberedList">Gibbs, Isaac and Candes, Emmanuel. (2021). <em class="italic">Adaptive Conformal Inference Under Distribution Shift</em>. Advances in Neural Information Processing Systems. <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/0d441de75945e5acbc865406fc9a2559-Paper.pdf"><span class="url">https://proceedings.neurips.cc/paper_files/paper/2021/file/0d441de75945e5acbc865406fc9a2559-Paper.pdf</span></a>.</li>
    </ol>
    <h1 id="_idParaDest-443" class="heading-1">Further reading</h1>
    <p class="normal">To learn more about the topics that were covered in this chapter, take a look at the following resources.</p>
    <ul>
      <li class="bulletList"><em class="italic">The Gradient Boosters VI(B): NGBoost</em>: <a href="https://deep-and-shallow.com/2020/06/27/the-gradient-boosters-vib-ngboost/"><span class="url">https://deep-and-shallow.com/2020/06/27/the-gradient-boosters-vib-ngboost/</span></a></li>
      <li class="bulletList"><em class="italic">Dive into Deep Learning, Chapter 5.6:</em> <a href="https://d2l.ai/chapter_multilayer-perceptrons/dropout.html"><span class="url">https://d2l.ai/chapter_multilayer-perceptrons/dropout.html</span></a></li>
      <li class="bulletList"><em class="italic">Bayesian Inference by Marco Taboga</em>: <a href="https://www.statlect.com/fundamentals-of-statistics/Bayesian-inference"><span class="url">https://www.statlect.com/fundamentals-of-statistics/Bayesian-inference</span></a></li>
      <li class="bulletList"><em class="italic">Seeing Theory: Bayesian Inference</em>: <a href="https://seeing-theory.brown.edu/bayesian-inference/index.html"><span class="url">https://seeing-theory.brown.edu/bayesian-inference/index.html</span></a></li>
      <li class="bulletList"><em class="italic">A Tutorial on Sparse Gaussian Processes and Variational Inference by Felix Leibfried et al.</em>: <a href="https://arxiv.org/pdf/2012.13962"><span class="url">https://arxiv.org/pdf/2012.13962</span></a></li>
      <li class="bulletList"><em class="italic">A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification</em> <em class="italic">by Anastasios N. Angelopoulos and Stephen Bates. (2021):</em> <a href="https://arxiv.org/abs/2107.07511  "><span class="url">https://arxiv.org/abs/2107.07511</span></a></li>
    </ul>
    <p class="normal"><a href="https://arxiv.org/abs/2107.07511  "/></p>
    <h1 class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/mts"><span class="url">https://packt.link/mts</span></a></p>
    <p class="normal"><img src="../Images/QR_Code15080603222089750.png" alt=""/></p>
    <h1 class="heading-1">Leave a Review!</h1>
    <p class="normal">Thank you for purchasing this book from Packt Publishing—we hope you enjoy it! Your feedback is invaluable and helps us improve and grow. Once you’ve completed reading it, please take a moment to leave an Amazon review; it will only take a minute, but it makes a big difference for readers like you.</p>
    <p class="normal">Scan the QR or visit the link to receive a free ebook of your choice.</p>
    <p class="normal"><a href="Chapter_17.xhtml"><span class="url">https://packt.link/NzOWQ</span></a></p>
    <p class="normal"><img src="../Images/review1.jpg" alt="A qr code with black squares  Description automatically generated"/></p>
  </div>
</body></html>