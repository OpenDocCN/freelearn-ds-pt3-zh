<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer024">
			<h1 id="_idParaDest-56"><a id="_idTextAnchor056"/>Chapter 3: Data Cleansing and Integration</h1>
			<p>In the previous chapter, you were introduced to the first step of the data analytics process – that is, ingesting raw, transactional data from various source systems into a cloud-based data lake. Once we have the raw data available, we need to process, clean, and transform it into a format that helps with extracting meaningful, actionable business insights. This process of cleaning, processing, and transforming raw data is known as data cleansing and integration. This is what you will learn about in this chapter.</p>
			<p>Raw data sourced from operational systems is not conducive for data analytics in its raw format. In this chapter, you will learn about various data integration techniques, which are useful in consolidating raw, transactional data from disparate source systems and joining them to enrich them and present the end user with a single, consolidated version of the truth. Then, you will learn how to clean and transform the form and structure of raw data into a format that is ready for data analytics using data cleansing techniques. Data cleansing broadly deals with fixing inconsistencies within data, dealing with bad and corrupt data, eliminating any duplicates within data, and standardizing data to meet the enterprise data standards and conventions. You will also learn about the challenges involved in using a cloud data lake as an analytics data store. Finally, you will be introduced to a modern data storage layer called Delta Lake to overcome these challenges. </p>
			<p>This chapter will equip you with essential skills for consolidating, cleaning, and transforming raw data into a structure that is ready for analytics, as well as provide you with useful techniques for building scalable, reliable, and analytics-ready data lakes in the cloud. As a developer, the topics included in this chapter will help you give your business users access to all of their data at all times, allowing them to draw actionable insights from their raw data much faster and easier.</p>
			<p>In this chapter, the following main topics will be covered:</p>
			<ul>
				<li>Transforming raw data into enriched meaningful data</li>
				<li>Building analytical data stores using cloud data lakes </li>
				<li>Consolidating data using data integration</li>
				<li>Making raw data analytics-ready using data cleansing</li>
			</ul>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor057"/>Technical requirements</h1>
			<p>In this chapter, we will be using the Databricks Community Edition to run our code (<a href="https://community.cloud.databricks.com">https://community.cloud.databricks.com</a>). Sign-up instructions can be found at <a href="https://databricks.com/try-databricks">https://databricks.com/try-databricks</a>. </p>
			<p>The code in this chapter can be downloaded from <a href="https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter03">https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/Chapter03</a>. </p>
			<p>The datasets for this chapter can be found at <a href="https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data">https://github.com/PacktPublishing/Essential-PySpark-for-Data-Analytics/tree/main/data</a>.</p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor058"/>Transforming raw data into enriched meaningful data</h1>
			<p>Every data analytics system consists of a few key stages, including data ingestion, data transformation, and <a id="_idIndexMarker193"/>loading into a data warehouse or a data lake. Only after the data passes through these stages does it become ready for consumption by end users for descriptive and predictive analytics. There are two common industry practices for undertaking this process, widely known as <strong class="bold">Extract, Transform, Load</strong> (<strong class="bold">ETL</strong>) and <strong class="bold">Extract, Load, Transform</strong> (<strong class="bold">ELT</strong>). In this section, you will explore both these <a id="_idIndexMarker194"/>methods of data processing <a id="_idIndexMarker195"/>and understand their key differences. You will also learn about the key advantages ELT has to offer over ETL in the context of big data analytics in the cloud.</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor059"/>Extracting, transforming, and loading data</h2>
			<p>This is the typical data processing methodology that's followed by almost all data warehousing systems. In this methodology, data is extracted from the source systems and stored in a temporary <a id="_idIndexMarker196"/>storage location such as a relational database, called the staging area. Then, the data in the staging area is integrated, cleansed, and transformed before being loaded into the data warehouse. The following diagram<a id="_idIndexMarker197"/> illustrates a typical ETL process:</p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="Images/B16736_03_01.jpg" alt="Figure 3.1 – Extract, transform, and load&#13;&#10;" width="1195" height="456"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – Extract, transform, and load</p>
			<p>As shown in the previous diagram, the ETL process consists of three main stages. We will discuss these in the following sections.</p>
			<h3>Extracting data from operational systems</h3>
			<p>The ETL stage involves extracting selective, raw, transactional data from multiple source systems and staging it at a temporary storage location. This step is equivalent to the data ingestion <a id="_idIndexMarker198"/>process, which you learned about in <a href="B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a>, <em class="italic">Data Ingestion</em>. The ETL process typically processes large volumes of data, though running it directly on the source systems might put excessive load on them. Operational <a id="_idIndexMarker199"/>systems are critical to the functionality of day-to-day business functions and it is not advisable to unnecessarily tax them. Thus, the <strong class="bold">Extract</strong> process extracts data from source systems during off-business hours and stores it in a staging area. Furthermore, ETL processing can happen on the data in the staging area, leaving operational systems to handle their core functions.</p>
			<h3>Transforming, cleaning, and integrating data </h3>
			<p>This stage involves <a id="_idIndexMarker200"/>various data <a id="_idIndexMarker201"/>transformation processes such as <a id="_idIndexMarker202"/>data integration, data cleansing, joining, filtering, splitting, standardization, validation, and more. This step converts raw <a id="_idIndexMarker203"/>transactional data into a clean, integrated, and enriched version <a id="_idIndexMarker204"/>that is ready for business analytics. We will dive deeper <a id="_idIndexMarker205"/>into this stage in the <em class="italic">Consolidating data using data integration</em> and <em class="italic">Making raw data analytics ready using data cleansing</em> sections of this chapter.</p>
			<h3>Loading data into a data warehouse</h3>
			<p>This is the final stage of the ETL process, where the transformed data is finally loaded into a persistent, historical <a id="_idIndexMarker206"/>data storage layer, such as a data warehouse. Typically, ETL processing systems accomplish the <strong class="bold">Transform</strong> and <strong class="bold">Load</strong> steps <a id="_idIndexMarker207"/>in a single flow, where raw data from the staging area is cleansed, integrated, and transformed according to the business rules and loaded into a warehouse, all in a single flow.</p>
			<h3>Pros and cons of ETL and data warehousing</h3>
			<p>Some advantages of the ETL methodology are that data is transformed and loaded into a structured analytical <a id="_idIndexMarker208"/>data store such as a data warehouse, which allows for efficient and performant analysis of the data. Since the ETL paradigm has been in existence for a few decades now, there are sophisticated platforms and tools on the market that can perform ETL in a very efficient manner in a single, unified flow. </p>
			<p>Another advantage of ETL is that since data is processed before being loaded into its final storage, there is the opportunity to either omit unwanted data or obscure sensitive data. This greatly helps with data regulatory and compliance requirements.</p>
			<p>However, ETL processes run in a batch processing manner and typically run once every night. Thus, new data is only available to end users once the ETL batch process has finished successfully. This creates a dependency on data engineers to efficiently run the ETL processes, and there is a considerable delay before end users can get access to the latest data. </p>
			<p>The data in the staging area is almost entirely cleared every time before the start of the next scheduled ETL load. Also, operational systems do not typically keep a historical record of the transactional data for more than a few years. This means that end users cease to have any access to historical raw data, other than the processed data in the data warehouse. This historical raw data could prove to be very useful for certain types of data analytics such as predictive analytics, but data warehouses generally do not retain it.</p>
			<p>The ETL process evolved around data warehousing concepts and is more suited for business intelligence workloads, in an on-premises type of setting. The highly structured and somewhat rigid nature of data warehouses makes ETL not very conducive for data science and machine learning, both of which deal with a lot of unstructured data. Moreover, the batch <a id="_idIndexMarker209"/>nature of the ETL process makes it unfit for real-time analytics. Also, ETL and data warehouses do not take full advantage of the cloud and cloud-based data lakes. That's why a <a id="_idIndexMarker210"/>new methodology for data processing has emerged called <strong class="bold">Extract, Load, and Transform</strong>, or <strong class="bold">ELT</strong>, which we will take a look at in the following section.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor060"/>Extracting, loading, and transforming data</h2>
			<p>In the ELT methodology, transactional <a id="_idIndexMarker211"/>data from source systems is ingested into a data lake in its original, raw format. The ingested, raw data in the data lake is then transformed either on-demand or in a scheduled manner. In the ELT process, raw data is directly staged on the data lake and is typically never purged. As a result, data can grow enormously in size and require virtually unlimited storage and compute capacity. On-premises data warehouses and data lakes were not designed to handle data at such an enormous scale. Thus, the ELT methodology is only made possible by modern cloud technologies that offer highly scalable and elastic compute and storage resources. The following diagram depicts a typical ELT process:</p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="Images/B16736_03_02.jpg" alt="Figure 3.2 – Extract, load, and transform&#13;&#10;" width="1184" height="475"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – Extract, load, and transform</p>
			<p>In the preceding diagram, raw data is either continuously or periodically ingested from multiple source systems into a data lake. Then, the raw data in the data lake is integrated, cleaned, and transformed before being stored back inside it. The clean and aggregated data in the data lake serves as a single source <a id="_idIndexMarker212"/>of truth for all types of downstream analytics.</p>
			<p>With ELT, virtually any amount of history can be maintained, and data can be made available as soon as it is created in the source systems. There is no requirement to pre-process data before ingesting it and since data lakes do not impose any strict requirements on the format or structure of data, ELT can ingest and store all kinds of structured, unstructured, and semi-structured data. Thus, the ETL process makes all the historical raw data available so that data transformation can become completely on demand.</p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor061"/>Advantages of choosing ELT over ETL</h2>
			<p>Some of the advantages of the ELT methodology are that data can be ingested at much faster speeds since <a id="_idIndexMarker213"/>no pre-processing steps are required. It is much more flexible with the kinds of data that can be ingested, helping unlock new analytics use cases such as data science and machine learning. ETL leverages elastic storage provided by cloud data lakes, helping organizations maintain a replica of transactional data, along with virtually unlimited history. ELT, typically being cloud-based, takes away the hassle of managing data replication and archiving as most cloud providers have <a id="_idIndexMarker214"/>managed services for these and guarantee <strong class="bold">Service-Level Agreements</strong> (<strong class="bold">SLAs</strong>).</p>
			<p>The ELT methodology is quickly becoming the de facto standard for big data processing in the cloud for organizations with huge amounts of transactional data. The ELT methodology <a id="_idIndexMarker215"/>of data processing is recommended for organizations that are already in the cloud or with a future cloud strategy.</p>
			<p>However, the ELT methodology in the cloud is still very nascent, and cloud data lakes do not offer any of the transactional or reliability guarantees that their data warehousing counterparts already offer. In the next section, you will explore some of the challenges involved in building cloud-based data lakes and some ways to overcome them.</p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor062"/>Building analytical data stores using cloud data lakes</h1>
			<p>In this section, you <a id="_idIndexMarker216"/>will explore the advantages afforded by cloud-based data lakes for big data analytics systems, and then <a id="_idIndexMarker217"/>understand some of the challenges facing big data analytics systems while leveraging cloud-based data <a id="_idIndexMarker218"/>analytics systems. You will also write a few <strong class="bold">PySpark</strong> code examples to experience these challenges first-hand. </p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor063"/>Challenges with cloud data lakes</h2>
			<p>Cloud-based data lakes offer unlimited, scalable, and relatively inexpensive data storage. They are offered <a id="_idIndexMarker219"/>as managed services by the individual cloud providers and offer availability, scalability, efficiency, and lower <strong class="bold">total cost of ownership</strong>. This helps <a id="_idIndexMarker220"/>organizations accelerate their digital innovation and achieve faster time to market. However, cloud data lakes are object storages that evolved primarily to solve the problem of storage scalability. They weren't designed to store highly structured, strongly typed, analytical data. Given this, there are a few challenges in using cloud-based data lakes as analytical storage systems.</p>
			<h3>Data reliability challenges with data lakes</h3>
			<p>Data lakes are <a id="_idIndexMarker221"/>not based on any underlying filesystem but on the object storage mechanism, which manages data as objects. Object storage represents data as objects with a unique identifier and its associated metadata. Object storages weren't designed to manage frequently changing transactional data. Thus, they have a few limitations regarding analytical data stores and data processing, such as eventual consistency, lack of transactional guarantees, and more. We will look at these in the following sections.</p>
			<h4>Eventual consistency of data</h4>
			<p>Cloud-based data <a id="_idIndexMarker222"/>lakes are distributed storage systems where data storage happens across multiple machines instead of a single machine. Distributed storage systems are governed by a theorem referred to as the CAP theorem. The <strong class="bold">CAP theorem</strong> states that a distributed storage <a id="_idIndexMarker223"/>system can be tuned for only two of the three parameters of CAP; that is, consistency, availability, and partition tolerance. Not guaranteeing strong availability and partition tolerance can lead to data loss or errors, so cloud-based data lakes prioritize these two so that they're made eventually consistent. </p>
			<p>Eventual consistency means that data written to a cloud data lake might not be available instantly. This could lead to <strong class="source-inline">FileNotFound</strong> errors in a data analytics system, where downstream business analytics processes try to read data from the data lake while it is being written by an ELT process.</p>
			<h4>Lack of transactional guarantees</h4>
			<p>A typical relational database provides transactional guarantees when data is being written. This simply means that a database operation either completely succeeds or completely fails, and that any consumer trying to read the data simultaneously doesn't get any inconsistent or incorrect data because of a database operation failure. </p>
			<p>Data lakes do not provide any such atomic transactional or durability guarantees. This means that it's up to the developer to clean up and manually roll back half-written, incomplete data from any failed jobs and reprocess the data all over again.</p>
			<p>Consider the<a id="_idIndexMarker224"/> following code snippet, where we are ingesting CSV data, converting it into Parquet format, and saving it to the data lake:</p>
			<p class="source-code">(spark</p>
			<p class="source-code">   .read</p>
			<p class="source-code">     .csv("/FileStore/shared_uploads/online_retail/")</p>
			<p class="source-code">   .write</p>
			<p class="source-code">     .mode("overwrite")</p>
			<p class="source-code">     .format("parquet")</p>
			<p class="source-code">     .save("/tmp/retail.parquet")</p>
			<p class="source-code">)</p>
			<p>Here, let's try <a id="_idIndexMarker225"/>and interrupt the job halfway through to simulate a Spark job failure. Upon browsing the data lake at <strong class="source-inline">/tmp/retail.parquet</strong>, you will notice a few half-written Parquet files. Let's try and read those Parquet files via another Spark job, as shown in the following code:</p>
			<p class="source-code">(spark</p>
			<p class="source-code">   .read</p>
			<p class="source-code">     .format("parquet")</p>
			<p class="source-code">     .load("dbfs:/tmp/retail.parquet/part-00006-tid-6775149024880509486-a83d662e-809e-4fb7-beef-208d983f0323-212-1-c000.snappy.parquet")</p>
			<p class="source-code">   .count()</p>
			<p class="source-code">) </p>
			<p>In the preceding code block, we have read a Parquet file that was the result of a data ingestion job that failed halfway through. The expected result, when we try to read this data on a data store that supports atomic transactions, is that either the query yields no results or it just fails because the data is incorrect. However, in the case of the preceding Spark job, we do get a few thousand records, which is incorrect. This is because of the lack of atomic transaction guarantees on the part of Apache Spark, as well as the data lake.</p>
			<h4>Lack of schema enforcement</h4>
			<p>Data lakes, being <a id="_idIndexMarker226"/>object stores, are not concerned with the structure and schema of data and are happy to store all and any data without performing any checks to make sure that data is consistent. Apache Spark also doesn't have any built-in mechanism to enforce a user-defined schema. This results in corrupt and bad data, with mismatched data types ending up in your data lake. This reduces data quality, which is critical for end user analytical applications.  </p>
			<p>Take a look at the following<a id="_idIndexMarker227"/> code example, where we have written an initial DataFrame with a few columns. The first column is of <strong class="source-inline">IntegerType</strong>, while the second column is of <strong class="source-inline">StringType</strong>. We wrote the first DataFrame to the data lake in Parquet format. Then, we generated a second DataFrame where both columns are of <strong class="source-inline">IntegerType</strong>. Then, we tried to append the second DataFrame to the original Parquet dataset already in the data lake, as shown in the following code:</p>
			<p class="source-code">from pyspark.sql.functions import lit</p>
			<p class="source-code">df1 = spark.range(3).withColumn("customer_id", lit("1"))</p>
			<p class="source-code">(df1</p>
			<p class="source-code">   .write</p>
			<p class="source-code">     .format("parquet")</p>
			<p class="source-code">     .mode("overwrite")</p>
			<p class="source-code">   .save("/tmp/customer")</p>
			<p class="source-code">)</p>
			<p class="source-code">df2 = spark.range(2).withColumn("customer_id", lit(2))</p>
			<p class="source-code">(df2</p>
			<p class="source-code">   .write</p>
			<p class="source-code">     .format("parquet")</p>
			<p class="source-code">     .mode("append")</p>
			<p class="source-code">   .save("/tmp/customer"))</p>
			<p>The expected result on a strongly typed analytical data store such as a data warehouse should be a data type mismatch error. However, neither Apache Spark nor the data lake or the Parquet data format itself, throw an error while we try to perform this operation, and the transaction seems to complete successfully. This is undesirable as we have allowed inconsistent data to enter our data lake. However, performing a read operation on the Parquet dataset would fail with a type mismatch, which could be confusing and quite difficult to debug. This error could have easily been caught during the data loading process if data lakes or Apache Spark came with data validation support. It is important to always <a id="_idIndexMarker228"/>validate the data's correctness and consistency before making it available for business analytics because business decision-makers depend on it.</p>
			<h3>Unifying batch and streaming</h3>
			<p>One of the key requirements of modern big data analytics systems is getting access to the latest data and insights in real time. Apache Spark comes with structured streaming to handle all real-time analytics requirements. Despite stream processing, batch processing remains a key aspect of big data analytics, and Apache Spark has done quite a good job of unifying both real-time and batch analytics via its Spark SQL Engine, which <a id="_idIndexMarker229"/>acts as the core abstraction layer for both batch and streaming Spark jobs. </p>
			<p>However, data lakes do not support any level of atomic transactions or isolation between different transactions <a id="_idIndexMarker230"/>on the same table or dataset. So, something like the <strong class="bold">Lambda Architecture</strong>, which you learned about in <a href="B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a>, <em class="italic">Data Ingestion</em>, would need to be employed to unify batch and stream processing pipelines. This results in separate data processing pipelines, separate code bases, and separate tables being maintained, one for batch processing and another stream processing. This architecture of your big data analytics system is very complex to design and maintain.</p>
			<h3>Updating and deleting data</h3>
			<p>In the ELT methodology, you are continuously ingesting new data into the data lake and maintaining a replica of your source transactions inside it, along with history over a certain period. Operational systems are constantly generating transactions. However, from time to time, you must update and delete records.</p>
			<p>Consider the example of an order that's been placed by a customer at an online retailer. The transaction goes through different phases, starting with the order being placed, the order being processed, the order getting ready for shipment, the order getting shipped, the order in transit, and the order being delivered. This change in the state of the transaction must be reflected in the data lake. </p>
			<p>This process of capturing the change in the state of data is known as <strong class="bold">change data capture</strong> (<strong class="bold">CDC</strong>) and is an <a id="_idIndexMarker231"/>essential process for maintaining a replica of the operating system in the data lake. CDC requires multiple <strong class="source-inline">UPDATE</strong> and <strong class="source-inline">DELETE</strong> operations in the data lake. Data lakes are append-only systems and not designed to handle a large number of arbitrary updates and deletes. Thus, implementing arbitrary updates and deletes increases the<a id="_idIndexMarker232"/> complexity of your ELT application.</p>
			<h3>Rolling back incorrect data</h3>
			<p>Earlier, you learned that data lakes do not support any atomic transactional guarantees on write operations. It is up to the data engineer to identify the incorrect records, clean them up, and reprocess the data again for failed jobs. For smaller datasets, this cleanup process could be as simple as truncating and loading the entire dataset. However, for larger datasets at a big data scale with petabytes of data, truncating and loading data is not at all feasible. Neither data lakes nor Apache Spark has an easy rollback option, requiring the data engineer to build complex mechanisms to handle failed jobs.</p>
			<p>A new class of modern data storage formats has emerged that tries to overcome the data lake challenges mentioned in the previous section. Some examples of these technologies are Apache Hudi, Apache Iceberg, and Delta Lake. In the following section, we will explore Delta Lake and see how it can help in overcoming various data lake challenges.</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor064"/>Overcoming data lake challenges with Delta Lake</h2>
			<p>In this section, you <a id="_idIndexMarker233"/>will be introduced to Delta Lake and understand how it helps overcome some of the challenges of data lakes. You will also write a few code examples to see Delta Lake in action.</p>
			<h3>Introduction to Delta Lake</h3>
			<p><strong class="bold">Delta Lake</strong> is an open source data storage layer that helps bring reliability, ACID transactional guarantees, schema validation, and evolution to cloud-based data lakes. Delta Lake also helps in <a id="_idIndexMarker234"/>unifying batch and stream processing. Delta Lake was created by Databricks, the original creators of Apache Spark, and it was designed to be completely compatible with all Apache Spark APIs.</p>
			<p>Delta Lake is made up of a set of versioned Parquet files, along with a write-ahead log called the <strong class="bold">transaction log</strong>. The Delta Transaction Log helps enable all the features of Delta Lake. Let's dive deeper into <a id="_idIndexMarker235"/>the inner workings of Delta Transaction Log to gain a better understanding of how Delta Lake operates.</p>
			<h3>Delta Lake transaction log</h3>
			<p>The <strong class="bold">Delta transaction log</strong> is based on a popular technique that's performed on relational databases known as <strong class="bold">write-ahead logging</strong> (<strong class="bold">WAL</strong>). This technique guarantees the <a id="_idIndexMarker236"/>atomicity and durability of write operations on a database. This is achieved by recording each write operation as a transaction in the write-ahead log before <a id="_idIndexMarker237"/>any data is written to the database. The Delta Transaction Log is based on the same technique as WAL, but here, WAL, as well as the data that's been written, is in files on the data lake.</p>
			<p>Let's try to understand the Delta Transaction Log using a simple Spark job that ingests CSV data into the data lake in Delta format, as shown in the following code block:</p>
			<p class="source-code">(spark</p>
			<p class="source-code">   .read</p>
			<p class="source-code">     .option("header", True)</p>
			<p class="source-code">     .option("inferSchema", True)</p>
			<p class="source-code">     .csv("/FileStore/shared_uploads/online_retail/")</p>
			<p class="source-code">   .write</p>
			<p class="source-code">     .format("delta")</p>
			<p class="source-code">     .save("/FileStore/shared_uploads/delta/online_retail")</p>
			<p class="source-code">)</p>
			<p>The preceding code reads CSV files from the data lake, infers the schema of the underlying data, along with the header, converts the data into Delta format, and saves the data in a different location on the data lake. Now, let's explore the Delta file's location on the data lake using the following command:</p>
			<p class="source-code">%fs ls /FileStore/shared_uploads/delta/online_retail</p>
			<p>After you execute the preceding command, you will notice the folder structure of the Delta location, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="Images/B16736_03_03.jpg" alt="Figure 3.3 – Delta folder structure&#13;&#10;" width="807" height="204"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – Delta folder structure</p>
			<p>In the preceding <a id="_idIndexMarker238"/>screenshot, you can see that a Delta Lake location contains two parts: a folder named <strong class="source-inline">_delta_log</strong> and a set of Parquet files. The <strong class="source-inline">_delta_log</strong> folder contains the Delta Transaction Log's files. Let's explore the transaction log using the following command:</p>
			<p class="source-code">%fs ls dbfs:/FileStore/shared_uploads/delta/online_retail/_delta_log/</p>
			<p>The preceding command displays the contents of the <strong class="source-inline">_delta_log</strong> folder, as shown in the following screenshot: </p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="Images/B16736_03_04.jpg" alt="Figure 3.4 – Delta transaction log &#13;&#10;" width="807" height="103"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4 – Delta transaction log </p>
			<p>In the preceding screenshot, we can see that the folder contains a few different types of files. There are also a few files with the<strong class="source-inline">.json</strong> extension. These JSON files are actual Delta Transaction Log files and contain an ordered record of all the successful transactions that are performed on the Delta table.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The previously used <strong class="source-inline">%fs</strong> filesystem commands are only available on the Databricks platform. You will need to use the appropriate command to browse the data lake that's appropriate for your Spark and data lake distribution.</p>
			<p>Delta Lake transactions can be any operations that are performed on the Delta table, such as inserts, updates, and deletes, or even metadata operations such as renaming the table, changing the table <a id="_idIndexMarker239"/>schema, and so on. Every time an operation takes place, a new record is appended to the Delta Transaction Log with actions such as <strong class="bold">Add file</strong>, <strong class="bold">Remove file</strong>, <strong class="bold">Update metadata</strong>, and so on. These actions are atomic units and are recorded in the order that they took place. They are called <strong class="bold">commits</strong>. </p>
			<p>After every 10 commits, Delta Lake generates a checkpoint file in Parquet format that contains all the <a id="_idIndexMarker240"/>transactions until that point in time. These periodic Parquet checkpoint files make it fast and easy for a Spark job to read and reconstruct the table's state. This can easily be illustrated with the help of the following Spark code:</p>
			<p class="source-code">spark.read.json("/FileStore/shared_uploads/delta/online_retail/_delta_log/").show()</p>
			<p>In the preceding line of code, we read the Delta Transaction Log just like any other JSON file by using a <strong class="source-inline">spark.read()</strong> function and created a Spark DataFrame. Every time a <strong class="source-inline">spark.read()</strong> command is run on a Delta Lake table, a small Spark job is executed to read the table's state, making metadata operations on Delta Lake completely scalable.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">%fs</strong> filesystem command to explore files on a data lake is only available on the Databricks platform. You would need to choose a mechanism appropriate for your Spark environment and data lake.</p>
			<p>Now that you have an understanding of the components of Delta Lake and the inner workings of the Delta Transaction Log, let's see how Delta Lake can help solve the challenges that data lakes face.</p>
			<h3>Improving data lake reliability with Delta Lake</h3>
			<p>Delta Lake, along with its transaction log, guarantees the atomicity and durability of data written to the <a id="_idIndexMarker241"/>data lake. Delta Lake only commits a transaction to the transaction log when all the data of the operation is completely written to the data lake. Any Delta-aware consumer reading data from a Delta table will always parse the Delta Transaction Log first to get the latest state of the Delta table.</p>
			<p>This way, if the data ingestion job fails midway, a Delta Transaction Log-aware consumer will parse the transaction log, get the last stable state of the table, and only read the data that has commits in the transaction log. Any half-written, dirty data that might be in the data lake will be completely ignored because such data will not have any commits in the transaction log. Thus, Delta Lake, coupled with its transaction log, makes data lakes more reliable by providing transactional atomicity and durability guarantees.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Both data readers and data writers need to be <em class="italic">Delta Transaction Log aware</em> to get the ACID transaction guarantees of Delta Lake. Any reader or writer using Apache Spark can be made fully <em class="italic">Delta Transaction Log aware</em> by just including the appropriate version of the Delta Lake library on the Spark cluster. Delta Lake also has connectors to external data processing systems such as Presto, Athena, Hive, Redshift, and Snowflake.</p>
			<h3>Enabling schema validation with Delta Lake</h3>
			<p>Clean and consistent data is an essential requirement for any kind of business analytics application. One of the easier ways to ensure that only clean data enters the data lake is to make sure the <a id="_idIndexMarker242"/>schema is validated during the data ingestion process. Delta Lake comes with a built-in schema validation mechanism <a id="_idIndexMarker243"/>and ensures that any data being written to Delta Lake conforms to the user-defined schema of the Delta table. Let's explore this feature by creating a new Delta table and trying to insert data with mismatching data types into it, as shown in the following code:</p>
			<p class="source-code">from pyspark.sql.functions import lit</p>
			<p class="source-code">df1 = spark.range(3).withColumn("customer_id", lit("1"))</p>
			<p class="source-code">(df1</p>
			<p class="source-code">   .write</p>
			<p class="source-code">     .format("delta")</p>
			<p class="source-code">     .mode("overwrite")</p>
			<p class="source-code">   .save("/tmp/delta/customer"))</p>
			<p class="source-code">df2 = spark.range(2).withColumn("customer_id", lit(2))</p>
			<p class="source-code">(df2</p>
			<p class="source-code">   .write</p>
			<p class="source-code">     .format("delta")</p>
			<p class="source-code">     .mode("append")</p>
			<p class="source-code">   .save("/tmp/delta/customer"))</p>
			<p>In the previous code snippet, we created a Spark DataFrame named <strong class="source-inline">df1</strong> with two columns, with <strong class="source-inline">StringType</strong> as the data type for both columns. We wrote this DataFrame <a id="_idIndexMarker244"/>to the data lake using the Delta Lake format. Then, we created another Spark DataFrame named <strong class="source-inline">df2</strong>, also with two <a id="_idIndexMarker245"/>columns, but their data types were set to <strong class="source-inline">LongType</strong> and <strong class="source-inline">IntegerType</strong>. </p>
			<p>Next, we tried to append the second DataFrame to the original Delta table. As expected, Delta Lake fails the operation and throws a <em class="italic">Failed to merge incompatible data types StringType and IntegerType</em> exception. This way, Delta Lake ensures data quality in data lakes by providing schema validation and enforcement during data ingestion.</p>
			<h3>Schema evolution support with Delta Lake</h3>
			<p>Another common use<a id="_idIndexMarker246"/> case during data ingestion and the ELT process is that the source schema might evolve from time to time and that it <a id="_idIndexMarker247"/>needs to be handled in the data lake. One such scenario is that new columns could be added to the source system tables. It is desirable to bring those new columns into our data lake table, without it affecting our already existing data. This process is generally known as <strong class="bold">schema evolution</strong>, and Delta Lake has <a id="_idIndexMarker248"/>built-in support for this. Let's explore schema evolution in Delta Lake with the following code example:</p>
			<p class="source-code">from pyspark.sql.functions import lit</p>
			<p class="source-code">df1 = spark.range(3)</p>
			<p class="source-code">(df1</p>
			<p class="source-code">   .write</p>
			<p class="source-code">     .format("delta")</p>
			<p class="source-code">     .mode("overwrite")</p>
			<p class="source-code">   .save("/tmp/delta/customer"))</p>
			<p class="source-code">df2 = spark.range(2).withColumn("customer_id", lit(2))</p>
			<p class="source-code">(df2</p>
			<p class="source-code">   .write</p>
			<p class="source-code">     .format("delta")</p>
			<p class="source-code">     .option("mergeSchema", True)</p>
			<p class="source-code">     .mode("append")</p>
			<p class="source-code">   .save("/tmp/delta/customer"))</p>
			<p>In the preceding code snippet, we created a Spark DataFrame named <strong class="source-inline">df1</strong> that has just one column labeled <strong class="source-inline">id</strong>. Then, we saved this DataFrame to the data lake in Delta Lake format. Then, we created a <a id="_idIndexMarker249"/>second Spark DataFrame named <strong class="source-inline">df2</strong> with<a id="_idIndexMarker250"/> two columns called <strong class="source-inline">id</strong> and <strong class="source-inline">customer_id</strong>. After, we appended the second DataFrame to the original Delta table that was created from <strong class="source-inline">df1</strong>. This time, we used the <strong class="source-inline">mergeSchema</strong> option. This <strong class="source-inline">mergeSchema</strong> option specifies that we are expecting new columns to be written to Delta Lake, and these need to be appended to the existing table. We can easily verify this by running the following command on the Delta table:</p>
			<p class="source-code">spark.read.format("delta").load("/tmp/delta/customer").show()</p>
			<p>In the previous code block, we are loading the data in the Delta table into a Spark DataFrame and calling the <strong class="source-inline">show()</strong> action to display the contents of the DataFrame, as shown in the following figure:</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="Images/B16736_03_05.jpg" alt="Figure 3.5 – Delta Lake schema evolution&#13;&#10;" width="276" height="326"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5 – Delta Lake schema evolution</p>
			<p>As you can see, new <strong class="bold">id</strong> column values, along with their corresponding <strong class="bold">customer_id</strong> column values, were <a id="_idIndexMarker251"/>inserted into the Delta table, and all the old <strong class="bold">id</strong> values<a id="_idIndexMarker252"/> with no previous <strong class="bold">column_id</strong> values were marked as null. With <strong class="source-inline">mergeSchema</strong> enabled, Delta Lake automatically adds the new column to the existing table and marks the values of the rows that did not exist previously as <strong class="source-inline">null</strong> values. </p>
			<h3>Arbitrary updates and deletes in data lakes with Delta Lake</h3>
			<p>Transactions not only get inserted into operating systems – they are also updated and deleted from time to time. In the ELT process, a replica of the source system data is maintained in the data lake. Thus, it becomes <a id="_idIndexMarker253"/>necessary to be able to not only insert data into data lakes but also update and delete it. However, data <a id="_idIndexMarker254"/>lakes are append-only storage systems with minimal or no support for any updates or deletes. Delta Lake, however, has full support for inserting, updating, and deleting records.</p>
			<p>Let's take a look at an example of how we can update and delete arbitrary data from Delta Lake, as shown in the following block of code:</p>
			<p class="source-code">from pyspark.sql.functions import lit</p>
			<p class="source-code">df1 = spark.range(5).withColumn("customer_id", lit(2))</p>
			<p class="source-code">df1.write.format("delta").mode("overwrite").save("/tmp/df1")</p>
			<p>In the <a id="_idIndexMarker255"/>preceding block of code, we created a Spark DataFrame with two columns labeled <strong class="source-inline">id</strong> and <strong class="source-inline">customer_id</strong>. <strong class="source-inline">id</strong> has values ranging from 1 through 5. We saved this table to the data lake using the Delta Lake <a id="_idIndexMarker256"/>format. Now, let's update the <strong class="source-inline">customer_id</strong> column where values of the <strong class="source-inline">id</strong> column are greater than <strong class="source-inline">2</strong>, as shown in the following code block:</p>
			<p class="source-code">%sql</p>
			<p class="source-code">UPDATE delta.`/tmp/df1` SET customer_id = 5 WHERE id &gt; 2;</p>
			<p class="source-code">SELECT * FROM delta.`/tmp/df1`;</p>
			<p>In the preceding code block, we updated the <strong class="source-inline">customer_id</strong> column using an <strong class="source-inline">UPDATE</strong> SQL clause and specified the condition via a <strong class="source-inline">WHERE</strong> clause, just as you would do on any RDBMS.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The <strong class="source-inline">%sql</strong> magic command specifies that we intend to execute SQL queries in the current notebook cell. Even though we did not explicitly create a table, we can still refer to the Delta Lake location as a table using the <strong class="source-inline">delta.`path-to-delta-table`</strong> syntax. </p>
			<p>The second SQL query reads the data back from the Delta table and displays it using the <strong class="source-inline">SELECT</strong> SQL clause, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="Images/B16736_03_06.jpg" alt="Figure 3.6 – Updates with Delta Lake&#13;&#10;" width="529" height="325"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6 – Updates with Delta Lake</p>
			<p>Here, we can verify that all the rows of the Delta table with the value of the <strong class="source-inline">id</strong> column greater than <strong class="source-inline">2</strong> were successfully updated. Thus, Delta Lake has full support for updating multiple <a id="_idIndexMarker257"/>arbitrary records at scale <a id="_idIndexMarker258"/>with a simple SQL-like syntax.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">A Delta table's metadata is entirely stored in the Delta Transaction Log itself. This makes registering Delta <a id="_idIndexMarker259"/>tables with an external <strong class="bold">metastore</strong> such as <strong class="bold">Hive</strong> completely optional. This makes it easier <a id="_idIndexMarker260"/>to just save the Delta table to the data lake and use it via Spark's DataFrame and SQL APIs seamlessly.</p>
			<p>Now, let's see how Delta supports deletes with the help of the following block of code:</p>
			<p class="source-code">%sql</p>
			<p class="source-code">DELETE FROM delta.`/tmp/df1` WHERE id = 4;</p>
			<p class="source-code">SELECT * FROM delta.`/tmp/df1`;</p>
			<p>In the preceding code snippet, we used the <strong class="source-inline">DELETE</strong> command to delete all the records that have an <strong class="source-inline">id</strong> of value <strong class="source-inline">4</strong>. The second query, where we used the <strong class="source-inline">SELECT</strong> clause, displays the contents of the Delta table after the <strong class="source-inline">DELETE</strong> operation, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="Images/B16736_03_07.jpg" alt="Figure 3.7 – Deletes with Delta Lake&#13;&#10;" width="531" height="243"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7 – Deletes with Delta Lake</p>
			<p>Here, we can easily verify that we no longer have any rows with an <strong class="source-inline">id</strong> value of <strong class="source-inline">4</strong>. Thus, Delta Lake <a id="_idIndexMarker261"/>also supports deleting arbitrary records at scale.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Delta Lake supports both SQL and DataFrame syntax for DELETES, UPDATES, and UPSERTS. A syntax reference can be found in the open source documentation, which is maintained at <a href="https://docs.delta.io/latest/delta-update.html#table-deletes-updates-and-merges">https://docs.delta.io/latest/delta-update.html#table-deletes-updates-and-merges</a>.</p>
			<p>Even <strong class="source-inline">DELETE</strong> and <strong class="source-inline">UPDATE</strong> operations on Delta Lake support the same transactional guarantees of atomicity and durability as write operations. However, an interesting thing to note <a id="_idIndexMarker262"/>is that every time a <strong class="source-inline">DELETE</strong> or <strong class="source-inline">UPDATE</strong> operation takes place, instead of updating or deleting any data in place, Delta Lake generates a brand-new file with the updated or deleted records and appends these new files to the existing Delta table. Then, Delta Lake creates a new <strong class="bold">commit</strong> for this write transaction in the transaction log and marks the older <strong class="bold">commits</strong> of the deleted or updated records as invalid.</p>
			<p>Thus, Delta Lake is never actually deleting or updating the actual data files in the data lake; it is just appending new files for any operation and updating the transaction log. Updating a smaller set of transaction log files is much faster and more efficient than updating a large number of very large data files. This process of updating and deleting records with Delta Lake is ultra-efficient and can be scaled to petabytes of data. This feature is very useful for use cases where customer arbitrary records need to be identified and deleted, such as in GDPR compliance use cases.</p>
			<p>One more interesting side effect of this technique of always appending data files and never deleting <a id="_idIndexMarker263"/>them is that Delta <a id="_idIndexMarker264"/>Lake maintains a historical audit record of all the changes that happen to the data. This audit log is maintained in the <strong class="bold">Delta transaction log</strong> and with its help, Delta Lake can travel back in time to <a id="_idIndexMarker265"/>reproduce a snapshot of a Delta table at that point. We will explore this feature in the next section. </p>
			<h3>Time travel and rollbacks with Delta Lake</h3>
			<p>Delta Lake keeps an <a id="_idIndexMarker266"/>audit log of how data has changed over time in its transaction log. It also maintains older versions of Parquet data files every time data <a id="_idIndexMarker267"/>changes. This gives Delta Lake the ability to reproduce a snapshot of<a id="_idIndexMarker268"/> the entire Delta table at that point. This feature is called <strong class="bold">Time Travel</strong>.</p>
			<p>You can easily explore the audit trail of a Delta table using the following SQL query:</p>
			<p class="source-code">%sql DESCRIBE HISTORY delta.`/tmp/df1`</p>
			<p>In the preceding Spark SQL query, we used the <strong class="source-inline">DESCRIBE HISTORY</strong> command to reproduce the entire audit log of the changes that happened to the Delta table, as shown here:</p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="Images/B16736_03_08.jpg" alt="Figure 3.8 – Time Travel with Delta Lake&#13;&#10;" width="800" height="169"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8 – Time Travel with Delta Lake</p>
			<p>In the preceding screenshot, you can see that this Delta table changed three times. First, data was inserted into the table, then the table was updated, and then records were deleted from the table. Delta Lake records all these events as transactions called <strong class="bold">commits</strong>. The timestamp of the commit event version number is also recorded in the change audit log. The timestamp or the table version number can be used to travel back in time to a particular snapshot of the Delta table using a SQL query, as follows:</p>
			<p class="source-code">%sql SELECT * from delta.`/tmp/delta/df1` VERSION AS OF 0</p>
			<p>In the preceding SQL query, we performed a Delta Time Travel to the original version of the table. Time Travel <a id="_idIndexMarker269"/>is very useful during data engineering and ELT <a id="_idIndexMarker270"/>processing for performing rollbacks on tables if a data ingestion process fails. Delta Time Travel can be used to restore a Delta table to a previous state, as shown here:</p>
			<p class="source-code">%sql </p>
			<p class="source-code">INSERT OVERWRITE delta.`/tmp/df1`</p>
			<p class="source-code">SELECT * from delta.`/tmp/df1` VERSION AS OF 0</p>
			<p>In the preceding SQL query, we overwrote the Delta table using a snapshot from a previous version of the table, all while making use of the <strong class="bold">Delta Time Travel</strong> feature.</p>
			<p>Another scenario where Delta Time Travel comes in handy is in data science and machine learning use cases. Data scientists often conduct multiple machine learning experiments by modifying the dataset that's used for experimentation. In the process, they end up maintaining multiple physical versions of the same dataset or table. Delta Lake can help eliminate these physical versions of tables with the help of Time Travel, since Delta has built-in data versioning. You will explore this technique in more detail in <a href="B16736_09_Final_JM_ePub.xhtml#_idTextAnchor164"><em class="italic">Chapter 9</em></a>, <em class="italic">Machine Learning Life Cycle Management</em>.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Delta continues to maintain versions of Parquet data files with every operation that mutates data. This means that older versions of data files get accumulated and Delta Lake doesn't automatically delete them. This could lead to a considerable increase in the size of the data lake over a while. To overcome this scenario, Delta Lake provides a <strong class="source-inline">VACUUM</strong> command to permanently remove older files that are no longer referenced by the Delta table. More information <a id="_idIndexMarker271"/>regarding the <strong class="source-inline">VACUUM</strong> command can be found at <a href="https://docs.delta.io/latest/delta-utility.html#vacuum">https://docs.delta.io/latest/delta-utility.html#vacuum</a>.</p>
			<h3>Unifying batch and stream processing using Delta Lake</h3>
			<p>Batch and real-time stream processing are essential components of any modern big data architecture. In <a href="B16736_02_Final_JM_ePub.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a>, <em class="italic">Data Ingestion</em>, you learned how to use Apache Spark for batch and real-time data ingestion. You also learned about the Lambda Architecture, which you can use <a id="_idIndexMarker272"/>to implement simultaneous batch and stream processing. An implementation of the Lambda Architecture with <a id="_idIndexMarker273"/>Apache Spark is still relatively complex as two separate data processing pipelines need to be implemented for batch and real-time processing. </p>
			<p>This complexity <a id="_idIndexMarker274"/>arises from the limitation of data lakes as they inherently do not provide any transactional, atomicity, or durability guarantees on write operations. Thus, batch and streaming processes cannot write data to the <a id="_idIndexMarker275"/>same table or location on the data lake. Since Delta Lake already solves this challenge of data lakes, a single Delta Lake can be used in conjunction with multiple batch and real-time pipelines, further simplifying the Lambda Architecture. You will explore this in more detail in <a href="B16736_04_Final_JM_ePub.xhtml#_idTextAnchor075"><em class="italic">Chapter 4</em></a><em class="italic">,</em> <em class="italic">Real-Time Data Analytics</em>.</p>
			<p>In summary, in this section, you learned that data lakes are instrumental in enabling truly scalable big data processing systems. However, they weren't built to be data analytics storage systems and have a few shortcomings, such as a lack of ACID transactional guarantees, as well as the ability to support the process of updating or deleting records, preserving data quality schema enforcement, or unification of batch and stream processing. You also learned how modern data storage layers such as Delta Lake can help overcome the challenges of data lakes and bring them closer to being true data analytics storage systems.</p>
			<p>Now that you have an understanding of how to make cloud-based data lakes more reliable and conducive for data analytics, you are ready to learn about the process of transforming raw transactional data into meaningful business insights. We will start by consolidating data from various disparate sources and creating a single, unified view of data.</p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor065"/>Consolidating data using data integration</h1>
			<p><strong class="bold">Data integration</strong> is an important step in both the ETL and ELT modes of data processing. Data integration <a id="_idIndexMarker276"/>is the process of combining and blending data from different data sources to create enriched data that happens to represent a single version of the truth. Data integration is different from data ingestion because data ingestion simply collects data from disparate sources and brings it to a central location, such as a data warehouse. On the <a id="_idIndexMarker277"/>other hand, data integration combines those disparate data sources to create a meaningful unified version of the data that represents all the dimensions of the data. There are multiple ways to perform data integration, and a few of them will be explored in this section.</p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor066"/>Data consolidation via ETL and data warehousing</h2>
			<p>Extracting, transforming, and loading data into data warehouses has been the best technique of data <a id="_idIndexMarker278"/>integration over the last few decades. One of the primary goals of data consolidation is to reduce the number of storage locations where the data resides. The <a id="_idIndexMarker279"/>ETL process extracts data from various source<a id="_idIndexMarker280"/> systems and then joins, filters, cleanses, and transforms the data according to user-specified business rules and then loads it into a central data warehouse. </p>
			<p>This way, ETL and data warehousing techniques, as well as the tools and technologies that have been purposely built for this, support data consolidation and data integration. Although ELT is a slightly different process than ETL and with Apache Spark, we intend to build a data lake, the techniques of data integration and data consolidation remain the same, even with ETL.</p>
			<p>Let's implement a data integration process using PySpark. As a first step, upload all the datasets provided with this chapter to a location where they can be accessed by your Spark cluster. In the case of Databricks Community Edition, the datasets can be directly uploaded to the data lake from within the <strong class="bold">File</strong> menu of the notebook. The links for the datasets and code files can be found in the <em class="italic">Technical requirements</em> section at the beginning of this chapter.</p>
			<p>Let's explore the schema of the two transactional datasets labeled <strong class="source-inline">online_retail.csv</strong> and <strong class="source-inline">online_retail_II.csv</strong> using the following block of code:</p>
			<p class="source-code">from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, StringType, DoubleType</p>
			<p class="source-code">schema = (StructType()</p>
			<p class="source-code">            .add("InvoiceNo", StringType(), True)</p>
			<p class="source-code">            .add("StockCode", StringType(), True)</p>
			<p class="source-code">            .add("Description", StringType(), True)</p>
			<p class="source-code">            .add("Quantity", StringType(), True)</p>
			<p class="source-code">            .add("InvoiceDate", StringType(), True)</p>
			<p class="source-code">            .add("UnitPrice", StringType(), True)</p>
			<p class="source-code">            .add("CustomerID", StringType(), True)</p>
			<p class="source-code">            .add("Country", StringType(), True))</p>
			<p class="source-code">df1 = spark.read.schema(schema).option("header", True).csv("dbfs:/FileStore/shared_uploads/online_retail/online_retail.csv")</p>
			<p class="source-code">df2 = spark.read.schema(schema).option("header", True).csv("dbfs:/FileStore/shared_uploads/online_retail/online_retail_II.csv")</p>
			<p class="source-code">df1.printSchema()</p>
			<p class="source-code">df2.printSchema()</p>
			<p>In the preceding code snippet, we did the following:</p>
			<ol>
				<li>We defined <a id="_idIndexMarker281"/>the schema of a Spark DataFrame as a <strong class="source-inline">StructType</strong> consisting of multiple StructFields. PySpark comes with <a id="_idIndexMarker282"/>these built-in structures to programmatically define the schema of a DataFrame.</li>
				<li>Then, we loaded the two CSV files into separate Spark DataFrames while using the <strong class="source-inline">schema</strong> option to specify the data schema we created during <em class="italic">Step 1</em>. We still specified the header option as <strong class="source-inline">True</strong> because the first line of the CSV file has a header defined and we need to ignore it.</li>
				<li>Finally, we printed the schema information of the two Spark DataFrames we created in <em class="italic">Step 2</em>.</li>
			</ol>
			<p>Now that we have the retail datasets from the CSV files loaded into Spark DataFrames, let's integrate them into a single dataset, as shown in the following lines of code:</p>
			<p class="source-code">retail_df = df1.union(df2)</p>
			<p class="source-code">retail_df.show()</p>
			<p>In the preceding <a id="_idIndexMarker283"/>code, we simply combined the two Spark DataFrames containing online retail transactional data to create a single Spark DataFrame <a id="_idIndexMarker284"/>by using the <strong class="source-inline">union()</strong> function. The union operation combines the two distinct DataFrames into a single DataFrame. The resultant consolidated dataset is labeled <strong class="source-inline">retail_df</strong>. We can verify the results using the <strong class="source-inline">show()</strong> function.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The <strong class="source-inline">union()</strong> function is a transformation and thus lazily evaluated. This means that as soon as <a id="_idIndexMarker285"/>you call a <strong class="source-inline">union()</strong> on two Spark DataFrames, Spark checks to see if the two DataFrames have the same number of columns and that their data types match. It doesn't manifest the DataFrames into memory yet. The <strong class="source-inline">show()</strong> function is an action, so Spark processes the transformations and manifests data in memory. However, the <strong class="source-inline">show()</strong> function only works on a small number of the DataFrame <a id="_idIndexMarker286"/>partitions and returns a sample set of the results to Spark Driver. Thus, this action helps verify our code quickly. </p>
			<p>Next, we have some data describing country codes and names stored in the <strong class="source-inline">country_codes.csv</strong> file. Let's integrate it with the <strong class="source-inline">retail_df</strong> DataFrame we created in the previous step by using the following block of code:</p>
			<p class="source-code">df3 = spark.read.option("header", True).option("delimiter", ";").csv("/FileStore/shared_uploads/countries_codes.csv")</p>
			<p class="source-code">country_df = (df3</p>
			<p class="source-code">   .withColumnRenamed("OFFICIAL LANG CODE", "CountryCode")</p>
			<p class="source-code">   .withColumnRenamed("ISO2 CODE", "ISO2Code")</p>
			<p class="source-code">   .withColumnRenamed("ISO3 CODE", "ISO3Code")</p>
			<p class="source-code">   .withColumnRenamed("LABEL EN", "CountryName")</p>
			<p class="source-code">   .withColumnRenamed("Geo Shape", "GeoShape")</p>
			<p class="source-code">   .drop("ONU CODE")</p>
			<p class="source-code">   .drop("IS ILOMEMBER")</p>
			<p class="source-code">   .drop("IS RECEIVING QUEST")</p>
			<p class="source-code">   .drop("LABEL FR")</p>
			<p class="source-code">   .drop("LABEL SP")</p>
			<p class="source-code">   .drop("geo_point_2d")</p>
			<p class="source-code">)</p>
			<p class="source-code">integrated_df = retail_df.join(country_df, retail_df.Country == country_df.CountryName, "left_outer")</p>
			<p>In the previous <a id="_idIndexMarker287"/>code snippet, we did the following:</p>
			<ol>
				<li value="1">We loaded the <strong class="source-inline">country_codes.csv</strong> file into a Spark DataFrame, with the <strong class="source-inline">header</strong> option set to <strong class="source-inline">True</strong> and the file delimiter specified as <strong class="source-inline">";"</strong>.</li>
				<li>We renamed <a id="_idIndexMarker288"/>a few column names to follow standard naming conventions using the <strong class="source-inline">withColumnRenamed()</strong> function. We dropped a few other columns that we thought were not necessary for any of our business use cases. This resulted in a DataFrame labeled <strong class="source-inline">country_df</strong> that contains the country code and other descriptive columns.</li>
				<li>Then, we joined this DataFrame to the <strong class="source-inline">retail_df</strong> DataFrame from the previous step. We used a <strong class="bold">left outer join</strong> here because we wanted to preserve all the rows from the <strong class="source-inline">retail_df</strong> DataFrame, irrespective of whether they have a matching record in the <strong class="source-inline">country_df</strong> DataFrame.</li>
				<li>The resultant <strong class="source-inline">integrated_df</strong> DataFrame contains online retail transactional data that's been enriched with descriptive columns from the <strong class="source-inline">country_codes.csv</strong> dataset.</li>
			</ol>
			<p>We also have another dataset named <strong class="source-inline">adult.data</strong> that contains the income dataset from the US census. Let's integrate this dataset with the already integrated and enriched retail transactional dataset, as shown in the following lines of code:</p>
			<p class="source-code">from pyspark.sql.functions import monotonically_increasing_id</p>
			<p class="source-code">income_df = spark.read.schema(schema).csv("/FileStore/shared_uploads/adult.data").withColumn("idx", monotonically_increasing_id())</p>
			<p class="source-code">retail_dfx = retail_df.withColumn("CustomerIDx", monotonically_increasing_id())</p>
			<p class="source-code">income_dfx = income_df.withColumn("CustomerIDx", monotonically_increasing_id()) </p>
			<p class="source-code">income_df = spark.read.schema(schema).csv("/FileStore/shared_uploads/adult.data").withColumn("idx", monotonically_increasing_id())</p>
			<p class="source-code">retail_dfx = integrated_df.withColumn("RetailIDx", monotonically_increasing_id())</p>
			<p class="source-code">income_dfx = income_df.withColumn("IncomeIDx", monotonically_increasing_id()) </p>
			<p class="source-code">retail_enriched_df = retail_dfx.join(income_dfx, retail_dfx.RetailIDx == income_dfx.IncomeIDx, "left_outer")</p>
			<p>In the previous code snippet, we did the following:</p>
			<ol>
				<li value="1">We created a <a id="_idIndexMarker289"/>Spark DataFrame from the income dataset using the <strong class="source-inline">spark.read.csv()</strong> function. This is a comma-delimited file <a id="_idIndexMarker290"/>with a header, so we used the appropriate options. As a result, we have a DataFrame called <strong class="source-inline">income_df</strong>, with a few columns related to consumer demographics and their income levels.</li>
				<li>Then, we added two <strong class="bold">surrogate key</strong> columns to both the <strong class="source-inline">income_df</strong> and <strong class="source-inline">integrated_df</strong> DataFrames so that they can be joined. We achieved this using the <strong class="source-inline">monotonically_increasing_id()</strong> function, which generates unique incremental numbers.</li>
				<li>The two DataFrames were then joined based on the newly generated <strong class="bold">surrogate key</strong> columns. We used a <strong class="bold">left outer join</strong> as we intended to preserve all the rows of the <strong class="source-inline">integrated_df</strong> DataFrame, regardless of whether they have corresponding matching rows in the <strong class="source-inline">income_df</strong> DataFrame. The result is integrated, enriched, retail transactional data, with the country and the <a id="_idIndexMarker291"/>customer demographic and income information in a single, unified dataset.</li>
			</ol>
			<p>This intermediate dataset can be useful for performing <strong class="bold">data cleansing</strong> and producing further clean and <a id="_idIndexMarker292"/>consistent dataset. So, let's persist this dataset on the data <a id="_idIndexMarker293"/>lake using the Delta Lake format with the name <strong class="source-inline">retail_enriched.delta</strong>, as shown in the following code:</p>
			<p class="source-code">(retail_enriched_df</p>
			<p class="source-code">   .coalesce(1)</p>
			<p class="source-code">   .write</p>
			<p class="source-code">     .format("delta", True)</p>
			<p class="source-code">     .mode("overwrite")</p>
			<p class="source-code">    .save("/FileStore/shared_uploads/retail.delta"))</p>
			<p>In the previous code block, we reduced the number of partitions of the <strong class="source-inline">retailed_enriched_df</strong> DataFrame to a single partition using the <strong class="source-inline">coalesce()</strong> function. This produces a single portable Parquet file.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">One of the biggest challenges with learning and experimenting with big data analytics is finding clean and useful datasets. In the preceding code example, we had to introduce a surrogate key to join two independent datasets. In real-world scenarios, you would never force a join between datasets unless the datasets are related and a common join key exists between them. </p>
			<p>Thus, using Spark's DataFrame operations or using Spark SQL, you can integrate data from disparate sources and create an enriched and meaningful dataset that represents a single version of the truth.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor067"/>Integrating data using data virtualization techniques</h2>
			<p><strong class="bold">Data virtualization</strong>, as the name implies, is a virtual process where a data virtualization layer <a id="_idIndexMarker294"/>acts as a logical layer on top of all the disparate data sources. This virtual layer acts as a conduit for business users to seamlessly access the required data in real time. The advantage data virtualization has over the traditional <strong class="bold">ETL</strong> and <strong class="bold">ELT</strong> processes is that it doesn't require any data movement, and just <a id="_idIndexMarker295"/>exposes an integrated <a id="_idIndexMarker296"/>view of data to business users. When business users try to access the data, the data virtualization layer queries the underlying datasets and retrieves data in real time. </p>
			<p>The advantage of the data virtualization layer is that it completely bypasses any data movement, saving any <a id="_idIndexMarker297"/>time and resources that would typically be invested in this process. It can present data in real time with minimal to no latency as it directly fetches data from the source systems. </p>
			<p>The disadvantage of data virtualization is that it is not a widely adopted technique and the products that do <a id="_idIndexMarker298"/>offer it come at a premium price. Apache Spark doesn't support data virtualization in its purest sense. However, Spark does <a id="_idIndexMarker299"/>support a type of data virtualization technique called <strong class="bold">data federation</strong>, which you will learn about in the next section.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor068"/>Data integration through data federation</h2>
			<p><strong class="bold">Data federation</strong> is a type of data virtualization technique that uses a virtual database, also called a federated database, to provide a consolidated and homogeneous view of heterogeneous data sources. The idea here is to access any data anywhere from a single data processing and <a id="_idIndexMarker300"/>metadata layer. Apache Spark SQL Engine supports data federation, where Spark data sources can be used to define external data sources for seamless access from within Spark SQL. With Spark SQL, multiple data sources can be used with a single SQL query, without you having to consolidate and transform the datasets first. </p>
			<p>Let's take a look at a code example to learn how to achieve data federation with Spark SQL:</p>
			<p class="source-code">%sql</p>
			<p class="source-code">CREATE TABLE mysql_authors IF NOT EXISTS</p>
			<p class="source-code">USING org.apache.spark.sql.jdbc</p>
			<p class="source-code">OPTIONS (</p>
			<p class="source-code">  url "jdbc:mysql://localhost:3306/pysparkdb",</p>
			<p class="source-code">  dbtable "authors",</p>
			<p class="source-code">  user "@@@@@@",</p>
			<p class="source-code">  password "######"</p>
			<p class="source-code">);</p>
			<p>In the previous block of code, we created a table with MySQL as the source. Here, the table we created with Spark is just a pointer to the actual table in MySQL. Every time this Spark table is <a id="_idIndexMarker301"/>queried, it fetches data from the underlying MySQL table over a JDBC connection. Let's create another table from a Spark DataFrame and save it in CSV format, as follows:</p>
			<p class="source-code">from pyspark.sql.functions import rand, col</p>
			<p class="source-code">authors_df = spark.range(16).withColumn("salary", rand(10)*col("id")*10000)</p>
			<p class="source-code">authors_df.write.format("csv").saveAsTable("author_salary")</p>
			<p>In the preceding code block, we generated a Spark Dataframe with 16 rows and 2 columns. The first column, labeled <strong class="source-inline">id</strong>, is just an incremental number, while the second column, labeled <strong class="source-inline">salary</strong>, is a random number that was generated using the built-in <strong class="source-inline">rand()</strong> function. We saved the DataFrame to the data lake and registered it with Spark's built-in Hive metastore using the <strong class="source-inline">saveAsTable()</strong> function. Now that we have two tables, each residing in a separate data source, let's see how we can use them together in a federated query via Spark SQL, as shown in the following code:</p>
			<p class="source-code">%sql</p>
			<p class="source-code">SELECT</p>
			<p class="source-code">  m.last_name,</p>
			<p class="source-code">  m.first_name,</p>
			<p class="source-code">  s.salary</p>
			<p class="source-code">FROM</p>
			<p class="source-code">  author_salary s</p>
			<p class="source-code">  JOIN mysql_authors m ON m.uid = s.id</p>
			<p class="source-code">ORDER BY s.salary DESC</p>
			<p>In the preceding SQL query, we joined the MySQL table to the CSV table residing on the data lake <a id="_idIndexMarker302"/>in the same query to produce an integrated view of data. This has demonstrated the data federation capabilities of Apache Spark.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Certain specialist data processing engines are designed purely to be federated databases, such as Presto. Presto is a distributed Massively Parallel Processing (MPP) query engine for big data that was designed to give very fast query performance on any data, anywhere. One advantage of using Apache Spark over Presto is that it supports data federation, along with other use cases such as batch and real-time analytics, data science, machine learning, and interactive SQL analytics, all with a single unified engine. This makes the user experience much more seamless. However, it is also very common for organizations to leverage several big data technologies for different use cases.</p>
			<p>To summarize, data integration is the process of consolidating and combining data from disparate data sources to produce meaningful data that gives a single version of the truth. There are several techniques surrounding data integration, including consolidating data using ETL or ELT techniques and data federation. In this section, you learned how to leverage these techniques using Apache Spark to achieve an integrated view of your data. The next step <a id="_idIndexMarker303"/>of your data analytics journey is to learn how to clean messy and dirty data via a process called <strong class="bold">data cleansing</strong>. </p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor069"/>Making raw data analytics-ready using data cleansing </h1>
			<p>Raw transactional data can have many kinds of inconsistencies, either inherent to the data itself or developed during movement between various data processing systems, during the data <a id="_idIndexMarker304"/>ingestion process. The data integration process can also introduce inconsistencies in data. This is because data <a id="_idIndexMarker305"/>is being consolidated from disparate systems with their own mechanism for data representation. This data is not very clean, can have a few bad and corrupt records, and needs to be cleaned before it is ready to generate meaningful business insights using a process known as <strong class="bold">data cleansing</strong>.</p>
			<p>Data cleansing is a part of the data analytics process and cleans data by fixing bad and corrupt data, removing <a id="_idIndexMarker306"/>duplicates, and selecting a set of data that's useful for a wide set of business use cases. When data is combined from disparate sources, there might be inconsistencies in the data types, including mislabeled or redundant data. Thus, data cleansing also incorporates <a id="_idIndexMarker307"/>data standardization to bring integrated data up to an enterprise's standards and conventions. </p>
			<p>The goal of data cleansing is to produce clean, consistent, and pristine data that is ready for the final step of generating meaningful and actionable insights from raw transactional data. In this section, you will learn about the various steps involved in the data cleansing process.</p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor070"/>Data selection to eliminate redundancies</h2>
			<p>Once data has been integrated from various sources, there might be redundancies in the integrated dataset. There might be fields that are not required by your business analytics teams. The first <a id="_idIndexMarker308"/>step of data cleansing is identifying these unnecessary data elements and removing them. </p>
			<p>Let's perform data selection on the integrated dataset we produced in the <em class="italic">Data consolidation via ETL and data warehousing</em> section. We need to look at the table schema first to see what columns we have and what their data types are. We can do this using the following line of code:</p>
			<p class="source-code">retail_enriched_df.printSchema()</p>
			<p>The result of the preceding line of code shows all columns, and we can easily spot that the <strong class="source-inline">Country</strong> and <strong class="source-inline">CountryName</strong> columns are redundant. We also have some columns that were introduced in the dataset for data integration, and they are not very useful for downstream analytics. Let's clean up the unwanted and redundant columns from the integrated dataset, as shown in the following block of code:</p>
			<p class="source-code">retail_clean_df = (retail_enriched_df</p>
			<p class="source-code">                    .drop("Country")</p>
			<p class="source-code">                    .drop("ISO2Code")</p>
			<p class="source-code">                    .drop("ISO3Code")</p>
			<p class="source-code">                    .drop("RetailIDx")</p>
			<p class="source-code">                    .drop("idx")</p>
			<p class="source-code">                    .drop("IncomeIDx")</p>
			<p class="source-code">                   )</p>
			<p>In the preceding code snippet, we used the <strong class="source-inline">drop()</strong> DataFrame operation to eliminate unwanted columns. Now that <a id="_idIndexMarker309"/>we have selected the right data columns from the integrated dataset, the next step is to identify and eliminate any duplicate rows.</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor071"/>De-duplicating data</h2>
			<p>The first step of the deduplication process is to check if we have any duplicate rows to begin with. We can do <a id="_idIndexMarker310"/>this using a combination of DataFrame operations, as shown in the following block of code:</p>
			<p class="source-code">(retail_enriched_df</p>
			<p class="source-code">   .select("InvoiceNo", "InvoiceDate")</p>
			<p class="source-code">   .groupBy("InvoiceNo", "InvoiceDate")</p>
			<p class="source-code">   .count()</p>
			<p class="source-code">   .show())</p>
			<p>The preceding lines of code show the count of all the rows after grouping the rows by the <strong class="source-inline">InvoiceNo</strong>,  <strong class="source-inline">InvoiceDate</strong>, and <strong class="source-inline">StockCode</strong> columns. Here, we are assuming that the <strong class="source-inline">InvoiceNo</strong>,  <strong class="source-inline">InvoiceDate</strong>, and <strong class="source-inline">StockCode</strong> column combination is unique and that they form the <strong class="bold">composite key</strong> for the dataset. The expected result of the preceding query is that every row has a count of <strong class="source-inline">1</strong>. However, in the results, we can see that some rows have counts greater than <strong class="source-inline">1</strong>, which suggests that there might be duplicate rows in the dataset. This should be inspected manually, once you've sampled a few rows that show duplicates. This is to ensure they are duplicate rows. We can do this using the following block of code:</p>
			<p class="source-code">(retail_enriched_df.where("InvoiceNo in ('536373', '536382', '536387') AND StockCode in ('85123A', '22798', '21731')")</p>
			<p class="source-code">   .display()</p>
			<p class="source-code">)</p>
			<p>In the preceding query, we checked a sample of the <strong class="source-inline">InvoiceNo</strong> and <strong class="source-inline">StockCode</strong> values to see if the <a id="_idIndexMarker311"/>returned data contains duplicates. Just eyeballing the results, we can see that there are duplicates in the dataset. We need to eliminate these duplicates. Fortunately, PySpark comes with a handy function called <strong class="source-inline">drop_duplicates()</strong> to just do that, as shown in the following line of code:</p>
			<p class="source-code">retail_nodupe = retail_clean_df.drop_duplicates(["InvoiceNo", "InvoiceDate", "StockCode"])</p>
			<p>In the preceding line of code, we used the <strong class="source-inline">drop_duplicates()</strong> function to eliminate duplicates based on a subset of columns. Let's see if it eliminated the duplicate rows by using the following line of code:</p>
			<p class="source-code">(retail_nodupe</p>
			<p class="source-code">   .select("InvoiceNo", "InvoiceDate", "StockCode")</p>
			<p class="source-code">   .groupBy("InvoiceNo", "InvoiceDate", "StockCode")</p>
			<p class="source-code">   .count()</p>
			<p class="source-code">   .where("count &gt; 1")</p>
			<p class="source-code">   .show())</p>
			<p>The previous code groups the rows based on the <strong class="bold">composite key</strong> and checks the count of each group. The result is an empty dataset, meaning that all the duplicates have been successfully eliminated.</p>
			<p>So far, we have dropped unwanted columns from the integrated dataset and eliminated duplicates. During the data selection step, we noticed that all the columns were of the <strong class="source-inline">string</strong> type and that the column's names were following different naming conventions. This can be rectified using the data standardization process.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor072"/>Standardizing data</h2>
			<p><strong class="bold">Data standardization</strong> refers to where we make sure all the columns adhere to their proper data types. This is also <a id="_idIndexMarker312"/>where we bring all the column names up to our enterprise naming standards and conventions. This can be achieved in PySpark using the following DataFrame operation:</p>
			<p class="source-code">retail_final_df = (retail_nodupe.selectExpr(</p>
			<p class="source-code">    "InvoiceNo AS invoice_num", "StockCode AS stock_code", </p>
			<p class="source-code">    "description AS description", "Quantity AS quantity", </p>
			<p class="source-code">    "CAST(InvoiceDate AS TIMESTAMP) AS invoice_date", </p>
			<p class="source-code">    "CAST(UnitPrice AS DOUBLE) AS unit_price", </p>
			<p class="source-code">    "CustomerID AS customer_id",</p>
			<p class="source-code">    "CountryCode AS country_code",</p>
			<p class="source-code">    "CountryName AS country_name", "GeoShape AS geo_shape",</p>
			<p class="source-code">    "age", "workclass AS work_class", </p>
			<p class="source-code">    "fnlwgt AS final_weight", "education", </p>
			<p class="source-code">    "CAST('education-num' AS NUMERIC) AS education_num", </p>
			<p class="source-code">    "'marital-status' AS marital_status", "occupation", </p>
			<p class="source-code">    "relationship", "race", "gender", </p>
			<p class="source-code">    "CAST('capital-gain' AS DOUBLE) AS capital_gain", </p>
			<p class="source-code">    "CAST('capital-loss' AS DOUBLE) AS capital_loss", </p>
			<p class="source-code">    "CAST('hours-per-week' AS DOUBLE) AS hours_per_week", </p>
			<p class="source-code">    "'native-country' AS native_country")</p>
			<p class="source-code">)</p>
			<p>In the preceding block of code, we essentially have a SQL <strong class="source-inline">SELECT</strong> query that casts columns to their proper data types and aliases column names so that they follow proper Pythonic naming standards. The result is a final dataset that contains data from various sources integrated into a cleansed, deduplicated, and standardized data format.</p>
			<p>This final dataset, which is the result of the data integration and data cleansing phases of the data analytics process, is ready to be presented to business users for them to run their business <a id="_idIndexMarker313"/>analytics on. Thus, it makes sense to persist this dataset onto the data lake and make it available for end user consumption, as shown in the following line of code:</p>
			<p class="source-code">retail_final_df.write.format("delta").save("dbfs:/FileStore/shared_uploads/delta/retail_silver.delta")</p>
			<p>In the preceding line of code, we saved our final version of the pristine transactional data in the data lake in Delta Lake format.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">It is an industry convention to call transactional data that's been replicated straight from the source system <strong class="bold">bronze</strong> data, cleansed and integrated transactional data <strong class="bold">silver</strong> data, and aggregated <a id="_idIndexMarker314"/>and summarized data <strong class="bold">gold</strong> data. The data analytics process, in a nutshell, is a continuous <a id="_idIndexMarker315"/>process of ingesting bronze data and transforming it <a id="_idIndexMarker316"/>into silver and gold data, until it is ready to be converted into actionable business insights.</p>
			<p>To summarize the data cleaning process, we took the result set of the data integration process, removed any redundant and unnecessary columns, eliminated duplicate rows, and brought the data columns up to enterprise standards and conventions. All these data processing steps were implemented using the DataFrame API, which is powered by Spark SQL Engine. It can easily scale out this process to terabytes and petabytes of data. </p>
			<p class="callout-heading">Tip</p>
			<p class="callout">In this chapter, data integration and data cleansing have been presented as two independent and mutually exclusive processes. However, in real-life use cases, it is a very common practice to implement these two steps together as a single data processing pipeline.</p>
			<p>The result of the data integration and data cleansing process is usable, clean, and meaningful data that is ready for consumption by business analytics users. Since we are working at a big data scale here, data must be structured and presented in a way that improves the performance of business analytics queries. You will learn about this in the following section.</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor073"/>Optimizing ELT processing performance with data partitioning</h2>
			<p><strong class="bold">Data partitioning</strong> is a process where a large dataset is physically split into smaller parts. This way, when <a id="_idIndexMarker317"/>a query requires a portion of the larger dataset, it can <a id="_idIndexMarker318"/>scan and load a subset of the partitions. This technique of eliminating partitions that are <a id="_idIndexMarker319"/>not required by the query is called <strong class="bold">partition pruning</strong>.</p>
			<p><strong class="bold">Predicate pushdown</strong> is another technique where parts of a query that filter, slice, and dice data, called the <strong class="bold">predicate</strong>, are <a id="_idIndexMarker320"/>pushed down to the data storage layer. It then becomes the <a id="_idIndexMarker321"/>data storage layer's responsibility <a id="_idIndexMarker322"/>to filter out all the partitions not required by the query.</p>
			<p>Traditional RDBMS and data warehouses have always supported data partitioning, partition pruning, and predicate pushdown. Semi-structured file formats such as CSV and JSON support data partitioning and partition pruning but do not support predicate pushdown. Apache Spark fully supports all three. With predicate pushdown, Spark can delegate the task of filtering out data to the underlying data storage layer, thus reducing the amount of data that needs to be loaded into Spark's memory and then processed. </p>
			<p>Structured data formats such as Parquet, ORC, and Delta Lake fully support partitioning pruning and predicate pushdown. This helps Spark's Catalyst Optimizer generate the best possible query execution plan. This is a strong reason to favor structured file formats such as Apache Parquet with Spark over semi-structured data formats. </p>
			<p>Consider that your data lake has historical data spanning several years and that your typical queries involve only a few months to a few years of data at a time. You can choose to store your data completely unpartitioned, with all the data in a single folder. Alternatively, you can partition your data by year and month attributes, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="Images/B16736_03_09.jpg" alt="Figure 3.9 – Data partitioning&#13;&#10;" width="898" height="625"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9 – Data partitioning</p>
			<p>On the right-hand side of the preceding diagram, we have unpartitioned data. This pattern of data storage makes data storage a little easier because we just keep appending new data to the same folder over and over again. However, after a certain point, the data becomes unmanageable and makes it difficult to perform <a id="_idIndexMarker323"/>any updates or deletes. Moreover, Apache Spark would need to read the entire dataset into memory, losing any advantages that partition pruning and predicate pushdown could have offered.</p>
			<p>On the right-hand side of the diagram, data is partitioned by year and then by month. This makes <a id="_idIndexMarker324"/>writing data a little more involved as the Spark application would need to choose the right partition every time before writing data. However, this is a small penalty compared to the efficiency and performance that's gained with updates and deletes, as well as downstream queries. Queries on such partitioned data will be orders of magnitude faster as they make full use of partition pruning and predicate pushdown. Thus, it is recommended to partition data with an appropriate partition key to get the best performance and efficiency out of your data lake.</p>
			<p>Since data partitioning plays a crucial role in determining the performance of downstream analytical queries, it is important to choose the right partitioning column. As a general <a id="_idIndexMarker325"/>rule of thumb, choose a partition column with low cardinality. Partition sizes of at least one gigabyte <a id="_idIndexMarker326"/>are practical and typically, a date-based column makes for a good candidate for the partition key.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Recursive file listing on cloud-based object storage is usually slow and expensive. So, using hierarchical partitioning on cloud-based object stores is not very efficient and thus not recommended. This could be a performance bottleneck when more than one partition key is <a id="_idIndexMarker327"/>required. Databricks's proprietary version of Delta Lake, along with their Delta Engine, supports techniques such as <strong class="bold">dynamic file pruning</strong> and <strong class="bold">Z-order</strong> multi-dimensional indexes to help solve the problems of hierarchical partitioning on <a id="_idIndexMarker328"/>cloud-based data lakes. You can read more about them at <a href="https://docs.databricks.com/delta/optimizations/dynamic-file-pruning.html">https://docs.databricks.com/delta/optimizations/dynamic-file-pruning.html</a>. However, these techniques are not available in the open source version of Delta Lake yet.</p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor074"/>Summary</h1>
			<p>In this chapter, you learned about two prominent methodologies of data processing known as <strong class="bold">ETL</strong> and <strong class="bold">ELT</strong> and saw the advantages of using ETL to unlock more analytics use cases than what's possible with ETL. By doing this, you understood the scalable storage and compute requirements of ETL and how modern cloud technologies help enable the ELT way of data processing. Then, you learned about the shortcomings of using cloud-based data lakes as analytics data stores, such as having a lack of atomic transactional and durability guarantees. After, you were introduced to Delta Lake as a modern data storage layer designed to overcome the shortcomings of cloud-based data lakes. You learned about the data integration and data cleansing techniques, which help consolidate raw transactional data from disparate sources to produce clean, pristine data that is ready to be presented to end users to generate meaningful insights. You also learned how to implement each of the techniques used in this chapter using DataFrame operations and Spark SQL. You gained skills that are essential for transforming raw transactional data into meaningful, enriched data using the ELT methodology for big data at scale.</p>
			<p>Typically, the data cleansing and integration processes are performance-intensive and are implemented in a batch processing manner. However, in big data analytics, you must get the latest transactional data to the end users as soon as it is generated at the source. This is very helpful in tactical decision-making and is made possible by real-time data analytics, which you will learn about in the next chapter.</p>
		</div>
	</div></body></html>