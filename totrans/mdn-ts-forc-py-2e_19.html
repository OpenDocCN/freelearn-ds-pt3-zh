<html><head></head><body>
  <div id="_idContainer848" class="Basic-Text-Frame">
    <h1 class="chapterNumber">16</h1>
    <h1 id="_idParaDest-347" class="chapterTitle">Specialized Deep Learning Architectures for Forecasting</h1>
    <p class="normal">Our journey through the world of <strong class="keyWord">deep learning</strong> (<strong class="keyWord">DL</strong>) is coming to an end. In the previous chapter, we were introduced to the global paradigm of forecasting and saw how we can make a simple model such as a <strong class="keyWord">Recurrent Neural Network</strong> (<strong class="keyWord">RNN</strong>) perform close to the high benchmark set by global machine learning models. In this chapter, we are going to review a few popular DL architectures that were designed specifically for time series forecasting. With these more sophisticated model architectures, we will be better equipped to handle problems in the wild that call for more powerful models than vanilla RNNs and LSTMs.</p>
    <p class="normal">In this chapter, we will be covering these main topics:</p>
    <ul>
      <li class="bulletList">The need for specialized architectures</li>
      <li class="bulletList">Introduction to NeuralForecast</li>
      <li class="bulletList">Neural Basis Expansion Analysis for Interpretable Time Series Forecasting</li>
      <li class="bulletList">Neural Basis Expansion Analysis for Interpretable Time Series Forecasting with Exogenous Variables</li>
      <li class="bulletList">Neural Hierarchical Interpolation for Time Series Forecasting</li>
      <li class="bulletList">Autoformer</li>
      <li class="bulletList">LTSF-Linear family</li>
      <li class="bulletList">Patch Time Series Forecasting</li>
      <li class="bulletList">iTransformer</li>
      <li class="bulletList">Temporal Fusion Transformer</li>
      <li class="bulletList">TSMixer</li>
      <li class="bulletList">Time Series Dense Encoder</li>
    </ul>
    <h1 id="_idParaDest-348" class="heading-1">Technical requirements</h1>
    <p class="normal">You will need to set up an Anaconda environment by following the instructions in the <em class="italic">Preface</em> to get a working environment with all the packages and datasets required for the code in this book.</p>
    <p class="normal">The code associated with this chapter can be found at <a href="https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python/tree/main/notebooks/Chapter16"><span class="url">https://github.com/PacktPublishing/Modern-Time-Series-Forecasting-with-Python/tree/main/notebooks/Chapter16</span></a>.</p>
    <p class="normal">You will need to run the following notebooks for this chapter:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">02-Preprocessing_London_Smart_Meter_Dataset.ipynb</code> in <code class="inlineCode">Chapter02</code></li>
      <li class="bulletList"><code class="inlineCode">01-Setting_up_Experiment_Harness.ipynb</code> in <code class="inlineCode">Chapter04</code></li>
      <li class="bulletList"><code class="inlineCode">01-Feature_Engineering.ipynb</code> in <code class="inlineCode">Chapter06</code></li>
    </ul>
    <h1 id="_idParaDest-349" class="heading-1">The need for specialized architectures</h1>
    <p class="normal">Inductive bias, or learning bias, refers to a set of assumptions a learning algorithm makes to generalize<a id="_idIndexMarker1218"/> the function it learns on training data to unseen data. Inductive bias is not inherently a bad thing and is different from “bias” in the context of bias and variance in learning theory. We use and design inductive bias either through model architectures or through feature engineering. For instance, a <strong class="keyWord">Convolutional Neural Network</strong> (<strong class="keyWord">CNN</strong>) works<a id="_idIndexMarker1219"/> better on images than a <a id="_idIndexMarker1220"/>standard <strong class="keyWord">Feed Forward Network</strong> (<strong class="keyWord">FFN</strong>) on pure pixel input because the CNN has the locality and spatial bias that FFNs do not have. Although the FFN is theoretically a universal approximator, we can learn better models with the inductive bias the CNN has.</p>
    <p class="normal">Deep learning is thought to be a completely data-driven approach where the feature engineering and final task are learned end to end, thus avoiding the inductive bias that the modelers bake in while designing the features. But that view is not entirely correct. These inductive biases, which used to be put in through the features, now make their way through the design of architecture. Every DL architecture has its own inductive bias, which is why some types of models perform better on some types of data. For instance, a CNN works well on images, but not as much on sequences because the spatial inductive bias and translational equivariance that the CNN brings to the table are most effective on images.</p>
    <p class="normal">In an ideal world, we <a id="_idIndexMarker1221"/>would have an infinite supply of good, annotated data and we would be able to learn entirely data-driven networks with no strong inductive bias. But sadly, in the real world, we will never have enough data to learn such complex functions. This is where designing the right kind of inductive bias makes or breaks the DL system. We used to heavily rely on RNNs for sequences and they had a strong auto-regressive inductive bias baked into them. But later, Transformers, which have a much weaker inductive bias for sequences, came in, and with large amounts of data, they were able to learn better functions for sequences. Therefore, this decision about how strong an inductive bias we bake into models is an important question in designing DL architectures.</p>
    <p class="normal">Over the years, many DL architectures have been proposed specifically for time series forecasting and each of them has its own inductive bias attached to it. We’ll not be able to review every single one of those models, but we will cover the major ones that made a lasting impact on the field. We will also look at how we can use a few open-source libraries to train those models on our data. </p>
    <p class="normal">We will exclusively focus on models that can handle the global modeling paradigm, directly or indirectly. This is because of the infeasibility of training separate models for each time series when we are forecasting at scale.</p>
    <p class="normal">We are going to look at a few popular architectures developed for time series forecasting. One of the major factors influencing the inclusion of a model is also the availability of stable open-source frameworks that support these models. This is in no way a complete list because there are many architectures we are not covering here. I’ll try and share a few links in the <em class="italic">Further reading</em> section to get you started on your journey of exploration.</p>
    <p class="normal">Before we get into the meat of the chapter, let’s understand the library we are going to use for it.</p>
    <h1 id="_idParaDest-350" class="heading-1">Introduction to NeuralForecast</h1>
    <p class="normal">NeuralForecast <a id="_idIndexMarker1222"/>is yet another library from the wonderful folks at NIXTLA. You might recall the name from <em class="chapterRef">Chapter 4</em>, <em class="italic">Setting a Strong Baseline Forecast</em>, where we used <code class="inlineCode">statsforecast</code> for classical time series models like ARIMA, ETS, and so on. They have a whole suite of open-source libraries for time series forecasting (<code class="inlineCode">mlforecast</code> for machine learning based forecasts, <code class="inlineCode">hierarchicalforecast</code> for reconciling forecasts for hierarchical data, <code class="inlineCode">utilsforecast</code> with some utilities for forecasting, <code class="inlineCode">datasetsforecast</code> with some ready-to-use datasets, and <code class="inlineCode">TimeGPT</code>, their foundational model for time series).</p>
    <p class="normal">Since we have<a id="_idIndexMarker1223"/> learned how to use <code class="inlineCode">statsforecast</code>, extending that to <code class="inlineCode">neuralforecast</code> is going to be easy because both libraries maintain similar APIs, structure, and ways of working. <code class="inlineCode">neuralforecast</code> offers both classic and cutting-edge deep learning models in an easy-to-use API, which makes it perfect for the practical side of the chapter.</p>
    <p class="normal">NeuralForecast is structured to offer an intuitive and flexible API that integrates seamlessly with modern data science workflows. The package includes implementations of several prominent models, each catering to different aspects of time series forecasting.</p>
    <h2 id="_idParaDest-351" class="heading-2">Common parameters and configurations</h2>
    <p class="normal">Similar<a id="_idIndexMarker1224"/> to <code class="inlineCode">statsforecast</code>, <code class="inlineCode">neuralforecast</code> also expects the input data to be in a particular form:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">ds</code>: This column should have the time index. It can either be a datetime column or an integer column which represents time.</li>
      <li class="bulletList"><code class="inlineCode">y</code>: This column should have the time series we are trying to forecast.</li>
      <li class="bulletList"><code class="inlineCode">unique_id</code>: This column lets us differentiate different time series with a unique ID that we choose. It can be the household ID in our data or any other uniquely identifying ID that we give.</li>
    </ul>
    <p class="normal">Most models in the <code class="inlineCode">neuralforecast</code> package share a set of common parameters that control aspects like:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">stat_exog_list</code>: This is a list of static continuous columns.</li>
      <li class="bulletList"><code class="inlineCode">hist_exog_list</code>: This is a list of temporal exogenous features for which the history is available.</li>
      <li class="bulletList"><code class="inlineCode">futr_exog_list</code>: This is a list of temporal exogenous features for which the future is available.</li>
      <li class="bulletList"><code class="inlineCode">learning_rate</code>: This dictates the speed at which a model learns. A higher rate might converge faster but can overshoot optimal weights, while a lower rate ensures more stable convergence at the cost of speed.</li>
      <li class="bulletList"><code class="inlineCode">batch_size</code>: This influences the amount of data fed into the model at each training step, affecting both memory usage and training dynamics.</li>
      <li class="bulletList"><code class="inlineCode">max_steps</code>: This defines the maximum number of epochs – the number of times the entire dataset is passed forward and backward through the neural network. It is max because we can also add early stopping and, in that case, the number of epochs can be lower than this as well.</li>
      <li class="bulletList"><code class="inlineCode">loss</code>: This is<a id="_idIndexMarker1225"/> the metric used to gauge the difference between predicted values and actual values, guiding the optimization process. For a list of loss functions included, refer to the NIXTLA documentation: <a href="https://nixtlaverse.nixtla.io/neuralforecast/losses.pytorch.html"><span class="url">https://nixtlaverse.nixtla.io/neuralforecast/losses.pytorch.html</span></a></li>
      <li class="bulletList"><code class="inlineCode">scaler_type</code>: This is a string indicating the type of temporal normalization used. Temporal normalization does the scaling for each instance of the batch separately at the window level. Some examples include <code class="inlineCode">['minmax,' 'robust,' 'standard']</code>. This is only applicable for window-based models like NBEATS and TimesNet and not recurrent models like RNNs. For a full list of scalers, check the NIXTLA temporal scalers: <a href="https://nixtlaverse.nixtla.io/neuralforecast/common.scalers.html"><span class="url">https://nixtlaverse.nixtla.io/neuralforecast/common.scalers.html</span></a>.</li>
      <li class="bulletList"><code class="inlineCode">early_stop_patience_steps</code>: If defined, this sets the number of steps we will wait without any improvement in validation scores before stopping training.</li>
      <li class="bulletList"><code class="inlineCode">random_seed</code>: This defines the random seed, which is essential for reproducibility.</li>
    </ul>
    <div class="note">
      <p class="normal"><strong class="keyWord">Practitioner’s tip</strong>:</p>
      <p class="normal">The parameters <code class="inlineCode">stat_exog_list</code>, <code class="inlineCode">hist_exog_list</code>, and <code class="inlineCode">futr_exog_list</code> are only available to models which support them. Do check the documentation of the model you are going to use to see if it supports these parameters. Some models support all three, some only support <code class="inlineCode">futr_exog_list</code>, and so on. The entire list of models and what is available can be found here: <a href="https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html"><span class="url">https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html</span></a>.</p>
      <p class="normal">Also, if you have some features for which you only have historical data, they go in <code class="inlineCode">hist_exog_list</code>. If you have some features for which you have both historical and future data, they go in both <code class="inlineCode">hist_exog_list</code> and <code class="inlineCode">futr_exog_list</code>.</p>
    </div>
    <p class="normal">Apart from these common model parameters, <code class="inlineCode">neuralforecast</code> also has a core class, <code class="inlineCode">NeuralForecast</code>, which orchestrates the training (just like we have <code class="inlineCode">StatsForecast</code> in <code class="inlineCode">statsforecast</code>). Similar to <code class="inlineCode">statsforecast</code>, this is where we define the list of models we need to forecast and so on. Let’s look at a few parameters in this class as well:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">models</code>: This defines a list of models that we need to fit or predict.</li>
      <li class="bulletList"><code class="inlineCode">freq</code>: This sets the frequency of the time series we want to forecast. It can either be a string (a valid pandas or polars offset alias) or an integer. This is used for generating future dataframes for prediction and should be defined according to the data you have. </li>
    </ul>
    <p class="bulletList">If <code class="inlineCode">ds</code> is a datetime column, then <code class="inlineCode">freq</code> can be a string indicating the frequency of repetition (like ‘D’ for days, ‘H’ for hours, etc.) or an integer indicating a multiplier of the default unit (usually days) to determine the interval between each date. And if <code class="inlineCode">ds</code> is a numerical column, then <code class="inlineCode">freq</code> should also be a numerical column indicating a fixed numerical increment between values.</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">local_scaler_type</code>: This is an alternate way to scale the time series. While <code class="inlineCode">scaler_type</code> in Windows-based models scales the time series for each window, this scales each time series as a pre-processing step. For each <code class="inlineCode">unique_id</code>, this step scales the time series separately and stores the scalers so that the inverse transformations can be applied while predicting.</li>
    </ul>
    <p class="normal">A typical<a id="_idIndexMarker1226"/> workflow involving <code class="inlineCode">neuralforecast</code> looks as below:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> neuralforecast <span class="hljs-keyword">import</span> NeuralForecast
<span class="hljs-keyword">from</span> neuralforecast.models <span class="hljs-keyword">import</span> LSTM
horizon = <span class="hljs-number">12</span>
models = [LSTM(h=horizon,
               max_steps=<span class="hljs-number">500</span>,
               scaler_type=<span class="hljs-string">'standard'</span>,
               encoder_hidden_size=<span class="hljs-number">64</span>,
               decoder_hidden_size=<span class="hljs-number">64</span>,),
          ]
nf = NeuralForecast(models=models, freq=<span class="hljs-string">'M'</span>)
nf.fit(df=Y_df)
Y_hat_df = nf.predict()
</code></pre>
    <h2 id="_idParaDest-352" class="heading-2">“Auto” models</h2>
    <p class="normal">One of the standout features of the <code class="inlineCode">neuralforecast</code> package is the inclusion of “auto” models. These<a id="_idIndexMarker1227"/> models automate the process of hyperparameter tuning and model selection, simplifying the workflow for users. By utilizing techniques from <strong class="keyWord">automated machine learning</strong> (<strong class="keyWord">AutoML</strong>), these models can adapt their architecture and settings based on the dataset, significantly reducing the manual effort involved in the model configuration. They have intelligent default ranges defined so that, even if you don’t declare any ranges to tune, they will take the default ranges and tune the models. Additional information can be found here: <a href="https://nixtlaverse.nixtla.io/neuralforecast/models.html"><span class="url">https://nixtlaverse.nixtla.io/neuralforecast/models.html</span></a>.</p>
    <h2 id="_idParaDest-353" class="heading-2">Exogenous features</h2>
    <p class="normal">NeuralForecast <a id="_idIndexMarker1228"/>can also easily incorporate exogenous variables into the forecasting process (depending on the capability of the model). Exogenous features, which are external influences that can affect the target variable, are crucial for improving forecasting accuracy, especially when these external factors significantly impact the outcome. Many models within the <code class="inlineCode">neuralforecast</code> package can integrate such features to refine predictions by accounting for additional information that may not be present in the time series data itself.</p>
    <p class="normal">For instance, the inclusion of holiday effects, weather conditions, or economic indicators as exogenous variables can provide critical insights that pure historical data cannot. This feature is especially useful in models like NBEATSx, NHITS, and TSMixerx within the package, which can model complex interactions between both the historical and future exogenous inputs. By handling exogenous features effectively, NeuralForecast enhances the models’ ability to forecast accurately in real-world scenarios where external factors play a pivotal role. To check which models can handle exogenous information, refer to the documentation on the website: <a href="https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html"><span class="url">https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html</span></a></p>
    <p class="normal">Now, without further ado, let’s get started on the first model on the list.</p>
    <h1 id="_idParaDest-354" class="heading-1">Neural Basis Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS)</h1>
    <p class="normal">The first <a id="_idIndexMarker1229"/>model that used some components from DL (we can’t call it DL because it is essentially a mix of DL and classical statistics) and made a splash in the field was a model that won the M4 competition (univariate) in 2018. This was a model by Slawek Smyl from Uber (at the time) and was a Frankenstein-style mix of exponential smoothing and an <a id="_idIndexMarker1230"/>RNN, dubbed <strong class="keyWord">ES-RNN</strong> (<em class="italic">Further reading</em> has links to a newer and faster implementation of the model that uses GPU acceleration). This led to Makridakis et al. putting forward an argument that “<em class="italic">hybrid approaches and combinations of methods are the way forward</em>.” The creators of the <strong class="keyWord">N-BEATS</strong> model aspired to challenge this conclusion by designing a pure DL architecture for time series forecasting. They succeeded in this when they created a model that beat all other methods in the M4 competition (although they didn’t publish it in time to participate in the competition). It is a very unique architecture, taking a lot of inspiration from signal processing. Let’s take a deeper look and understand the architecture.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research paper by Makridakis et al. and the blog post by Slawek Smyl are cited in the <em class="italic">References</em> section as <em class="italic">1</em> and <em class="italic">2</em>, respectively.</p>
    </div>
    <p class="normal">We need to establish a bit of context and terminology before moving ahead with the explanation. The core problem that they are solving is univariate forecasting, which means it is similar to classical methods such as exponential smoothing and ARIMA in the sense that it takes only the history of the time series to generate a forecast. There is no provision to include other covariates in the model. The model is shown a window from the history and is asked to predict the next few timesteps. The window of history is referred to as <a id="_idIndexMarker1231"/>the <strong class="keyWord">lookback period</strong> and the future<a id="_idIndexMarker1232"/> timesteps are the <strong class="keyWord">forecast period</strong>.</p>
    <h2 id="_idParaDest-355" class="heading-2">The architecture of N-BEATS</h2>
    <p class="normal">The <a id="_idIndexMarker1233"/>N-BEATS architecture was different from the existing architectures (at the time) in a few aspects:</p>
    <ul>
      <li class="bulletList">Instead of the common encoder-decoder (or sequence-to-sequence) formulation, N-BEATS formulates the problem as a multivariate regression problem.</li>
      <li class="bulletList">Most of the other architectures at the time were relatively shallow (~5 LSTM layers). However, N-BEATS used the residual principle to stack many basic blocks (we will explain this shortly) and the paper has shown that we can stack up to 150 layers and still facilitate efficient learning.</li>
      <li class="bulletList">The<a id="_idIndexMarker1234"/> model lets us extend it to human-interpretable output, still in a principled way.</li>
    </ul>
    <p class="normal">Let’s look at the architecture and go deeper:</p>
    <figure class="mediaobject"><img src="../Images/B22389_16_01.png" alt="Figure 16.1 – N-BEATS architecture "/></figure>
    <p class="packt_figref">Figure 16.1: N-BEATS architecture</p>
    <p class="normal">We can see three columns of <em class="italic">rectangular blocks</em>, each one an exploded view of another. Let’s start at the leftmost (which is the most granular view) and then go up step by step, building up to the architecture. At the top, there is a representative time series, which has a lookback window and a forecast period.</p>
    <h3 id="_idParaDest-356" class="heading-3">Blocks</h3>
    <p class="normal">The <a id="_idIndexMarker1235"/>fundamental learning unit in N-BEATS is a <code class="inlineCode">block</code>. Each block, <em class="italic">l</em>, takes in an input, (<em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">l</sub>), of the size of the lookback period and generates two outputs: a forecast, (<img src="../Images/B22389_16_001.png" alt=""/>), and a backcast, (<img src="../Images/B22389_16_002.png" alt=""/>). The backcast is the block’s own best prediction of the lookback period. It is synonymous with fitted values in the classical sense; they tell us how the stack would have predicted the lookback window using the function it has learned. The block input is first processed by a stack of four standard, fully connected layers (complete with a bias term and non-linear activation), transforming the input into a hidden representation, <em class="italic">h</em><sub class="subscript-italic" style="font-style: italic;">l</sub>. Now, this hidden representation is transformed by two separate linear layers (no bias or non-linear activation) to something the paper calls expansion coefficients for the backcast and forecast, <img src="../Images/B22389_16_003.png" alt=""/> and <img src="../Images/B22389_16_004.png" alt=""/>, respectively. </p>
    <p class="normal">The last part of the block takes these expansion coefficients and maps them to the output using a set of basis layers (<img src="../Images/B22389_16_005.png" alt=""/> and <img src="../Images/B22389_16_006.png" alt=""/>). We will talk about the basis layers in a bit more detail later, but for now, just understand that they take the expansion coefficients and transform them into the desired outputs (<img src="../Images/B22389_16_001.png" alt=""/> and<img src="../Images/B22389_16_002.png" alt=""/>).</p>
    <h3 id="_idParaDest-357" class="heading-3">Stacks</h3>
    <p class="normal">Now, let’s <a id="_idIndexMarker1236"/>move one layer up the abstraction to the middle column of <em class="italic">Figure 16.1</em>. It shows how different blocks are arranged in a <code class="inlineCode">stack</code>, <em class="italic">s</em>. All the blocks in a stack share the same kind of basis layers and therefore are grouped as a stack. As we saw earlier, each block has two outputs, <img src="../Images/B22389_16_001.png" alt=""/> and <img src="../Images/B22389_16_002.png" alt=""/>. The blocks are arranged in a residual manner, each block processing and cleaning the time series step by step. The input to a block, <em class="italic">l</em>, is <img src="../Images/B22389_16_011.png" alt=""/> . At each step, the backcast generated by the block is subtracted from the input to that block before it’s passed on to the next layer. All the forecast outputs of all the blocks in a stack are added up to make the <em class="italic">stack forecast</em>:</p>
    <p class="center"><img src="../Images/B22389_16_012.png" alt=""/></p>
    <p class="normal">The residual backcast from the last block in a stack is the <em class="italic">stack residual</em> (<em class="italic">x</em><sup class="superscript">s</sup>).</p>
    <h3 id="_idParaDest-358" class="heading-3">The overall architecture</h3>
    <p class="normal">With that, we<a id="_idIndexMarker1237"/> can move to the rightmost column of <em class="italic">Figure 16.1</em>, which shows the top-level view of the architecture. We saw that each stack has two outputs—a stack forecast (<em class="italic">y</em><sup class="superscript">s</sup>) and a stack residual (<em class="italic">x</em><sup class="superscript">s</sup>). There can be <em class="italic">N</em> stacks that make up the N-BEATS model. Each stack is chained together so that for any stack (<em class="italic">s</em>), the stack residual out of the previous stack (<em class="italic">x</em><sup class="superscript">s-1</sup>) is the input and the stack generates two outputs: the stack forecast (<em class="italic">y</em><sup class="superscript">s</sup>) and the stack residual (<em class="italic">x</em><sup class="superscript">s</sup>). Finally, the N-BEATS forecast, <img src="../Images/B22389_05_001.png" alt=""/>, is the additive sum of all the stack forecasts:</p>
    <p class="center"><img src="../Images/B22389_16_014.png" alt=""/></p>
    <p class="normal">Now that we have understood what the model is doing, we need to come back to one point that we left for later—<strong class="keyWord">basis functions</strong>.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Disclaimer</strong>:</p>
      <p class="normal">The explanation here is to mostly aid intuition, so we might be hand-waving over a few mathematical concepts. For a more rigorous treatment of the subject, you should refer to mathematical books/articles that cover the topic. For example, <em class="italic">Functions as Vector Spaces</em> from the <em class="italic">Further reading</em> section and <em class="italic">Function Spaces</em> (<a href="https://cns.gatech.edu/~predrag/courses/PHYS-6124-12/StGoChap2.pdf"><span class="url">https://cns.gatech.edu/~predrag/courses/PHYS-6124-12/StGoChap2.pdf</span></a>).</p>
    </div>
    <h3 id="_idParaDest-359" class="heading-3">Basis functions and interpretability</h3>
    <p class="normal">To understand what <a id="_idIndexMarker1238"/>basis functions are, we need to understand a <a id="_idIndexMarker1239"/>concept from linear algebra. We talked about vector spaces in <em class="chapterRef">Chapter 11</em>, <em class="italic">Introduction to Deep Learning</em>, and gave you a geometric interpretation of vectors and vector spaces. We talked about how a vector is a point in the <em class="italic">n</em>-dimensional vector space. We had that discussion regarding regular Euclidean space (<em class="italic">R</em><sup class="superscript-italic" style="font-style: italic;">n</sup>), which is intended to represent physical space. Euclidean spaces are defined with an origin and an orthonormal basis. An orthonormal basis is a unit vector (magnitude=1) and they are orthogonal (in simple intuition, at 90 degrees) to each other. Therefore, a vector, <img src="../Images/B22389_16_015.png" alt=""/>, can be written as <img src="../Images/B22389_16_016.png" alt=""/>, where <img src="../Images/B22389_16_017.png" alt=""/> and <img src="../Images/B22389_16_018.png" alt=""/> are the orthonormal basis. You may remember this from high school.</p>
    <p class="normal">Now, there is a <a id="_idIndexMarker1240"/>branch of mathematics that views a function as a point in a vector space (at which point, we call it a functional space). This comes from the fact that all the mathematical conditions that need to be satisfied for a vector space (things such as additivity, associativity, and so on) are valid if we consider functions instead of points. To better drive that intuition, let’s consider a function, <em class="italic">f</em>(<em class="italic">x</em>) = 2<em class="italic">x</em> + 4<em class="italic">x</em><sup class="superscript">2</sup>. We can consider this function as a vector in the function space with basis <em class="italic">x</em> and <em class="italic">x</em><sup class="superscript">2</sup>. Now, the coefficients, 2 and 4, can be changed to give us different functions; this can be any real number from -<img src="../Images/B22389_11_014.png" alt=""/> to +<img src="../Images/B22389_11_014.png" alt=""/>. This space of all functions that can have a basis of <em class="italic">x</em> and <em class="italic">x</em><sup class="superscript">2</sup> is the functional space, and every function in the function space can be defined as a linear combination of the basis functions. We can have the basis of any arbitrary function, which gives us a lot of flexibility. From a machine learning perspective, searching for the best function in this functional space automatically means that we are restricting the function search so that we have some properties defined by the basis functions.</p>
    <p class="normal">Coming back to N-BEATS, we talked about the expansion coefficients, <img src="../Images/B22389_16_021.png" alt=""/>and <img src="../Images/B22389_16_022.png" alt=""/>, which are mapped to the output using a set of basis layers (<img src="../Images/B22389_16_005.png" alt=""/> and <img src="../Images/B22389_16_006.png" alt=""/>). A basis layer can also be thought of as a basis function because we know that a layer is nothing but a function that maps its inputs to its outputs. Therefore, by learning the expansion coefficients, we are essentially searching for the best function that can represent the output but is constrained by the basis functions we choose.</p>
    <p class="normal">There are two modes in which N-BEATS operates: <em class="italic">generic</em> and <em class="italic">interpretable</em>. The N-BEATS paper shows that under both modes, N-BEATS managed to beat the best in the M4 competition. Generic mode is where we do not have any basis function constraining the function search. We can also think of this as setting the basis function to be the identity function. So, in this mode, we are leaving the function completely learned by the model through a linear projection of the basis coefficients. This mode lacks human interpretability because we don’t have any idea how the different functions are learned and what each stack signifies.</p>
    <p class="normal">But if we have fixed <a id="_idIndexMarker1241"/>basis functions that constrain the function space, we can bring in more interpretability. For instance, if we have a basis function that constrains the output to represent the trends for all the blocks in a stack, we can say that the forecast output of that stack represents the trend component. Similarly, if we have another basis function that constrains the output to represent the seasonality for all the blocks in a stack, we can say that the forecast output of the stack represents seasonality.</p>
    <p class="normal">This is exactly what the paper has proposed as well. They have defined specific basis functions that capture trend and seasonality, and including such blocks makes the final forecast more interpretable by giving us a decomposition. The trend basis function is a polynomial of a small degree, <em class="italic">p</em>. So, as long as <em class="italic">p</em> is low, such as 1, 2, or 3, it forces the forecast output to mimic the trend component. For the seasonality basis function, the authors chose a Fourier basis (similar to the one we saw in <em class="chapterRef">Chapter 6</em>, <em class="italic">Feature Engineering for Time Series Forecasting</em>). This forces the forecast output to be functions of these sinusoidal basis functions that mimic seasonality. In other words, the model learns to combine these sinusoidal waves with different coefficients to reconstruct the seasonality pattern as best as possible.</p>
    <p class="normal">For a deeper understanding of these basis functions and how they are structured, I have linked to a <em class="italic">Kaggle notebook</em> in the <em class="italic">Further reading</em> section that provides a clear explanation of the trend and seasonality basis functions. The associated notebook also has an additional section that visualizes the first few basis functions of seasonality. Along with the original paper, these additional readings will help you solidify your understanding.</p>
    <p class="normal">N-BEATS wasn’t designed to be a global model, but it does well in the global setting. The M4 competition was a collection of unrelated time series and the N-BEATS model was trained so that the model was exposed to all those series and learned a common function to forecast each time series in the dataset. This, along with ensembling multiple N-BEATS models with different lookback windows, was the success formula for the M4 competition.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research paper by Boris Oreshkin et al. (N-BEATS) is cited in the <em class="italic">References</em> section as <em class="italic">3</em>.</p>
    </div>
    <h2 id="_idParaDest-360" class="heading-2">Forecasting with N-BEATS</h2>
    <p class="normal">N-BEATS, along with<a id="_idIndexMarker1242"/> many other specialized architectures we will explore in this chapter, are implemented in NIXTLA’s NeuralForecast packages. First, let’s look at the initialization parameters of the implementation.</p>
    <p class="normal">The <code class="inlineCode">NBEATS</code> class in NeuralForecast has lots of parameters, but here are the most important ones:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">stack_types</code>: This defines the number of stacks that we need to have in the N-BEATS model. This should be a list of strings (<em class="italic">generic</em>, <em class="italic">trend</em>, or <em class="italic">seasonality</em>) denoting the number and type of stacks. Examples include <code class="inlineCode">["trend", "seasonality"]</code>, <code class="inlineCode">["trend", "seasonality", "generic"]</code>, and <code class="inlineCode">["generic", "generic", "generic"]</code>. However, if the entire network is generic, we can just have a single generic stack with more blocks as well.</li>
      <li class="bulletList"><code class="inlineCode">n_blocks</code>: This is a list of integers signifying the number of blocks in each stack that we have defined. If we had defined <code class="inlineCode">stack_types</code> as <code class="inlineCode">["trend", "seasonality"]</code>, and we want three blocks each, we can set <code class="inlineCode">n_blocks</code> to <code class="inlineCode">[3,3]</code>.</li>
      <li class="bulletList"><code class="inlineCode">input_size</code>: This is an integer which contains the autoregressive units (lags) to be tested. </li>
      <li class="bulletList"><code class="inlineCode">shared_weights</code>: This is a list of Booleans signifying whether the weights generating the expansion coefficients are shared with other blocks in a stack. It is recommended to share the weights in the interpretable stacks and not share them in the identity stacks.</li>
    </ul>
    <p class="normal">There are several other parameters, but these are not as important. A full list of parameters and their descriptions can be found at <a href="https://nixtlaverse.nixtla.io/neuralforecast/models.nbeats.html"><span class="url">https://nixtlaverse.nixtla.io/neuralforecast/models.nbeats.html</span></a>.</p>
    <p class="normal">Since the strength of the model is in forecasting slightly longer durations, we can do a single-shot 48-step horizon simply by setting the forecast horizon parameter <code class="inlineCode">h = 48</code>.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">The complete code for training N-BEATS can be found in the <code class="inlineCode">01-NBEATS_NeuralForecast.ipynb</code> notebook in the <code class="inlineCode">Chapter16</code> folder.</p>
    </div>
    <h2 id="_idParaDest-361" class="heading-2">Interpreting N-BEATS forecasting</h2>
    <p class="normal">N-BEATS, if we are running it in the interpretable model, also gives us more interpretability by<a id="_idIndexMarker1243"/> separating the forecast into trend and seasonality. To get the interpretable output, we can call the <code class="inlineCode">decompose</code> function. We must ensure that, in our initial parameters, we include the stack type for the trend and seasonal components: <code class="inlineCode">stack_types = ['trend','seasonality']</code>.</p>
    <pre class="programlisting code"><code class="hljs-code">model_interpretable = model_untuned.models[<span class="hljs-number">0</span>]
dataset, *_ = TimeSeriesDataset.from_df(df = training_df, id_col=<span class="hljs-string">'LCLid'</span>,time_col=<span class="hljs-string">'timestamp'</span>,target_col=<span class="hljs-string">'energy_consumption'</span>)
y_hat = model_interpretable.decompose(dataset=dataset)
</code></pre>
    <p class="normal">This will return us an <code class="inlineCode">array</code> from which trend and seasonality can be accessed, like <code class="inlineCode">y_hat =[0,1]</code>. The order of trend or seasonality depends on how you include it in stack_types, though the default is <code class="inlineCode">['seasonality','trend']</code>, meaning seasonality is <code class="inlineCode">y_hat =[0,1]</code> and trend is <code class="inlineCode">y_hat =[0,1]</code>. </p>
    <p class="normal">Let’s see how one of the household predictions decomposed:</p>
    <figure class="mediaobject"><img src="../Images/B22389_16_02.png" alt="Figure 16.2 – Decomposed predictions from N-BEATS (interpretable) "/></figure>
    <p class="packt_figref">Figure 16.2: Decomposed predictions from N-BEATS (interpretable)</p>
    <p class="normal">With all its <a id="_idIndexMarker1244"/>success, N-BEATS was still a univariate model. It was not able to take in any external information, apart from its history. This was fine for the M4 competition, where all the time series in question were also univariate. However, many real-world time series problems come with additional explanatory variables (or exogenous variables). Let’s look at a slight modification that was made to N-BEATS that enabled exogenous variables.</p>
    <h1 id="_idParaDest-362" class="heading-1">Neural Basis Expansion Analysis for Interpretable Time Series Forecasting with Exogenous Variables (N-BEATSx)</h1>
    <p class="normal">Olivares et al. proposed <a id="_idIndexMarker1245"/>an extension of the N-BEATS model by making it compatible with exogenous variables. The overall structure is the same (with blocks, stacks, and residual connections) as N-BEATS (<em class="italic">Figure 16.1</em>), so we will only be focusing on <a id="_idIndexMarker1246"/>the key differences and additions that the <strong class="keyWord">N-BEATSx</strong> model puts forward.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research paper by Olivares et al. (N-BEATSx) is cited in the <em class="italic">References</em> section as <em class="italic">4</em>.</p>
    </div>
    <h2 id="_idParaDest-363" class="heading-2">Handling exogenous variables</h2>
    <p class="normal">In N-BEATS, the<a id="_idIndexMarker1247"/> input to a block was the lookback window, <em class="italic">y</em><sup class="superscript-italic" style="font-style: italic;">b</sup>. But here, the input to a block is both the lookback window, <em class="italic">y</em><sup class="superscript-italic" style="font-style: italic;">b</sup>, and the array of exogenous variables, <em class="italic">x</em>. These exogenous variables can be of two types: time-varying and static. The static variables are encoded using a static feature encoder. This is nothing but a single-layer FC that encodes the static information into a dimension specified by the user. Now, the encoded static information, the time-varying exogenous variables, and the lookback window are concatenated to form the input for a block so that the hidden state representation, <em class="italic">h</em><sub class="subscript-italic" style="font-style: italic;">l</sub>, of block <em class="italic">l</em> is not <em class="italic">FC</em>(<em class="italic">y</em><sup class="superscript-italic" style="font-style: italic;">b</sup>) like in N-BEATS, but <em class="italic">FC</em>([<em class="italic">y</em><sup class="superscript-italic" style="font-style: italic;">b</sup>;<em class="italic">x</em>]), where [;] represents concatenation. This way, the exogenous information is part of the input to every block as it is concatenated with the residual at each step.</p>
    <h2 id="_idParaDest-364" class="heading-2">Exogenous blocks</h2>
    <p class="normal">In addition to<a id="_idIndexMarker1248"/> this, the paper also proposes a new kind of block—an <em class="italic">exogenous block</em>. The exogenous block takes in the concatenated lookback window and exogenous variables (just like any other block) as input and produces a backcast and forecast:</p>
    <p class="center"><img src="../Images/B22389_16_025.png" alt=""/></p>
    <p class="normal">Here, <em class="italic">N</em><sub class="subscript-italic" style="font-style: italic;">x</sub> is the number of exogenous features.</p>
    <p class="normal">Here, we can see that the exogenous forecast is the linear combination of the exogenous variables and that the weights for this linear combination are learned by the expansion coefficients, <img src="../Images/B22389_16_026.png" alt=""/>. The paper refers to this configuration as the interpretable exogenous block because, by using the expansion weights, we can define the importance of each exogenous variable and even figure out the exact part of the forecast, which is because of a particular exogenous variable.</p>
    <p class="normal">N-BEATSx also<a id="_idIndexMarker1249"/> has a generic version (which is not interpretable) of the exogenous block. In this block, the exogenous variables are passed through an encoder that learns a context vector, <em class="italic">C</em><sub class="subscript-italic" style="font-style: italic;">l</sub>, and the forecast is generated using the following formula:</p>
    <p class="center"><img src="../Images/B22389_16_027.png" alt=""/></p>
    <p class="normal">They proposed two encoders: a <strong class="keyWord">Temporal Convolutional Network</strong> (<strong class="keyWord">TCN</strong>) and <strong class="keyWord">WaveNet</strong> (a network similar to the <a id="_idIndexMarker1250"/>TCN, but<a id="_idIndexMarker1251"/> with dilation to expand the receptive field). The <em class="italic">Further reading</em> section contains resources if you wish to learn more about WaveNet, an architecture that originated in the sound domain.</p>
    <div class="note">
      <p class="normal">N-BEATSx is also implemented in NIXTLA <code class="inlineCode">neuralforecast</code>, however, at the time of writing, it cannot yet handle categorical data. Thus, we will need to encode the categorical features into numerical representations (like we did in <em class="chapterRef">Chapter 10</em>, <em class="italic">Global Forecasting Models</em>) before using <code class="inlineCode">neuralforecast</code>.</p>
    </div>
    <p class="normal">The research paper also showed that N-BEATSx outperformed N-BEATS, ES-RNN, and other benchmarks on electricity price forecasting considerably.</p>
    <p class="normal">Continuing with the legacy of N-BEATS, we will now talk about another modification to the architecture that makes it suitable for long-term forecasting.</p>
    <h1 id="_idParaDest-365" class="heading-1">Neural Hierarchical Interpolation for Time Series Forecasting (N-HiTS)</h1>
    <p class="normal">Although there<a id="_idIndexMarker1252"/> has been a good amount of work from DL to tackle time series forecasting, very little focus has been on long-horizon forecasting. Despite recent progress, long-horizon forecasting remains a challenge for two reasons:</p>
    <ul>
      <li class="bulletList">The expressiveness required to truly capture the variation</li>
      <li class="bulletList">The computational complexity</li>
    </ul>
    <p class="normal">Attention-based methods (Transformers) and N-BEATS-like methods scale quadratically in memory and the computational cost concerning the forecasting horizon.</p>
    <p class="normal">The authors claim that <a id="_idIndexMarker1253"/>N-HiTS drastically cuts long-forecasting compute costs while simultaneously showing 25% accuracy improvements compared to existing Transformer-based architectures across a large array of multi-variate forecasting datasets.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research paper by Challu et al. on N-HiTS is cited in the <em class="italic">References</em> section as <em class="italic">5</em>.</p>
    </div>
    <h2 id="_idParaDest-366" class="heading-2">The Architecture of N-HiTS</h2>
    <p class="normal">N-HiTS<a id="_idIndexMarker1254"/> can be considered as an alteration to N-BEATS because the two share a large part of their architectures. <em class="italic">Figure 16.1</em>, which shows the N-BEATS architecture, is still valid for N-HiTS. N-HiTS also has stacks of blocks arranged in a residual manner; it differs only in the kind of blocks it uses. For instance, there is no provision for interpretable blocks. All the blocks in N-HiTS are generic. While N-BEATS tries to decompose the signal into different patterns (trend, seasonality, and so on), N-HiTS tries to decompose the signal into multiple frequencies and forecast them separately. </p>
    <p class="normal">To enable this, a few key improvements have been proposed:</p>
    <ul>
      <li class="bulletList">Multi-rate data sampling</li>
      <li class="bulletList">Hierarchical interpolation</li>
      <li class="bulletList">Synchronizing the rate of input sampling with a scale of output interpolation across the blocks</li>
    </ul>
    <h3 id="_idParaDest-367" class="heading-3">Multi-rate data sampling</h3>
    <p class="normal">N-HiTS incorporates <a id="_idIndexMarker1255"/>sub-sampling layers before the fully connected blocks so that the resolution of the input to each block is different. This is similar to smoothing the signal with different resolutions so that each block is looking at a pattern that occurs at different resolutions—for instance, if one block looks at the input every day, another block looks at the output every week, and so on. This way, when arranged with different blocks looking at different resolutions, the model will be able to predict patterns that occur in those resolutions. This significantly reduces the memory footprint and the computation required as well, because instead of looking at all <em class="italic">H</em> steps of the lookback window, we are looking at smaller series (such as H/2, H/4, and so on).</p>
    <p class="normal">N-HiTS accomplishes <a id="_idIndexMarker1256"/>this using a Max Pooling or Average Pooling layer of kernel size <em class="italic">k</em><sub class="subscript-italic" style="font-style: italic;">l</sub> on the lookback window. A pooling operation is similar to a convolution operation, but the function that is used is non-learnable. In <em class="chapterRef">Chapter 12</em>, <em class="italic">Building Blocks of Deep Learning for Time Series</em>, we learned about convolutions, kernels, stride, and so on. While a convolution uses weights that are learned from data while training, a pooling operation uses a non-learnable and fixed function to aggregate the data in the receptive field of a kernel. Common examples of these functions are the maximum, average, sum, and so on. N-HiTS uses <code class="inlineCode">MaxPool1d</code> or <code class="inlineCode">AvgPool1d</code> (in <code class="inlineCode">PyTorch</code> terminology) with different kernel sizes for different blocks. Each pooling operation also has a stride equal to the kernel, resulting in non-overlapping windows over which we do the aggregation operation. To refresh our memory, let’s see what max pooling with <code class="inlineCode">kernel=2</code> and <code class="inlineCode">stride=2</code> looks like:</p>
    <figure class="mediaobject"><img src="../Images/B22389_16_03.png" alt="Figure 16.3 – Max pooling on one dimension – kernel=2, stride=2 "/></figure>
    <p class="packt_figref">Figure 16.3: Max pooling on one dimension—kernel = 2, stride = 2</p>
    <p class="normal">Therefore, a larger kernel size will tend to cut more high-frequency (or small-timescale) components from the input. This way, the block is forced to focus on larger-scale patterns. The paper calls<a id="_idIndexMarker1257"/> this <strong class="keyWord">multi-rate signal sampling</strong>.</p>
    <h3 id="_idParaDest-368" class="heading-3">Hierarchical interpolation</h3>
    <p class="normal">In a standard <a id="_idIndexMarker1258"/>multi-step forecasting setting, the model must forecast <em class="italic">H</em> timesteps. As <em class="italic">H</em> becomes larger, the compute requirements increase and lead to an explosion of expressive power the model needs to have. </p>
    <p class="normal">Training a model with such a large expressive power, without overfitting, is a challenge in itself. To combat these issues, N-HiTS proposes a <a id="_idIndexMarker1259"/>technique called <strong class="keyWord">temporal interpolatio</strong><strong class="keyWord">n</strong> <code class="inlineCode">(</code>not the simple interpolation between two known points in time, but something specific to the architecture).</p>
    <p class="normal">The pooled<a id="_idIndexMarker1260"/> input (which we saw in the previous section) goes into the block along with the usual mechanism to generate expansion coefficients and finally gets converted into forecast output. But here, instead of setting the dimension of the expansion coefficients as <em class="italic">H</em>, N-HiTS sets them as <em class="italic">r</em><sub class="subscript-italic" style="font-style: italic;">l</sub> X <em class="italic">H</em>, where <em class="italic">r</em><sub class="subscript-italic" style="font-style: italic;">l</sub> is the <strong class="keyWord">expressiveness ratio</strong>. This <a id="_idIndexMarker1261"/>parameter essentially reduces the forecast output dimension and thus controls the issues we discussed in the previous paragraph. To recover the original sampling rate and predict all the <em class="italic">H</em> points in the forecast horizon, we can use an interpolation function. There are many options for the interpolation functions—linear, nearest neighbor, cubic, and so on. All these options can easily be implemented in <code class="inlineCode">PyTorch</code> using the <code class="inlineCode">interpolate</code> function.</p>
    <h3 id="_idParaDest-369" class="heading-3">Synchronizing the input sampling and output interpolation</h3>
    <p class="normal">In addition <a id="_idIndexMarker1262"/>to proposing the input sampling through pooling and output interpolation, N-HiTS also proposes arranging them in different blocks in a particular way. The authors argue that hierarchical interpolation can only happen the right way if the expressiveness ratios are distributed across blocks in a manner that is synchronized with the multi-rate sampling. Blocks closer to the input should have a smaller expressiveness ratio, <em class="italic">r</em><sub class="subscript-italic" style="font-style: italic;">l</sub>, and larger kernel sizes, <em class="italic">k</em><sub class="subscript-italic" style="font-style: italic;">l</sub>. This means that the blocks closer to the input will generate larger resolution patterns (because of aggressive interpolation) while being forced to look at aggressively subsampled input signals. The paper proposes exponentially increasing expressiveness ratios as we move from the initial block to the last block to handle a wide range of frequency bands. The official N-HiTS implementation uses the following formula to set the expressiveness ratios and pooling kernels:</p>
    <pre class="programlisting code"><code class="hljs-code">pooling_sizes = np.exp2(
    np.<span class="hljs-built_in">round</span>(np.linspace(<span class="hljs-number">0.49</span>, np.log2(prediction_length / <span class="hljs-number">2</span>), n_stacks))
)
pooling_sizes = [<span class="hljs-built_in">int</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> pooling_sizes[::-<span class="hljs-number">1</span>]]
downsample_frequencies = [
    <span class="hljs-built_in">min</span>(prediction_length, <span class="hljs-built_in">int</span>(np.power(x, <span class="hljs-number">1.5</span>))) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> pooling_sizes
]
</code></pre>
    <p class="normal">We can also provide explicit <code class="inlineCode">pooling_sizes</code> and <code class="inlineCode">downsampling_fequencies</code> to reflect known cycles of the time series (weekly seasonality, monthly seasonality, and so on). The<a id="_idIndexMarker1263"/> core principle of N-BEATS (one block removing the effect it captures from the signal and passing it on to the next block) is used here as well so that, at each level, the patterns or frequencies that a block captures are removed from the input signal before being passed on to the next block. In the end, the final forecast is the sum of all such individual block forecasts.</p>
    <h2 id="_idParaDest-370" class="heading-2">Forecasting with N-HiTS</h2>
    <p class="normal"><em class="italic">N-HiTS</em> is implemented<a id="_idIndexMarker1264"/> in NIXTLA forecasting. We can use the same framework we were working with for NBEATS and extend it to train <em class="italic">N-HiTS</em> on our data. What’s even better is that the implementation supports exogenous variables, the same way N-BEATSx handles exogenous variables (although without the exogenous block). First, let’s look at the initialization parameters of the implementation.</p>
    <p class="normal">The <code class="inlineCode">NHITS</code> class in <code class="inlineCode">neuralforecast</code> has the following parameters:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">n_blocks</code>: This is a list of integers signifying the number of blocks to be used in each stack. For instance, <code class="inlineCode">[1,1,1]</code> means there will be three stacks with one block each.</li>
      <li class="bulletList"><code class="inlineCode">n_pool_kernel_size</code>: This is a list of integers that defines the pooling size (<em class="italic">k</em><sub class="subscript-italic" style="font-style: italic;">l</sub>) for each stack. This is an optional parameter, and if provided, we can have more control over how the pooling happens in the different stacks. Using an ordering of higher to lower improves results.</li>
      <li class="bulletList"><code class="inlineCode">pooling_mode</code>: This defines the kind of pooling to be used. It should be either <code class="inlineCode">'MaxPool1d'</code> or <code class="inlineCode">'AvgPool1d'</code>.</li>
      <li class="bulletList"><code class="inlineCode">n_freq_downsample</code>: This is a list of integers that defines the expressiveness ratios (<em class="italic">r</em><sub class="subscript-italic" style="font-style: italic;">l</sub>) for each stack. This is an optional parameter, and if provided, we can have more control over how the interpolation happens in the different stacks.</li>
    </ul>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">The complete code for training N-HiTS can be found in the <code class="inlineCode">02-NHiTS_NeuralForecast.ipynb</code> notebook in the <code class="inlineCode">Chapter16</code> folder.</p>
    </div>
    <p class="normal">Now, let’s shift our focus and look at a few modifications of the Transformer model to make it better for time series forecasting.</p>
    <h1 id="_idParaDest-371" class="heading-1">Autoformer</h1>
    <p class="normal">Recently, Transformer <a id="_idIndexMarker1265"/>models have shown superior performance in capturing long-term patterns than standard RNNs. One of the major factors of that is the fact that self-attention, which powers Transformers, can reduce the length that the relevant sequence information has to be held on to before it can be used for prediction. In other words, in an RNN, if the timestep 12 steps before holds important information, that information has to be stored in the RNN through 12 updates before it can be used for prediction. But with self-attention in Transformers, the model is free to create a shortcut between lag 12 and the current step directly because of the lack of recurrence in the structure.</p>
    <p class="normal">But the same self-attention is also the reason why we can’t scale vanilla Transformers to long sequences. In the previous section, we discussed how long-term forecasting is a challenge because of two reasons: the expressiveness required to truly capture the variation and computational complexity. Self-attention, with its quadratic computational complexity, contributes to the second reason.</p>
    <p class="normal">The research community has recognized this challenge and has put a lot of effort into devising efficient transformers through many techniques, such as downsampling, low-rank approximations, sparse attention, and so on. For a detailed account of such techniques, refer to the link for <em class="italic">Efficient Transformers: A Survey</em> in the <em class="italic">Further reading</em> section.</p>
    <p class="normal"><code class="inlineCode">Autoformer</code> is another model that is designed for long-term forecasting. Autoformer invents a new kind of attention and couples it with aspects from time series decomposition. Let’s take a look at what makes Autoformer special.</p>
    <h2 id="_idParaDest-372" class="heading-2">The architecture of the Autoformer model</h2>
    <p class="normal">The<a id="_idIndexMarker1266"/> Autoformer model is a modification of Transformers. The following are its major contributions:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Uniform Input Representation</strong>: A methodical way to include the history of the series along <a id="_idIndexMarker1267"/>with other information, which will help in capturing long-term signals such as the week, month, holidays, and so on</li>
      <li class="bulletList"><strong class="keyWord">Generative-style decoder</strong>: Used to generate the long-term horizon in a single <a id="_idIndexMarker1268"/>forward pass instead of via dynamic recurrence</li>
      <li class="bulletList"><strong class="keyWord">AutoCorrelation mechanism</strong>: An alternative<a id="_idIndexMarker1269"/> to standard dot product attention, which<a id="_idIndexMarker1270"/> takes into account sub-series similarity rather than point-to-point similarity</li>
      <li class="bulletList"><strong class="keyWord">Decomposition architecture</strong>: A specially designed architecture that separates <a id="_idIndexMarker1271"/>seasonality, trend, and residual in a time series while modeling it</li>
    </ul>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research paper by Wu et al. on Autoformer is cited in the <em class="italic">References</em> section as <em class="italic">9</em>.</p>
    </div>
    <h3 id="_idParaDest-373" class="heading-3">Uniform Input Representation</h3>
    <p class="normal">RNNs capture <a id="_idIndexMarker1272"/>time series patterns with their recurrent structure, so they only need the sequence; they don’t need information about the timestamp to extract the patterns. However, the self-attention in Transformers is done via point-wise operations that are performed in sets (the order doesn’t matter in a set). Typically, we include positional encodings to capture the order of the sequence. Instead of using positional encodings, we can use richer information, such as hierarchical timestamp information (such as weeks, months, years, and so on). This is what the authors proposed<a id="_idIndexMarker1273"/> through <strong class="keyWord">Uniform Input Representation</strong>.</p>
    <p class="normal">Uniform Input Representation uses three types of embeddings to capture the history of the time series, the sequence of values in the time series, and the global timestamp information. The sequence of values in the time series is captured by the standard positional embedding of the <code class="inlineCode">d_model</code> dimension.</p>
    <p class="normal">Uniform Input Representation uses a one-dimensional convolutional layer with <code class="inlineCode">kernel=3</code> and <code class="inlineCode">stride=1</code> to project the history (which is scalar or one-dimensional) into an embedding of <code class="inlineCode">d_model</code> dimensions. This is<a id="_idIndexMarker1274"/> referred to as <strong class="keyWord">value embedding</strong>.</p>
    <p class="normal">The global timestamp information is embedded by a learnable embedding of <code class="inlineCode">d_model</code> dimensions with limited vocabulary in a mechanism that is identical to embedding categorical variables into fixed-size vectors (<em class="chapterRef">Chapter 15</em>, <em class="italic">Strategies for Global Deep Learning Forecasting Models</em>). This is<a id="_idIndexMarker1275"/> referred to as <strong class="keyWord">temporal embedding</strong>.</p>
    <p class="normal">Now that we have three embeddings of the same dimension, <code class="inlineCode">d_model</code>, all we need to do is add them together to get the Uniform Input Representation.</p>
    <h3 id="_idParaDest-374" class="heading-3">Generative-style decoder</h3>
    <p class="normal">The <a id="_idIndexMarker1276"/>standard way of inferencing a Transformer model is by decoding one token at a time. This autoregressive process is time-consuming and repeats a lot of calculations for each step. To alleviate this problem, the Autoformer model adopts a more generative fashion where the entire forecasting horizon is generated in a single forward pass.</p>
    <p class="normal">In NLP, it is a popular <a id="_idIndexMarker1277"/>technique to use a special token (START) to start the dynamic decoding process. Instead of choosing a special token for this purpose, the Autoformer model chooses a sample from the input sequence, such as an earlier slice before the output window. For instance, if we say the input window is <em class="italic">t</em><sub class="subscript">1</sub> to <em class="italic">t</em><sub class="subscript-italic" style="font-style: italic;">w</sub>, we will sample a sequence of length <em class="italic">C</em> from the input, <em class="italic">t</em><sub class="subscript-italic" style="font-style: italic;">w-c</sub> to <em class="italic">t</em><sub class="subscript-italic" style="font-style: italic;">w</sub>, and include this sequence as the starting sequence of the decoder. To make the model predict the entire horizon in a single forward pass, we can extend the decoder input tensor so that its length is <em class="italic">C</em> + <em class="italic">H</em>, where <em class="italic">H</em> is the length of the prediction horizon. The initial <em class="italic">C</em> tokens are filled with the sample sequence from the input, and the rest are filled as zeros—that is, <img src="../Images/B22389_16_028.png" alt=""/>. This is just the target. Although <img src="../Images/B22389_16_029.png" alt=""/> has zeros filled in for the prediction horizon, this is just for the target. The other information, such as the global timestamps, is included in <img src="../Images/B22389_16_029.png" alt=""/>. Sufficient masking of the attention matrix is also employed so that each position does not attend to future positions, thus maintaining the autoregressive nature of the prediction.</p>
    <p class="normal">Now, let’s look at the time series decomposition architecture.</p>
    <h3 id="_idParaDest-375" class="heading-3">Decomposition architecture</h3>
    <p class="normal">We saw this idea<a id="_idIndexMarker1278"/> of decomposition back in <em class="chapterRef">Chapter 3</em>, <em class="italic">Analyzing and Visualizing Time Series Data</em>, and even in this chapter (N-BEATS). Autoformer <a id="_idIndexMarker1279"/>successfully renovated the Transformer architecture into a deep-decomposition architecture:</p>
    <figure class="mediaobject"><img src="../Images/B22389_16_04.png" alt="Figure 16.5 – Autoformer architecture "/></figure>
    <p class="packt_figref">Figure 16.4: Autoformer architecture</p>
    <p class="normal">It is easier to understand the overall architecture first and then dive deeper into the details. In <em class="italic">Figure 16.4</em>, there are boxes labeled <strong class="keyWord">Auto-Correlation</strong> and <strong class="keyWord">Series Decomp</strong>. For now, just know that auto-correlation is a type of attention and that series decomposition is a particular block that decomposes the signal into trend-cyclical and seasonal components.</p>
    <h4 class="heading-4">Encoder</h4>
    <p class="normal">With the <a id="_idIndexMarker1280"/>level of abstraction discussed in the preceding section, let’s understand what is happening in the encoder:</p>
    <ol>
      <li class="numberedList" value="1">The uniform representation of the time series, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">en</sub>, is the input to the encoder. The input is passed <a id="_idIndexMarker1281"/>through an <strong class="keyWord">Auto-Correlation</strong> block (for self-attention) whose output is <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">ac</sub>.</li>
      <li class="numberedList">The uniform representation, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">en</sub>, is added back to <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">ac</sub> as a residual connection, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">ac</sub> =<em class="italic"> x</em><sub class="subscript-italic" style="font-style: italic;">ac</sub> + <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">en</sub>.</li>
      <li class="numberedList">Now, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">ac</sub> is passed <a id="_idIndexMarker1282"/>through a <strong class="keyWord">Series Decomp</strong> block, which decomposes the signal into a trend-cyclical component (<em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">T</sub>) and a seasonal component, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">seas</sub>.</li>
      <li class="numberedList">We discard <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">T</sub> and pass <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">seas</sub> to a Feed Forward network, which gives <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">FF</sub> as an output.</li>
      <li class="numberedList"><em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">seas</sub> is again <a id="_idIndexMarker1283"/>added to <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">FF</sub> as a residual connection, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">seas</sub> = <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">FF</sub> + <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">seas</sub>.</li>
      <li class="numberedList">Finally, this <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">seas</sub> is passed through another <strong class="keyWord">Series Decomp</strong> layer, which again decomposes the signal into the trend, <img src="../Images/B22389_16_031.png" alt=""/>, and a seasonal component, <img src="../Images/B22389_16_032.png" alt=""/>.</li>
      <li class="numberedList">We discard <img src="../Images/B22389_16_031.png" alt=""/>, and pass on <img src="../Images/B22389_16_032.png" alt=""/> as the final output from one block of the encoder.</li>
      <li class="numberedList">There may be <em class="italic">N</em> blocks of encoders stacked together, one taking in the output of the previous encoder as input.</li>
    </ol>
    <p class="normal">Now, let’s shift our attention to the decoder block.</p>
    <h4 class="heading-4">Decoder</h4>
    <p class="normal">The <a id="_idIndexMarker1284"/>Autoformer model uses a START token-like mechanism by including a sampled window from the input sequence. But instead of just taking the sequence, Autoformer does a bit of special processing on it. Autoformer uses the bulk of its learning power to learn seasonality. The output of the transformer is also just the seasonality. Therefore, instead of including the complete window from the input sequence, Autoformer decomposes the signal and only includes the seasonal component in the START token. Let’s look at this process step by step:</p>
    <ol>
      <li class="numberedList" value="1">If the input (the context window) is <em class="italic">x</em>, we decompose it with the <strong class="keyWord">Series Decomp</strong> block into <img src="../Images/B22389_16_035.png" alt=""/> and <img src="../Images/B22389_16_036.png" alt=""/>.</li>
      <li class="numberedList">Now, we sample <em class="italic">C</em> timesteps from the end of <img src="../Images/B22389_16_036.png" alt=""/> and append <em class="italic">H</em> zeros, where <em class="italic">H</em> is the forecast horizon, and construct <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">ds</sub>.</li>
      <li class="numberedList">This <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">ds</sub> is then used to create a uniform representation, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dec</sub>.</li>
      <li class="numberedList">Meanwhile, we sample <em class="italic">C</em> timesteps from the end of <img src="../Images/B22389_16_035.png" alt=""/> and append <em class="italic">H</em> timesteps with the series mean (<em class="italic">mean</em>(<em class="italic">x</em>)), where <em class="italic">H</em> is the forecast horizon, and construct <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dt</sub>.</li>
    </ol>
    <p class="normal">This <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dec</sub> is then used as the input for the decoder. This is what happens in the decoder:</p>
    <ol>
      <li class="numberedList" value="1">The input, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dec</sub> , is first passed through an <code class="inlineCode">Auto-Correlation</code> (for self-attention) block whose output is <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dac</sub>.</li>
      <li class="numberedList">The uniform representation, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dec</sub> , is added back to <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dac</sub> as a residual connection, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dac</sub> =<em class="italic"> x</em><sub class="subscript-italic" style="font-style: italic;">dac</sub> +<em class="italic"> x</em><sub class="subscript-italic" style="font-style: italic;">dec</sub>.</li>
      <li class="numberedList">Now, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dac</sub> is passed through a <strong class="keyWord">Series Decomp</strong> block that decomposes the signal into a trend-cyclical component (<em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dT</sub><sub class="subscript">1</sub>) and a seasonal component, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dseas</sub>.</li>
      <li class="numberedList">In the <a id="_idIndexMarker1285"/>decoder, we do not discard the trend component; instead, we save it. This is because we will be adding all the trend components with the trend in it (<em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dt</sub>) to come up with the overall trend part (<em class="italic">T</em>).</li>
      <li class="numberedList">The seasonal output from the <strong class="keyWord">Series Decomp</strong> block (<em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dseas</sub>), along with the output from the encoder (<img src="../Images/B22389_16_032.png" alt=""/>), is then passed into another <strong class="keyWord">Auto-Correlation</strong> block where cross-attention between the decoder sequence and encoder sequence is calculated. Let the output of this block be <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">cross</sub>.</li>
      <li class="numberedList">Now, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dseas</sub> is added back to <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">cross</sub> as a residual connection, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">cross</sub> =<em class="italic"> x</em><sub class="subscript-italic" style="font-style: italic;">cross</sub> + <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dseas</sub>.</li>
      <li class="numberedList"><em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">cross</sub> is again passed through a <strong class="keyWord">Series Decomp</strong> block, which splits <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">cross</sub> into two components— <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dT2</sub> and <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dseas2</sub>.</li>
      <li class="numberedList"><em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dseas</sub> is then transformed <a id="_idIndexMarker1286"/>using a <strong class="keyWord">Feed Forward</strong> network into <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dff</sub> and <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dseas</sub> is added to it in a residual connection, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dff</sub>= <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dff</sub> + <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dseas</sub>.</li>
      <li class="numberedList">Finally, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dff</sub> is passed through yet another <strong class="keyWord">Series Decomp</strong> block, which decomposes it into two components—<em class="italic"> x</em><sub class="subscript-italic" style="font-style: italic;">dT3</sub> and <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dseas3</sub>. <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dseas3</sub> is the final output of the decoder, which captures seasonality.</li>
      <li class="numberedList">Another output is the residual trend, <img src="../Images/B22389_16_040.png" alt=""/>, which is a projection of the summation of all the trend components extracted in the decoder’s <strong class="keyWord">Series Decomp</strong> blocks. The<a id="_idIndexMarker1287"/> projection layer is a <strong class="keyWord">Conv1d</strong> layer, which projects the extracted trend to the desired output dimension: <img src="../Images/B22389_16_041.png" alt=""/>.</li>
      <li class="numberedList"><em class="italic">M</em> such decoder layers are stacked on top of each other, each one feeding its output as the input to the next one.</li>
      <li class="numberedList">The residual trend, <img src="../Images/B22389_16_040.png" alt=""/>, of each decoder layer gets added to the trend init, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dt</sub>, to model the overall trend component (<em class="italic">T</em>).</li>
      <li class="numberedList">The <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">dseas3</sub> of the final decoder layer is considered to be the overall seasonality component and is projected to the desired output dimension (<em class="italic">S</em>) using a linear layer.</li>
      <li class="numberedList">Finally, the prediction or the forecast <em class="italic">X</em><sub class="subscript-italic" style="font-style: italic;">out</sub>= <em class="italic">T</em> + <em class="italic">S</em>.</li>
    </ol>
    <p class="normal">The whole <a id="_idIndexMarker1288"/>architecture is cleverly designed so that the relatively stable and easy-to-predict part of the time series (the trend-cyclical) is removed and the difficult-to-capture seasonality can be modeled well.</p>
    <p class="normal">Now, how does the <strong class="keyWord">Series Decomp</strong> block decompose the series? The mechanism may be familiar to you already: <code class="inlineCode">AvgPool1d</code> with some padding so that it maintains the same size as the input. This acts like a moving average over the specified kernel width.</p>
    <p class="normal">We have been talking about the <strong class="keyWord">Auto-Correlation</strong> block throughout this explanation. Now, let’s understand the ingenuity of the <strong class="keyWord">Auto-Correlation</strong> block.</p>
    <h3 id="_idParaDest-376" class="heading-3">Auto-correlation mechanism</h3>
    <p class="normal">Autoformer<a id="_idIndexMarker1289"/> uses an auto-correlation mechanism in place of standard scaled dot product attention. This discovers sub-series similarity <a id="_idIndexMarker1290"/>based on periodicity and uses this similarity to aggregate similar sub-series. This clever mechanism breaks the information bottleneck by expanding the point-wise operation of the scaled dot product attention to a sub-series level operation. The initial part of the overall mechanism is similar to the standard attention procedure, where we project the query, key, and values into the same dimension using weight matrices. The key difference is the attention weight calculation and how they are used to calculate the values. This mechanism achieves this by using two salient sub-mechanisms: discovering period-based dependencies and time delay aggregation.</p>
    <h4 class="heading-4">Period-based dependencies</h4>
    <p class="normal">Autoformer uses <a id="_idIndexMarker1291"/>autocorrelation as the key measure of similarity. Auto-correlation, as we know, represents the similarity between a given time series, <em class="italic">X</em><sub class="subscript-italic" style="font-style: italic;">t</sub>, and its lagged series. For instance, <img src="../Images/B22389_16_043.png" alt=""/> is the autocorrelation between the time series <em class="italic">X</em><sub class="subscript-italic" style="font-style: italic;">t</sub> and <img src="../Images/B22389_16_044.png" alt=""/>. Autoformer considers this autocorrelation as the unnormalized confidence of the particular lag. Therefore, from the list of all <img src="../Images/B22389_16_045.png" alt=""/>, we choose <em class="italic">k</em> most possible lags and use <em class="italic">softmax</em> to convert these unnormalized confidences into probabilities. We <a id="_idIndexMarker1292"/>use these probabilities as weights to aggregate relevant sub-series (we will talk about this in the next section).</p>
    <p class="normal">The autocorrelation calculation is not the most efficient operation and Autoformer suggests an alternative to <a id="_idIndexMarker1293"/>make the calculation faster. Based on the <strong class="keyWord">Wiener–Khinchin theorem</strong> in <strong class="keyWord">Stochastic Processes</strong> (this is outside the scope of the book, but for<a id="_idIndexMarker1294"/> those who are interested, I have included a link in the <em class="italic">Further reading</em> section), autocorrelation can also be calculated using <strong class="keyWord">Fast Fourier Transform</strong> (<strong class="keyWord">FFT</strong>). The<a id="_idIndexMarker1295"/> process can be seen as follows:</p>
    <p class="center"><img src="../Images/B22389_16_046.png" alt=""/></p>
    <p class="normal">Here, <img src="../Images/B22389_16_047.png" alt=""/> denotes the FFT and <img src="../Images/B22389_16_048.png" alt=""/> denotes the conjugate operation (the conjugate of a complex number is the number with the same real part and an imaginary part, which is equal in magnitude but with the sign reversed. The mathematics around this is outside the scope of this book). </p>
    <p class="normal">This can easily be written in PyTorch as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># calculating the FFT of Query and Key</span>
q_fft = torch.fft.rfft(queries.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>).contiguous(), dim=-<span class="hljs-number">1</span>)
k_fft = torch.fft.rfft(keys.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>).contiguous(), dim=-<span class="hljs-number">1</span>)
<span class="hljs-comment"># Multiplying the FFT of Query with Conjugate FFT of Key</span>
res = q_fft * torch.conj(k_fft)
</code></pre>
    <p class="normal">Now, <img src="../Images/B22389_16_049.png" alt=""/> is in the spectral domain. To bring it back to the real domain, we need to do an inverse FFT: </p>
    <p class="center"><img src="../Images/B22389_16_050.png" alt=""/></p>
    <p class="normal">Here, <img src="../Images/B22389_16_051.png" alt=""/> denotes the inverse FFT. In PyTorch, we can do this easily:</p>
    <pre class="programlisting code"><code class="hljs-code">corr = torch.fft.irfft(res, dim=-<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">When the<a id="_idIndexMarker1296"/> query and key are the same, this calculates self-attention; when they are different, they calculate cross-attention.</p>
    <p class="normal">Now, all we need to do is take the top-k values from <code class="inlineCode">corr</code> and use them to aggregate the sub-series.</p>
    <h4 class="heading-4">Time delay aggregation</h4>
    <p class="normal">We have <a id="_idIndexMarker1297"/>identified the major lags that are auto-correlated using the FFT and inverse-FFT. For a more concrete example, the dataset we have been working on (<em class="italic">London Smart Meter Dataset</em>) has a half-hourly frequency and has strong daily and weekly seasonality. Therefore, the auto-correlation identification may have picked out 48 and 48*7 as the two most important lags. In the standard attention mechanism, we use the calculated probability as weights to aggregate the value. Autoformer also does something similar, but instead of applying the weights to points, it applies them to sub-series.</p>
    <p class="normal">Autoformer does this by shifting the time series by the lag, <img src="../Images/B22389_16_045.png" alt=""/>, and then using the lag’s weight to aggregate them:</p>
    <p class="center"><img src="../Images/B22389_16_053.png" alt=""/></p>
    <p class="normal">Here, <img src="../Images/B22389_16_054.png" alt=""/> is the <em class="italic">softmax</em>-ed probabilities on the <em class="italic">top-k</em> autocorrelations.</p>
    <p class="normal">In our example, we can think of this as shifting the series by 48 timesteps so that the previous day’s timesteps are aligned with the current day and then using the weight of the 48 lag to scale it. Then, we can move on to the 48*7 lag, align the previous week’s timesteps with the current week, and then use the weight of the 48*7 lag to scale it. So, in the end, we will get a weighted mixture of the seasonality patterns that we can observe daily and weekly. Since these weights are learned by the model, we can hypothesize that different blocks learn to focus on different seasonalities, and thus as a whole, the blocks learn the overall pattern in the time series.</p>
    <h2 id="_idParaDest-377" class="heading-2">Forecasting with Autoformer</h2>
    <p class="normal"><code class="inlineCode">Autoformer</code> is <a id="_idIndexMarker1298"/>implemented in NIXTLA forecasting. We can use the same framework we were working with for NBEATS and extend it to train <code class="inlineCode">Autoformer</code> on our data. First, let’s look at the initialization parameters of the implementation.</p>
    <div class="note">
      <p class="normal">We have to keep in mind that the Autoformer model does not support exogenous variables. The only additional information it officially supports is global timestamp information such as the week, month, and so on, along with holiday information. We can technically extend this to any categorical feature (static or dynamic), but no real-valued information is currently supported.</p>
    </div>
    <p class="normal">Let’s look at the initialization parameters of the implementation.</p>
    <p class="normal">The <code class="inlineCode">Autoformer</code> class has<a id="_idIndexMarker1299"/> the following major parameters:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">distil</code>: This is a Boolean flag for turning the attention distillation off and on.</li>
      <li class="bulletList"><code class="inlineCode">encoder_layers</code>: This is an integer representing the number of encoder layers.</li>
      <li class="bulletList"><code class="inlineCode">decoder_layers</code>: This is an integer representing the number of decoder layers.</li>
      <li class="bulletList"><code class="inlineCode">n_head</code>: This is an integer representing the number of attention heads.</li>
      <li class="bulletList"><code class="inlineCode">conv_hidden_size</code>: This is an integer parameter that specifies the channels of the convolutional encoder, which can be thought of similarly to controlling the number of kernels or filters in the convolutional layers. The number of channels effectively determines how many different filters are applied to the input data, each capturing different features.</li>
      <li class="bulletList"><code class="inlineCode">activation</code>: This is a string that takes in one of two values—<code class="inlineCode">relu</code> or <code class="inlineCode">gelu</code>. This is the activation to be used in the encoder and decoder layers.</li>
      <li class="bulletList"><code class="inlineCode">factor</code>: This is an int value that helps us control the top-k values that will be selected in the Auto Correlation mechanism we discussed. <code class="inlineCode">top_k = int(self.factor * math.log(length))</code> is the exact formula used, but we can treat <em class="italic">k</em> as a factor to control the top <em class="italic">K</em> selection.</li>
      <li class="bulletList"><code class="inlineCode">dropout</code>: This <a id="_idIndexMarker1300"/>is a float between 0 and 1, which determines the strength of the dropout in the network.</li>
    </ul>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">The complete code for training the Autoformer model can be found in the <code class="inlineCode">03-Autoformer_NeuralForecast.ipynb</code> notebook in the <code class="inlineCode">Chapter16</code> folder.</p>
    </div>
    <p class="normal">Let’s switch tracks and look at a family of simple linear models that were proposed to challenge Transformers in <strong class="keyWord">Long-Term Time Series Forecasting</strong> (<strong class="keyWord">LTSF</strong>).</p>
    <h1 id="_idParaDest-378" class="heading-1">LTSF-Linear family of models</h1>
    <p class="normal">There <a id="_idIndexMarker1301"/>has been a lot of debate on whether Transformers are right for forecasting problems, how popular Transformer papers haven’t used strong baselines to show their superiority, how the order-agnostic attention mechanism may not be the best way to approach strongly ordered time series, and so on. The criticism was more pronounced for Long-Term Time Series Forecasting as it relies more on the extraction of strong trends and seasonalities. In 2023, Ailing Zeng et al. decided to put the Transformer models to the test and conducted a wide study using 5 multivariate datasets, pitting five Transformer models (FEDFormer, Autoformer, Informer, Pyraformer, and LogTrans) against a set of simple linear models that they proposed. Surprisingly, the simple linear models they proposed beat all the Transformer models comfortably.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research papers by Ailing Zeng et al. and the different Transformer models, FEDFormer, Autoformer, Informer, Pyraformer, and LogTrans, are cited in the <em class="italic">References</em> section as <em class="italic">14</em>, <em class="italic">16</em>, <em class="italic">9</em>, <em class="italic">8</em>, <em class="italic">15</em>, and <em class="italic">17</em> respectively.</p>
    </div>
    <p class="normal">There are three models in the family of LTSF models that the authors proposed:</p>
    <ol>
      <li class="numberedList" value="1">Linear</li>
      <li class="numberedList">D-Linear</li>
      <li class="numberedList">N-Linear</li>
    </ol>
    <p class="normal">These models are so simple that it’s almost embarrassing that they outperformed the Transformer models. But once you get to know them a bit more, you might appreciate the simple but effective inductive biases that have been built into the model. Let’s look at them one by one.</p>
    <h2 id="_idParaDest-379" class="heading-2">Linear</h2>
    <p class="normal">Just as<a id="_idIndexMarker1302"/> the name suggests, this is a simple linear model. It takes the context window and applies a linear layer to predict the forecast horizon. It also considers different time series as separate channels and applies different linear layers to each of them. In <code class="inlineCode">PyTorch</code>, all we need to have is an <code class="inlineCode">nn.Linear</code> layer for each of the channels:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Declare nn.Linear for each channel</span>
layers = nn.ModuleList([nn.Linear(context_window, forecast_horizon) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_timeseries)])
<span class="hljs-comment">## Forward Method ##</span>
<span class="hljs-comment"># Now use these layers once you get the input (Batch, Context Length, Channel)</span>
forecast = [layers[i](<span class="hljs-built_in">input</span>[:,:,i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_timeseries)]
</code></pre>
    <p class="normal">This embarrassingly simple model was able to outperform a few Transformer models like the Informer, LogTrans, and so on.</p>
    <h2 id="_idParaDest-380" class="heading-2">D-Linear</h2>
    <p class="normal">D-Linear<a id="_idIndexMarker1303"/> took the simple linear model and injected a decomposition prior into it. We saw in <em class="chapterRef">Chapter 3</em> how we can decompose a time series into trend, seasonality, and residual. D-Linear does exactly that and uses a moving average (the window or the kernel size is a hyperparameter) and separates the input time series, <em class="italic">x</em>, into trend, <em class="italic">t</em> (the moving average), and the rest, <em class="italic">r</em> (seasonality + residual). Now, it proceeds to apply separate linear layers to <em class="italic">t</em> and <em class="italic">r</em> separately, and finally add them back together for the final forecast. Let’s look at a simplified <code class="inlineCode">PyTorch</code> implementation:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Declare nn.Linear for each channel, trend and seasonality separately</span>
trend_layers = nn.ModuleList([nn.Linear(context_window, forecast_horizon) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_timeseries)])
seasonality_layers = nn.ModuleList([nn.Linear(context_window, forecast_horizon) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_timeseries)])
<span class="hljs-comment">## Forward Method ##</span>
<span class="hljs-comment"># Now use these layers once you get the input (Batch, Context Length, Channel)</span>
<span class="hljs-comment"># series_decomp is a function extracting trend using moving aveages</span>
trend, seasonality = series_decomp(<span class="hljs-built_in">input</span>)
trend_forecast = [trend_layers[i]( trend[:,:,i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_timeseries)]
seasonality_forecast = [seasonality_layers[i]( seasonality [:,:,i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_timeseries)]
forecast = [trend_forecast[i] + seasonality_forecast[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_timeseries)]
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1304"/>decomposition prior in the model helps it perform better than a simple linear model consistently and it also outperforms all the Transformer models in the study in almost all the datasets used.</p>
    <h2 id="_idParaDest-381" class="heading-2">N-Linear</h2>
    <p class="normal">The authors <a id="_idIndexMarker1305"/>also proposed another model, which added another very simple modification to the linear model. This modification was to handle the distributional shifts in data that are inherent in time series data. In N-Linear, we just extract the last value in the input context and subtract it from the entire series (in a sort of normalization) and then use the linear layer for prediction. Now, once the output from the linear layer is available, we add back the last value that we subtracted earlier. In PyTorch, a simple implementation would look like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Declare nn.Linear for each channel</span>
layers = nn.ModuleList([nn.Linear(context_window, forecast_horizon) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_timeseries)])
<span class="hljs-comment">## Forward Method ##</span>
<span class="hljs-comment"># Extract the last value once you get the input (Batch, Context Length, Channel)</span>
<span class="hljs-comment"># Get the last value of time series</span>
last_value = sample_data[:,-<span class="hljs-number">1</span>:,:]
<span class="hljs-comment"># Normalize the time series</span>
norm_ts = sample_data - last_value
<span class="hljs-comment"># Use the linear layers</span>
output = [layers[i](norm_ts[:,:,i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_timeseries)]
<span class="hljs-comment"># Add back the last value</span>
forecast = [o + last_value[:,:,i] <span class="hljs-keyword">for</span> i, o <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(output)]
</code></pre>
    <p class="normal">N-Linear<a id="_idIndexMarker1306"/> models also perform quite well in comparison to the other Transfomer models in the study. In most of the datasets that were part of the study, N-Linear or D-Linear came out to be the top-performing model, which is quite telling.</p>
    <p class="normal">This paper exposed some major flaws in the way we were using Transformer models for time series forecasting, especially for multivariate time series problems. A typical input to a transformer is of the form (<em class="italic">Batch</em> x <em class="italic">Time steps</em> x <em class="italic">Embedding</em>). The most common way to forecast multivariate time series is to pass in all the time series or other features in a time step as the embedding. This results in seemingly unrelated values being embedded in a single token and mixed together in the attention mechanism (which itself isn’t strongly ordered). This leads to a “muddled’ representation and thereby Transformers might be struggling to wean out the real patterns from the data.</p>
    <div class="note">
      <p class="normal">This paper had such an impact that many newer models, including PatchTST and iTransformer, which we will be seeing later in the chapter, have used these models as benchmarks and showed that they perform better than them. This underlines the need for strong and simple methods to be reserved as strong baselines so that we aren’t misled by the “coolness” of any algorithm.</p>
    </div>
    <p class="normal">Now let’s see how we can also use these simple linear models and get good long-term forecasts.</p>
    <h2 id="_idParaDest-382" class="heading-2">Forecasting with the LTSF-Linear family</h2>
    <p class="normal">NLinear and DLinear are <a id="_idIndexMarker1307"/>implemented in NIXTLA forecasting with the same framework we have seen in the prior models.</p>
    <p class="normal">Let’s look at the initialization parameters of the implementation.</p>
    <p class="normal">The <code class="inlineCode">DLinear</code> class has similar parameters to many of the other models. Some callouts are the following major parameters:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">moving_avg_window</code>: This is an integer value of the window size used for trend-seasonality decomposition. This value should be an odd integer.</li>
      <li class="bulletList"><code class="inlineCode">exclude_insample_y</code>: This is a boolean value to skip the autoregressive features.</li>
    </ul>
    <p class="normal">The <code class="inlineCode">NLinear</code> class has<a id="_idIndexMarker1308"/> no additional parameters because it is just an input window to the output window map.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">The complete code for training the D-Linear model can be found in the <code class="inlineCode">04-DLinear_NeuralForecast.ipynb</code> notebook, and for the N-Linear model, in the <code class="inlineCode">05-NLinear_NeuralForecast.ipynb</code> notebook in the <code class="inlineCode">Chapter16</code> folder. </p>
    </div>
    <div class="packt_tip">
      <p class="normal"><strong class="keyWord">Practitioner’s tip</strong>:</p>
      <p class="normal">The jury is out on this debate as Transformers are modified more and more to suit time series forecasting. There might always be datasets where using a Transformer-based model gives you better performance than some other class of models. As practitioners, we should be able to suspend disbelief and try different classes of models to see which one fits well for our use case. After all, we only care about the dataset we are trying to forecast.</p>
    </div>
    <p class="normal">Now, let’s look at a modification of how Transformers can be used for time series that learned from the insights of the LTSF-Linear paper and showed that it can outperform the simple linear models we just saw.</p>
    <h1 id="_idParaDest-383" class="heading-1">Patch Time Series Transformer (PatchTST)</h1>
    <p class="normal">In 2021, Alexey Dosovitskiy et al. proposed Vision Transformer, which introduced the Transformer <a id="_idIndexMarker1309"/>architecture which was widely successful in Natural Language Processing to Vision. Although not the first to introduce patching, they applied it in a way that works really well for vision. The design broke up an image into patches and fed the transformer each patch in sequence.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research paper by Alexey Dosovitskiy et al. on Vision Tranformers and Yuqi Nie et al. on PatchTST are cited in the <em class="italic">References</em> section as <em class="italic">12</em> and <em class="italic">13</em>, respectively.</p>
    </div>
    <p class="normal">Fast-forward <a id="_idIndexMarker1310"/>to 2023, and we have the same patching design applied to time series forecasting. Yuqi Nie et al. proposed <strong class="keyWord">Patch Time Series Transformer</strong> (<strong class="keyWord">PatchTST</strong>) by adopting the patching design for time series. They were motivated by the apparent ineffectiveness of more complicated Transformer designs (like Autoformer and Informer) on time series forecasting. </p>
    <p class="normal">In 2023, Zheng et al. showed up many Transformer models by comparing them with a simple linear model which outperformed most of the Transformer models on common benchmarks. One of the key insights from the paper was that the point-wise application of time series to Transformer architecture doesn’t capture the locality information and strong ordering in time series data. Therefore, the authors proposed a simpler alternative that performs better than the linear models and solves the problem of including long context windows to Transformers without blowing up the memory and compute requirements.</p>
    <h2 id="_idParaDest-384" class="heading-2">The architecture of the PatchTST model</h2>
    <p class="normal">The<a id="_idIndexMarker1311"/> PatchTST model is a modification of Transformers. The following are its major contributions:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Patching</strong>: A methodical way to include the history of the series along with other information, which will help in capturing long-term signals such as the week, month, holidays, and so on.</li>
      <li class="bulletList"><strong class="keyWord">Channel-independence</strong>: A conceptual way to process multi-variate time series as separate, independent time series. Although I wouldn’t call this a major contribution, this is indeed something we need to be aware of.</li>
    </ul>
    <p class="normal"> Let’s take a look at these in a bit more detail.</p>
    <h3 id="_idParaDest-385" class="heading-3">Patching</h3>
    <p class="normal">We saw <a id="_idIndexMarker1312"/>some adaptations of Transformers for time series forecasting earlier in the chapter. All of them focused on making attention mechanisms adapt to time series forecasting and longer context windows. But all of them used attention in a pointwise manner. Let’s use a diagram to make the point clearer and introduce patching.</p>
    <figure class="mediaobject"><img src="../Images/B22389_16_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.5: Patched vs non-patched time series inputs to Transformers</p>
    <p class="normal">In <em class="italic">Figure 16.5</em>, we<a id="_idIndexMarker1313"/> are considering a time series with 8 time steps as an example. On the left-hand side, we can see how all the other transformer architectures we have discussed handle the time series. They use some mechanism, like the Uniform Representation in AutoFormer, to convert a time series point into a k-dimensional embedding and then feed it to the Transformer architecture point by point. The attention mechanism for each point is calculated by looking at all the other points in the context window.</p>
    <p class="normal">The PatchTST paper claims that this kind of point-wise attention for time series doesn’t capture the locality effectively and proposes converting the time series into patches and feeding those patches to the Transformer instead. Patching is nothing but making the time series into shorter time series in a process very similar (or almost identical) to the sliding window operation we saw earlier in the book. The major difference is that this patching is done after we have already sampled a window from the larger time series.</p>
    <p class="normal">Patching is typically defined by a couple of parameters:</p>
    <ul>
      <li class="bulletList">Patch Length (<em class="italic">P</em>) is the length of each sub-time series or patch.</li>
      <li class="bulletList">Stride (<em class="italic">S</em>) is the length of the non-overlapping region between two consecutive patches. More intuitively, this is the number of time steps we move in each iteration of patching. This holds the exact same meaning as stride in convolutions.</li>
    </ul>
    <p class="normal">With these<a id="_idIndexMarker1314"/> two parameters fixed, a time series of length <em class="italic">L</em> would result in <img src="../Images/B22389_16_055.png" alt=""/> patches. Here, we also pad repeated numbers of the last value to the end of the original sequence to ensure each patch is of the same size.</p>
    <p class="normal">In <em class="italic">Figure 16.5</em>, we can see that we have illustrated the patching process of a time series with length <img src="../Images/B22389_16_056.png" alt=""/>, with <img src="../Images/B22389_16_057.png" alt=""/>, and <img src="../Images/B22389_16_058.png" alt=""/>. Using the formula we saw just now, we can calculate <img src="../Images/B22389_16_059.png" alt=""/>. We can also see that the last value, 8, has been repeated at the end as a padding to make the last patch length also 4.</p>
    <p class="normal">Now, each of these patches is considered as the embedding, of sorts, and passed into and processed by the Transformer architecture. With this kind of input patching, for a given context of <img src="../Images/B22389_16_060.png" alt=""/>, the number of input tokens to the Transformer can be reduced to, approximately, <img src="../Images/B22389_16_061.png" alt=""/>. This means that the computational complexity and memory usage are also reduced by a factor of <img src="../Images/B22389_16_062.png" alt=""/>. This enables the model to process longer context windows with the same hardware constraints, thus possibly enhancing the forecasting performance of the model.</p>
    <p class="normal">Now, let’s look at channel independence.</p>
    <h3 id="_idParaDest-386" class="heading-3">Channel independence</h3>
    <p class="normal">A <a id="_idIndexMarker1315"/>multivariate time series can be thought of as a multi-channel signal. Transformer inputs can either be a single channel or multiple. Most of the other Transformer-based models capable of multivariate forecasting take the approach where the channels are mixed together and processed. Or, in other words, input tokens take in information from all time series and project it to a shared embedding space, mixing information. But other simpler approaches process each channel separately, and the authors of PatchTST bring that independence to Transformers.</p>
    <p class="normal">In practice, this<a id="_idIndexMarker1316"/> is very simple. Let’s try to understand it with an example. Consider a dataset where there are <img src="../Images/B22389_16_063.png" alt=""/> time series, making it a multi-variate time series. So, the input to the PatchTST would be <img src="../Images/B22389_16_064.png" alt=""/>, where <img src="../Images/B22389_16_065.png" alt=""/> is the batch size and <img src="../Images/B22389_16_066.png" alt=""/> is the length of the context window. After patching, it becomes <img src="../Images/B22389_16_067.png" alt=""/>, where <img src="../Images/B22389_16_068.png" alt=""/> is the number of patches and <img src="../Images/B22389_16_069.png" alt=""/> is the patch length. Now, to process this multi-variate signal in a channel-independent way, we just reshape the tensor such that each of the M time series becomes another sample in the batch, i.e., <img src="../Images/B22389_16_070.png" alt=""/>, where <img src="../Images/B22389_16_071.png" alt=""/>.</p>
    <p class="normal">While this independence brings some desirable properties to the model, it also means that any interaction between different time series is ignored as they are treated as completely independent. The model is still trained in a global model paradigm and will benefit from cross-learning, but any explicit interaction between different time series (like two time series varying together) is not captured.</p>
    <p class="normal">Apart from these major components, the architecture is pretty similar to vanilla Transformer architecture. Now, let’s look at how we can practically forecast using PatchTST.</p>
    <h2 id="_idParaDest-387" class="heading-2">Forecasting with PatchTST</h2>
    <p class="normal">PatchTST is <a id="_idIndexMarker1317"/>implemented in NIXTLA forecasting. The same framework as used previously can be used here with PatchTST as well.</p>
    <p class="normal">Let’s look at the initialization parameters of the implementation.</p>
    <p class="normal">The <code class="inlineCode">PatchTST</code> class has the following major parameters:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">encoder_layers</code>: This is an integer representing the number of encoder layers.</li>
      <li class="bulletList"><code class="inlineCode">hidden_size</code>: This parameter sets the size of the embeddings and the encoders, directly influencing the model’s capacity and its ability to capture information from the data. This is the activation to be used in the encoder and decoder layers.</li>
      <li class="bulletList"><code class="inlineCode">patch_len</code> &amp; <code class="inlineCode">stride</code>: These parameters define how the input sequence is divided into patches, which affects how the model perceives temporal dependencies. <code class="inlineCode">patch_len</code> controls the length of each segment, while stride affects the overlap between these segments.</li>
      <li class="bulletList"><code class="inlineCode">stride</code>: This parameter sets the size of the embeddings and the encoders, directly influencing the model’s capacity and its ability to capture information from the data. This is the activation to be used in the encoder and decoder layers.</li>
    </ul>
    <p class="normal">Regularization<a id="_idIndexMarker1318"/> parameters:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">dropout</code>: This is a float between 0 and 1, which determines the strength of the dropout in the network.</li>
      <li class="bulletList"><code class="inlineCode">fc_dropout</code>: This is a float value that is the linear layer dropout.</li>
      <li class="bulletList"><code class="inlineCode">head_dropout</code>: This is a float value that is the flatten layer dropout.</li>
      <li class="bulletList"><code class="inlineCode">attn_dropout</code>: This is a float value that is the attention layer dropout.<div class="note">
          <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
          <p class="normal">The complete code for training the PatchTST model can be found in the <code class="inlineCode">06-PatchTST_NeuralForecast.ipynb</code> notebook in the <code class="inlineCode">Chapter16</code> folder.</p>
        </div>
      </li>
    </ul>
    <p class="normal">Now, let’s look at another Transformer-based model that took the innovation from PatchTST and turned it on its head for good effect, outperforming the LTSF-Linear models.</p>
    <h1 id="_idParaDest-388" class="heading-1">iTransformer</h1>
    <p class="normal">We<a id="_idIndexMarker1319"/> have already talked at length about the inadequacies of Transformer architectures in handling multivariate time series, namely the inefficient capture of locality, the order-agnostic attention mechanism muddling up information across time steps, and so on. In 2024, Yong Liu et al. took a slightly different view of this problem and, in their own words, “an extreme case of patching.”</p>
    <h2 id="_idParaDest-389" class="heading-2">The architecture of iTransformer</h2>
    <p class="normal">They <a id="_idIndexMarker1320"/>proposed that it is not that the Transformer architecture is ineffective for time series forecasting, but rather it is improperly used. The authors suggested that we flip the inputs to the Transformer architecture so that the attention isn’t applied across time steps but rather across variates or different series/features on the time series. <em class="italic">Figure 16.6</em> shows the difference clearly.</p>
    <figure class="mediaobject"><img src="../Images/B22389_16_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.6: Transformers vs iTransformers—the difference</p>
    <p class="normal">In vanilla Transformers, we use the input as (<em class="italic">Batch</em> x <em class="italic">Time steps</em> x <em class="italic">Embeddings</em> <em class="italic">(features)</em>), the attention gets applied across the time steps, and eventually, the Position-Wise Feed Forward Network mixes the different features into a Variate-Mixed Representation. But when you flip the input to (<em class="italic">Batch</em> x <em class="italic">Embeddings</em> <em class="italic">(features)</em> x <em class="italic">Timesteps</em>), the attention gets calculated across the variables and the Position-Wise Feed Forward Network mixes the time leaving variates separate in a Variate-Unmixed Representation.</p>
    <p class="normal">This “flipping” comes with some more benefits. Now that attention isn’t calculated across time, we can include very large context windows with minimal computational and memory constraints (remember that the computational and memory complexity comes from the O(N<sup class="superscript">2</sup>) of the attention mechanism). In fact, the paper suggests including the entire time series history as the context window. On the other hand, we need to be mindful of the number of features or concurrent time series we include in the model.</p>
    <p class="normal">A typical Transformer architecture has these major components:</p>
    <ul>
      <li class="bulletList">Attention mechanism</li>
      <li class="bulletList">Feed forward network</li>
      <li class="bulletList">Layer normalization</li>
    </ul>
    <p class="normal">In the<a id="_idIndexMarker1321"/> inverted version, we already saw that attention is applied across variates and <a id="_idIndexMarker1322"/>the <strong class="keyWord">Feed Forward Network</strong> (<strong class="keyWord">FFN</strong>) learns generalizable representations of the lookback window for the final prediction of the forecast. The layer normalization also works out well in the inverted version. In standard Transformers, layer normalization is typically used to normalize the multivariate representation of each time step. But in the inverted version, we normalize each variate separately across time. This is similar to the normalization we were doing in the N-Linear model and has been proven to work well on non-stationary time series problems.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research paper by Yong Liu et al. on iTransformers is cited in the <em class="italic">References</em> section as <em class="italic">18</em>.</p>
    </div>
    <h2 id="_idParaDest-390" class="heading-2">Forecasting with iTransformer</h2>
    <p class="normal">iTransformer is<a id="_idIndexMarker1323"/> implemented in NIXTLA forecasting. The same framework as was used previously can be used here with iTransformer as well.</p>
    <p class="normal">The <code class="inlineCode">iTransformer</code> class has the following major parameters:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">n_series</code>: This is an integer representing the number of time series.</li>
      <li class="bulletList"><code class="inlineCode">e_layers</code>: This is an integer representing the number of encoder layers.</li>
      <li class="bulletList"><code class="inlineCode">d_layers</code>: This is an integer representing the number of decoder layers.</li>
      <li class="bulletList"><code class="inlineCode">d_ff</code>: This is an integer representing the number of kernels in the 1-dimensional convolutional layers used in the encoder and decoder layers.</li>
    </ul>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">The complete code for training the iTransformer model can be found in the <code class="inlineCode">07-iTransformer_NeuralForecast.ipynb</code> notebook in the <code class="inlineCode">Chapter16</code> folder.</p>
    </div>
    <p class="normal">Now, let’s look at one more, very successful, architecture that is well-designed to utilize all kinds of information in a global context.</p>
    <h1 id="_idParaDest-391" class="heading-1">Temporal Fusion Transformer (TFT)</h1>
    <p class="normal">TFT is a<a id="_idIndexMarker1324"/> model that is thoughtfully designed from the ground up to make the most efficient use of all the different kinds of information in a global modeling context—static and dynamic variables. TFT also has interpretability at the heart of all design decisions. The result is a high-performing, interpretable, and global DL model.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research paper by Lim et al. on TFT is cited in the <em class="italic">References</em> section as <em class="italic">10</em>.</p>
    </div>
    <p class="normal">At first glance, the model architecture looks complicated and daunting. But once you peel the onion, it is quite simple and ingenious. We will take this one level of abstraction at a time to ease you into the full model. Along the way, there will be many black boxes I’m going to ask you to take for granted, but don’t worry—we will open every one of them as we dive deeper.</p>
    <h2 id="_idParaDest-392" class="heading-2">The architecture of TFT</h2>
    <p class="normal">Let’s <a id="_idIndexMarker1325"/>establish some notations and a setting before we start. We have a dataset with <em class="italic">I</em> unique time series and each entity, <em class="italic">i</em>, has some static variables (<em class="italic">s</em><sub class="subscript-italic" style="font-style: italic;">i</sub>). The collection of all static variables of all entities can be represented by <em class="italic">S</em>. We also have the context window of length <em class="italic">k</em>. Along with this, we have the time-varying variables, which have one distinction—for some variables, we do not have the future data (unknown), and for other variables, we know the future (known). Let’s denote all the time-varying information (the context window, known, and unknown time-varying variables) from the context window’s input, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t-k</sub>…<em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>. The known time-varying variables for the future are denoted using <img src="../Images/B22389_16_072.png" alt=""/>, where <img src="../Images/B22389_16_045.png" alt=""/> is the forecast horizon. With these notations, we are ready to look at the first level of abstraction:</p>
    <figure class="mediaobject"><img src="../Images/B22389_16_07.png" alt="Figure 16.6 – TFT – a high-level overview "/></figure>
    <p class="packt_figref">Figure 16.7: TFT—a high-level overview</p>
    <p class="normal">There is a<a id="_idIndexMarker1326"/> lot to unpack here. Let’s start with the static variables, <em class="italic">S</em>. First, the static variables are passed through a <strong class="keyWord">Variable Selection Network</strong> (<strong class="keyWord">VSN</strong>). The <a id="_idIndexMarker1327"/>VSN does instance-wise feature selection and performs some non-linear processing on the inputs. This processed input is fed <a id="_idIndexMarker1328"/>into a bunch of <strong class="keyWord">Static Covariate Encoders</strong> (<strong class="keyWord">SEs</strong>). The SE block is designed to integrate the static metadata in a principled way.</p>
    <p class="normal">If you follow the arrows from the SE block in <em class="italic">Figure 16.6</em>, you will see that the static covariates are used in three (four distinct outputs) different places in the architecture. We will see how these are used in each of these places when we talk about them. But all these different places may be looking at different aspects of the static information. To allow the model this flexibility, the processed and variable-selected output is fed into four <a id="_idIndexMarker1329"/>different <strong class="keyWord">Gated Residual Networks</strong> (<strong class="keyWord">GRNs</strong>), which, in turn, generate four outputs—<em class="italic"> c</em><sub class="subscript-italic" style="font-style: italic;">s</sub>, <em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">e</sub>, <em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">c</sub>, and <em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">h</sub>. We will explain what a GRN is later, but for now, just understand that it is a block capable of non-linear processing, along with a residual connection, which enables it to bypass the non-linear processing if needed.</p>
    <p class="normal">The<a id="_idIndexMarker1330"/> past inputs,<em class="italic"> x</em><sub class="subscript-italic" style="font-style: italic;">t-k</sub>…<em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>, and the future known inputs, <img src="../Images/B22389_16_072.png" alt=""/>, are also passed through separate VSNs and these processed outputs are fed into a <strong class="keyWord">Locality Enhancement</strong> (<strong class="keyWord">LE</strong>) Seq2Seq layer. We <a id="_idIndexMarker1331"/>can think of LE as a way to encode the local context and temporal ordering into the embeddings of each timestep. This is similar to the positional embeddings in vanilla Transformers. We can also see similar attempts in the <code class="inlineCode">Conv1d</code> layers that were used to encode the history in the uniform representation in the Autoformer models. We will see what is happening inside the LE later, but for now, just understand it captures the local context conditioned on other observed variables and static information. Let’s call the output of the <a id="_idIndexMarker1332"/>block <strong class="keyWord">Locality Encoded Context Vectors</strong> ( <img src="../Images/B22389_16_075.png" alt=""/>, and <img src="../Images/B22389_16_076.png" alt=""/>).</p>
    <div class="note">
      <p class="normal">The terminology, notation, and grouping of major blocks are not the same as in the original paper. I have changed these to make them more accessible and understandable.</p>
    </div>
    <p class="normal">Now, these LE context vectors are <a id="_idIndexMarker1333"/>fed into a <strong class="keyWord">Temporal Fusion Decoder</strong> (<strong class="keyWord">TFD</strong>). The TFD applies a slight variation of multi-head self-attention in a Transfomer model-like manner and produces the <strong class="keyWord">Decoded Representation</strong> (<img src="../Images/B22389_16_077.png" alt=""/>). Finally, this decoded representation is passed <a id="_idIndexMarker1334"/>through a <strong class="keyWord">Gated Linear Unit</strong> (<strong class="keyWord">GLU</strong>) and an <strong class="keyWord">Add &amp; Norm</strong> block <a id="_idIndexMarker1335"/>that adds the LE context vectors as a residual connection.</p>
    <p class="normal">A GLU is a unit that helps the model decide how much information it needs to allow to flow through. We can think of it as <a id="_idIndexMarker1336"/>a learned information throttle that is widely used in <strong class="keyWord">Natural Language Processing</strong> (<strong class="keyWord">NLP</strong>) architectures. The formula is really simple:</p>
    <p class="center"><img src="../Images/B22389_16_078.png" alt=""/></p>
    <p class="normal">Here, <em class="italic">W</em> and <em class="italic">V</em> are learnable weight matrices, <em class="italic">b</em> and <em class="italic">c</em> are learnable biases, <img src="../Images/B22389_03_004.png" alt=""/> is an activation function, and <img src="../Images/B22389_16_080.png" alt=""/> is the Hadamard product operator (element-wise multiplication).</p>
    <p class="normal">The <strong class="keyWord">Add &amp; Norm</strong> block<a id="_idIndexMarker1337"/> is the same as <a id="_idIndexMarker1338"/>in the vanilla Transformer; we discussed this back in <em class="chapterRef">Chapter 14</em>, <em class="italic">Attention and Transformers for Time Series</em>.</p>
    <p class="normal">Now, to top it all off, we have a <code class="inlineCode">Dense</code> layer (linear layer with bias) that projects the output of the <code class="inlineCode">Add &amp; Norm</code> block to the desired output dimensions.</p>
    <p class="normal">And with that, it is time for us to step one level down in our abstraction.</p>
    <h3 id="_idParaDest-393" class="heading-3">Locality Enhancement Seq2Seq layer</h3>
    <p class="normal">Let’s peel <a id="_idIndexMarker1339"/>back the onion and see what’s happening inside the LE Seq2Seq layer. Let’s start with a figure:</p>
    <figure class="mediaobject"><img src="../Images/B22389_16_08.png" alt="Figure 16.7 – TFT – LE Seq2Seq layer "/></figure>
    <p class="packt_figref">Figure 16.8: TFT—LE Seq2Seq layer</p>
    <p class="normal">The<a id="_idIndexMarker1340"/> LE uses a Seq2Seq architecture to capture the local context. The process starts with the processed past inputs. The LSTM encoder takes in these past inputs, <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t-k</sub>…<em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub> . <em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">h</sub> <em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">c</sub> from the static covariate encoder acts as the initial hidden states of the LSTM. The encoder processes each timestep at a time, producing hidden states at each time step, <em class="italic">H</em><sub class="subscript-italic" style="font-style: italic;">t-k</sub>…<em class="italic">H</em><sub class="subscript-italic" style="font-style: italic;">t</sub>. The last hidden states (context vector) are now passed on to the LSTM decoder, which processes the known future inputs, <img src="../Images/B22389_16_072.png" alt=""/>, and produces the hidden states at each of the future timesteps, <img src="../Images/B22389_16_082.png" alt=""/>. Finally, all these hidden states are passed<a id="_idIndexMarker1341"/> through a <strong class="keyWord">GLU + AddNorm</strong> block with the residual connection from before the LSTM processing. The outputs are the LE context vectors ( <img src="../Images/B22389_16_075.png" alt=""/> and <img src="../Images/B22389_16_076.png" alt=""/>).</p>
    <p class="normal">Now, let’s look at the next block: the TFD.</p>
    <h3 id="_idParaDest-394" class="heading-3">Temporal fusion decoder</h3>
    <p class="normal">Let’s start this discussion with another figure:</p>
    <figure class="mediaobject"><img src="../Images/B22389_16_09.png" alt="Figure 16.8 – Temporal Fusion Transformer – Temporal Fusion Decoder "/></figure>
    <p class="packt_figref">Figure 16.9: Temporal Fusion Transformer—Temporal Fusion Decoder</p>
    <p class="normal">The LE context <a id="_idIndexMarker1342"/>vectors from both the past input and known future input are concatenated into a single LE context vector. Now, this can be thought of as the position-encoded tokens in the Transformer paradigm. The first thing the TFD does is enrich these encodings with static information, <em class="italic">c</em><sub class="subscript-italic" style="font-style: italic;">e</sub>, that was created from the static covariate encoder. This was concatenated with the embeddings. A position-wise GRN is used to enrich the embeddings. These<a id="_idIndexMarker1343"/> enriched embeddings are now used as the query, key, and values for the <strong class="keyWord">Masked Interpretable Multi-Head Attention</strong> block.</p>
    <p class="normal">The paper posits that the <strong class="keyWord">Masked Interpretable Multi-Head Attention</strong> block learns long-term dependencies across<a id="_idIndexMarker1344"/> time steps. The local dependencies are already captured by the LE Seq2Seq layer in the embeddings, but the point-wise long-term dependencies are captured by <strong class="keyWord">Masked Interpretable Multi-Head Attention</strong>. This block also enhances the interpretability of the architecture. The attention weights that are generated in the process give us some indication of the major timesteps involved in the process. However, the multi-head attention has one drawback from the interpretability perspective. </p>
    <p class="normal">In vanilla multi-head attention, we use separate projection weights for the values, which means that the values for each head are different and hence the attention weights are not straightforward to interpret.</p>
    <p class="normal">TFT gets <a id="_idIndexMarker1345"/>over this limitation by employing a <em class="italic">single shared weight matrix</em> for projecting the values into the attention dimension. Even with the shared value projection weights, because of the individual query and key projection weights, each head can learn different temporal patterns. In addition to this, TFT also employs masking to make sure information from the future is not used in operations. We discussed this type of causal masking in <em class="chapterRef">Chapter 14</em>, <em class="italic">Attention and Transformers for Time Series</em>. With these two modifications, TFT names this layer <strong class="keyWord">Masked Interpretable Multi-Head Attention</strong>.</p>
    <p class="normal">And with that, it’s time to open the last and most granular level of abstraction we have been using.</p>
    <h3 id="_idParaDest-395" class="heading-3">Gated residual networks</h3>
    <p class="normal">We have <a id="_idIndexMarker1346"/>been talking about GRNs for some time now; so far, we have just taken them at face value. Let’s understand what is happening inside a GRN—one of the most basic building blocks of a TFT.</p>
    <p class="normal">Let’s look at a schematic diagram of a GRN to understand it better:</p>
    <figure class="mediaobject"><img src="../Images/B22389_16_10.png" alt="Figure 16.9 – TFT – GRN (left) and VSN (right) "/></figure>
    <p class="packt_figref">Figure 16.10: TFT—GRN (left) and VSN (right)</p>
    <p class="normal">The GRN takes<a id="_idIndexMarker1347"/> in two inputs: the primary input, <em class="italic">a</em>, and the external context, <em class="italic">c</em>. The context, <em class="italic">c</em>, is an optional input and is treated as zero if it’s not present. First, both the inputs, <em class="italic">a</em> and <em class="italic">c</em>, are transformed by<a id="_idIndexMarker1348"/> separate dense layers and a subsequent activation function—the <strong class="keyWord">Exponential Linear Unit</strong> (<strong class="keyWord">ELU</strong>) (<a href="https://pytorch.org/docs/stable/generated/torch.nn.ELU.html"><span class="url">https://pytorch.org/docs/stable/generated/torch.nn.ELU.html</span></a>).</p>
    <p class="normal">Now, the transformed <em class="italic">a</em> and <em class="italic">c</em> inputs are added together and then transformed again using another <code class="inlineCode">Dense</code> layer. Finally, this is passed through a <strong class="keyWord">GLU+Add &amp; Norm</strong> layer with residual connections from the original <em class="italic">a</em>. This structure bakes in enough non-linearity to learn complex interactions between the inputs, but at the same time lets the model ignore those non-linearities through a residual connection. Therefore, such a block allows the model to scale the computation required up or down based on the data.</p>
    <h3 id="_idParaDest-396" class="heading-3">Variable selection networks</h3>
    <p class="normal">The<a id="_idIndexMarker1349"/> last building block of the TFT is the VSN. VSNs enable TFT to do instance-wise variable selection. Most real-world time series datasets have many variables that do not have a lot of predictive power, so being able to select the ones that do have predictive power automatically will help the model pick out relevant patterns. <em class="italic">Figure 16.9</em> (right) shows this VSN.</p>
    <p class="normal">These additional variables can be categorical or continuous. TFT uses entity embeddings to convert the categorical features into numerical vectors of the dimension that we desire (<em class="italic">d</em><sub class="subscript-italic" style="font-style: italic;">model</sub>). We talked about this in <em class="chapterRef">Chapter 15</em>, <em class="italic">Strategies for Global Deep Learning Forecasting Models</em>. The continuous features are linearly transformed (independently) into the same dimension, <em class="italic">d</em><sub class="subscript-italic" style="font-style: italic;">model</sub>. This gives us the transformed inputs, <img src="../Images/B22389_16_085.png" alt=""/>, where <em class="italic">m</em> is the number of features and <em class="italic">t</em> is the timestep. We can concatenate all these embeddings (flatten them) and that flattened representation can be represented as <img src="../Images/B22389_16_086.png" alt=""/>.</p>
    <p class="normal">Now, there <a id="_idIndexMarker1350"/>are two parallel streams in which these embeddings are processed—one for non-linear processing of the embeddings and another to do feature selection. Each of these embeddings is processed by separate GRNs (but shared for all timesteps) to give us the non-linearly processed ones, <img src="../Images/B22389_16_085.png" alt=""/>. In another stream, the VSN processes the flattened representation, <img src="../Images/B22389_16_086.png" alt=""/>, along with optional context information, <em class="italic">c</em>, and processes it through a GRN with a softmax activation. This gives us a weight, <em class="italic">v</em><sub class="subscript-italic" style="font-style: italic;">t</sub>, which is a vector of length <em class="italic">m</em>. This <em class="italic">v</em><sub class="subscript-italic" style="font-style: italic;">t</sub> is now used in a weighted sum of all the non-linearly processed feature embeddings, <img src="../Images/B22389_16_085.png" alt=""/>, which is calculated as follows:</p>
    <p class="center"><img src="../Images/B22389_16_090.png" alt=""/></p>
    <h2 id="_idParaDest-397" class="heading-2">Forecasting with TFT</h2>
    <p class="normal"><em class="italic">TFT</em> is<a id="_idIndexMarker1351"/> implemented in NIXTLA forecasting. We can use the same framework we were working with for NBEATS and extend it to train <em class="italic">TFT</em> on our data. Additionally, NIXTLA supports exogenous variables, the same way N-BEATSx handles exogenous variables. First, let’s look at the initialization parameters of the implementation.</p>
    <p class="normal">The <code class="inlineCode">TFT</code> class in NIXTLA has the following major parameters:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">hidden_size</code>: This is an integer representing the hidden dimension across the model. This is the dimension in which all the GRNs work, the VSN, the LSTM hidden sizes, the self-attention hidden sizes, and so on. Arguably, this is the most important hyperparameter in the model.</li>
      <li class="bulletList"><code class="inlineCode">n_head</code>: This is an integer representing the number of attention heads.</li>
      <li class="bulletList"><code class="inlineCode">dropout</code>: This is a <a id="_idIndexMarker1352"/>float between 0 and 1, which determines the strength of the dropout in the Variable Selection Networks.</li>
      <li class="bulletList"><code class="inlineCode">attn_dropout</code>: This is a float between 0 and 1, which determines the strength of the dropout in the decoder’s attention layer.</li>
    </ul>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">The complete code for training TFT can be found in the <code class="inlineCode">08-TFT_NeuralForecast.ipynb</code> notebook in the <code class="inlineCode">Chapter16</code> folder.</p>
    </div>
    <h2 id="_idParaDest-398" class="heading-2">Interpreting TFT</h2>
    <p class="normal">TFT <a id="_idIndexMarker1353"/>approaches interpretability from a slightly different perspective than N-BEATS. While N-BEATS gives us a decomposed output for interpretability, TFT gives us visibility into how the model has interpreted the variables it has used. On account of the VSNs, we have ready access to feature weights. Like the feature importance we get from tree-based models, TFT gives us access to similar scores. Because of the self-attention layer, the attention weights can also be interpreted to help us understand which time steps hold a large enough weightage in the attention mechanism.</p>
    <p class="normal">PyTorch Forecasting makes this possible by performing a few steps. First, we get the raw predictions using <code class="inlineCode">mode="raw"</code> in the <code class="inlineCode">predict</code> function. Then, we use those raw predictions in the <code class="inlineCode">interpret_output</code> function. There is a parameter called <code class="inlineCode">reduction</code> in the <code class="inlineCode">interpret_output</code> function that decides how to aggregate the weights across different instances. We know that TFT does instance-wise feature selection in VSNs and attention is also done instance-wise. <code class="inlineCode">'mean'</code> is a good option for looking at the global interpretability:</p>
    <pre class="programlisting code"><code class="hljs-code">raw_predictions, x = best_model.predict(val_dataloader, mode=<span class="hljs-string">"raw"</span>, return_x=<span class="hljs-literal">True</span>)
interpretation = best_model.interpret_output(raw_predictions, reduction=<span class="hljs-string">"sum"</span>)
</code></pre>
    <p class="normal">This <code class="inlineCode">interpretation</code> variable is a dictionary with weights for different aspects of the model, such as <code class="inlineCode">attention</code>, <code class="inlineCode">static_variables</code>, <code class="inlineCode">encoder_variables</code>, and <code class="inlineCode">decoder_variables</code>. PyTorch Forecasting also provides us with an easy way to visualize this <a id="_idIndexMarker1354"/>importance:</p>
    <pre class="programlisting code"><code class="hljs-code">best_model.plot_interpretation(interpretation)
</code></pre>
    <p class="normal">This generates four plots:</p>
    <figure class="mediaobject"><img src="../Images/B22389_16_11.png" alt="Figure 16.10 – Interpreting TFT "/></figure>
    <p class="packt_figref">Figure 16.11: Interpreting TFT</p>
    <p class="normal">We can also look at each instance and plot similar visualizations for each prediction we make. All we need to do is use <code class="inlineCode">reduction="none"</code> and then plot it ourselves. The accompanying notebook explores how to do that and more.</p>
    <p class="normal">Now, let’s switch tracks and look at some models that proved that simple MLPs are also more than capable of matching or beating Transformer-based models.</p>
    <h1 id="_idParaDest-399" class="heading-1">TSMixer</h1>
    <p class="normal">While the<a id="_idIndexMarker1355"/> Transformer-based models were forging ahead with steam, a parallel track of research started by <a id="_idIndexMarker1356"/>using <strong class="keyWord">Multi-Layer Perceptrons</strong> (<strong class="keyWord">MLPs</strong>) instead of Transformers as the key learning unit. The trend kicked off in 2021 when MLP-Mixer showed that one can attain state-of-the-art performance in vision problems by using just MLPs, replacing Convolutional Neural Networks. And so, similar mixer architectures using MLPs as the key learning component started popping up in all domains. In 2023, Si-An Chen et al. from Google brought mixing MLPs into time series forecasting.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research paper by Si-An et al. on TSMixer is cited in the <em class="italic">References</em> section as <em class="italic">19</em>.</p>
    </div>
    <h2 id="_idParaDest-400" class="heading-2">The architecture of the TSMixer model</h2>
    <p class="normal">TSMixer <a id="_idIndexMarker1357"/>really took inspiration from the Transformer model but tried to replicate similar processes with an MLP. Let’s use <em class="italic">Figure 16.10</em> to understand the similarities and differences.</p>
    <figure class="mediaobject"><img src="../Images/B22389_16_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.12: Transformer vs TSMixer</p>
    <p class="normal">If you look at the Transformer block, we can see that there is a Multi-Head Attention that looks across timesteps, and “mixes” them together using attention. Then those outputs are passed on to the Position-Wise Feed Forward networks, which “mix” the different <a id="_idIndexMarker1358"/>features together. Drawing inspiration from these, the TSMixer also has a Time-Mixing component and a Feature-Mixing component in a Mixer block. There is a Temporal Projection that takes the output from the Mixer block and projects it to the output space.</p>
    <p class="normal">Let’s take this one level of explanation at a time. <em class="italic">Figure 16.11</em> shows the entire architecture.</p>
    <figure class="mediaobject"><img src="../Images/B22389_16_13.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.13: TSMixer Architecture</p>
    <p class="normal">The input, which is a multivariate time series is fed into N mixer layers, which process it sequentially and the output from the final mixer layer is fed into the temporal projection layer, which converts the learned representation into the actual forecast. Although the figure and the paper refer to “features,” they aren’t features in the way we have been discussing in this book. Here, “features” means other time series in a multi-variate setting.</p>
    <p class="normal">Now, let’s double-click on Mixer Layer and see the Time Mixing and Feature Mixing inside a block.</p>
    <h3 id="_idParaDest-401" class="heading-3">Mixer Layer</h3>
    <figure class="mediaobject"><img src="../Images/B22389_16_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.14: TSMixer—Mixer block</p>
    <p class="normal">The <a id="_idIndexMarker1359"/>input is of the form (<em class="italic">Batch Size</em> x <em class="italic">Features</em> x <em class="italic">Time Steps</em>) and is first passed through the Time Mixing block. The input is first transposed into the form (<em class="italic">Batch Size</em> x <em class="italic">Time Steps</em> x <em class="italic">Features</em>) such that the weights in the Time Mixing MLP are mixing timesteps. Now, this “time-mixed” output is passed to the Feature Mixing MLP, which uses its weights to mix the different features to give the final learned representation. Batch Normalization layers and residual connections are added in between to make the model more robust and learn deeper and smoother connections.</p>
    <p class="normal">Given an input matrix <img src="../Images/B22389_16_091.png" alt=""/>, Time Mixing can be represented mathematically as:</p>
    <p class="center"><img src="../Images/B22389_16_092.png" alt=""/></p>
    <p class="normal">Feature mixing is actually a two-layer MLP, one projecting to a hidden dimension, <em class="italic">H</em><sub class="subscript-italic" style="font-style: italic;">inner</sub>, and the next projecting from <em class="italic">H</em><sub class="subscript-italic" style="font-style: italic;">inner</sub> to the output dimension, <em class="italic">H</em>. If not specified explicitly, this defaults to the original number of features (or number of time series), <em class="italic">C</em>.</p>
    <p class="center"><img src="../Images/B22389_16_093.png" alt=""/></p>
    <p class="center"><img src="../Images/B22389_16_094.png" alt=""/></p>
    <p class="normal">Therefore<a id="_idIndexMarker1360"/> the entire Mixer layer can be represented as:</p>
    <p class="center"><img src="../Images/B22389_16_095.png" alt=""/></p>
    <p class="normal">Now, this output is passed through the Temporal Projection layer to get the forecast.</p>
    <h3 id="_idParaDest-402" class="heading-3">Temporal Projection Layer</h3>
    <figure class="mediaobject"><img src="../Images/B22389_16_15.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.15: TSMixer—Temporal Projection Layer</p>
    <p class="normal">The <a id="_idIndexMarker1361"/>temporal projection layer is nothing but a fully connected layer applied to the time domain. This is identical to the simple linear model we saw earlier, where we apply a fully connected layer to the input context to get the forecast. Instead of applying the layer to the input, TSMixer applies this layer to the “mixed” output from the Mixer Layer.</p>
    <p class="normal">The output from the previous layer is in the form (<em class="italic">Batch Size</em> x <em class="italic">Time Steps</em> x <em class="italic">Features</em>). This is transposed to (<em class="italic">Batch Size</em> x <em class="italic">Features</em> x <em class="italic">Time Steps</em>) and then passed through a fully connected layer, which projects the input into (<em class="italic">Batch Size</em> x <em class="italic">Features</em> x <em class="italic">Forecast Horizon</em>) to get the final forecast.</p>
    <p class="normal">Given <img src="../Images/B22389_16_096.png" alt=""/> as the output of the <em class="italic">k</em>-th Mixer Layer and forecast horizon, <em class="italic">T</em>:</p>
    <p class="center"><img src="../Images/B22389_16_097.png" alt=""/></p>
    <p class="normal">But how do <a id="_idIndexMarker1362"/>we include additional features? Many time series problems have static features and dynamic (future-looking) features, which adds quite a bit of information to the problem. The architecture we have discussed so far doesn’t let you include them. For this reason, the authors proposed a slight tweak to include this additional information, TSMixerx.</p>
    <h3 id="_idParaDest-403" class="heading-3">TSMixerx—TSMixer with auxiliary information</h3>
    <p class="normal">Following <a id="_idIndexMarker1363"/>the same notation as before, consider we have the input time series (or collection of time series), <img src="../Images/B22389_16_098.png" alt=""/>. Now, we would have a few historical features, <img src="../Images/B22389_16_099.png" alt=""/>, some future-looking features, <img src="../Images/B22389_16_100.png" alt=""/>, and some static features, <img src="../Images/B22389_16_101.png" alt=""/>. To effectively include all this additional information, the authors defined another unit of learning called the Conditional Feature Mixing layer and then used it in a way that assimilates all the information.</p>
    <p class="normal">The <strong class="keyWord">Conditional Feature Mixing</strong> (<strong class="keyWord">CFM</strong>) layer is almost identical to the Feature Mixing layer, except<a id="_idIndexMarker1364"/> for an additional layer to process the static information along with the features. The static information is first repeated across time steps and projected into the output dimension using a linear layer. This is then concatenated with the input features and the concatenated input is then “mixed” together and projected to the output dimension.</p>
    <p class="normal">Mathematically, it can be represented as:</p>
    <p class="center"><img src="../Images/B22389_16_102.png" alt=""/></p>
    <p class="center"><img src="../Images/B22389_16_103.png" alt=""/></p>
    <p class="normal">where <img src="../Images/B22389_16_104.png" alt=""/> means concatenation and <em class="italic">Expand</em> means repeating the static information for all the time steps.</p>
    <p class="normal">Now, let’s see how the CFM layer is used in the overall TSMixerx architecture.</p>
    <figure class="mediaobject"><img src="../Images/B22389_16_16.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.16: TSMixerx—architecture using exogenous variables</p>
    <p class="normal">First off, we <a id="_idIndexMarker1365"/>have <em class="italic">X</em> and <img src="../Images/B22389_16_105.png" alt=""/>, which have <em class="italic">L</em> timesteps, which is the length of the context window. Therefore, we concatenate both and use a simple temporal projection layer to project the combined tensor into <img src="../Images/B22389_16_106.png" alt=""/>, where <em class="italic">T</em> is the length of the forecast horizon. This is also the length of the future-looking features, <em class="italic">Z</em>. Now, we combine this with the static information using a CFM layer, which projects them into a hidden dimension, <em class="italic">H</em>. Formally, this step is represented as:</p>
    <p class="center"><img src="../Images/B22389_16_107.png" alt=""/></p>
    <p class="normal">Now, we want to conditionally mix the future-looking features, <em class="italic">Z</em>, as well with the static information, <em class="italic">S</em>. Therefore, we use a CFM layer to do that and project this combined information into a hidden dimension, <em class="italic">H</em>.</p>
    <p class="center"><img src="../Images/B22389_16_108.png" alt=""/></p>
    <p class="normal">At this point, we have <img src="../Images/B22389_16_109.png" alt=""/> and <img src="../Images/B22389_16_110.png" alt=""/>, which are both in <img src="../Images/B22389_16_111.png" alt=""/> dimensions. So, we use another CFM layer to mix these features further conditioned on <img src="../Images/B22389_16_062.png" alt=""/>. This gives us the first feature mixed latent representation, <img src="../Images/B22389_16_113.png" alt=""/>.</p>
    <p class="center"><img src="../Images/B22389_16_114.png" alt=""/></p>
    <p class="normal">Now, this latent representation is passed through <img src="../Images/B22389_16_115.png" alt=""/> subsequent CFMs (similar to regular TSMixer architecture), where <img src="../Images/B22389_16_116.png" alt=""/> is the total number of Mixer layers, to give us <img src="../Images/B22389_16_117.png" alt=""/>, the final latent representation. There are <img src="../Images/B22389_16_115.png" alt=""/> layers because the first Mixer layer is already defined and is different from the rest in just the input dimensions.</p>
    <p class="center"><img src="../Images/B22389_16_119.png" alt=""/></p>
    <p class="normal">Now, we can<a id="_idIndexMarker1366"/> use a simple linear layer to project this output into the desired output dimension. If it is a point prediction for a single time series, we can project it to <img src="../Images/B22389_16_120.png" alt=""/>. In case we are predicting the M time series, then we can project it to <img src="../Images/B22389_16_121.png" alt=""/>.</p>
    <h2 id="_idParaDest-404" class="heading-2">Forecasting with TSMixer and TSMixerx</h2>
    <p class="normal">TSMixer is<a id="_idIndexMarker1367"/> implemented in NIXTLA forecasting with the same framework <a id="_idIndexMarker1368"/>we have seen in the prior models.</p>
    <p class="normal">Let’s look at the initialization parameters of the implementation.</p>
    <p class="normal">The <code class="inlineCode">TSMixer</code> class has the following major parameters:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">n_series</code>: This is an integer value indicating the number of time series.</li>
      <li class="bulletList"><code class="inlineCode">n_block</code>: This is an integer value indicating the number of mixing layers used in the model.</li>
      <li class="bulletList"><code class="inlineCode">ff_dim</code>: This is an integer value indicating the number of units to use for the second feed forward layer.</li>
      <li class="bulletList"><code class="inlineCode">revin</code>: This is a Boolean value that, if True, uses Reversible instance Normalization to process inputs and outputs (ICLR 2022 paper: <a href="https://openreview.net/forum?id=cGDAkQo1C0p"><span class="url">https://openreview.net/forum?id=cGDAkQo1C0p</span></a>).</li>
    </ul>
    <p class="normal">Similar to NBEATX, there is a <code class="inlineCode">TSMixerx</code> class that can take exogenous information. To forecast with exogenous information, you would add appropriate information into the parameters below:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">futr_exog_list</code>: This takes a list of future exogenous columns.</li>
      <li class="bulletList"><code class="inlineCode">hist_exog_list</code>: This takes a list of historical exogenous columns.</li>
      <li class="bulletList"><code class="inlineCode">stag_exog_list</code>: This is a list of exogenous columns.</li>
    </ul>
    <div class="note">
      <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
      <p class="normal">The complete code for training the TSMixer model can be found in the <code class="inlineCode">09-TSMixer_NeuralForecast.ipynb</code> notebook in the <code class="inlineCode">Chapter16</code> folder.</p>
    </div>
    <p class="normal">Let’s<a id="_idIndexMarker1369"/> now <a id="_idIndexMarker1370"/>look at one more MLP-based architecture which has shown that it performs better than PatchTST and the Linear family of models we saw earlier.</p>
    <h1 id="_idParaDest-405" class="heading-1">Time Series Dense Encoder (TiDE)</h1>
    <p class="normal">We saw <a id="_idIndexMarker1371"/>earlier in the chapter that a linear family of models outperformed quite a lot of Transformer models. In 2023, Das et al. from Google proposed a model that extends that idea into non-linearity. They argued that the linear models will fall short where there are inherent non-linearities in the dependence between the future and the past. The inclusion of covariates compounds this problem.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Reference check</strong>:</p>
      <p class="normal">The research paper by Das et al. on TiDE is cited in the <em class="italic">References</em> section as <em class="italic">20</em>.</p>
    </div>
    <p class="normal">Therefore, they introduced a simple and efficient <strong class="keyWord">Multi-Layer Perceptron</strong> (<strong class="keyWord">MLP</strong>) based architecture for<a id="_idIndexMarker1372"/> long-term time series forecasting. The model essentially encodes the past of the time series, along with the covariates using dense MLPs, and then decodes this latent representation into a forecast. The model assumes channel independence (similar to PatchTST) and considers different related time series in a multivariate problem as separate time series.</p>
    <h2 id="_idParaDest-406" class="heading-2">The architecture of the TiDE model</h2>
    <p class="normal">The<a id="_idIndexMarker1373"/> architecture has two main components—an encoder and a decoder. But all through the architecture, one learning component they call a Residual block is reused. Let’s take a look at the Residual block first.</p>
    <h3 id="_idParaDest-407" class="heading-3">Residual block</h3>
    <p class="normal">The<a id="_idIndexMarker1374"/> residual block is an MLP with a ReLU and a subsequent linear projection enabling a residual connection. <em class="italic">Figure 16.14</em> shows a residual block.</p>
    <figure class="mediaobject"><img src="../Images/B22389_16_17.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.17: TiDE: residual block</p>
    <p class="normal">We define the layer by setting a hidden dimension and an output dimension. The first Dense layer transforms the input to the hidden dimension and then ReLU non-linearity is applied to the output. This output is then linearly projected to the output dimension and a dropout layer is stacked on top of that. The residual connection is then added to the output by projecting the input into the output dimension using another linear projection. And to top it all off, the output is passed through Layer Normalization.</p>
    <p class="normal">Let <img src="../Images/B22389_16_122.png" alt=""/> be the input to the block, <em class="italic">h</em> be the hidden dimension, and <em class="italic">o</em> be the output dimension. Then, the Residual block can be represented as:</p>
    <p class="center"><img src="../Images/B22389_16_123.png" alt=""/></p>
    <p class="normal">Let’s establish <a id="_idIndexMarker1375"/>some notation to help us with the rest of the explanation. There are <em class="italic">N</em> time series in the dataset, <em class="italic">L</em> is the length of the lookback window, and <em class="italic">H</em> is the length of the forecast. So, the lookback of the i<sup class="superscript">th</sup> time series can be represented as <img src="../Images/B22389_16_124.png" alt=""/>, and its forecast is <img src="../Images/B22389_16_125.png" alt=""/>. The r-dimensional dynamic covariates at time <img src="../Images/B22389_16_126.png" alt=""/> are represented by <img src="../Images/B22389_16_127.png" alt=""/>. Static features of the i<sup class="superscript">th</sup> time series are <img src="../Images/B22389_16_128.png" alt=""/>.</p>
    <p class="normal">Now, let’s look at the larger architecture in <em class="italic">Figure 16.15</em>.</p>
    <figure class="mediaobject"><img src="../Images/B22389_16_18.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.18: TiDE: overall architecture</p>
    <h3 id="_idParaDest-408" class="heading-3">Encoder</h3>
    <p class="normal">The<a id="_idIndexMarker1376"/> encoder is tasked with mapping the lookback window and the corresponding covariates into a dense latent representation. The first step<a id="_idIndexMarker1377"/> is a <strong class="keyWord">Linear Projection</strong> of the dynamic covariates, <img src="../Images/B22389_16_129.png" alt=""/>, into <img src="../Images/B22389_16_130.png" alt=""/>, where <img src="../Images/B22389_16_131.png" alt=""/>, called <em class="italic">temporal width</em>, is much smaller than <em class="italic">r</em>. We use the Residual block for this projection.</p>
    <p class="center"><img src="../Images/B22389_16_132.png" alt=""/></p>
    <p class="normal">From a programmatic perspective (where <em class="italic">B</em> is the batch size), if the input dimension of the dynamic covariates is <img src="../Images/B22389_16_133.png" alt=""/>, we project it to <img src="../Images/B22389_16_134.png" alt=""/>.</p>
    <p class="normal">This is done so that when we flatten the time series and its covariates before feeding it through the encoder, the dimension of the resulting tensor doesn’t explode. That brings us to the next step, which is the flattening of the tensors and concatenating them. The flattening and concatenation operation looks something like this:</p>
    <ol>
      <li class="numberedList" value="1">Lookback window: <img src="../Images/B22389_16_135.png" alt=""/></li>
      <li class="numberedList">Dynamic covariates: <img src="../Images/B22389_16_136.png" alt=""/></li>
      <li class="numberedList">Static information: <img src="../Images/B22389_16_137.png" alt=""/></li>
      <li class="numberedList">Concatenated representation: <img src="../Images/B22389_16_138.png" alt=""/></li>
    </ol>
    <p class="normal">Now, this concatenated representation is passed through a stack of <em class="italic">n</em><sub class="subscript-italic" style="font-style: italic;">e</sub> Residual blocks to encode them into a dense latent representation.</p>
    <p class="center"><img src="../Images/B22389_16_139.png" alt=""/></p>
    <p class="normal">In the programmatic perspective, the dimensions get transformed from <img src="../Images/B22389_16_138.png" alt=""/> to <img src="../Images/B22389_16_141.png" alt=""/>, where <em class="italic">H</em> is the hidden size of the latent representation.</p>
    <p class="normal">Now that we have the latent representation, let’s look at how we can decode the forecast from this.</p>
    <h3 id="_idParaDest-409" class="heading-3">Decoder</h3>
    <p class="normal">Just like the<a id="_idIndexMarker1378"/> encoder, the decoder also has two separate steps. In the first step, we use a stack of <em class="italic">n</em><sub class="subscript-italic" style="font-style: italic;">d</sub> of Residual Blocks to decode the latent representation into a decoded vector of dimension, <img src="../Images/B22389_16_142.png" alt=""/>, where <img src="../Images/B22389_16_143.png" alt=""/> is the <em class="italic">decoder output dimension</em>. This decoded vector is reshaped into a <img src="../Images/B22389_16_144.png" alt=""/> dimensional vector.</p>
    <p class="center"><img src="../Images/B22389_16_145.png" alt=""/></p>
    <p class="center"><img src="../Images/B22389_16_146.png" alt=""/></p>
    <p class="normal">Now, we use a <strong class="keyWord">Temporal Decoder</strong> to convert this decoded vector into predictions. The temporal decoder is just a Residual block that takes in the concatenated decoded vector, <img src="../Images/B22389_16_147.png" alt=""/> and the encoded future exogenous vector, <img src="../Images/B22389_16_148.png" alt=""/>. The authors argue that this residual connection allows some future covariates to affect the forecast in a stronger way. For instance, if one of the future covariates is holidays in a retail forecasting problem, then you want that variable to have a strong influence on the forecast. </p>
    <p class="normal">This residual connection helps the model enable that “highway” if needed.</p>
    <p class="center"><img src="../Images/B22389_16_149.png" alt=""/></p>
    <p class="normal">Finally, we add a Global Residual Connection that linearly maps the lookback window to the prediction <img src="../Images/B22389_16_150.png" alt=""/>, after a linear mapping to the right dimension. This ensures that the linear model that we saw earlier in the chapter becomes a subclass of the TiDE model.</p>
    <h2 id="_idParaDest-410" class="heading-2">Forecasting with TiDE</h2>
    <p class="normal"><em class="italic">TiDE</em> is implemented<a id="_idIndexMarker1379"/> in NIXTLA forecasting with the same framework we have seen in the prior models.</p>
    <p class="normal">Let’s look at the initialization parameters of the implementation.</p>
    <p class="normal">The <code class="inlineCode">TIDE</code> class has the following major parameters:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">decoder_output_dim</code>: An integer that controls the number of units in the output of the decoder <img src="../Images/B22389_16_151.png" alt=""/>. It directly impacts the dimensionality of the decoded sequence and can influence the model’s ability to reconstruct the target series.</li>
      <li class="bulletList"><code class="inlineCode">temporal_decoder_dim</code>: An integer that defines the output size of the temporal decoder. Although we discussed that the output of the temporal decoder<a id="_idIndexMarker1380"/> is the final forecast, <code class="inlineCode">NeuralfForecast</code> has a uniform map from network output to desired output dimension. Therefore, <code class="inlineCode">temporal_decoder_dim</code> denotes the dimension of the penultimate layer, which will finally be transformed into the final output. The size of the dimension determines how much information you are allowing to pass on to the final forecasting layer.</li>
      <li class="bulletList"><code class="inlineCode">num_encoder_layers</code>: The number of encoder layers stacked on top of each other.</li>
      <li class="bulletList"><code class="inlineCode">num_decoder_layers</code>: The number of decoder layers stacked on top of each other</li>
      <li class="bulletList"><code class="inlineCode">temporal_width</code>: An integer that affects the lower temporal projected dimension <img src="../Images/B22389_16_152.png" alt=""/>, influencing how exogenous data is projected and processed. It plays a role in how the model incorporates and learns from exogenous information.</li>
      <li class="bulletList"><code class="inlineCode">layernorm</code>: This Boolean flag determines whether Layer Normalization is applied. Layer Normalization can stabilize and accelerate training, which might lead to better performance, especially in deeper networks.</li>
    </ul>
    <p class="normal">Additionally, TIDE can handle exogenous information, which can be included in the below parameters:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">futr_exog_list</code>: This takes a list of future exogenous columns</li>
      <li class="bulletList"><code class="inlineCode">hist_exog_list</code>: This takes a list of historical exogenous columns</li>
      <li class="bulletList"><code class="inlineCode">stag_exog_list</code>: This is a list of exogenous columns<div class="note">
          <p class="normal"><strong class="keyWord">Notebook alert</strong>:</p>
          <p class="normal">The complete code for training the TIDE model can be found in the <code class="inlineCode">10-TIDE_NeuralForecast.ipynb</code> notebook in the <code class="inlineCode">Chapter16</code> folder.</p>
        </div>
      </li>
    </ul>
    <p class="normal">We have covered a few popular specialized architectures for time series forecasting, but this is in no way a complete list. There are so many model architectures and techniques out there. I <a id="_idIndexMarker1381"/>have included a few in the <em class="italic">Further reading</em> section to get you started.</p>
    <p class="normal">Congratulations on making it through probably one of the toughest and densest chapters in this book. Give yourself a pat on the back, sit back, and relax.</p>
    <h1 id="_idParaDest-411" class="heading-1">Summary</h1>
    <p class="normal">Our journey with deep learning for time series has finally reached a conclusion with us reviewing a few specialized architectures for time series forecasting. We got an understanding of why it makes sense to have specialized architectures for time series and forecasting and went on to understand how different models such as <em class="italic">N-BEATS</em>, <em class="italic">N-BEATSx</em>, <em class="italic">N-HiTS</em>, <em class="italic">Autoformer</em>, <em class="italic">TFT</em>, <em class="italic">PatchTST</em>, <em class="italic">TiDE</em>, and <em class="italic">TSMixer</em> work. In addition to covering the architecture and theory behind it, we also looked at how we can use these models on real datasets using <code class="inlineCode">neuralforecast</code> by NIXTLA. We never know which model will work better with our dataset, but as practitioners, we need to develop intuition that will guide us in the experimentation. Knowing how these models work behind the scenes is essential in developing that intuition and will help us experiment more efficiently.</p>
    <p class="normal">This brings this part of this book to a close. At this point, you should be much more comfortable with using DL for time series forecasting problems.</p>
    <p class="normal">In the next chapter, let’s look at another important topic in forecasting—probabilistic forecasting.</p>
    <h1 id="_idParaDest-412" class="heading-1">References</h1>
    <p class="normal">The following is a list of the references that we used throughout this chapter:</p>
    <ol>
      <li class="numberedList" value="1">Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. (2020). <em class="italic">The M4 Competition: 100,000 time series and 61 forecasting methods</em>. International Journal of Forecasting, Volume 36, Issue 1. Pages 54–74. <a href="https://doi.org/10.1016/j.ijforecast.2019.04.014"><span class="url">https://doi.org/10.1016/j.ijforecast.2019.04.014</span></a>.</li>
      <li class="numberedList">Slawek Smyl. (2018). <em class="italic">M4 Forecasting Competition: Introducing a New Hybrid ES-RNN Model</em>. <a href="https://www.uber.com/blog/m4-forecasting-competition/"><span class="url">https://www.uber.com/blog/m4-forecasting-competition/</span></a>.</li>
      <li class="numberedList">Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. (2020). <em class="italic">N-BEATS: Neural basis expansion analysis for interpretable time series forecasting</em>. 8<sup class="superscript">th</sup> International Conference on Learning Representations, (ICLR). <a href="https://openreview.net/forum?id=r1ecqn4YwB"><span class="url">https://openreview.net/forum?id=r1ecqn4YwB</span></a>.</li>
      <li class="numberedList">Kin G. Olivares and Cristian Challu and Grzegorz Marcjasz and R. Weron and A. Dubrawski. (2022). <em class="italic">Neural basis expansion analysis with exogenous variables: Forecasting electricity prices with NBEATSx</em>. International Journal of Forecasting, 2022. <a href="https://www.sciencedirect.com/science/article/pii/S0169207022000413"><span class="url">https://www.sciencedirect.com/science/article/pii/S0169207022000413</span></a>.</li>
      <li class="numberedList">Cristian Challu and Kin G. Olivares and Boris N. Oreshkin and Federico Garza and Max Mergenthaler-Canseco and Artur Dubrawski. (2022). <em class="italic">N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting</em>. arXiv preprint arXiv: Arxiv-2201.12886. <a href="https://arxiv.org/abs/2201.12886"><span class="url">https://arxiv.org/abs/2201.12886</span></a>.</li>
      <li class="numberedList">Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Gomez, Aidan N, Kaiser, Lukasz, and Polosukhin, Illia. (2017). <em class="italic">Attention is All you Need.</em> Advances in Neural Information Processing Systems. <a href="https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html"><span class="url">https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</span></a>.</li>
      <li class="numberedList">Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. (2021). <em class="italic">Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</em>. Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI} 2021. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17325"><span class="url">https://ojs.aaai.org/index.php/AAAI/article/view/17325</span></a>.</li>
      <li class="numberedList">Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. (2021). <em class="italic">Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting</em>. Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6–14, 2021. <a href="https://proceedings.neurips.cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html"><span class="url">https://proceedings.neurips.cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html</span></a>.</li>
      <li class="numberedList">Bryan Lim, Sercan Ö. Arik, Nicolas Loeff, and Tomas Pfister. (2019). <em class="italic">Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting</em>. International Journal of Forecasting, Volume 37, Issue 4, 2021, Pages 1,748–1,764. <a href="https://www.sciencedirect.com/science/article/pii/S0169207021000637"><span class="url">https://www.sciencedirect.com/science/article/pii/S0169207021000637</span></a>.</li>
      <li class="numberedList">Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. (2021). <em class="italic">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</em>. 9th International Conference on Learning Representations, ICLR 2021. <a href="https://openreview.net/forum?id=YicbFdNTTy"><span class="url">https://openreview.net/forum?id=YicbFdNTTy</span></a>.</li>
      <li class="numberedList">Yuqi Nie, Nam H. Nguyen, and Phanwadee Sinthong and J. Kalagnanam. (2022). <em class="italic">A Time Series is Worth 64 Words: Long-term Forecasting with Transformers</em>. 10th International Conference on Learning Representations, ICLR 2022. <a href="https://openreview.net/forum?id=Jbdc0vTOcol"><span class="url">https://openreview.net/forum?id=Jbdc0vTOcol</span></a>.</li>
      <li class="numberedList">Ailing Zeng and Mu-Hwa Chen, L. Zhang, and Qiang Xu. (2023). <em class="italic">Are Transformers Effective for Time Series Forecasting?</em> AAAI Conference on Artificial Intelligence. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26317"><span class="url">https://ojs.aaai.org/index.php/AAAI/article/view/26317</span></a>.</li>
      <li class="numberedList">Liu, Shizhan and Yu, Hang and Liao, Cong and Li, Jianguo and Lin, Weiyao and Liu, Alex X and Dustdar, Schahram. (2022). <em class="italic">Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting</em>. International Conference on Learning Representations. <a href="https://openreview.net/pdf?id=0EXmFzUn5I"><span class="url">https://openreview.net/pdf?id=0EXmFzUn5I</span></a>.</li>
      <li class="numberedList">Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong. (2022). <em class="italic">{FEDformer}: Frequency enhanced decomposed transformer for long-term series forecasting</em>. Proc. 39<sup class="superscript">th</sup> International Conference on Machine Learning (ICML 2022). <a href="https://proceedings.mlr.press/v162/zhou22g.html"><span class="url">https://proceedings.mlr.press/v162/zhou22g.html</span></a>.</li>
      <li class="numberedList">Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. (2019). <em class="italic">Enhancing the locality and breaking the memory bottle neck of transformer on time series forecasting</em>. Advances in Neural Information Processing Systems. <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf"><span class="url">https://proceedings.neurips.cc/paper_files/paper/2019/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf</span></a>.</li>
      <li class="numberedList">Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. (2024). <em class="italic">iTransformer: Inverted Transformers Are Effective for Time Series Forecasting</em>.12<sup class="superscript">th</sup> International Conference on Learning Representations, ICLR 2024. <a href="https://openreview.net/forum?id=JePfAI8fah"><span class="url">https://openreview.net/forum?id=JePfAI8fah</span></a>.</li>
      <li class="numberedList">Si-An Chen and Chun-Liang Li and Sercan O Arik and Nathanael Christian Yoder and Tomas Pfister. (2023). <em class="italic">TSMixer: An All-MLP Architecture for Time Series Forecasting</em>. Transactions on Machine Learning Research. <a href="https://openreview.net/forum?id=wbpxTuXgm0"><span class="url">https://openreview.net/forum?id=wbpxTuXgm0</span></a>.</li>
      <li class="numberedList">Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan Mathur, Rajat Sen, and Rose Yu. (2023). <em class="italic">Long-term Forecasting with TiDE: Time-series Dense Encoder</em>. Transactions on Machine Learning Research. <a href="https://openreview.net/forum?id=pCbC3aQB5W"><span class="url">https://openreview.net/forum?id=pCbC3aQB5W</span></a>.</li>
    </ol>
    <h1 id="_idParaDest-413" class="heading-1">Further reading</h1>
    <p class="normal">You can check out the following resources for further reading:</p>
    <ul>
      <li class="bulletList"><em class="italic">Fast ES-RNN: A GPU Implementation of the ES-RNN Algorithm</em>: <a href="https://arxiv.org/abs/1907.03329"><span class="url">https://arxiv.org/abs/1907.03329</span></a> and <a href="https://github.com/damitkwr/ESRNN-GPU"><span class="url">https://github.com/damitkwr/ESRNN-GPU</span></a></li>
      <li class="bulletList"><em class="italic">Functions as Vector Spaces</em>: <a href="https://www.youtube.com/watch?v=NvEZol2Q8rs"><span class="url">https://www.youtube.com/watch?v=NvEZol2Q8rs</span></a></li>
      <li class="bulletList"><em class="italic">Forecast with N-BEATS</em>, by Gaetan Dubuc: <a href="https://www.kaggle.com/code/gatandubuc/forecast-with-n-beats-interpretable-model/notebook"><span class="url">https://www.kaggle.com/code/gatandubuc/forecast-with-n-beats-interpretable-model/notebook</span></a></li>
      <li class="bulletList"><em class="italic">WaveNet: A Generative Model for Audio</em>, by DeepMind: <a href="https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio"><span class="url">https://www.deepmind.com/blog/wavenet-a-generative-model-for-raw-audio</span></a></li>
      <li class="bulletList"><em class="italic">What is Residual Connection?</em>, by Wanshun Wong: <a href="https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55"><span class="url">https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55</span></a></li>
      <li class="bulletList"><em class="italic">Efficient Transformers: A Survey</em>, by Tay et al.: <a href="https://arxiv.org/abs/2009.06732"><span class="url">https://arxiv.org/abs/2009.06732</span></a></li>
      <li class="bulletList"><em class="italic">Autocorrelation and the Wiener-Khinchin theorem</em>: <a href="https://www.itp.tu-berlin.de/fileadmin/a3233/grk/pototskyLectures2012/pototsky_lectures_part1.pdf"><span class="url">https://www.itp.tu-berlin.de/fileadmin/a3233/grk/pototskyLectures2012/pototsky_lectures_part1.pdf</span></a></li>
      <li class="bulletList"><em class="italic">Modelling Long- and Short-Term Temporal Patterns with Deep Neural Networks</em>, by Lai et al.: <a href="https://dl.acm.org/doi/10.1145/3209978.3210006"><span class="url">https://dl.acm.org/doi/10.1145/3209978.3210006</span></a> and <a href="https://github.com/cure-lab/SCINet"><span class="url">https://github.com/cure-lab/SCINet</span></a></li>
      <li class="bulletList"><em class="italic">Think Globally, Act Locally: A Deep Neural Network Approach to High-Dimensional Time Series Forecasting</em>, by Sen et al.: <a href="https://proceedings.neurips.cc/paper/2019/hash/3a0844cee4fcf57de0c71e9ad3035478-Abstract.html"><span class="url">https://proceedings.neurips.cc/paper/2019/hash/3a0844cee4fcf57de0c71e9ad3035478-Abstract.html</span></a> and <a href="https://github.com/rajatsen91/deepglo"><span class="url">https://github.com/rajatsen91/deepglo</span></a></li>
    </ul>
    <h1 class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/mts"><span class="url">https://packt.link/mts</span></a></p>
    <p class="normal"><img src="../Images/QR_Code15080603222089750.png" alt=""/></p>
  </div>
</body></html>