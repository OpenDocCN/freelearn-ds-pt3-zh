["```py\nuniq_blocks = train_df.file.unique().tolist()\nsel_blocks = sorted(uniq_blocks, key=lambda x: int(x.replace(\"block_\",\"\")))[:len(uniq_blocks)//2]\ntrain_df = train_df.loc[train_df.file.isin(sel_blocks)]\ntest_df = test_df.loc[test_df.file.isin(sel_blocks)]\nsel_lclids = train_df.LCLid.unique().tolist() \n```", "```py\nfrom pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\nfrom pytorch_tabular.models import FTTransformerConfig\nfrom pytorch_tabular import TabularModel \n```", "```py\ndata_config = DataConfig(\n    target=[target], #target should always be a list\n    continuous_cols=[\n        \"visibility\",\n        \"windBearing\",\n        …\n        \"timestamp_Is_month_start\",\n    ],\n    categorical_cols=[\n        \"holidays\",\n        …\n        \"LCLid\"\n    ],\n    normalize_continuous_features=True\n)\ntrainer_config = TrainerConfig(\n    auto_lr_find=True, # Runs the LRFinder to automatically derive a learning rate\n    batch_size=1024,\n    max_epochs=1000,\n    auto_select_gpus=True,\n    gpus=-1\n)\noptimizer_config = OptimizerConfig() \n```", "```py\nmodel_config = FTTransformerConfig(\n    task=\"regression\",\n    num_attn_blocks=3,\n    num_heads=4,\n    transformer_head_dim=64,\n    attn_dropout=0.2,\n    ff_dropout=0.1,\n    out_ff_layers=\"32\",\n    metrics=[\"mean_squared_error\"]\n) \n```", "```py\ntabular_model.fit(train=train_df) \n```", "```py\ntabular_model.save_model(\"notebooks/Chapter13/ft_transformer_global\") \n```", "```py\ntabular_model = TabularModel.load_from_checkpoint(\"notebooks/Chapter13/ft_transformer_global\") \n```", "```py\nforecast_df = tabular_model.predict(test_df)\nagg_metrics, eval_metrics_df = evaluate_forecast(\n    y_pred=forecast_df[f\"{target}_prediction\"],\n    test_target=forecast_df[\"energy_consumption\"],\n    train_target=train_df[\"energy_consumption\"],\n    model_name=model_config._model_name,\n) \n```", "```py\ndatamodule = TimeSeriesDataModule(data = sample_df[[target]],\n        n_val = sample_val_df.shape[0],\n        n_test = sample_test_df.shape[0],\n        window = 48, # giving enough memory to capture daily seasonality\n        horizon = 1, # single step\n        normalize = \"global\", # normalizing the data\n        batch_size = 32,\n        num_workers = 0)\ndatamodule.setup() \n```", "```py\n# Getting a batch from the train_dataloader\nfor batch in datamodule.train_dataloader():\n    x, y = batch\n    break\nprint(\"Shape of x: \",x.shape) #-> torch.Size([32, 48, 1])\nprint(\"Shape of y: \",y.shape) #-> torch.Size([32, 1, 1]) \n```", "```py\nrnn_config = SingleStepRNNConfig(\n    rnn_type=\"RNN\",\n    input_size=1,\n    hidden_size=128,\n    num_layers=3,\n    bidirectional=True,\n    learning_rate=1e-3,\n    seed=42,\n)\nmodel = SingleStepRNNModel(rnn_config) \n```", "```py\nx, y = batch \n```", "```py\nx, _ = self.rnn(x) \n```", "```py\nx = self.fc(x) \n```", "```py\ny = torch.cat([x[:, 1:, :], y], dim=1) \n```", "```py\ny_hat, y = model(batch)\nprint(\"Shape of y_hat: \",y_hat.shape) #-> ([32, 48, 1])\nprint(\"Shape of y: \",y.shape) #-> ([32, 48, 1]) \n```", "```py\ntrainer = pl.Trainer(\n    auto_select_gpus=True,\n    gpus=-1,\n    min_epochs=5,\n    max_epochs=100,\n    callbacks=[pl.callbacks.EarlyStopping(monitor=\"valid_loss\", patience=3)],\n) \n```", "```py\ntrainer.fit(model, datamodule) \n```", "```py\ndef predict(self, batch):\n        y_hat, _ = self.forward(batch)\n        return y_hat[:, -1, :] \n```", "```py\npred = trainer.predict(model, datamodule.test_dataloader()) \n```", "```py\npred = torch.cat(pred).squeeze().detach().numpy() \n```", "```py\npred = pred * datamodule.train.std + datamodule.train.mean \n```", "```py\nself.encoder = nn.LSTM(\n                **encoder_params,\n                batch_first=True,\n            ) \n```", "```py\no, h = self.encoder(x) \n```", "```py\nself.decoder = nn.Linear(\n                    hidden_size*bi_directional_multiplier, horizon\n                ) \n```", "```py\nself.decoder = nn.Linear(\n                    hidden_size * bi_directional_multiplier * window_size, horizon\n                ) \n```", "```py\ny_hat = self.decoder(o[:,-1,:]).unsqueeze(-1) \n```", "```py\ny_hat = self.decoder(o.reshape(o.size(0), -1)).unsqueeze(-1) \n```", "```py\nself.decoder = nn.LSTM(\n                **decoder_params,\n                batch_first=True,\n            ) \n```", "```py\n01  y_hat = torch.zeros_like(y, device=y.device)\n02  dec_input = x[:, -1:, :]\n03  for i in range(y.size(1)):\n04      out, h = self.decoder(dec_input, h)\n05      out = self.fc(out)\n06      y_hat[:, i, :] = out.squeeze(1)\n07      #decide if we are going to use teacher forcing or not\n08      teacher_force = random.random() < teacher_forcing_ratio\n09      if teacher_force:\n10          dec_input = y[:, i, :].unsqueeze(1)\n11      else:\n12          dec_input = out \n```", "```py\nHORIZON = 48\nWINDOW = 48*2\ndatamodule = TimeSeriesDataModule(data = sample_df[[target]],\n        n_val = sample_val_df.shape[0],\n        n_test = sample_test_df.shape[0],\n        window = WINDOW,\n        horizon = HORIZON,\n        normalize = \"global\", # normalizing the data\n        batch_size = 32,\n        num_workers = 0) \n```", "```py\npred = trainer.predict(model, datamodule.test_dataloader())\n# pred is a list of outputs, one for each batch\npred = torch.cat(pred).squeeze().detach().numpy() \n```", "```py\npred = pred[0::48].ravel() \n```"]