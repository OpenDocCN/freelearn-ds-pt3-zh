<html><head></head><body>
		<div id="_idContainer944">
			<h1 class="chapter-number" id="_idParaDest-286"><a id="_idTextAnchor285"/>7</h1>
			<h1 id="_idParaDest-287"><a id="_idTextAnchor286"/>Using Regression and Forecasting</h1>
			<p>One of the most important tasks that a statistician or data scientist has is to generate a systematic understanding of the relationship between two sets of data. This can mean a <em class="italic">continuous</em> relationship between two sets of data, where one value depends directly on the value of another variable. Alternatively, it can mean a categorical relationship, where one value is categorized according to another. The tool for working with these kinds of problems is <em class="italic">regression</em>. In its most basic form, regression involves fitting a straight line through a scatter plot of the two sets of data and performing some analysis to see how well this line <em class="italic">fits</em> the data. Of course, we often need something more sophisticated to model more complex relationships that exist in the <span class="No-Break">real world.</span></p>
			<p>Forecasting typically refers to learning trends in time series data with the aim of predicting values in the future. Time series data is data that evolves over a period of time, and usually exhibits a high degree of noise and oscillatory behavior. Unlike more simple data, time series data usually has complex dependencies between consecutive values; for instance, a value may depend on both of the previous values, and perhaps even on the previous <em class="italic">noise</em>. Time series modeling is important across science and economics, and there are a variety of tools for modeling time series data. The basic technique for working with <a id="_idIndexMarker709"/>time series data is called <strong class="bold">autoregressive integrated moving average</strong> (<strong class="bold">ARIMA</strong>). This model incorporates two underlying components: an <strong class="bold">autoregressive</strong> (<strong class="bold">AR</strong>) component<a id="_idIndexMarker710"/> and a <strong class="bold">moving average</strong> (<strong class="bold">MA</strong>) component, to<a id="_idIndexMarker711"/> construct a model for the <span class="No-Break">observed data.</span></p>
			<p>In this chapter, we will learn how to model the relationship between two sets of data, quantify how strong this relationship is, and generate forecasts about other values (the future). Then, we will learn how to use logistic regression, which is a variation of a simple linear model, in classification problems. Finally, we will build models for time series data using ARIMA and build on these models for different kinds of data. We will finish this chapter by using a library called Prophet to automatically generate a model for time <span class="No-Break">series data.</span></p>
			<p>In the first three recipes, we will learn how to perform various kinds of regression to simple data. In the next four recipes, we will learn about various techniques for working with time series data. The final recipe deals with an alternative means of summarizing time series data for various purposes using <span class="No-Break">signature methods.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following recipes:</span></p>
			<ul>
				<li>Using basic <span class="No-Break">linear regression</span></li>
				<li>Using <span class="No-Break">multilinear regression</span></li>
				<li>Classifying using <span class="No-Break">logarithmic regression</span></li>
				<li>Modeling time series data <span class="No-Break">with ARMA</span></li>
				<li>Forecasting from time series data <span class="No-Break">using ARIMA</span></li>
				<li>Forecasting seasonal data <span class="No-Break">using ARIMA</span></li>
				<li>Using Prophet to model time <span class="No-Break">series data</span></li>
				<li>Using signatures to summarize time <span class="No-Break">series data</span></li>
			</ul>
			<p>Let’s <span class="No-Break">get started!</span></p>
			<h1 id="_idParaDest-288"><a id="_idTextAnchor287"/>Technical requirements</h1>
			<p>In this chapter, as usual, we will need the NumPy package imported under the <strong class="source-inline">np</strong> alias, the Matplotlib <strong class="source-inline">pyplot</strong> module imported as <strong class="source-inline">plt</strong>, and the Pandas package imported as <strong class="source-inline">pd</strong>. We can do this using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd</pre>
			<p>We will also need some new packages in this chapter. The <strong class="source-inline">statsmodels</strong> package is used for regression and time series analysis, the <strong class="source-inline">scikit-learn</strong> package (<strong class="source-inline">sklearn</strong>) provides general data science and machine learning tools, and the Prophet package (<strong class="source-inline">prophet</strong>) is used for automatically modeling time series data. These packages can be installed using your favorite package manager, such <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">pip</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
python3.10 -m pip install statsmodels sklearn prophet</pre>
			<p>The Prophet package can prove difficult to install on some operating systems because of its dependencies. If installing <strong class="source-inline">prophet</strong> causes a problem, you might want to try using the Anaconda distribution of Python and its package manager, <strong class="source-inline">conda</strong>, which handles the dependencies <span class="No-Break">more rigorously:</span></p>
			<pre class="source-code">
conda install prophet</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">Previous versions of the Prophet library (prior to version 1.0) were called <strong class="source-inline">fbprophet</strong>, whereas the newer versions of Prophet are just <strong class="source-inline">prophet</strong>. </p>
			<p>Finally, we also need a small module called <strong class="source-inline">tsdata</strong> that is contained in the repository for this chapter. This module contains a series of utilities for producing sample time <span class="No-Break">series data.</span></p>
			<p>The code for this chapter can be found in the <span class="No-Break"><strong class="source-inline">Chapter 07</strong></span> folder of the GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Applying-Math-with-Python-2nd-Edition/tree/main/Chapter%2007"><span class="No-Break">https://github.com/PacktPublishing/Applying-Math-with-Python-2nd-Edition/tree/main/Chapter%2007</span></a><span class="No-Break">.</span></p>
			<p>Using basic <span class="No-Break">linear regression</span></p>
			<p>Linear regression <a id="_idIndexMarker712"/>is a tool for modeling the dependence between two sets of data so that we can eventually use this model to make predictions. The name comes from the fact that we form a linear model (straight line) of one set of data based on a second. In the literature, the variable that we wish to model is frequently<a id="_idIndexMarker713"/> called the <em class="italic">response</em> variable, and the variable that we are using in this model is <a id="_idIndexMarker714"/>the <span class="No-Break"><em class="italic">predictor</em></span><span class="No-Break"> variable.</span></p>
			<p>In this recipe, we’ll learn<a id="_idIndexMarker715"/> how to use the <strong class="source-inline">statsmodels</strong> package to<a id="_idIndexMarker716"/> perform simple linear regression to model the relationship between two sets <span class="No-Break">of data.</span></p>
			<h2 id="_idParaDest-289"><a id="_idTextAnchor288"/>Getting ready</h2>
			<p>For this <a id="_idIndexMarker717"/>recipe, we will need the <strong class="source-inline">statsmodels.api</strong> module imported under the <strong class="source-inline">sm</strong> alias, the NumPy package imported as <strong class="source-inline">np</strong>, the <a id="_idIndexMarker718"/>Matplotlib <strong class="source-inline">pyplot</strong> module imported as <strong class="source-inline">plt</strong>, and an instance of a NumPy default random number generator. All this can be achieved with the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
import statsmodels.api as sm
import numpy as np
import matplotlib.pyplot as plt
from numpy.random import default_rng
rng = default_rng(12345)</pre>
			<p>Let’s see how to use the <strong class="source-inline">statsmodels</strong> package to perform basic <span class="No-Break">linear regression.</span></p>
			<h2 id="_idParaDest-290"><a id="_idTextAnchor289"/>How to do it...</h2>
			<p>The following steps outline how to use the <strong class="source-inline">statsmodels</strong> package to perform a simple linear regression on two sets <span class="No-Break">of data:</span></p>
			<ol>
				<li value="1">First, we generate some example data that we can analyze. We’ll generate two sets of data that will illustrate a good fit and a less <span class="No-Break">good fit:</span><pre class="console">
x = np.linspace(0, 5, 25)</pre><pre class="console">
rng.shuffle(x)</pre><pre class="console">
trend = 2.0</pre><pre class="console">
shift = 5.0</pre><pre class="console">
y1 = trend*x + shift + rng.normal(0, 0.5, size=25)</pre><pre class="console">
y2 = trend*x + shift + rng.normal(0, 5, size=25)</pre></li>
				<li>A good first step in performing regression analysis is to create a scatter plot of the datasets. We’ll do this on the same set <span class="No-Break">of axes:</span><pre class="console">
fig, ax = plt.subplots()</pre><pre class="console">
ax.scatter(x, y1, c="k", marker="x",</pre><pre class="console">
    label="Good correlation")</pre><pre class="console">
ax.scatter(x, y2, c="k", marker="o",</pre><pre class="console">
    label="Bad correlation")</pre><pre class="console">
ax.legend()</pre><pre class="console">
ax.set_xlabel("X"),</pre><pre class="console">
ax.set_ylabel("Y")</pre><pre class="console">
ax.set_title("Scatter plot of data with best fit lines")</pre></li>
				<li>We <a id="_idIndexMarker719"/>need to use the <strong class="source-inline">sm.add_constant</strong> utility routine so that the modeling step will include a <span class="No-Break">constant </span><span class="No-Break"><a id="_idIndexMarker720"/></span><span class="No-Break">value:</span><pre class="console">
pred_x = sm.add_constant(x)</pre></li>
				<li>Now, we can create an <strong class="source-inline">OLS</strong> model for our first set of data and use the <strong class="source-inline">fit</strong> method to fit the model. We then print a summary of the data using the <span class="No-Break"><strong class="source-inline">summary</strong></span><span class="No-Break"> method:</span><pre class="console">
model1 = sm.OLS(y1, pred_x).fit()</pre><pre class="console">
print(model1.summary())</pre></li>
				<li>We repeat the model fitting for the second set of data and print <span class="No-Break">the summary:</span><pre class="console">
model2 = sm.OLS(y2, pred_x).fit()</pre><pre class="console">
print(model2.summary())</pre></li>
				<li>Now, we create a new range of <img alt="" src="image/Formula_07_001.png"/> values using <strong class="source-inline">linspace</strong> that we can use to plot the trend lines on our scatter plot. We need to add the <strong class="source-inline">constant</strong> column to interact with the models that we <span class="No-Break">have created:</span><pre class="console">
model_x = sm.add_constant(np.linspace(0, 5))</pre></li>
				<li>Next, we use the <strong class="source-inline">predict</strong> method on the model objects so that we can use the model to predict the response value at each of the <img alt="" src="image/Formula_07_002.png"/> values we generated in the <span class="No-Break">previous step:</span><pre class="console">
model_y1 = model1.predict(model_x)</pre><pre class="console">
model_y2 = model2.predict(model_x)</pre></li>
				<li>Finally, we plot the model data computed in the previous two steps on top of the <span class="No-Break">scatter plot:</span><pre class="console">
ax.plot(model_x[:, 1], model_y1, 'k')</pre><pre class="console">
ax.plot(model_x[:, 1], model_y2, 'k--')</pre></li>
			</ol>
			<p>The scatter<a id="_idIndexMarker721"/> plot, along with the best fit lines (the models) we <a id="_idIndexMarker722"/>added, can be seen in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer802">
					<img alt="Figure 7.1 - Scatter plot of data with lines of best fit computed using least squares regression.&#13;&#10;" src="image/7.1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 - Scatter plot of data with lines of best fit computed using least squares regression.</p>
			<p>The solid line indicates the line fitted to the well-correlated data (marked by <strong class="bold">x</strong> symbols) and the dashed line indicates the line fitted to the poorly correlated data (marked by dots). We can see in the plot that the two best-fit lines are fairly similar, but the line fitted (dashed) to the data with lots of noise has drifted away from the true model <img alt="" src="image/Formula_07_003.png"/> defined in <span class="No-Break"><em class="italic">step 1</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-291"><a id="_idTextAnchor290"/>How it works...</h2>
			<p>Elementary <a id="_idIndexMarker723"/>mathematics tells us that the equation of a straight <a id="_idIndexMarker724"/>line is given by <span class="No-Break">the following:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_004.png"/></p>
			<p>Here, <img alt="" src="image/Formula_07_005.png"/> is the value at which the line meets the <img alt="" src="image/Formula_07_006.png"/>-axis, usually called the <img alt="" src="image/Formula_07_007.png"/>-intercept, and <img alt="" src="image/Formula_07_008.png"/> is the gradient of the line. In the linear regression context, we are trying to find a relationship between the response variable, <img alt="" src="image/Formula_07_009.png"/>, and the predictor variable, <img alt="" src="image/Formula_07_010.png"/>, which has the form of a straight line so that the <span class="No-Break">following occurs:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_011.png"/></p>
			<p>Here, <img alt="" src="image/Formula_07_012.png"/> and <img alt="" src="image/Formula_07_013.png"/> are now parameters that are to be found. We can write this in a different way, <span class="No-Break">as follows:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_014.png"/></p>
			<p>Here, <img alt="" src="image/Formula_07_015.png"/> is an error term, which, in general, depends on <img alt="" src="image/Formula_07_016.png"/>. To find the “best” model, we need to find values for the <img alt="" src="image/Formula_07_017.png"/> and <img alt="" src="image/Formula_07_018.png"/> parameters for which the error term, <img alt="" src="image/Formula_07_019.png"/>, is minimized (in an appropriate sense). The basic method for finding the values of the parameters such that this error is minimized is the method of least squares, which gives its name to the type of regression used<a id="_idIndexMarker725"/> here: <em class="italic">ordinary least squares</em>. Once we have used this method to establish some relationship between a response variable and a predictor variable, our next task is to assess how well this model actually represents this relationship. For this, we form the <em class="italic">residuals</em> given by the <span class="No-Break">following equation:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_020.png"/></p>
			<p>We do this for each of the data points, <img alt="" src="image/Formula_07_021.png"/> and <img alt="" src="image/Formula_07_022.png"/>. In order to provide a rigorous statistical analysis of how well we have modeled the relationship between the data, we need the residuals to satisfy certain assumptions. First, we need them to be independent in the sense of probability. Second, we need them to be normally distributed about 0 with a common variance (in practice, we can relax these slightly and still make reasonable comments about the accuracy of <span class="No-Break">the model).</span></p>
			<p>In this recipe, we generated response data from the predictor data using a linear relationship. The difference between the two response datasets we created is the “size” of the error at each value. For the first dataset, <strong class="source-inline">y1</strong>, the residuals were normally distributed with a standard deviation of 0.5, whereas for the second dataset, <strong class="source-inline">y2</strong>, the residuals have a standard deviation of 5.0. We can see this variability in the scatter plot shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.1</em>, where the data for <strong class="source-inline">y1</strong> is generally very close to the best fit line – which closely matches the actual relationship that was used to generate the data – whereas the <strong class="source-inline">y2</strong> data is much further from the <span class="No-Break">best-fit line.</span></p>
			<p>The <strong class="source-inline">OLS</strong> object<a id="_idIndexMarker726"/> from the <strong class="source-inline">statsmodels</strong> package is the main<a id="_idIndexMarker727"/> interface for ordinary least squares regression. We provide the response data and the predictor data as arrays. In order to have a constant term in the model, we need to add a column of ones in the predictor data. The <strong class="source-inline">sm.add_constant</strong> routine is a simple utility for adding this constant column. The <strong class="source-inline">fit</strong> method of the <strong class="source-inline">OLS</strong> class computes the parameters for the model and returns a results object (<strong class="source-inline">model1</strong> and <strong class="source-inline">model2</strong>) that contains the parameters for the best fit model. The <strong class="source-inline">summary</strong> method creates a string containing information about the model and various statistics about the goodness of fit. The <strong class="source-inline">predict</strong> method applies the model to new data. As the name suggests, it can be used to make predictions using <span class="No-Break">the model.</span></p>
			<p>There are two basic statistics reported in the summary that give us information about the fit. The first is the <img alt="" src="image/Formula_07_023.png"/> value, or the adjusted version, which measures the variability explained by the model against the total variability. This number is defined as follows. First, define the <span class="No-Break">following quantities:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_024.png"/></p>
			<p>Here, <img alt="" src="image/Formula_07_025.png"/> are the residuals defined previously and <img alt="" src="image/Formula_07_026.png"/> is the mean of the data. We then define <img alt="" src="image/Formula_07_027.png"/> and its <span class="No-Break">adjusted counterpart:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_028.png"/></p>
			<p>In the latter equation, <img alt="" src="image/Formula_07_029.png"/> is the size of the sample and <img alt="" src="image/Formula_07_030.png"/> is the number of variables in the model (including the <img alt="" src="image/Formula_07_031.png"/>-intercept <img alt="" src="image/Formula_07_032.png"/>). A higher value indicates a better fit, with a best possible value of 1. Note that the ordinary <img alt="" src="image/Formula_07_033.png"/> value tends to be overly optimistic, especially when the model contains more variables, so it is usually better to look at the <span class="No-Break">adjusted version.</span></p>
			<p>The second is the<a id="_idIndexMarker728"/> F statistic p-value. This is a hypothesis test that at least one of the coefficients of the model is non-zero. As with ANOVA testing (see <em class="italic">Testing Hypotheses with ANOVA</em>, <a href="B19085_06.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>), a small p-value indicates that the model is significant, meaning that the model is more likely to accurately model <span class="No-Break">the data.</span></p>
			<p>In this <a id="_idIndexMarker729"/>recipe, the<a id="_idIndexMarker730"/> first model, <strong class="source-inline">model1</strong>, has an adjusted <img alt="" src="image/Formula_07_034.png"/> value of 0.986, indicating that the model very closely fits the data, and a p-value of 6.43e-19, indicating high significance. The second model has an adjusted <img alt="" src="image/Formula_07_035.png"/> value of 0.361, which indicates that the model less closely fits the data, and a p-value of 0.000893, which also indicates high significance. Even though the second model less closely fits the data, in terms of statistics, that is not to say that it is not useful. The model is still significant, although less so than the first model, but it doesn’t account for all of the variability (or at least a significant portion of it) in the data. This could be indicative of additional (non-linear) structures in the data, or that the data is less correlated, which means there is a weaker relationship between the response and predictor data (due to the way we constructed the data, we know that the latter <span class="No-Break">is true).</span></p>
			<h2 id="_idParaDest-292"><a id="_idTextAnchor291"/>There’s more...</h2>
			<p>Simple linear regression<a id="_idIndexMarker731"/> is a good general-purpose tool in a statistician’s toolkit. It is excellent for finding the nature of the relationship between two sets of data that are known (or suspected) to be connected in some way. The statistical measurement of how much one set of data depends on<a id="_idIndexMarker732"/> another is called <em class="italic">correlation</em>. We can measure correlation using a correlation coefficient, such<a id="_idIndexMarker733"/> as <em class="italic">Spearman’s rank correlation coefficient</em>. A high positive correlation coefficient indicates a strong positive relationship between the data, such as that seen in this recipe, while a high negative correlation coefficient indicates a strong negative relationship, where the slope of the best-fit line through the data is negative. A correlation coefficient of 0 means that the data is not correlated: there is no relationship between <span class="No-Break">the data.</span></p>
			<p>If the sets of data are clearly related but not in a linear (straight line) relationship, then it might follow a polynomial relationship where, for example, one value is related to the other squared. Sometimes, you can apply a transformation, such as a logarithm, to one set of data and then use linear regression to fit the transformed data. Logarithms are especially useful when there is a power-law relationship between the two sets <span class="No-Break">of data.</span></p>
			<p>The <strong class="source-inline">scikit-learn</strong> package<a id="_idIndexMarker734"/> also provides facilities for performing ordinary least squares regression. However, their implementation does not offer an easy way to generate goodness-of-fit statistics, which are often useful when performing a linear regression in isolation. The <strong class="source-inline">summary</strong> method on the <strong class="source-inline">OLS</strong> object is very convenient for producing all the required fitting information, along with the <span class="No-Break">estimated coefficients.</span></p>
			<h1 id="_idParaDest-293"><a id="_idTextAnchor292"/>Using multilinear regression</h1>
			<p>Simple linear regression, as <a id="_idIndexMarker735"/>seen in the previous recipe, is excellent for producing simple models of a relationship between one response variable and one predictor variable. Unfortunately, it is far more common to have a single response variable that depends on many predictor variables. Moreover, we might not know which variables from a collection make good predictor variables. For this task, we need <span class="No-Break">multilinear regression.</span></p>
			<p>In this recipe, we will learn how to<a id="_idIndexMarker736"/> use multilinear regression to explore the relationship between a response variable and several <span class="No-Break">predictor variables.</span></p>
			<h2 id="_idParaDest-294"><a id="_idTextAnchor293"/>Getting ready</h2>
			<p>For this recipe, we will need the NumPy package imported as <strong class="source-inline">np</strong>, the Matplotlib <strong class="source-inline">pyplot</strong> module imported as <strong class="source-inline">plt</strong>, the Pandas package imported as <strong class="source-inline">pd</strong>, and an instance of the NumPy default random number generator created using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
from numpy.random import default_rng
rng = default_rng(12345)</pre>
			<p>We will also need the <strong class="source-inline">statsmodels</strong>.<strong class="source-inline">api</strong> module imported as <strong class="source-inline">sm</strong>, which can be imported using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
import statsmodels.api as sm</pre>
			<p>Let’s see how to fit a multilinear regression model to <span class="No-Break">some data.</span></p>
			<h2 id="_idParaDest-295"><a id="_idTextAnchor294"/>How to do it...</h2>
			<p>The following steps <a id="_idIndexMarker737"/>show you how to use multilinear regression to explore the relationship between several predictors and a <span class="No-Break">response variable:</span></p>
			<ol>
				<li value="1">First, we need to create the predictor data to analyze. This will take the form of a Pandas DataFrame with four terms. We will add the constant term at this stage by adding a column <span class="No-Break">of ones:</span><pre class="console">
p_vars = pd.DataFrame({</pre><pre class="console">
    "const": np.ones((100,)),</pre><pre class="console">
    "X1": rng.uniform(0, 15, size=100),</pre><pre class="console">
    "X2": rng.uniform(0, 25, size=100),</pre><pre class="console">
    "X3": rng.uniform(5, 25, size=100)</pre><pre class="console">
})</pre></li>
				<li>Next, we will generate the response data using only the first <span class="No-Break">two variables:</span><pre class="console">
residuals = rng.normal(0.0, 12.0, size=100)</pre><pre class="console">
Y = -10.0 + 5.0*p_vars["X1"] - 2.0*p_vars["X2"] +    residuals</pre></li>
				<li>Now, we’ll <a id="_idIndexMarker738"/>produce scatter plots of the response data against each of the <span class="No-Break">predictor variables:</span><pre class="console">
fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True,</pre><pre class="console">
    tight_layout=True)</pre><pre class="console">
ax1.scatter(p_vars["X1"], Y, c="k")</pre><pre class="console">
ax2.scatter(p_vars["X2"], Y, c="k")</pre><pre class="console">
ax3.scatter(p_vars["X3"], Y, c="k")</pre></li>
				<li>Then, we’ll add axis labels and titles to each scatter plot since this is <span class="No-Break">good practice:</span><pre class="console">
ax1.set_title("Y against X1")</pre><pre class="console">
ax1.set_xlabel("X1")</pre><pre class="console">
ax1.set_ylabel("Y")</pre><pre class="console">
ax2.set_title("Y against X2")</pre><pre class="console">
ax2.set_xlabel("X2")</pre><pre class="console">
ax3.set_title("Y against X3")</pre><pre class="console">
ax3.set_xlabel("X3") </pre></li>
			</ol>
			<p>The resulting plots can be seen in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer836">
					<img alt="Figure 7.2 - Scatter plots of the response data against each of the predictor variables&#13;&#10;" src="image/7.2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 - Scatter plots of the response data against each of the predictor variables</p>
			<p>As we can <a id="_idIndexMarker739"/>see, there appears to be some correlation between the response data and the first two predictor columns, <strong class="source-inline">X1</strong> and <strong class="source-inline">X2</strong>. This is what we expect, given how we generated <span class="No-Break">the data.</span></p>
			<ol>
				<li value="5">We use the same <strong class="source-inline">OLS</strong> class to perform multilinear regression; that is, providing the response array and the <span class="No-Break">predictor DataFrame:</span><pre class="console">
model = sm.OLS(Y, p_vars).fit()</pre><pre class="console">
print(model.summary())</pre></li>
			</ol>
			<p>The first half of the output of the <strong class="source-inline">print</strong> statement is <span class="No-Break">as follows:</span></p>
			<pre class="console">
                   OLS Regression Results           
===========================================
Dep. Variable:        y            R-squared:0.769
Model:              OLS             Adj. R-squared:0.764
Method: Least Squares  F-statistic:161.5
Date: Fri, 25 Nov 2022 Prob (F-statistic):1.35e-31
Time: 12:38:40      Log-Likelihood:-389.48
No. Observations: 100 AIC:        785.0           
Df Residuals: 97              BIC:     792.8
Df Model: 2
Covariance Type:  nonrobust </pre>
			<p>This <a id="_idIndexMarker740"/>gives us a summary of the model, various parameters, and various goodness-of-fit characteristics such as the <strong class="source-inline">R-squared</strong> values (0.77 and 0.762), which indicate that the fit is reasonable but not very good. The second half of the output contains information about the <span class="No-Break">estimated coefficients:</span></p>
			<pre class="console">
=========================================
         coef           std err      t     P&gt;|t|  [0.025    0.975]
-----------------------------------------------------------------------
const -11.1058  2.878  -3.859 0.000  -16.818  -5.393
X1      4.7245  0.301  15.672   0.00    4.126      5.323
X2     -1.9050  0.164 -11.644  0.000   -2.230   -1.580
=========================================
Omnibus:           0.259        Durbin-Watson:     1.875
Prob(Omnibus): 0.878       Jarque-Bera (JB):  0.260
Skew: 0.115             Prob(JB):         0.878
Kurtosis: 2.904         Cond. No          38.4
=========================================
Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</pre>
			<p>In the summary data, we can see that the <strong class="source-inline">X3</strong> variable is not significant since it has a p-value <span class="No-Break">of 0.66.</span></p>
			<ol>
				<li value="6">Since the <a id="_idIndexMarker741"/>third predictor variable is not significant, we eliminate this column and perform the <span class="No-Break">regression again:</span><pre class="console">
second_model = sm.OLS(</pre><pre class="console">
    Y, p_vars.loc[:, "const":"X2"]).fit()</pre><pre class="console">
print(second_model.summary())</pre></li>
			</ol>
			<p>This results in a small increase in the <span class="No-Break">goodness-of-fit statistics.</span></p>
			<h2 id="_idParaDest-296"><a id="_idTextAnchor295"/>How it works...</h2>
			<p>Multilinear regression works in much the same way as simple linear regression. We follow the same procedure here as in the previous recipe, where we use the <strong class="source-inline">statsmodels</strong> package to fit a multilinear model to our data. Of course, there are some differences behind the scenes. The model we produce using multilinear regression is very similar in form to the simple linear model from the previous recipe. It has the <span class="No-Break">following form:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_036.png"/></p>
			<p>Here, <img alt="" src="image/Formula_07_037.png"/> is the response variable, <img alt="" src="image/Formula_07_038.png"/> represents the predictor variables, <img alt="" src="image/Formula_07_039.png"/> is the error term, and <img alt="" src="image/Formula_07_040.png"/> is the parameters to be computed. The same requirements are also necessary for this context: residuals must be independent and normally distributed with a mean of 0 and a common <span class="No-Break">standard deviation.</span></p>
			<p>In this recipe, we provided our predictor data as a Pandas DataFrame rather than a plain NumPy array. Notice that the names of the columns have been adopted in the summary data that we printed. Unlike the first recipe, <em class="italic">Using basic linear regression</em>, we included the constant column in this DataFrame, rather than using the <strong class="source-inline">add_constant</strong> utility <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">statsmodels</strong></span><span class="No-Break">.</span></p>
			<p>In the output of the first regression, we can see that the model is a reasonably good fit with an adjusted <img alt="" src="image/Formula_07_041.png"/> value of 0.762, and is highly significant (we can see this by looking at the regression F statistic p-value). However, looking closer at the individual parameters, we can see that both of the first two predictor values are significant, but the constant and the third predictor are less so. In particular, the third predictor parameter, <strong class="source-inline">X3</strong>, is<a id="_idIndexMarker742"/> not significantly different from 0 and has a p-value of 0.66. Given that our response data was constructed without using this variable, this shouldn’t come as a surprise. In the final step of the analysis, we repeat the regression without the predictor variable, <strong class="source-inline">X3</strong>, which is a mild improvement to <span class="No-Break">the fit.</span></p>
			<h1 id="_idParaDest-297"><a id="_idTextAnchor296"/>Classifying using logarithmic regression</h1>
			<p>Logarithmic regression<a id="_idIndexMarker743"/> solves a different problem from ordinary linear regression. It is commonly used for classification problems where, typically, we wish to classify data into two distinct groups, according to a number of predictor variables. Underlying this technique is a transformation that’s performed using logarithms. The original classification problem is transformed into a problem of constructing a model for <a id="_idIndexMarker744"/>the <strong class="bold">log-odds</strong>. This model can be completed with simple linear regression. We apply the inverse transformation to the linear model, which leaves us with a model of the probability that the desired outcome will occur, given the predictor data. The transform we apply here is called the <strong class="bold">logistic function</strong>, which<a id="_idIndexMarker745"/> gives its name to the method. The probability we obtain can then be used in the classification problem we originally aimed <span class="No-Break">to solve.</span></p>
			<p>In this recipe, we will learn how to perform logistic regression and use this technique in <span class="No-Break">classification problems.</span></p>
			<h2 id="_idParaDest-298"><a id="_idTextAnchor297"/>Getting ready</h2>
			<p>For this recipe, we will need the NumPy package imported as <strong class="source-inline">np</strong>, the Matplotlib <strong class="source-inline">pyplot</strong> module imported as <strong class="source-inline">plt</strong>, the Pandas package imported as <strong class="source-inline">pd</strong>, and an instance of the NumPy default random number generator to be created using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
from numpy.random import default_rng
rng = default_rng(12345)</pre>
			<p>We also need several components from the <strong class="source-inline">scikit-learn</strong> package to perform logistic regression. These can be imported <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report</pre>
			<h2 id="_idParaDest-299"><a id="_idTextAnchor298"/>How to do it...</h2>
			<p>Follow <a id="_idIndexMarker746"/>these steps to use logistic regression<a id="_idIndexMarker747"/> to solve a simple <span class="No-Break">classification problem:</span></p>
			<ol>
				<li value="1">First, we need to create some sample data that we can use to demonstrate how to use logistic regression. We start by creating the <span class="No-Break">predictor variables:</span><pre class="console">
df = pd.DataFrame({</pre><pre class="console">
    "var1": np.concatenate([</pre><pre class="console">
        rng.normal(3.0, 1.5, size=50),</pre><pre class="console">
        rng.normal(-4.0, 2.0, size=50)]),</pre><pre class="console">
    "var2": rng.uniform(size=100),</pre><pre class="console">
    "var3": np.concatenate([</pre><pre class="console">
        rng.normal(-2.0, 2.0, size=50),</pre><pre class="console">
        rng.normal(1.5, 0.8, size=50)])</pre><pre class="console">
})</pre></li>
				<li>Now, we use two of our three predictor variables to create our response variable as a series of <span class="No-Break">Boolean values:</span><pre class="console">
score = 4.0 + df["var1"] - df["var3"]</pre><pre class="console">
Y = score &gt;= 0</pre></li>
				<li>Next, we <a id="_idIndexMarker748"/>scatterplot<a id="_idIndexMarker749"/> the points, styled according to the response variable, of the <strong class="source-inline">var3</strong> data against the <strong class="source-inline">var1</strong> data, which are the variables used to construct the <span class="No-Break">response variable:</span><pre class="console">
fig1, ax1 = plt.subplots()</pre><pre class="console">
ax1.plot(df.loc[Y, "var1"], df.loc[Y, "var3"],</pre><pre class="console">
    "ko", label="True data")</pre><pre class="console">
ax1.plot(df.loc[~Y, "var1"], df.loc[~Y, "var3"],</pre><pre class="console">
    "kx", label="False data")</pre><pre class="console">
ax1.legend()</pre><pre class="console">
ax1.set_xlabel("var1")</pre><pre class="console">
ax1.set_ylabel("var3")</pre><pre class="console">
ax1.set_title("Scatter plot of var3 against var1")</pre></li>
			</ol>
			<p>The resulting plot can be seen in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer843">
					<img alt="Figure 7.3 – Scatter plot of the var3 data against var1, with classification marked&#13;&#10;" src="image/7.3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Scatter plot of the var3 data against var1, with classification marked</p>
			<ol>
				<li value="4">Next, we <a id="_idIndexMarker750"/>create a <strong class="source-inline">LogisticRegression</strong> object from the <strong class="source-inline">scikit-learn</strong> package<a id="_idIndexMarker751"/> and fit the model to <span class="No-Break">our data:</span><pre class="console">
model = LogisticRegression()</pre><pre class="console">
model.fit(df, Y)</pre></li>
				<li>Next, we prepare some extra data, different from what we used to fit the model, to test the accuracy of <span class="No-Break">our model:</span><pre class="console">
test_df = pd.DataFrame({</pre><pre class="console">
    "var1": np.concatenate([</pre><pre class="console">
        rng.normal(3.0, 1.5, size=50),</pre><pre class="console">
        rng.normal(-4.0, 2.0, size=50)]),</pre><pre class="console">
    "var2": rng.uniform(size=100),</pre><pre class="console">
    "var3": np.concatenate([</pre><pre class="console">
        rng.normal(-2.0, 2.0, size=50),</pre><pre class="console">
        rng.normal(1.5, 0.8, size=50)])</pre><pre class="console">
})</pre><pre class="console">
test_scores = 4.0 + test_df["var1"] - test_df["var3"]</pre><pre class="console">
test_Y = test_scores &gt;= 0</pre></li>
				<li>Then, we <a id="_idIndexMarker752"/>generate<a id="_idIndexMarker753"/> predicted results based on our logistic <span class="No-Break">regression model:</span><pre class="console">
test_predicts = model.predict(test_df)</pre></li>
				<li>Finally, we use the <strong class="source-inline">classification_report</strong> utility from <strong class="source-inline">scikit-learn</strong> to print a summary of predicted classification against known response values to test the accuracy of the model. We print this summary to <span class="No-Break">the Terminal:</span><pre class="console">
print(classification_report(test_Y, test_predicts))</pre></li>
			</ol>
			<p>The report that’s generated by this routine looks <span class="No-Break">as follows:</span></p>
			<pre class="console">
              precision    recall      f1-score   support
       False       0.82      1.00        0.90        18
        True        1.00      0.88        0.93        32
accuracy                     0.92                   50
   macro avg     0.91      0.94       0.92         50
weighted avg    0.93      0.92       0.92         50</pre>
			<p>The report <a id="_idIndexMarker754"/>here contains <a id="_idIndexMarker755"/>information about the performance of the classification model on the test data. We can see that the reported precision and recall are good, indicating that there were relatively few false positive and false <span class="No-Break">negative identifications.</span></p>
			<h2 id="_idParaDest-300"><a id="_idTextAnchor299"/>How it works...</h2>
			<p>Logistic regression <a id="_idIndexMarker756"/>works by forming a linear model of the <em class="italic">log-odds</em> ratio (or <em class="italic">logit</em>), which, for a single predictor variable, <img alt="" src="image/Formula_07_042.png"/>, has the <span class="No-Break">following form:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_043.png"/></p>
			<p>Here, <img alt="" src="image/Formula_07_044.png"/> represents the probability of a true outcome in response to the given predictor, <img alt="" src="image/Formula_07_045.png"/>. Rearranging this gives a variation of the logistic function for <span class="No-Break">the probability:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_046.png"/></p>
			<p>The parameters for the log-odds are estimated using a maximum <span class="No-Break">likelihood method.</span></p>
			<p>The <strong class="source-inline">LogisticRegression</strong> class from the <strong class="source-inline">linear_model</strong> module in <strong class="source-inline">scikit-learn</strong> is an implementation of logistic regression that is very easy to use. First, we create a new model instance of this class, with any custom parameters that we need, and then use the <strong class="source-inline">fit</strong> method on this object to fit (or train) the model to the sample data. Once this fitting is done, we can access the parameters that have been estimated using the <strong class="source-inline">get_params</strong> method. </p>
			<p>The <strong class="source-inline">predict</strong> method on the fitted model allows us to pass in new (unseen) data and make predictions about the classification of each sample. We could also get the probability estimates that are actually given by the logistic function using the <span class="No-Break"><strong class="source-inline">predict_proba</strong></span><span class="No-Break"> method.</span></p>
			<p>Once we have built a model for predicting the classification of data, we need to validate the model. This means we have to test the model with some previously unseen data and check whether it correctly classifies the new data. For this, we can use <strong class="source-inline">classification_report</strong>, which takes a new set of data and the predictions generated by the model and computes several summary values about the performance of the model. The first reported value is <a id="_idIndexMarker757"/>the <strong class="bold">precision</strong>, which is the ratio of the number of true positives to the number of predicted positives. This measures how well the model avoids labeling values as positive when they are not. The second reported value is <a id="_idIndexMarker758"/>the <strong class="bold">recall</strong>, which is the ratio of the number of true positives to the number of true positives plus the number of false negatives. This measures the ability of the model to find positive samples within the collection. A related score (not included in the report) is <a id="_idIndexMarker759"/>the <strong class="bold">accuracy</strong>, which is the ratio of the number of correct classifications to the total number of classifications. This measures the ability of the model to correctly <span class="No-Break">label samples.</span></p>
			<p>The<a id="_idIndexMarker760"/> classification<a id="_idIndexMarker761"/> report we generated using the <strong class="source-inline">scikit-learn</strong> utility performs a comparison between the predicted results and the known response values. This is a common method for validating a model before using it to make actual predictions. In this recipe, we saw that the reported precision for each of the categories (<strong class="source-inline">True</strong> and <strong class="source-inline">False</strong>) was <strong class="source-inline">1.00</strong>, indicating that the model performed perfectly in predicting the classification with this data. In practice, it is unlikely that the precision of a model will be 100%. </p>
			<h2 id="_idParaDest-301"><a id="_idTextAnchor300"/>There’s more...</h2>
			<p>There are lots of packages that offer tools for using logistic regression for classification problems. The <strong class="source-inline">statsmodels</strong> package has the <strong class="source-inline">Logit</strong> class for creating logistic regression models. We used the <strong class="source-inline">scikit-learn</strong> package in this recipe, which has a similar interface. <strong class="source-inline">scikit-learn</strong> is a <a id="_idIndexMarker762"/>general-purpose machine learning library and has a variety of other tools for <span class="No-Break">classification problems.</span></p>
			<h1 id="_idParaDest-302"><a id="_idTextAnchor301"/>Modeling time series data with ARMA</h1>
			<p>Time series, as the <a id="_idIndexMarker763"/>name suggests, track a value over a sequence of distinct time intervals. They are particularly important in the finance industry, where stock values are tracked over time and used to make predictions – known as forecasting – of the value at some point in the future. Good predictions coming from this kind of data can be used to make better investments. Time series also appear in many other common situations, such as weather monitoring, medicine, and any places where data is derived from sensors <span class="No-Break">over time.</span></p>
			<p>Time series, unlike <a id="_idIndexMarker764"/>other types of data, do not usually have independent data points. This means that the methods that we use for modeling independent data will not be particularly effective. Thus, we need to use alternative techniques to model data with this property. There are two ways in which a value in a time series can depend on previous values. The first is where there is a direct relationship between the value and one or more previous values. This is the <em class="italic">autocorrelation</em> property<a id="_idIndexMarker765"/> and is modeled by an <em class="italic">AR</em> model. The second is where the noise that’s added to the value depends on one or more previous noise terms. This is modeled by an <em class="italic">MA</em> model. The number of terms involved in either of these models is called the <em class="italic">order</em> of <span class="No-Break">the model.</span></p>
			<p>In this recipe, we will learn how to create a model for stationary time series data with <span class="No-Break">ARMA terms.</span></p>
			<h2 id="_idParaDest-303"><a id="_idTextAnchor302"/>Getting ready</h2>
			<p>For this recipe, we<a id="_idIndexMarker766"/> need the Matplotlib <strong class="source-inline">pyplot</strong> module<a id="_idIndexMarker767"/> imported as <strong class="source-inline">plt</strong> and the <strong class="source-inline">statsmodels</strong> package <strong class="source-inline">api</strong> module imported as <strong class="source-inline">sm</strong>. We also need to import the <strong class="source-inline">generate_sample_data</strong> routine from the <strong class="source-inline">tsdata</strong> package from this book’s repository, which uses NumPy and Pandas to generate sample data <span class="No-Break">for analysis:</span></p>
			<pre class="source-code">
from tsdata import generate_sample_data</pre>
			<p>To avoid repeatedly setting colors in plotting functions, we do some one-time setup to set the plotting <span class="No-Break">color here:</span></p>
			<pre class="source-code">
from matplotlib.rcsetup import cycler
plt.rc("axes", prop_cycle=cycler(c="k"))</pre>
			<p>With this set up, we can now see how to generate an ARMA model for some time <span class="No-Break">series data.</span></p>
			<h2 id="_idParaDest-304"><a id="_idTextAnchor303"/>How to do it...</h2>
			<p>Follow these steps to create an ARMA model for stationary time <span class="No-Break">series data:</span></p>
			<ol>
				<li value="1">First, we need to generate the sample data that we <span class="No-Break">will analyze:</span><pre class="console">
sample_ts, _ = generate_sample_data()</pre></li>
				<li>As always, the first step in the analysis is to produce a plot of the data so that we can visually identify <span class="No-Break">any structure:</span><pre class="console">
ts_fig, ts_ax = plt.subplots()</pre><pre class="console">
sample_ts.plot(ax=ts_ax, label="Observed",</pre><pre class="console">
    ls="--", alpha=0.4)</pre><pre class="console">
ts_ax.set_title("Time series data")</pre><pre class="console">
ts_ax.set_xlabel("Date")</pre><pre class="console">
ts_ax.set_ylabel("Value")</pre></li>
			</ol>
			<p>The <a id="_idIndexMarker768"/>resulting plot can be seen in the <span class="No-Break">following</span><span class="No-Break"><a id="_idIndexMarker769"/></span><span class="No-Break"> figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer849">
					<img alt="Figure 7.4 - Plot of the time series data that we will analyze (there doesn’t appear to be a trend in this data)&#13;&#10;" src="image/7.4.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 - Plot of the time series data that we will analyze (there doesn’t appear to be a trend in this data)</p>
			<p>Here, we<a id="_idIndexMarker770"/> can see that there doesn’t <a id="_idIndexMarker771"/>appear to be an underlying trend, which means that the data is likely to be stationary (a time series is said to be <strong class="bold">stationary</strong> if its <a id="_idIndexMarker772"/>statistical properties do not vary with time. This often manifests in the form of an upward or <span class="No-Break">downward trend).</span></p>
			<ol>
				<li value="3">Next, we compute the augmented Dickey-Fuller test. This is a hypothesis test for whether a time series is stationary or not. The null hypothesis is that the time series is <span class="No-Break">not stationary:</span><pre class="console">
adf_results = sm.tsa.adfuller(sample_ts)</pre><pre class="console">
adf_pvalue = adf_results[1]</pre><pre class="console">
print("Augmented Dickey-Fuller test:\nP-value:",</pre><pre class="console">
    adf_pvalue)</pre></li>
			</ol>
			<p>The reported <strong class="source-inline">adf_pvalue</strong> is 0.000376 in this case, so we reject the null hypothesis and conclude that the series <span class="No-Break">is stationary.</span></p>
			<ol>
				<li value="4">Next, we need to determine the order of<a id="_idIndexMarker773"/> the model that we should fit. For this, we’ll plot the <strong class="bold">autocorrelation function</strong> (<strong class="bold">ACF</strong>) and<a id="_idIndexMarker774"/> the <strong class="bold">partial autocorrelation function</strong> (<strong class="bold">PACF</strong>) for the <span class="No-Break">time series:</span><pre class="console">
ap_fig, (acf_ax, pacf_ax) = plt.subplots(</pre><pre class="console">
    2, 1, tight_layout=True)</pre><pre class="console">
sm.graphics.tsa.plot_acf(sample_ts, ax=acf_ax, </pre><pre class="console">
    title="Observed autocorrelation")</pre><pre class="console">
sm.graphics.tsa.plot_pacf(sample_ts, ax=pacf_ax, </pre><pre class="console">
    title="Observed partial autocorrelation")</pre><pre class="console">
acf_ax.set_xlabel("Lags")</pre><pre class="console">
pacf_ax.set_xlabel("Lags")</pre><pre class="console">
pacf_ax.set_ylabel("Value")</pre><pre class="console">
acf_ax.set_ylabel("Value")</pre></li>
			</ol>
			<p>The <a id="_idIndexMarker775"/>plots of the ACF and PACF for our time series<a id="_idIndexMarker776"/> can be seen in the following figure. These plots suggest the existence of both AR and <span class="No-Break">MA processes:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer850">
					<img alt="Figure 7.5 - The ACF and PACF for the sample time series data&#13;&#10;" src="image/7.5.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 - The ACF and PACF for the sample time series data</p>
			<ol>
				<li value="5">Next, we create an ARMA model for the data, using the <strong class="source-inline">ARIMA</strong> class from the <strong class="source-inline">tsa</strong> module. This model will have an order 1 AR component and an order 1 <span class="No-Break">MA component:</span><pre class="console">
arma_model = sm.tsa.ARIMA(sample_ts, order=(1, 0, 1))</pre></li>
				<li>Now, we fit the model to the data and get the resulting model. We print a summary of these results to <span class="No-Break">the Terminal:</span><pre class="console">
arma_results = arma_model.fit()</pre><pre class="console">
print(arma_results.summary())</pre></li>
				<li>The summary data given for the fitted model is <span class="No-Break">as follows:</span><pre class="console">
                     ARMA Model Results</pre><pre class="console">
=========================================</pre><pre class="console">
Dep. Variable: y No.             Observations:         366</pre><pre class="console">
Model: ARMA(1, 1)               Log Likelihood -513.038</pre><pre class="console">
Method: css-mle S.D. of innovations        </pre><pre class="console">
                                                 0.982</pre><pre class="console">
Date: Fri, 01 May 2020 AIC  1034.077</pre><pre class="console">
Time: 12:40:00              BIC  1049.687</pre><pre class="console">
Sample: 01-01-2020     HQIC  1040.280</pre><pre class="console">
            - 12-31-2020</pre><pre class="console">
==================================================</pre><pre class="console">
coef         std       err            z  P&gt;|z|  [0.025   0.975]</pre><pre class="console">
---------------------------------------------------------------------</pre><pre class="console">
const  -0.0242  0.143  -0.169  0.866  -0.305  0.256</pre><pre class="console">
ar.L1.y 0.8292  0.057  14.562  0.000   0.718  0.941</pre><pre class="console">
ma.L1.y -0.5189 0.090  -5.792 0.000  -0.695  -0.343</pre><pre class="console">
                                             Roots</pre><pre class="console">
=========================================</pre><pre class="console">
                    Real  Imaginary  Modulus </pre><pre class="console">
Frequency </pre><pre class="console">
---------------------------------------------------------</pre><pre class="console">
AR.1        1.2059  +0.0000j  1.2059</pre><pre class="console">
0.0000</pre><pre class="console">
MA.1        1.9271  +0.0000j  1.9271</pre><pre class="console">
0.0000</pre><pre class="console">
---------------------------------------------------</pre></li>
			</ol>
			<p>Here, we can see that both of the estimated parameters for the AR and MA components are significantly different from 0. This is because the value in the <strong class="source-inline">P &gt;|z|</strong> column is 0 to 3 <span class="No-Break">decimal places.</span></p>
			<ol>
				<li value="8">Next, we <a id="_idIndexMarker777"/>need to verify that there is no additional <a id="_idIndexMarker778"/>structure remaining in the residuals (error) of the predictions from our model. For this, we plot the ACF and PACF of <span class="No-Break">the residuals:</span><pre class="console">
residuals = arma_results.resid</pre><pre class="console">
rap_fig, (racf_ax, rpacf_ax) = plt.subplots(</pre><pre class="console">
    2, 1, tight_layout=True)</pre><pre class="console">
sm.graphics.tsa.plot_acf(residuals, ax=racf_ax, </pre><pre class="console">
    title="Residual autocorrelation")</pre><pre class="console">
sm.graphics.tsa.plot_pacf(residuals, ax=rpacf_ax, </pre><pre class="console">
    title="Residual partial autocorrelation")</pre><pre class="console">
racf_ax.set_xlabel("Lags")</pre><pre class="console">
rpacf_ax.set_xlabel("Lags")</pre><pre class="console">
rpacf_ax.set_ylabel("Value")</pre><pre class="console">
racf_ax.set_ylabel("Value")</pre></li>
			</ol>
			<p>The ACF and PACF of the residuals can be seen in the following figure. Here, we can see that there are no significant spikes at lags other than 0, so we conclude that there is no structure remaining in <span class="No-Break">the residuals:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer851">
					<img alt="Figure 7.6 - The ACF and PACF for the residuals from our model&#13;&#10;" src="image/7.6.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 - The ACF and PACF for the residuals from our model</p>
			<ol>
				<li value="9">Now that we <a id="_idIndexMarker779"/>have verified that our model is not <a id="_idIndexMarker780"/>missing any structure, we plot the values that are fitted to each data point on top of the actual time series data to see whether the model is a good fit for the data. We plot this model in the plot we created in <span class="No-Break"><em class="italic">step 2</em></span><span class="No-Break">:</span><pre class="console">
fitted = arma_results.fittedvalues</pre><pre class="console">
fitted.plot(ax=ts_ax, label="Fitted")</pre><pre class="console">
ts_ax.legend()</pre></li>
			</ol>
			<p>The updated plot can be seen in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer852">
					<img alt="Figure 7.7 – Plot of the fitted time series data over the observed time series data&#13;&#10;" src="image/7.7.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – Plot of the fitted time series data over the observed time series data</p>
			<p>The fitted<a id="_idIndexMarker781"/> values give a reasonable approximation of the <a id="_idIndexMarker782"/>behavior of the time series but reduce the noise from the <span class="No-Break">underlying structure.</span></p>
			<h2 id="_idParaDest-305"><a id="_idTextAnchor304"/>How it works...</h2>
			<p>The ARMA model that we used in this recipe is a basic means of modeling the behavior of stationary time series. The two parts of an ARMA model are the AR and MA parts, which model the dependence of the terms and noise, respectively, on previous terms and noise. In practice, time series are usually not stationary, and we have to perform some kind of transformation to make this the case before we can fit an <span class="No-Break">ARMA model.</span></p>
			<p>An order 1 AR model has the <span class="No-Break">following form:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_047.png"/></p>
			<p>Here, <img alt="" src="image/Formula_07_048.png"/> represents the parameters and <img alt="" src="image/Formula_07_049.png"/> is the noise at a given step. The noise is usually assumed to be normally distributed with a mean of 0 and a standard deviation that is constant across all the time steps. The <img alt="" src="image/Formula_07_050.png"/> value represents the value of the time series at the time step, <img alt="" src="image/Formula_07_051.png"/>. In this model, each value depends on the previous value, although it can also depend on some constants and some noise. The model will give rise to a stationary time series precisely when the <img alt="" src="image/Formula_07_052.png"/> parameter lies strictly between -1 <span class="No-Break">and 1.</span></p>
			<p>An order 1 MA model is very similar to an AR model and is given by the <span class="No-Break">following equation:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_053.png"/></p>
			<p>Here, the variants of <img alt="" src="image/Formula_07_054.png"/> are parameters. Putting these two models together gives us an ARMA(1,1) model, which has the <span class="No-Break">following form:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_055.png"/></p>
			<p>In general, we can have an ARMA(p,q) model that has an order <img alt="" src="image/Formula_07_056.png"/> AR component and an order q MA component. We usually refer to the quantities, <img alt="" src="image/Formula_07_057.png"/> and <img alt="" src="image/Formula_07_058.png"/>, as the orders of <span class="No-Break">the model.</span></p>
			<p>Determining the <a id="_idIndexMarker783"/>orders of the AR and MA components is the <a id="_idIndexMarker784"/>most tricky aspect of constructing an ARMA model. The ACF and PACF give some information about this, but even then, it can be quite difficult. For example, an AR process will show some kind of decay or oscillating pattern on the ACF as lag increases, and a small number of peaks on the PACF and values that are not significantly different from zero beyond that. The number of peaks that appear on the PAF plot can be taken as the order of the process. For an MA process, the reverse is true. There is usually a small number of significant peaks on the ACF plot, and a decay or oscillating pattern on the PACF plot. Of course, sometimes, this <span class="No-Break">isn’t obvious.</span></p>
			<p>In this recipe, we plotted the ACF and PACF for our sample time series data. In the autocorrelation plot in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.5</em> (top), we can see that the peaks decay rapidly until they lie within the confidence interval of zero (meaning they are not significant). This suggests the presence of an AR component. On the partial autocorrelation plot in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.5</em> (bottom), we can see that there are only two peaks that can be considered not zero, which suggests an AR process of order 1 or 2. You should try to keep the order of the model as small as possible. Due to this, we chose an order 1 AR component. With this assumption, the second peak on the partial autocorrelation plot is indicative of decay (rather than an isolated peak), which suggests the presence of an MA process. To keep the model simple, we try an order 1 MA process. This is how the model that we used in this recipe was decided on. Notice that this is not an exact process, and you might have decided differently. </p>
			<p>We use the augmented Dickey-Fuller test to test the likelihood that the time series that we have observed is stationary. This is a statistical test, such as those seen in <a href="B19085_06.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Working with Data and Statistics</em>, that generates a test statistic from the data. This test statistic, in turn, determines a p-value that is used to determine whether to accept or reject the null hypothesis. For this test, the null hypothesis is that a unit root is present in the time series that’s been sampled. The alternative hypothesis – the one we are really interested in – is that the observed time series is (trend) stationary. If the p-value is sufficiently small, then we can conclude with the specified confidence that the observed time series is stationary. In this recipe, the p-value was 0.000 to 3 decimal places, which indicates a strong likelihood that the series is stationary. Stationarity is an essential assumption for using the ARMA model for <span class="No-Break">the data.</span></p>
			<p>Once we have<a id="_idIndexMarker785"/> determined that the series is stationary, and <a id="_idIndexMarker786"/>decided on the orders of the model, we have to fit the model to the sample data that we have. The parameters of the model are estimated using a maximum likelihood estimator. In this recipe, the learning of the parameters is done using the <strong class="source-inline">fit</strong> method, in <span class="No-Break"><em class="italic">step 6</em></span><span class="No-Break">.</span></p>
			<p>The <strong class="source-inline">statsmodels</strong> package provides various tools for working with time series, including utilities for calculating – and plotting – ACF and PACF of time series data, various test statistics, and creating ARMA models for time series. There are also some tools for automatically estimating the order of <span class="No-Break">the model.</span></p>
			<p>We <a id="_idIndexMarker787"/>can use the <strong class="bold">Akaike information criterion</strong> (<strong class="bold">AIC</strong>), <strong class="bold">Bayesian information criterion</strong> (<strong class="bold">BIC</strong>), and <strong class="bold">Hannan-Quinn Information Criterion</strong> (<strong class="bold">HQIC</strong>) quantities to <a id="_idIndexMarker788"/>compare this model to other models<a id="_idIndexMarker789"/> to see which model best describes the data. A smaller value is better in <span class="No-Break">each case.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">When using ARMA to model time series data, as in all kinds of mathematical modeling tasks, it is best to pick the simplest model that describes the data to the extent that is needed. For ARMA models, this usually means picking the smallest order model that describes the structure of the <span class="No-Break">observed data.</span></p>
			<h2 id="_idParaDest-306"><a id="_idTextAnchor305"/>There’s more...</h2>
			<p>Finding the best combination of orders for an ARMA<a id="_idIndexMarker790"/> model can be quite difficult. Often, the best way to fit a model is to test multiple different configurations and pick the order that produces the best fit. For example, we could have tried ARMA(0,1) or ARMA(1, 0) in this recipe, and compared it to the ARMA(1,1) model we used to see which produced the best fit by considering the AIC statistic reported in the summary. In fact, if we build these models, we will see that the AIC value for ARMA(1,1) – the model we used in this recipe – is the “best” of these <span class="No-Break">three models.</span></p>
			<h1 id="_idParaDest-307"><a id="_idTextAnchor306"/>Forecasting from time series data using ARIMA</h1>
			<p>In the previous recipe, we<a id="_idIndexMarker791"/> generated a model for a stationary time series using an ARMA model, which consists of an AR component and an MA component. Unfortunately, this model cannot accommodate time series that have some underlying trend; that is, they are not stationary time series. We can often get around this by <em class="italic">differencing</em> the observed time series one or more times until we obtain a stationary time series that can be modeled using ARMA. The incorporation of differencing into an ARMA model is called an <span class="No-Break">ARIMA model.</span></p>
			<p>Differencing<a id="_idIndexMarker792"/> is the process of computing the difference between consecutive terms in a sequence of data – so, applying first-order differencing amounts to subtracting the value at the current step from the value at the next step (<img alt="" src="image/Formula_07_059.png"/>). This has the effect of removing the underlying upward or downward linear trend from the data. This helps to reduce an arbitrary time series to a stationary time series that can be modeled using ARMA. Higher-order differencing can remove higher-order trends to achieve <span class="No-Break">similar effects.</span></p>
			<p>An ARIMA model has three parameters, usually labeled <img alt="" src="image/Formula_07_060.png"/>, <img alt="" src="image/Formula_07_061.png"/>, and <img alt="" src="image/Formula_07_062.png"/>. The <img alt="" src="image/Formula_07_063.png"/> and <img alt="" src="image/Formula_07_064.png"/> order parameters are the order of the AR component and the MA component, respectively, just as they are for the ARMA model. The third order parameter, <img alt="" src="image/Formula_07_065.png"/>, is the order of differencing to be applied. An ARIMA model with these orders is usually written as ARIMA (<img alt="" src="image/Formula_07_066.png"/>, <img alt="" src="image/Formula_07_067.png"/>, <img alt="" src="image/Formula_07_068.png"/>). Of course, we <a id="_idIndexMarker793"/>will need to determine what order differencing should be included before we start fitting <span class="No-Break">the model.</span></p>
			<p>In this recipe, we will learn how to fit an ARIMA model to a non-stationary time series and use this model to generate forecasts about <span class="No-Break">future values.</span></p>
			<h2 id="_idParaDest-308"><a id="_idTextAnchor307"/>Getting ready</h2>
			<p>For this recipe, we will need the NumPy package imported as <strong class="source-inline">np</strong>, the Pandas package imported as <strong class="source-inline">pd</strong>, the Matplotlib <strong class="source-inline">pyplot</strong> module as <strong class="source-inline">plt</strong>, and the <strong class="source-inline">statsmodels.api</strong> module imported as <strong class="source-inline">sm</strong>. We will also need the utility for creating sample time series data from the <strong class="source-inline">tsdata</strong> module, which is included in this <span class="No-Break">book’s repository:</span></p>
			<pre class="source-code">
from tsdata import generate_sample_data</pre>
			<p>As in the previous recipe, we use the Matplotlib <strong class="source-inline">rcparams</strong> to set the color for all plots in <span class="No-Break">the recipe:</span></p>
			<pre class="source-code">
from matplotlib.rcsetup import cycler
plt.rc("axes", prop_cycle=cycler(c="k"))</pre>
			<h2 id="_idParaDest-309"><a id="_idTextAnchor308"/>How to do it...</h2>
			<p>The following steps<a id="_idIndexMarker794"/> show you how to construct an ARIMA model for time series data and <a id="_idIndexMarker795"/>use this model to <span class="No-Break">make forecasts:</span></p>
			<ol>
				<li value="1">First, we load the sample data using the <span class="No-Break"><strong class="source-inline">generate_sample_data</strong></span><span class="No-Break"> routine:</span><pre class="console">
sample_ts, test_ts = generate_sample_data(</pre><pre class="console">
    trend=0.2, undiff=True)</pre></li>
				<li>As usual, the next step is to plot the time series so that we can visually identify the trend of <span class="No-Break">the data:</span><pre class="console">
ts_fig, ts_ax = plt.subplots(tight_layout=True)</pre><pre class="console">
sample_ts.plot(ax=ts_ax, label="Observed")</pre><pre class="console">
ts_ax.set_title("Training time series data")</pre><pre class="console">
ts_ax.set_xlabel("Date")</pre><pre class="console">
ts_ax.set_ylabel("Value")</pre></li>
			</ol>
			<p>The resulting plot can be seen in the following figure. As we can see, there is a clear upward trend in the data, so the time series is certainly <span class="No-Break">not stationary:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer875">
					<img alt="Figure 7.8 – Plot of the sample time series&#13;&#10;" src="image/7.8.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – Plot of the sample time series</p>
			<p>There is an obvious<a id="_idIndexMarker796"/> positive<a id="_idIndexMarker797"/> trend in <span class="No-Break">the data.</span></p>
			<ol>
				<li value="3">Next, we difference the series to see whether one level of differencing is sufficient to remove <span class="No-Break">the trend:</span><pre class="console">
diffs = sample_ts.diff().dropna()</pre></li>
				<li>Now, we plot the ACF and PACF for the differenced time series: <pre class="console">
ap_fig, (acf_ax, pacf_ax) = plt.subplots(2, 1, </pre><pre class="console">
    tight_layout=True)</pre><pre class="console">
sm.graphics.tsa.plot_acf(diffs, ax=acf_ax)</pre><pre class="console">
sm.graphics.tsa.plot_pacf(diffs, ax=pacf_ax)</pre><pre class="console">
acf_ax.set_ylabel("Value")</pre><pre class="console">
acf_ax.set_xlabel("Lag")</pre><pre class="console">
pacf_ax.set_xlabel("Lag")</pre><pre class="console">
pacf_ax.set_ylabel("Value")</pre></li>
			</ol>
			<p>The <a id="_idIndexMarker798"/>ACF and PACF can be seen in the following figure. We<a id="_idIndexMarker799"/> can see that there do not appear to be any trends left in the data and that there appears to be both an AR component and an <span class="No-Break">MA component:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer876">
					<img alt="Figure 7.9 - ACF and PACF for the differenced time series&#13;&#10;" src="image/7.9.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.9 - ACF and PACF for the differenced time series</p>
			<ol>
				<li value="5">Now, we <a id="_idIndexMarker800"/>construct the ARIMA model with order 1 differencing, an <a id="_idIndexMarker801"/>AR component, and an MA component. We fit this to the observed time series and print a summary of <span class="No-Break">the model:</span><pre class="console">
model = sm.tsa.ARIMA(sample_ts, order=(1,1,1))</pre><pre class="console">
fitted = model.fit()</pre><pre class="console">
print(fitted.summary())</pre></li>
			</ol>
			<p>The summary information that’s printed looks <span class="No-Break">as follows:</span></p>
			<pre class="console">
             SARIMAX Results                   
===========================================
Dep. Variable:      y   No.  Observations:  366
Model:  ARIMA(1, 0, 1)         Log Likelihood  -513.038
Date:  Fri, 25 Nov 2022         AIC            1034.077
Time:              13:17:24         BIC            1049.687
Sample:      01-01-2020        HQIC           1040.280
                 - 12-31-2020                         
Covariance Type:                  opg           
=========================================
              coef    std err     z       P&gt;|z|   [0.025   0.975]
-----------------------------------------------------------------------
const  -0.0242  0.144  -0.168  0.866   -0.307   0.258
ar.L1   0.8292  0.057  14.512  0.000   0.717    0.941
ma.L1  -0.5189  0.087  -5.954  0.000  -0.690  -0.348
sigma2  0.9653  0.075  12.902  0.000  0.819   1.112
=========================================
Ljung-Box (L1) (Q):      0.04   Jarque-Bera (JB):  0.59
Prob(Q):                        0.84  Prob(JB):                0.74
Heteroskedasticity (H): 1.15   Skew:                    -0.06
Prob(H) (two-sided):     0.44   Kurtosis:                 2.84
=========================================
Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).</pre>
			<p>Here, we<a id="_idIndexMarker802"/> can see that all 3 of our estimated coefficients<a id="_idIndexMarker803"/> are significantly different from 0 since all three have 0 to 3 decimal places in the <span class="No-Break"><strong class="source-inline">P&gt;|z|</strong></span><span class="No-Break"> column.</span></p>
			<ol>
				<li value="6">Now, we<a id="_idIndexMarker804"/> can use the <strong class="source-inline">get_forecast</strong> method to generate<a id="_idIndexMarker805"/> predictions of future values and generate a summary DataFrame from these predictions. This also returns the standard error and confidence intervals <span class="No-Break">for predictions:</span><pre class="console">
forecast =fitted.get_forecast(steps=50).summary_frame()</pre></li>
				<li>Next, we plot the forecast values and their confidence intervals on the figure containing the time <span class="No-Break">series data:</span><pre class="console">
forecast["mean"].plot(</pre><pre class="console">
    ax=ts_ax, label="Forecast", ls="--")</pre><pre class="console">
ts_ax.fill_between(forecast.index,</pre><pre class="console">
                   forecast["mean_ci_lower"],</pre><pre class="console">
                   forecast["mean_ci_upper"],</pre><pre class="console">
                   alpha=0.4)</pre></li>
				<li>Finally, we add the actual future values to generate, along with the sample in <em class="italic">step 1</em>, to the plot (it might be easier if you repeat the plot commands from <em class="italic">step 1</em> to regenerate the whole <span class="No-Break">plot here):</span><pre class="console">
test_ts.plot(ax=ts_ax, label="Actual", ls="-.")</pre><pre class="console">
ts_ax.legend()</pre></li>
			</ol>
			<p>The final plot containing the time series with the forecast and the actual future values can be seen in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer877">
					<img alt="Figure 7.10 - Sample time series with forecast values and actual future values for comparison&#13;&#10;" src="image/7.10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 - Sample time series with forecast values and actual future values for comparison</p>
			<p>Here, we can <a id="_idIndexMarker806"/>see that the actual future values are within the confidence<a id="_idIndexMarker807"/> interval for the <span class="No-Break">forecast values.</span></p>
			<h2 id="_idParaDest-310"><a id="_idTextAnchor309"/>How it works...</h2>
			<p>The ARIMA model – with orders <img alt="" src="image/Formula_07_069.png"/>, <img alt="" src="image/Formula_07_070.png"/>, and <img alt="" src="image/Formula_07_071.png"/> – is simply an ARMA (<img alt="" src="image/Formula_07_072.png"/>,<img alt="" src="image/Formula_07_073.png"/>) model that’s applied to a time series. This is obtained by applying differencing of order <img alt="" src="image/Formula_07_074.png"/> to the original time series data. It is a fairly simple way to generate a model for time series data. The <strong class="source-inline">statsmodels</strong> <strong class="source-inline">ARIMA</strong> class handles the creation of a model, while the <strong class="source-inline">fit</strong> method fits this model to <span class="No-Break">the data.</span></p>
			<p>The model is fit to the data using a maximum likelihood method and the final estimates for the parameters – in this case, one parameter for the AR component, one for the MA component, the constant trend parameter, and the variance of the noise. These parameters are reported in the summary. From this output, we can see that the estimates for the AR coefficient (<strong class="source-inline">0.9567</strong>) and the MA constant (<strong class="source-inline">-0.6407</strong>) are very good approximations of the true estimates that were used to generate the data, which were <strong class="source-inline">0.8</strong> for the AR coefficient and <strong class="source-inline">-0.5</strong> for the MA coefficient. These parameters are set in the <strong class="source-inline">generate_sample_data</strong> routine from the <strong class="source-inline">tsdata.py</strong> file in the code repository for this chapter. This generates the sample data in <em class="italic">step 1</em>. You might have noticed that the constant parameter (<strong class="source-inline">1.0101</strong>) is not <strong class="source-inline">0.2</strong>, as specified in the <strong class="source-inline">generate_sample_data</strong> call in <em class="italic">step 1</em>. In fact, it is not so far from the actual drift of the <span class="No-Break">time series.</span></p>
			<p>The <strong class="source-inline">get_forecast</strong> method on the fitted model (the output of the <strong class="source-inline">fit</strong> method) uses the model to make predictions about the value after a given number of steps. In this recipe, we forecast for up to 50 time steps beyond the range of the sample time series. The output of the command in <em class="italic">step 6</em>  is a DataFrame containing the forecast values, the standard error for the forecasts, and the upper and lower bounds for the confidence <a id="_idIndexMarker808"/>interval (by default, 95% confidence) of <a id="_idIndexMarker809"/>the forecasts. </p>
			<p>When you construct an ARIMA model for time series data, you need to make sure you use the smallest order differencing that removes the underlying trend. Applying more differencing than is<a id="_idIndexMarker810"/> necessary is called <em class="italic">over-differencing</em> and can lead to problems with <span class="No-Break">the model.</span></p>
			<h1 id="_idParaDest-311"><a id="_idTextAnchor310"/>Forecasting seasonal data using ARIMA</h1>
			<p>Time series often display periodic behavior so that peaks or dips in the value appear at regular intervals. This behavior is called <em class="italic">seasonality</em> in <a id="_idIndexMarker811"/>the analysis of time series. The methods we have used thus far in this chapter to model time series data obviously do not account for seasonality. Fortunately, it is relatively easy to adapt the standard ARIMA model to incorporate seasonality, resulting in what is sometimes called a <a id="_idIndexMarker812"/><span class="No-Break">SARIMA model.</span></p>
			<p>In this recipe, we will learn how to model time series data that includes seasonal behavior and use this model to <span class="No-Break">produce forecasts.</span></p>
			<h2 id="_idParaDest-312"><a id="_idTextAnchor311"/>Getting ready</h2>
			<p>For this recipe, we <a id="_idIndexMarker813"/>will need the NumPy package imported<a id="_idIndexMarker814"/> as <strong class="source-inline">np</strong>, the Pandas package imported as <strong class="source-inline">pd</strong>, the Matplotlib <strong class="source-inline">pyplot</strong> module as <strong class="source-inline">plt</strong>, and the <strong class="source-inline">statsmodels</strong> <strong class="source-inline">api</strong> module imported as <strong class="source-inline">sm</strong>. We will also need the utility for creating sample time series data from the <strong class="source-inline">tsdata</strong> module, which is included in this <span class="No-Break">book’s repository:</span></p>
			<pre class="source-code">
from tsdata import generate_sample_data</pre>
			<p>Let’s see how to produce an ARIMA model that takes seasonal variations <span class="No-Break">into account.</span></p>
			<h2 id="_idParaDest-313"><a id="_idTextAnchor312"/>How to do it...</h2>
			<p>Follow these steps to produce a seasonal ARIMA model for sample time series data and use this<a id="_idIndexMarker815"/> model to <a id="_idIndexMarker816"/><span class="No-Break">produce forecasts:</span></p>
			<ol>
				<li value="1">First, we use the <strong class="source-inline">generate_sample_data</strong> routine to generate a sample time series <span class="No-Break">to analyze:</span><pre class="console">
sample_ts, test_ts = generate_sample_data(undiff=True,</pre><pre class="console">
    seasonal=True)</pre></li>
				<li>As usual, our first step is to visually inspect the data by producing a plot of the sample <span class="No-Break">time series:</span><pre class="console">
ts_fig, ts_ax = plt.subplots(tight_layout=True)</pre><pre class="console">
sample_ts.plot(ax=ts_ax, title="Time series",</pre><pre class="console">
    label="Observed")</pre><pre class="console">
ts_ax.set_xlabel("Date")</pre><pre class="console">
ts_ax.set_ylabel("Value")</pre></li>
			</ol>
			<p>The plot of the sample time series data can be seen in the following figure. Here, we can see that there seem to be periodic peaks in <span class="No-Break">the data:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer884">
					<img alt="Figure 7.11 - Plot of the sample time series data&#13;&#10;" src="image/7.11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.11 - Plot of the sample time series data</p>
			<ol>
				<li value="3">Next, we<a id="_idIndexMarker817"/> plot the ACF and PACF for<a id="_idIndexMarker818"/> the sample <span class="No-Break">time series:</span><pre class="console">
ap_fig, (acf_ax, pacf_ax) = plt.subplots(2, 1,</pre><pre class="console">
    tight_layout=True)</pre><pre class="console">
sm.graphics.tsa.plot_acf(sample_ts, ax=acf_ax)</pre><pre class="console">
sm.graphics.tsa.plot_pacf(sample_ts, ax=pacf_ax)</pre><pre class="console">
acf_ax.set_xlabel("Lag")</pre><pre class="console">
pacf_ax.set_xlabel("Lag")</pre><pre class="console">
acf_ax.set_ylabel("Value")</pre><pre class="console">
pacf_ax.set_ylabel("Value")</pre></li>
			</ol>
			<p>The ACF <a id="_idIndexMarker819"/>and PACF for the sample time series<a id="_idIndexMarker820"/> can be seen in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer885">
					<img alt="Figure 7.12 - The ACF and PACF for the sample time series&#13;&#10;" src="image/7.12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.12 - The ACF and PACF for the sample time series</p>
			<p>These plots possibly indicate the existence of AR components, but also a significant spike in the PACF with <span class="No-Break">lag </span><span class="No-Break"><strong class="bold">7</strong></span><span class="No-Break">.</span></p>
			<ol>
				<li value="4">Next, we difference the time series and produce plots of the ACF and PACF for the differenced series. This should make the order of the <span class="No-Break">model clearer:</span><pre class="console">
diffs = sample_ts.diff().dropna()</pre><pre class="console">
dap_fig, (dacf_ax, dpacf_ax) = plt.subplots(</pre><pre class="console">
    2, 1, tight_layout=True)</pre><pre class="console">
sm.graphics.tsa.plot_acf(diffs, ax=dacf_ax, </pre><pre class="console">
    title="Differenced ACF")</pre><pre class="console">
sm.graphics.tsa.plot_pacf(diffs, ax=dpacf_ax, </pre><pre class="console">
    title="Differenced PACF")</pre><pre class="console">
dacf_ax.set_xlabel("Lag")</pre><pre class="console">
dpacf_ax.set_xlabel("Lag")</pre><pre class="console">
dacf_ax.set_ylabel("Value")</pre><pre class="console">
dpacf_ax.set_ylabel("Value")</pre></li>
			</ol>
			<p>The ACF and PACF for the differenced time series can be seen in the following figure. We can see that there is definitely a seasonal component with <span class="No-Break">lag 7:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer886">
					<img alt="Figure 7.13 -  Plot of the ACF and PACF for the differenced time series&#13;&#10;" src="image/7.13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.13 -  Plot of the ACF and PACF for the differenced time series</p>
			<ol>
				<li value="5">Now, we<a id="_idIndexMarker821"/> need to create a <strong class="source-inline">SARIMAX</strong> object<a id="_idIndexMarker822"/> that holds the model, with an ARIMA order of <strong class="source-inline">(1, 1, 1)</strong> and a SARIMA order of <strong class="source-inline">(1, 0, 0, 7)</strong>. We fit this model to the sample time series and print summary statistics. We plot the predicted values on top of the time <span class="No-Break">series data:</span><pre class="console">
model = sm.tsa.SARIMAX(sample_ts, order=(1, 1, 1), </pre><pre class="console">
    seasonal_order=(1, 0, 0, 7))</pre><pre class="console">
fitted_seasonal = model.fit()</pre><pre class="console">
print(fitted_seasonal.summary())</pre></li>
			</ol>
			<p>The first half <a id="_idIndexMarker823"/>of the summary statistics that<a id="_idIndexMarker824"/> are printed to the terminal are <span class="No-Break">as follows:</span></p>
			<pre class="console">
             SARIMAX Results                   
===========================================
Dep. Variable:      y   No.  Observations:     366
Model:ARIMA(1, 0, 1)       Log Likelihood  -513.038
Date:   Fri, 25 Nov 2022      AIC    1027.881
Time:  14:08:54                   BIC    1043.481
Sample:01-01-2020            HQIC  1034.081
            - 12-31-2020    
Covariance Type:         opg</pre>
			<p>As before, the first half contains some information about the model, parameters, and fit. The second half of the summary (here) contains information about the estimated <span class="No-Break">model coefficients:</span></p>
			<pre class="console">
=========================================
              coef   std err       z      P&gt;|z|   [0.025  0.975]
-----------------------------------------------------
ar.L1  0.7939   0.065  12.136  0.000   0.666  0.922
ma.L1  -0.4544  0.095  -4.793  0.000  -0.640  -0.269
ar.S.L7 0.7764  0.034  22.951  0.000   0.710  0.843
sigma2  0.9388  0.073  2.783   0.000   0.795  1.083
=========================================
Ljung-Box (L1) (Q):     0.03        Jarque-Bera (JB): 0.47
Prob(Q):                0.86               Prob(JB): 0.79
Heteroskedasticity (H): 1.15        Skew: -0.03
Prob(H) (two-sided):    0.43         Kurtosis: 2.84
=========================================
Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).</pre>
			<ol>
				<li value="6">This model appears<a id="_idIndexMarker825"/> to be a reasonable fit, so we<a id="_idIndexMarker826"/> move ahead and forecast <strong class="source-inline">50</strong> time steps into <span class="No-Break">the future:</span><pre class="console">
forecast_result = fitted_seasonal.get_forecast(steps=50)</pre><pre class="console">
forecast_index = pd.date_range("2021-01-01", periods=50)</pre><pre class="console">
forecast = forecast_result.predicted_mean</pre></li>
				<li>Finally, we add the forecast values to the plot of the sample time series, along with the confidence interval for <span class="No-Break">these forecasts:</span><pre class="console">
forecast.plot(ax=ts_ax, label="Forecasts", ls="--")</pre><pre class="console">
conf = forecast_result.conf_int()</pre><pre class="console">
ts_ax.fill_between(forecast_index, conf["lower y"],</pre><pre class="console">
    conf["upper y"], alpha=0.4)</pre><pre class="console">
test_ts.plot(ax=ts_ax, label="Actual future", ls="-.")</pre><pre class="console">
ts_ax.legend()</pre></li>
			</ol>
			<p>The final plot of the time series, along with the predictions and the confidence interval for the forecasts, can be seen in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer887">
					<img alt="Figure 7.14 - Plot of the sample time series, along with the forecasts and confidence interval&#13;&#10;" src="image/7.14.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.14 - Plot of the sample time series, along with the forecasts and confidence interval</p>
			<p>As we can see, the forecast evolution follows roughly the same upward trajectory as the final portion of observed data, and the confidence region for predictions expands quickly. We can see that the actual future values dip down again after the end of the observed data but do stay within the <span class="No-Break">confidence interval.</span></p>
			<h2 id="_idParaDest-314"><a id="_idTextAnchor313"/>How it works...</h2>
			<p>Adjusting an <a id="_idIndexMarker827"/>ARIMA model to incorporate seasonality is a<a id="_idIndexMarker828"/> relatively simple task. A seasonal component is similar to an AR component, where the lag starts at some number larger than 1. In this recipe, the time series exhibits seasonality with period 7 (weekly), which means that the model is approximately given by the <span class="No-Break">following equation:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_075.png"/></p>
			<p>Here, <img alt="" src="image/Formula_07_076.png"/> and <img alt="" src="image/Formula_07_077.png"/> are the parameters and <img alt="" src="image/Formula_07_078.png"/> is the noise at time step <img alt="" src="image/Formula_07_079.png"/>. The standard ARIMA model is easily adapted to include this additional <span class="No-Break">lag term.</span></p>
			<p>The SARIMA model<a id="_idIndexMarker829"/> incorporates this additional seasonality into the ARIMA model. It has four additional order terms on top of the three for the underlying ARIMA model. These four additional parameters are the seasonal AR, differencing, and MA components, along with the period of seasonality. In this recipe, we took the seasonal AR as order 1, with no seasonal differencing or MA components (order 0), and a seasonal period of 7. This gives us the additional parameters (1, 0, 0, 7) that we used in <em class="italic">step 5</em> of <span class="No-Break">this recipe.</span></p>
			<p>Seasonality<a id="_idIndexMarker830"/> is clearly important in modeling time series data that is measured over a period of time covering days, months, or years. It usually incorporates some kind of seasonal component based on the time frame that they occupy. For example, a time series of national power consumption measured hourly over several days would probably have a 24-hour seasonal component since power consumption will likely fall during the <span class="No-Break">night hours.</span></p>
			<p>Long-term seasonal patterns might be hidden if the time series data that you are analyzing does not cover a sufficiently large time period for the pattern to emerge. The same is true for trends in the data. This can lead to some interesting problems when trying to produce long-term forecasts from a relatively short period represented by <span class="No-Break">observed data.</span></p>
			<p>The <strong class="source-inline">SARIMAX</strong> class<a id="_idIndexMarker831"/> from the <strong class="source-inline">statsmodels</strong> package provides the means of modeling time series data using a seasonal ARIMA model. In fact, it can also model external factors that have an additional effect on the model, sometimes called <em class="italic">exogenous regressors</em> (we will not cover these here). This class works much like the <strong class="source-inline">ARMA</strong> and <strong class="source-inline">ARIMA</strong> classes <a id="_idIndexMarker832"/>that we used in the previous recipes. First, we create the model object by providing the data and orders for both the ARIMA process and the seasonal process, and then use the <strong class="source-inline">fit</strong> method on this object to create a fitted model object. We use<a id="_idIndexMarker833"/> the <strong class="source-inline">get_forecasts</strong> method to <a id="_idIndexMarker834"/>generate an object holding the forecasts and confidence interval data that we can then plot, thus producing <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-315"><a id="_idTextAnchor314"/>There’s more...</h2>
			<p>There is a small difference in the interface between the <strong class="source-inline">SARIMAX</strong> class<a id="_idIndexMarker835"/> used in this recipe and the <strong class="source-inline">ARIMA</strong> class<a id="_idIndexMarker836"/> used in the previous recipe. At the time of writing, the <strong class="source-inline">statsmodels</strong> package (v0.11) includes a second <strong class="source-inline">ARIMA</strong> class that builds on top of the <strong class="source-inline">SARIMAX</strong> class, thus providing the same interface. However, at the time of writing, this new <strong class="source-inline">ARIMA</strong> class does not offer the same functionality as that used in <span class="No-Break">this recipe.</span></p>
			<h1 id="_idParaDest-316"><a id="_idTextAnchor315"/>Using Prophet to model time series data </h1>
			<p>The tools we have seen so far for modeling time series data are very general and flexible methods, but they require some knowledge of time series analysis in order to be set up. The analysis needed to construct a good model that can be used to make reasonable predictions for the future can be intensive and time-consuming, and may not be viable for your application. The Prophet<a id="_idIndexMarker837"/> library is designed to automatically model time series data quickly, without the need for input from the user, and make predictions for <span class="No-Break">the future.</span></p>
			<p>In this recipe, we will learn how to use Prophet to produce forecasts from a sample <span class="No-Break">time series.</span></p>
			<h2 id="_idParaDest-317"><a id="_idTextAnchor316"/>Getting ready</h2>
			<p>For this recipe, we will need the Pandas package imported as <strong class="source-inline">pd</strong>, the Matplotlib <strong class="source-inline">pyplot</strong> package imported as <strong class="source-inline">plt</strong>, and the <strong class="source-inline">Prophet</strong> object from the Prophet library, which can be imported using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
from prophet import Prophet</pre>
			<p>Prior to version 1.0, the <strong class="source-inline">prophet</strong> library was called <strong class="source-inline">fbprophet</strong>. </p>
			<p>We also need to import the <strong class="source-inline">generate_sample_data</strong> routine from the <strong class="source-inline">tsdata</strong> module, which is included in the code repository for <span class="No-Break">this book:</span></p>
			<pre class="source-code">
from tsdata import generate_sample_data</pre>
			<p>Let’s see how to use the Prophet package to quickly generate models of time <span class="No-Break">series data.</span></p>
			<h2 id="_idParaDest-318"><a id="_idTextAnchor317"/>How to do it...</h2>
			<p>The following<a id="_idIndexMarker838"/> steps show you how to use the Prophet <a id="_idIndexMarker839"/>package to generate forecasts for a sample <span class="No-Break">time series:</span></p>
			<ol>
				<li value="1">First, we use <strong class="source-inline">generate_sample_data</strong> to generate the sample time <span class="No-Break">series data:</span><pre class="console">
sample_ts, test_ts = generate_sample_data(</pre><pre class="console">
    undiffTrue,trend=0.2)</pre></li>
				<li>We need to convert the sample data into a DataFrame that <span class="No-Break">Prophet expects:</span><pre class="console">
df_for_prophet = pd.DataFrame({</pre><pre class="console">
    "ds": sample_ts.index,    # dates</pre><pre class="console">
    "y": sample_ts.values    # values</pre><pre class="console">
})</pre></li>
				<li>Next, we make a model using the <strong class="source-inline">Prophet</strong> class and fit it to the sample <span class="No-Break">time series:</span><pre class="console">
model = Prophet()</pre><pre class="console">
model.fit(df_for_prophet)</pre></li>
				<li>Now, we create a new DataFrame that contains the time intervals for the original time series, plus the additional periods for <span class="No-Break">the forecasts:</span><pre class="console">
forecast_df = model.make_future_dataframe(periods=50)</pre></li>
				<li>Then, we use the <strong class="source-inline">predict</strong> method to produce the forecasts along the time periods we <span class="No-Break">just created:</span><pre class="console">
forecast = model.predict(forecast_df)</pre></li>
				<li>Finally, we<a id="_idIndexMarker840"/> plot the predictions on<a id="_idIndexMarker841"/> top of the sample time series data, along with the confidence interval and the true <span class="No-Break">future values:</span><pre class="console">
fig, ax = plt.subplots(tight_layout=True)</pre><pre class="console">
sample_ts.plot(ax=ax, label="Observed", title="Forecasts", c="k")</pre><pre class="console">
forecast.plot(x="ds", y="yhat", ax=ax, c="k", </pre><pre class="console">
    label="Predicted", ls="--")</pre><pre class="console">
ax.fill_between(forecast["ds"].values, forecast["yhat_lower"].values, </pre><pre class="console">
    forecast["yhat_upper"].values, color="k", alpha=0.4)</pre><pre class="console">
test_ts.plot(ax=ax, c="k", label="Future", ls="-.")</pre><pre class="console">
ax.legend()</pre><pre class="console">
ax.set_xlabel("Date")</pre><pre class="console">
ax.set_ylabel("Value")</pre></li>
			</ol>
			<p>The plot of the<a id="_idIndexMarker842"/> time series, along with forecasts, can<a id="_idIndexMarker843"/> be seen in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer893">
					<img alt="Figure 7.15 - Plot of sample time series data, along with forecasts and a confidence interval&#13;&#10;" src="image/7.15.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.15 - Plot of sample time series data, along with forecasts and a confidence interval</p>
			<p>We can see that the fit of the data up to (approximately) October 2020 is pretty good, but then a sudden dip in the observed data causes an abrupt change in the predicted values, which continues into the future. This can probably be rectified by tuning the settings of the Prophet prediction. </p>
			<h2 id="_idParaDest-319"><a id="_idTextAnchor318"/>How it works...</h2>
			<p>Prophet is a <a id="_idIndexMarker844"/>package that’s used to automatically<a id="_idIndexMarker845"/> produce models for time series data based on sample data, with little extra input needed from the user. In practice, it is very easy to use; we just need to create an instance of the <strong class="source-inline">Prophet</strong> class, call the <strong class="source-inline">fit</strong> method, and then we are ready to produce forecasts and understand our data using <span class="No-Break">the model.</span></p>
			<p>The <strong class="source-inline">Prophet</strong> class expects the data in a specific format: a DataFrame with columns named <strong class="source-inline">ds</strong> for the date/time index, and <strong class="source-inline">y</strong> for the response data (the time series values). This DataFrame should have integer indices. Once the model has been fit, we use <strong class="source-inline">make_future_dataframe</strong> to create a DataFrame in the correct format, with appropriate date intervals, and with additional rows for future time intervals. The <strong class="source-inline">predict</strong> method then takes this DataFrame and produces values using the model to populate these <a id="_idIndexMarker846"/>time intervals with predicted values. We<a id="_idIndexMarker847"/> also get other information, such as the confidence intervals, in this <span class="No-Break">forecast’s DataFrame.</span></p>
			<h2 id="_idParaDest-320"><a id="_idTextAnchor319"/>There’s more...</h2>
			<p>Prophet<a id="_idIndexMarker848"/> does a fairly good job of modeling time series data without any input from the user. However, the model can be customized using various methods from the <strong class="source-inline">Prophet</strong> class. For example, we could provide information about the seasonality of the data using the <strong class="source-inline">add_seasonality</strong> method of the <strong class="source-inline">Prophet</strong> class, prior to fitting <span class="No-Break">the model.</span></p>
			<p>There are alternative packages for automatically generating models for time series data. For example, popular machine learning libraries such as TensorFlow can be used to model time <span class="No-Break">series data.</span></p>
			<h1 id="_idParaDest-321"><a id="_idTextAnchor320"/>Using signatures to summarize time series data</h1>
			<p>Signatures <a id="_idIndexMarker849"/>are a mathematical construction that arises from rough path theory – a branch of mathematics established by Terry Lyons in the 1990s. The signature of a path is an abstract description of the variability of the path and, up to “tree-like equivalence,” the signature of a path is unique (for instance, two paths that are related by a translation will have the same signature). The signature is independent of parametrization and, consequently, signatures handle irregularly sampled <span class="No-Break">data effectively.</span></p>
			<p>Recently, signatures have found their way into the data science world as a means of summarizing time series data to be passed into machine learning pipelines (and for other applications). One of the reasons this is effective is because the signature of a path (truncated to a particular level) is always a fixed size, regardless of how many samples are used to compute the signature. One of the easiest applications of signatures is for classification (and outlier detection). For this, we often compute the <strong class="bold">expected signature</strong> – the <a id="_idIndexMarker850"/>component-wise mean of signatures – of a family of sampled paths that have the same underlying signal, and then compare the signatures of new samples to this expected signature to see whether they <span class="No-Break">are “close.”</span></p>
			<p>In terms of practical use, there are several Python packages for computing signatures from sampled paths. We’ll be using the <strong class="source-inline">esig</strong> package in this recipe, which is a reference package developed by Lyons and his team – the author is the maintainer of this package at the time of writing. There are alternative packages such as <strong class="source-inline">iisignature</strong> and <strong class="source-inline">signatory</strong> (based on PyTorch, but not actively developed). In this recipe, we will compute signatures for a collection of paths constructed by adding noise to two known signals and compare the expected signatures of each collection to the signature of the true signal and <span class="No-Break">one another.</span></p>
			<h2 id="_idParaDest-322"><a id="_idTextAnchor321"/>Getting ready</h2>
			<p>For this recipe, we <a id="_idIndexMarker851"/>will make use of the NumPy <a id="_idIndexMarker852"/>package (imported as <strong class="source-inline">np</strong> as usual) and the Matplotlib <strong class="source-inline">pyplot</strong> interface imported as <strong class="source-inline">plt</strong>. We will also need the <strong class="source-inline">esig</strong> package. Finally, we will create an instance of the default random number generator from the NumPy <strong class="source-inline">random</strong> library created <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
rng = np.random.default_rng(12345)</pre>
			<p>The seed will ensure that the data generated will <span class="No-Break">be reproducible.</span></p>
			<h2 id="_idParaDest-323"><a id="_idTextAnchor322"/>How to do it…</h2>
			<p>Follow the steps below to compute signatures for two signals and use these signatures to distinguish observed data from <span class="No-Break">each signal:</span></p>
			<ol>
				<li value="1">To start, let’s define some parameters that we will use in <span class="No-Break">the recipe:</span><pre class="console">
upper_limit = 2*np.pi</pre><pre class="console">
depth = 2</pre><pre class="console">
noise_variance = 0.1</pre></li>
				<li>Next, we <a id="_idIndexMarker853"/>define a utility function that <a id="_idIndexMarker854"/>we can use to add noise to each signal. The noise we add is simply Gaussian noise with mean 0 and variance <span class="No-Break">defined previously:</span><pre class="console">
def make_noisy(signal):</pre><pre class="console">
    return signal + rng.normal(0.0, noise_variance, size=signal.shape)</pre></li>
				<li>Now, we define functions that describe the true signals over the interval <img alt="" src="image/Formula_07_080.png"/> with irregular parameter values that are defined by taking drawing increments from an <span class="No-Break">exponential distribution:</span><pre class="console">
def signal_a(count):</pre><pre class="console">
    t = rng.exponential(</pre><pre class="console">
        upper_limit/count, size=count).cumsum()</pre><pre class="console">
    return t, np.column_stack(</pre><pre class="console">
        [t/(1.+t)**2, 1./(1.+t)**2])</pre><pre class="console">
def signal_b(count):</pre><pre class="console">
    t = rng.exponential(</pre><pre class="console">
        upper_limit/count, size=count).cumsum()</pre><pre class="console">
    return t, np.column_stack(</pre><pre class="console">
        [np.cos(t), np.sin(t)])</pre></li>
				<li>Let’s generate<a id="_idIndexMarker855"/> a sample signal and plot these <a id="_idIndexMarker856"/>to see what our true signals look like on <span class="No-Break">the plane:</span><pre class="console">
params_a, true_signal_a = signal_a(100)</pre><pre class="console">
params_b, true_signal_b = signal_b(100)</pre><pre class="console">
fig, ((ax11, ax12), (ax21, ax22)) = plt.subplots(</pre><pre class="console">
    2, 2,tight_layout=True)</pre><pre class="console">
ax11.plot(params_a, true_signal_a[:, 0], "k")</pre><pre class="console">
ax11.plot(params_a, true_signal_a[:, 1], "k--")</pre><pre class="console">
ax11.legend(["x", "y"])</pre><pre class="console">
ax12.plot(params_b, true_signal_b[:, 0], "k")</pre><pre class="console">
ax12.plot(params_b, true_signal_b[:, 1], "k--")</pre><pre class="console">
ax12.legend(["x", "y"])</pre><pre class="console">
ax21.plot(true_signal_a[:, 0], true_signal_a[:, 1], "k")</pre><pre class="console">
ax22.plot(true_signal_b[:, 0], true_signal_b[:, 1], "k")</pre><pre class="console">
ax11.set_title("Components of signal a")</pre><pre class="console">
ax11.set_xlabel("parameter")</pre><pre class="console">
ax11.set_ylabel("value")</pre><pre class="console">
ax12.set_title("Components of signal b")</pre><pre class="console">
ax12.set_xlabel("parameter")</pre><pre class="console">
ax12.set_ylabel("value")</pre><pre class="console">
ax21.set_title("Signal a")</pre><pre class="console">
ax21.set_xlabel("x")</pre><pre class="console">
ax21.set_ylabel("y")</pre><pre class="console">
ax22.set_title("Signal b")</pre><pre class="console">
ax22.set_xlabel("x")</pre><pre class="console">
ax22.set_ylabel("y")</pre></li>
			</ol>
			<p>The resulting<a id="_idIndexMarker857"/> plot is shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.16</em>. On the first<a id="_idIndexMarker858"/> row, we can see the plots of each component of the signal over the parameter interval. On the second row, we can see the <img alt="" src="image/Formula_07_081.png"/> component plotted against the <img alt="" src="image/Formula_07_082.png"/> <span class="No-Break">component:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer897">
					<img alt="Figure 7.16 - Components (top row) of signals a and b and the signals on the plane (bottom row)&#13;&#10;" src="image/7.16.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.16 - Components (top row) of signals a and b and the signals on the plane (bottom row)</p>
			<ol>
				<li value="5">Now, we <a id="_idIndexMarker859"/>use the <strong class="source-inline">stream2sig</strong> routine<a id="_idIndexMarker860"/> from the <strong class="source-inline">esig</strong> package to compute the signature of the two signals. This routine takes the stream data as the first argument and the depth (which determines the level at which the signature is truncated) as the second argument. We use the depth set in <em class="italic">step 1</em> for <span class="No-Break">this argument:</span><pre class="console">
signature_a = esig.stream2sig(true_signal_a, depth)</pre><pre class="console">
signature_b = esig.stream2sig(true_signal_b, depth)</pre><pre class="console">
print(signature_a, signature_b, sep="\n")</pre></li>
			</ol>
			<p>This will print the two signatures (as NumPy arrays) <span class="No-Break">as follows:</span></p>
			<pre class="console">
[ 1. 0.11204198 -0.95648657 0.0062767 -0.15236199 0.04519534 0.45743328]
[ 1.00000000e+00 7.19079669e-04 -3.23775977e-02 2.58537785e-07 3.12414826e+00 -3.12417155e+00 5.24154417e-04]</pre>
			<ol>
				<li value="6">Now, we generate<a id="_idIndexMarker861"/> several noisy signals using<a id="_idIndexMarker862"/> our <strong class="source-inline">make_noisy</strong> routine from <em class="italic">step 2</em>. Not only do we randomize the parametrization of the interval but also the number <span class="No-Break">of samples:</span><pre class="console">
sigs_a = np.vstack([esig.stream2sig(</pre><pre class="console">
    make_noisy(signal_a(</pre><pre class="console">
        rng.integers(50, 100))[1]), depth)</pre><pre class="console">
    for _ in range(50)])</pre><pre class="console">
sigs_b = np.vstack([esig.stream2sig(</pre><pre class="console">
    make_noisy(signal_b(</pre><pre class="console">
        rng.integers(50, 100))[1]), depth)</pre><pre class="console">
    for _ in range(50)])</pre></li>
				<li>Now, we compute the mean of each collection of signatures component by component to generate an “expected signature.” We can compare these to the true signal signatures and one another to illustrate the ability of signatures to discriminate between the <span class="No-Break">two signals:</span><pre class="console">
expected_sig_a = np.mean(sigs_a, axis=0)</pre><pre class="console">
expected_sig_b = np.mean(sigs_b, axis=0)</pre><pre class="console">
print(expected_sig_a, expected_sig_b, sep="\n")</pre></li>
			</ol>
			<p>This will print out the two expected signatures, <span class="No-Break">as follows:</span></p>
			<pre class="console">
[ 1. 0.05584373 -0.82468682 0.01351423 -0.1040297 0.0527106 0.36009198]
[ 1. -0.22457304 -0.05130969 0.07368485 3.0923422 -3.09672887 0.17059484]</pre>
			<ol>
				<li value="8">Finally, we <a id="_idIndexMarker863"/>print the maximum difference (in absolute value) between each expected signature and the corresponding <a id="_idIndexMarker864"/>true signal signature and between the two <span class="No-Break">expected signatures:</span><pre class="console">
print("Signal a", np.max(</pre><pre class="console">
    np.abs(expected_sig_a - signature_a)))</pre><pre class="console">
print("Signal b", np.max(</pre><pre class="console">
    np.abs(expected_sig_b -signature_b)))</pre><pre class="console">
print("Signal a vs signal b", np.max(</pre><pre class="console">
    np.abs(expected_sig_a - expected_sig_b)))</pre></li>
			</ol>
			<p>The results are <span class="No-Break">shown here:</span></p>
			<pre class="console">
Signal a 0.13179975589137582
Signal b 0.22529211936796972
Signal a vs signal b 3.1963719013938148</pre>
			<p>We can see that the difference between the expected signature and the true signature in each case is relatively small, whereas the difference between the two expected signatures is <span class="No-Break">relatively large.</span></p>
			<h2 id="_idParaDest-324"><a id="_idTextAnchor323"/>How it works…</h2>
			<p>The<a id="_idIndexMarker865"/> signature of a path <img alt="" src="image/Formula_07_083.png"/> (taking values in<a id="_idIndexMarker866"/> the <img alt="" src="image/Formula_07_084.png"/>-dimensional real space) over an interval <img alt="" src="image/Formula_07_085.png"/> is an element of the <strong class="bold">free tensor algebra</strong> over <img alt="" src="image/Formula_07_086.png"/> (in this notation, <img alt="" src="image/Formula_07_087.png"/> denotes the value of the<a id="_idIndexMarker867"/> path at time <img alt="" src="image/Formula_07_088.png"/>. You may prefer to think of this as <img alt="" src="image/Formula_07_089.png"/>). We denote this signature as <img alt="" src="image/Formula_07_090.png"/>. Formality aside, we realize the signature as a sequence of elements <span class="No-Break">as follows:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_091.png"/></p>
			<p>The superscripts denote the index within the free tensor. For example, the indices with two terms <img alt="" src="image/Formula_07_092.png"/> (degree 2) are like the rows and columns of a matrix. The first term of the signature is always 1. The following <img alt="" src="image/Formula_07_093.png"/>-terms are given by the increments in each of the component directions: if we write the path <img alt="" src="image/Formula_07_094.png"/> as a vector <img alt="" src="image/Formula_07_095.png"/>, then these terms are given by the <span class="No-Break">following formula:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_096.png"/></p>
			<p>The higher order terms are given by iterated integrals of these <span class="No-Break">component functions:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_097.png"/></p>
			<p>The full signature of a path is an infinite sequence – so for practical uses, we usually truncate at a particular <em class="italic">depth</em> that determines the maximum size of indices, such as <img alt="" src="image/Formula_07_098.png"/> here. </p>
			<p>This iterated integral definition is not especially useful in practice. Fortunately, when we sample a path and make the modest assumption that the path is linear between successive samples, then we can compute the signature by computing the product of tensor exponentials of increments. Concretely, if <img alt="" src="image/Formula_07_099.png"/> are sample values taken from our path <img alt="" src="image/Formula_07_100.png"/> at <img alt="" src="image/Formula_07_101.png"/>, respectively, all lying between <img alt="" src="image/Formula_07_102.png"/> and <img alt="" src="image/Formula_07_103.png"/>, then (assuming <img alt="" src="image/Formula_07_104.png"/> is linear between <img alt="" src="image/Formula_07_105.png"/> and <img alt="" src="image/Formula_07_106.png"/>) the signature is given by <span class="No-Break">the following:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_107.png"/></p>
			<p>Here, the <img alt="" src="image/Formula_07_108.png"/> symbol <a id="_idIndexMarker868"/>denotes multiplication in the <a id="_idIndexMarker869"/>free tensor algebra (this multiplication is defined by the concatenation of indices – so, for instance, the <img alt="" src="image/Formula_07_109.png"/>-th value on the left and the <img alt="" src="image/Formula_07_110.png"/>-th value on the right will contribute to the <img alt="" src="image/Formula_07_111.png"/>-th value in the result). Remember that these are exponents of free tensor objects – not the usual exponential function – which are defined using the familiar <span class="No-Break">power series:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_112.png"/></p>
			<p>When the constant term of a tensor <img alt="" src="image/Formula_07_113.png"/> is zero and we truncate the tensor algebra to depth <img alt="" src="image/Formula_07_114.png"/>, then the value of <img alt="" src="image/Formula_07_115.png"/> is exactly equal to the sum of the first <img alt="" src="image/Formula_07_116.png"/> terms of this sum, which is a finite sum that can be <span class="No-Break">computed efficiently.</span></p>
			<p>The importance of the signature of a path in a data science context is from the fact that the signature is representative of the path from the perspective of functions. Any continuous function defined on the path <img alt="" src="image/Formula_07_117.png"/> is approximately (in a very precise sense) a linear function defined on the signature. Thus, anything that can be learned about a path can also be learned from <span class="No-Break">the signature.</span></p>
			<p>The <strong class="source-inline">esig</strong> package is built on top of the <strong class="source-inline">libalgebra</strong> C++ library for computations involving the free tensor algebra (and other kinds of algebraic objects). The <strong class="source-inline">stream2sig</strong> routine from <strong class="source-inline">esig</strong> takes a sequence of path samples in the form of an <strong class="source-inline">N</strong> (number of samples) x <strong class="source-inline">d</strong> (number of dimensions) NumPy array and returns a flat NumPy array containing the components of the signature, laid out in sequence as described here. The second argument to <strong class="source-inline">stream2sig</strong> is the depth parameter <img alt="" src="image/Formula_07_118.png"/>, which we have chosen to be 2 in this recipe. The size of the signature array is determined only by the dimension of the space and the depth, and is given by the <span class="No-Break">following formula:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_119.png"/></p>
			<p>In the recipe, both of our paths were 2-dimensional, and signatures were computed to depth 2 so the signature has <img alt="" src="image/Formula_07_120.png"/> elements (notice that the number of samples varied in each case and were generated randomly and irregularly, yet the signature was the same size in <span class="No-Break">each case).</span></p>
			<p>Now that the <a id="_idIndexMarker870"/>theory is out of the way, let’s look at the <a id="_idIndexMarker871"/>recipe. We define two true paths (signals), which we called <em class="italic">signal a</em> and <em class="italic">signal b</em>. We draw samples from each signal by drawing parameter values <img alt="" src="image/Formula_07_121.png"/> with differences taken from an exponential distribution so that (on average) <img alt="" src="image/Formula_07_122.png"/>. Then, we feed these parameter values into the formula for the path (see <em class="italic">step 3</em>). In the latter steps, we also add Gaussian noise to generated paths with mean 0 and variance 0.1. This guarantees that our 2 signals are irregularly sampled and noisy – to demonstrate the robustness of signature calculations. </p>
			<p>Signal a is defined by the <span class="No-Break">following formula:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_123.png"/></p>
			<p>Because this is a nice (smooth) path over the interval <img alt="" src="image/Formula_07_124.png"/>, we can compute the signature precisely using the iterated integrals to (approximately) get the sequence: </p>
			<p><img alt="" src="image/Formula_07_125.png"/></p>
			<p>This is remarkably close to the computed signature for the signal as <span class="No-Break">given here:</span></p>
			<pre class="source-code">
[1. 0.11204198 -0.95648657 0.0062767 -0.15236199 0.04519534 0.45743328]</pre>
			<p>We expect a reasonable amount of error because our sampling is fairly coarse (only 100 points) and our parameter values might finish before <img alt="" src="image/Formula_07_126.png"/> because of the way <span class="No-Break">we randomized.</span></p>
			<p>Signal b is defined by the <span class="No-Break">following formula:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_127.png"/></p>
			<p>The component functions for this signal are also smooth, so we can compute the signature by computing the iterated integrals. Following this procedure, we see that the signature of the true signal is <span class="No-Break">the following:</span></p>
			<p class="IMG---Figure"><img alt="" src="image/Formula_07_128.png"/></p>
			<p>Comparing <a id="_idIndexMarker872"/>this to the compute value, we see that we’re <a id="_idIndexMarker873"/><span class="No-Break">fairly close:</span></p>
			<pre class="source-code">
[ 1.00000000e+00  7.19079669e-04 -3.23775977e-02  2.58537785e-07  3.12414826e+00 -3.12417155e+00  5.24154417e-04]</pre>
			<p>Again, we expect some error because of coarse sampling and not covering the parameter interval exactly (in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.16</em>, you can see that there are some substantial “straight sections” indicating that the parameter values are spaced far apart in some places on the plots for <span class="No-Break">signal b).</span></p>
			<p>In <em class="italic">step 6</em>, we generate a number of signatures for noisy samples taken from both signals, all with different and irregular time steps (the count of which is also randomly drawn between 50 and 100) and Gaussian noise. These are stacked into an array with <strong class="source-inline">N = 50</strong> rows and 7 columns (the size of the signature). We compute the row-wise mean of each array of signatures using the <strong class="source-inline">np.mean</strong> routine with <strong class="source-inline">axis=0</strong>. This produces an <em class="italic">expected signature</em> for each signal. We then compare these expected signatures to the “true signature” computed in <em class="italic">step 5</em> and one another. We can see that the difference between the two expected signatures is significantly larger (not in the statistical sense) than the difference between the expected and true signatures for each signal. This illustrates the discriminative power of the signature for classifying time <span class="No-Break">series data.</span></p>
			<h2 id="_idParaDest-325"><a id="_idTextAnchor324"/>There’s more…</h2>
			<p>The example problem<a id="_idIndexMarker874"/> we addressed in the recipe is extremely simple. Signatures have been used in a wide variety of contexts, including sepsis detection, handwriting recognition, natural language processing, human action recognition, and drone identification. Usually, signatures are used in tandem with a selection of “preprocessing steps” that address various deficiencies in the sampled data. For example, in the recipe, we deliberately chose signals that are bounded (and relatively small) on the interval in question. In practice, data will likely be spread more widely and in this case, the higher order terms in the signature can grow quite rapidly, which can have important consequences for numerical stability. These preprocessing steps include lead-lag transformations, pen-on-pen-off transformations, the missing data transformation, and time integration. Each of these has a specific role in making data more amenable to signature <span class="No-Break">based methods.</span></p>
			<p>Signatures contain a large amount of redundancy. Many of the higher order terms can be computed from the others because of the geometry. This means that we can reduce the number of terms we need without discarding any information about the path. This reduction involves projecting the signature (in the free tensor algebra) onto the log signature (in the free Lie algebra). The log signature is an alternative representation of the path that has fewer terms than the signature. Many of the properties remain true for log signatures, except that we lose linearity in the approximation of functions (this may or may not be<a id="_idIndexMarker875"/> important for <span class="No-Break">specific applications).</span></p>
			<h2 id="_idParaDest-326"><a id="_idTextAnchor325"/>See also</h2>
			<p>The theory of rough paths and signature methods is obviously too broad – and rapidly expanding – to cover in such a short space. Here are some sources where you can find additional information <span class="No-Break">about signatures:</span></p>
			<ul>
				<li>Lyons, T. and McLeod, A., 2022. <em class="italic">Signature Methods in Machine </em><span class="No-Break"><em class="italic">Learning</em></span><span class="No-Break"> </span><a href="https://arxiv.org/abs/2206.14674"><span class="No-Break">https://arxiv.org/abs/2206.14674</span></a></li>
				<li>Lyons, T., Caruana, M., and Lévy, T., 2004. <em class="italic">Differential Equations Driven by Rough Paths</em>, Springer, Ecole d’Eté de Probabilités de <span class="No-Break">Saint-Flour XXXIV</span></li>
				<li>Several Jupyter notebooks walking through analyzing time series data using signatures on the Datasig <span class="No-Break">website: </span><a href="https://datasig.ac.uk/examples"><span class="No-Break">https://datasig.ac.uk/examples</span></a><span class="No-Break">.</span></li>
			</ul>
			<h1 id="_idParaDest-327"><a id="_idTextAnchor326"/>Further reading</h1>
			<p>A good textbook on regression in statistics is the book <em class="italic">Probability and Statistics</em> by Mendenhall, Beaver, and Beaver, as mentioned in <a href="B19085_06.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Working with Data and Statistics</em>. The following books provide a good introduction to classification and regression in modern <span class="No-Break">data science:</span></p>
			<ul>
				<li>James, G. and Witten, D., 2013. <em class="italic">An Introduction To Statistical Learning: With Applications In R</em>. New <span class="No-Break">York: Springer.</span></li>
				<li>Müller, A. and Guido, S., 2016. <em class="italic">Introduction To Machine Learning With Python</em>. Sebastopol: <span class="No-Break">O’Reilly Media.</span></li>
			</ul>
			<p>A good introduction to time series analysis can be found in the <span class="No-Break">following book:</span></p>
			<ul>
				<li>Cryer, J. and Chan, K., 2008. <em class="italic">Time Series Analysis</em>. New <span class="No-Break">York: Springer.</span></li>
			</ul>
		</div>
	</body></html>